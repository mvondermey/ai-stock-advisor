[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üñ•Ô∏è  FORCE_CPU enabled - PyTorch models will run on CPU (allows higher parallelism)
[INTERNET] Connection verified, using local time with internet sync: 2025-12-29 21:11:02 UTC
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
Using initial balance: $100,000.00
   üìä After fractionable filter: 5903 tickers
   üìä After ADR/special filter: 5649 tickers (filtered out 254)
   ‚úÖ Removed 254 foreign ADRs/special securities (e.g., symbols ending in Y)
[SUCCESS] Fetched 5649 tradable US equity tickers from Alpaca.
Total unique tickers found: 5649
üöÄ Step 1: Batch downloading data for 5649 tickers from 2024-09-29 to 2025-12-29...
  (Requesting 456 days of data based on BACKTEST_DAYS=90 + TRAIN_LOOKBACK_DAYS=365)
  - Downloading batch 1/1 (5649 tickers)...
  ‚ÑπÔ∏è  Market data not available yet (markets closed or processing). Using cached data.
  üìÇ Checking cache for 5649 tickers...
  ‚úÖ Cache hit for A (312 rows, 100% coverage)
  ‚úÖ Cache hit for AA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAAU (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAOI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAON (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AAXJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABBV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ABXB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACES (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACET (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACGLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACIW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACWV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ACWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADME (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ADV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AENT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AER (312 rows, 100% coverage)
  ‚úÖ Cache hit for AES (312 rows, 100% coverage)
  ‚úÖ Cache hit for AESI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AESR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AEVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFGB (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFIF (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFK (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AFYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNCN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNCP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AGZD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AHCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AHH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIEQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIFU (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIRJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AISP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for AIZN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AJG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AKAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AKBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AKO-B (312 rows, 100% coverage)
  ‚úÖ Cache hit for AKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALLE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ALXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for AME (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMZN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AMZY (312 rows, 100% coverage)
  ‚úÖ Cache hit for AN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANDE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANET (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANEW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANIK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ANSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AON (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AORT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AOUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for APA (312 rows, 100% coverage)
  ‚úÖ Cache hit for APAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for APCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for APD (312 rows, 100% coverage)
  ‚úÖ Cache hit for APEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for APG (312 rows, 100% coverage)
  ‚úÖ Cache hit for APGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for APH (312 rows, 100% coverage)
  ‚úÖ Cache hit for API (312 rows, 100% coverage)
  ‚úÖ Cache hit for APIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for APLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for APLE (312 rows, 100% coverage)
  ‚úÖ Cache hit for APLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for APO (312 rows, 100% coverage)
  ‚úÖ Cache hit for APOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for APP (312 rows, 100% coverage)
  ‚úÖ Cache hit for APPF (312 rows, 100% coverage)
  ‚úÖ Cache hit for APPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for APPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for APRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for APTV (312 rows, 100% coverage)
  ‚úÖ Cache hit for APUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for APYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AQN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AQNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for AR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARES (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARKX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARMK (312 rows, 100% coverage)
  ‚úÖ Cache hit for AROC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AROW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARQQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARQT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARTNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ARWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASLE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASML (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASND (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ASX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATER (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATHA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATHM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATMU (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATON (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ATYR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AU (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AURA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for AUTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVAH (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVAV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVD (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVDE (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVES (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVIR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVMU (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVSU (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVUV (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for AWAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for AWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for AWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for AX (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXON (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AXTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AYI (312 rows, 100% coverage)
  ‚úÖ Cache hit for AZN (312 rows, 100% coverage)
  ‚úÖ Cache hit for AZO (312 rows, 100% coverage)
  ‚úÖ Cache hit for AZTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for AZZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for B (312 rows, 100% coverage)
  ‚úÖ Cache hit for BA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BABA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BACQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAER (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BALI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BALL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BALT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BANC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAND (312 rows, 100% coverage)
  ‚úÖ Cache hit for BANF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BANR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BARK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BATRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BATRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BATT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBBY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBCP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBDO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBUC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BBY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCML (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BCYC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BELFB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BEPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BETZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BF-A (312 rows, 100% coverage)
  ‚úÖ Cache hit for BF-B (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BFST (312 rows, 100% coverage)
  ‚úÖ Cache hit for BG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BHVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BILI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BILL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BILS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BILZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BINC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIOA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BITX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BIZD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKSY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BKV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLES (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLFY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLKB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLND (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BLZE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BND (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNED (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BNTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOKF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOND (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BORR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOTZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BOXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BPOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BPYPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BPYPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BPYPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRK-B (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BROS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRZE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BRZU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSET (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSJQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSJR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSJS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BST (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BSY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTCW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUFZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for BULZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for BURL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BUYZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BV (312 rows, 100% coverage)
  ‚úÖ Cache hit for BVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for BW (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWA (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BWZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BX (312 rows, 100% coverage)
  ‚úÖ Cache hit for BXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for BXMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for BXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for BXSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for BY (312 rows, 100% coverage)
  ‚úÖ Cache hit for BYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for BYND (312 rows, 100% coverage)
  ‚úÖ Cache hit for BYRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for BZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for BZAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for BZH (312 rows, 100% coverage)
  ‚úÖ Cache hit for BZUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for C (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CABO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CACC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CACI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CADE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CADL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CALF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CALM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CALX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAML (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CANE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CANG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CART (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CARZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CASH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CASS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CASY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CATH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CATO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CATY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CAVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBOE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBON (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CBZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCBG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCOI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CCU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CDZI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CECO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CEFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CEG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CELC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CELH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CEMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CENN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CENT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CENTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CENX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CEPU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CERS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CERT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CEVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFFN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGAU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGCP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGON (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CGXU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHIQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHKP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHRW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHWY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CHYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CII (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CINF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CING (312 rows, 100% coverage)
  ‚úÖ Cache hit for CINT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CION (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CIVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLOA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLOI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLOU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLOZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMBM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMCSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CME (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMND (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CMTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNET (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNOB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CNYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for COCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CODI (312 rows, 100% coverage)
  ‚úÖ Cache hit for COE (312 rows, 100% coverage)
  ‚úÖ Cache hit for COF (312 rows, 100% coverage)
  ‚úÖ Cache hit for COFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for COGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for COHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for COHU (312 rows, 100% coverage)
  ‚úÖ Cache hit for COIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for COKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for COLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for COLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for COLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for COLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for COLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for COM (312 rows, 100% coverage)
  ‚úÖ Cache hit for COMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for COMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for COMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for COMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CON (312 rows, 100% coverage)
  ‚úÖ Cache hit for CONL (312 rows, 100% coverage)
  ‚úÖ Cache hit for COO (312 rows, 100% coverage)
  ‚úÖ Cache hit for COOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for COP (312 rows, 100% coverage)
  ‚úÖ Cache hit for COPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for COR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CORN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CORP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CORT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CORZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for COSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for COST (312 rows, 100% coverage)
  ‚úÖ Cache hit for COTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for COUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for COWG (312 rows, 100% coverage)
  ‚úÖ Cache hit for COWZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPER (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CQP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CQQQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRBG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRBU (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRD-A (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRDO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CREX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRML (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRON (312 rows, 100% coverage)
  ‚úÖ Cache hit for CROX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CRWV (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSIQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSWC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTKB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CTXR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CULP (312 rows, 100% coverage)
  ‚úÖ Cache hit for CURB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CURE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CURV (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CUZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVGW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for CW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWST (312 rows, 100% coverage)
  ‚úÖ Cache hit for CWT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CXM (312 rows, 100% coverage)
  ‚úÖ Cache hit for CXSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for CXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for CXW (312 rows, 100% coverage)
  ‚úÖ Cache hit for CYBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for CYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for CYH (312 rows, 100% coverage)
  ‚úÖ Cache hit for CYRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for CYTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for CZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for CZNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for CZR (312 rows, 100% coverage)
  ‚úÖ Cache hit for D (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DALI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DASH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAWN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for DB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBAW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DCTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DDWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DECK (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEED (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DELL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DENN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DES (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DEW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAU (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFCF (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFSU (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DFVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGICA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGII (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGRW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DINO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DINT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DISV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DIVZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJTWW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DK (312 rows, 100% coverage)
  ‚úÖ Cache hit for DKL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DKNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DMXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DNUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOCN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOCU (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOGZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOLE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DON (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DORM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DOYU (312 rows, 100% coverage)
  ‚úÖ Cache hit for DPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DPST (312 rows, 100% coverage)
  ‚úÖ Cache hit for DPZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DRVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DT (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTST (312 rows, 100% coverage)
  ‚úÖ Cache hit for DTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUK (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUKB (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUO (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DURA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DV (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for DVYE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWAW (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for DWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DX (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXD (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for DXPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for DY (312 rows, 100% coverage)
  ‚úÖ Cache hit for DYN (312 rows, 100% coverage)
  ‚úÖ Cache hit for DYNF (312 rows, 100% coverage)
  ‚úÖ Cache hit for E (312 rows, 100% coverage)
  ‚úÖ Cache hit for EA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EAF (312 rows, 100% coverage)
  ‚úÖ Cache hit for EAGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EAGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EB (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBIZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for EBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECON (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ECX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ED (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for EDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EELV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EES (312 rows, 100% coverage)
  ‚úÖ Cache hit for EET (312 rows, 100% coverage)
  ‚úÖ Cache hit for EETH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFAV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EFZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EHAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for EHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EHTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIDO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EINC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELME (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ELVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMBJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EME (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMGF (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMQQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EMXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ENZL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EOLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EOSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPRF (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPU (312 rows, 100% coverage)
  ‚úÖ Cache hit for EPV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EQX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERII (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERX (312 rows, 100% coverage)
  ‚úÖ Cache hit for ERY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ES (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESGU (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESML (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ESTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ET (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHU (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETHZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETON (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ETSY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EU (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUFN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EUSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVER (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EW (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWCZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWG (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWH (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWJV (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWN (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWU (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWW (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EWZS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXFY (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXK (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EXTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for EYE (312 rows, 100% coverage)
  ‚úÖ Cache hit for EYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for EYPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZET (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZM (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZPW (312 rows, 100% coverage)
  ‚úÖ Cache hit for EZU (312 rows, 100% coverage)
  ‚úÖ Cache hit for F (312 rows, 100% coverage)
  ‚úÖ Cache hit for FA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FALN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FANG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FARM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAST (312 rows, 100% coverage)
  ‚úÖ Cache hit for FATE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FAZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBIZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FBY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCNCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FELE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FENG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FENY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FER (312 rows, 100% coverage)
  ‚úÖ Cache hit for FERG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FET (312 rows, 100% coverage)
  ‚úÖ Cache hit for FETH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FEZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FFWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FHLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FHN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FHTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for FICO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FICS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FID (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIHL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FINV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FINX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FISI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FISR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FISV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FITB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FITBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FITBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FITBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FITE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIW (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIXD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FIZZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLGB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLQL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLQM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLUX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FLYW (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMST (312 rows, 100% coverage)
  ‚úÖ Cache hit for FMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FND (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNGU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNKO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FORA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FORM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FORR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOXA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FOXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for FPXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FQAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FREL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRME (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FROG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRST (312 rows, 100% coverage)
  ‚úÖ Cache hit for FRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FSZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FTXR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUFU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FULC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FULT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FUTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FV (312 rows, 100% coverage)
  ‚úÖ Cache hit for FVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FVC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FVD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FVRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FWONA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FWONK (312 rows, 100% coverage)
  ‚úÖ Cache hit for FWRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FWRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXA (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXB (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXG (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXH (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXU (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for FXZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for FYBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for FYC (312 rows, 100% coverage)
  ‚úÖ Cache hit for FYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for FYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for FYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for G (312 rows, 100% coverage)
  ‚úÖ Cache hit for GABC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GARP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GASS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GATX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDXD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDXJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDXU (312 rows, 100% coverage)
  ‚úÖ Cache hit for GDYN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GENI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GERN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GES (312 rows, 100% coverage)
  ‚úÖ Cache hit for GETY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for GEVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GGAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GGB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GH (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHYB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GHYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIGB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GII (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIII (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GILD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GILT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GINN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GKOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLOB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLOF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GLXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GME (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMED (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GMRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOAU (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOOGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GOVZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for GP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPIQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GPTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GQRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GREK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRID (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRND (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRNQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GRWG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSEW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSID (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSST (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GSY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GT (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTEK (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTES (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for GUNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for GURU (312 rows, 100% coverage)
  ‚úÖ Cache hit for GUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for GUSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for GVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for GVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for GVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for GVIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for GWH (312 rows, 100% coverage)
  ‚úÖ Cache hit for GWRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for GWRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GWW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for GXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for GXDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for GXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for GXUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for GYRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for H (312 rows, 100% coverage)
  ‚úÖ Cache hit for HACK (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAFN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HALO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HASI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAUZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for HAYW (312 rows, 100% coverage)
  ‚úÖ Cache hit for HBAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HBM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HBNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HCSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for HE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEDJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEI-A (312 rows, 100% coverage)
  ‚úÖ Cache hit for HELE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HELO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEQT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HERO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HESM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEWJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HEZU (312 rows, 100% coverage)
  ‚úÖ Cache hit for HFGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HFWA (312 rows, 100% coverage)
  ‚úÖ Cache hit for HFXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HGER (312 rows, 100% coverage)
  ‚úÖ Cache hit for HGTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for HHH (312 rows, 100% coverage)
  ‚úÖ Cache hit for HI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for HII (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HISF (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HIW (312 rows, 100% coverage)
  ‚úÖ Cache hit for HKD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for HMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HMOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for HMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HNDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HNRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HNST (312 rows, 100% coverage)
  ‚úÖ Cache hit for HODL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOMZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HON (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for HOWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HP (312 rows, 100% coverage)
  ‚úÖ Cache hit for HPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for HPK (312 rows, 100% coverage)
  ‚úÖ Cache hit for HPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for HPQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HQH (312 rows, 100% coverage)
  ‚úÖ Cache hit for HQY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HR (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HROW (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for HRZN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSCZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HST (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HSY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HTZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUBG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HURN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HUYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for HVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HWC (312 rows, 100% coverage)
  ‚úÖ Cache hit for HWKN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HY (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYI (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for HYZD (312 rows, 100% coverage)
  ‚úÖ Cache hit for HZO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IART (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAU (312 rows, 100% coverage)
  ‚úÖ Cache hit for IAUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBCP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBHF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBMQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IBUY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICE (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICUI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ICVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDRV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IDYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IESC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IETC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IEZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IFV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IIGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IIIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IIIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IIPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJH (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJK (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ILTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMKTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IMXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for INBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for INBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for INBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for INCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for INCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for INCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for INCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDO (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for INDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for INFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for INFU (312 rows, 100% coverage)
  ‚úÖ Cache hit for INFY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ING (312 rows, 100% coverage)
  ‚úÖ Cache hit for INGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for INGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for INGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for INKM (312 rows, 100% coverage)
  ‚úÖ Cache hit for INMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for INN (312 rows, 100% coverage)
  ‚úÖ Cache hit for INNV (312 rows, 100% coverage)
  ‚úÖ Cache hit for INO (312 rows, 100% coverage)
  ‚úÖ Cache hit for INOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for INR (312 rows, 100% coverage)
  ‚úÖ Cache hit for INSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for INSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for INSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for INSW (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTF (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for INTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for INV (312 rows, 100% coverage)
  ‚úÖ Cache hit for INVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for INVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for INVH (312 rows, 100% coverage)
  ‚úÖ Cache hit for INVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for INVZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IONQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IONS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IOSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IOVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPKW (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQDF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQSU (312 rows, 100% coverage)
  ‚úÖ Cache hit for IQV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IREN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRON (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IRWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISCF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISPY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ISVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ITW (312 rows, 100% coverage)
  ‚úÖ Cache hit for IUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IUSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IUSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IUSV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVW (312 rows, 100% coverage)
  ‚úÖ Cache hit for IVZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWV (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IWY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IX (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for IXUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYC (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYE (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYF (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYH (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYK (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYR (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYW (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for IYZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for IZRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for J (312 rows, 100% coverage)
  ‚úÖ Cache hit for JAAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for JACK (312 rows, 100% coverage)
  ‚úÖ Cache hit for JAKK (312 rows, 100% coverage)
  ‚úÖ Cache hit for JAMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for JANT (312 rows, 100% coverage)
  ‚úÖ Cache hit for JANW (312 rows, 100% coverage)
  ‚úÖ Cache hit for JANX (312 rows, 100% coverage)
  ‚úÖ Cache hit for JAVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for JAZZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JBTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JCPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for JCPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JD (312 rows, 100% coverage)
  ‚úÖ Cache hit for JEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for JELD (312 rows, 100% coverage)
  ‚úÖ Cache hit for JEMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for JEPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JEPQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for JETS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for JGLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for JGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHML (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JHSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for JIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for JILL (312 rows, 100% coverage)
  ‚úÖ Cache hit for JIRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JJSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for JKHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for JKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMST (312 rows, 100% coverage)
  ‚úÖ Cache hit for JMUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for JNJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for JNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for JNUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for JOBY (312 rows, 100% coverage)
  ‚úÖ Cache hit for JOE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JOET (312 rows, 100% coverage)
  ‚úÖ Cache hit for JOUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for JOYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPME (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPST (312 rows, 100% coverage)
  ‚úÖ Cache hit for JPUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for JQUA (312 rows, 100% coverage)
  ‚úÖ Cache hit for JRVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for JSCP (312 rows, 100% coverage)
  ‚úÖ Cache hit for JSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JSMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for JSML (312 rows, 100% coverage)
  ‚úÖ Cache hit for JSTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for JTEK (312 rows, 100% coverage)
  ‚úÖ Cache hit for JUCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for JUST (312 rows, 100% coverage)
  ‚úÖ Cache hit for JVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for JXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for JXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for JYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for JYNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for KAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KALA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KALU (312 rows, 100% coverage)
  ‚úÖ Cache hit for KALV (312 rows, 100% coverage)
  ‚úÖ Cache hit for KAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KARO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KARS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KB (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBWP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KBWY (312 rows, 100% coverage)
  ‚úÖ Cache hit for KC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KCCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KD (312 rows, 100% coverage)
  ‚úÖ Cache hit for KDK (312 rows, 100% coverage)
  ‚úÖ Cache hit for KDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KELYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEMQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEY (312 rows, 100% coverage)
  ‚úÖ Cache hit for KEYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KFRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KFY (312 rows, 100% coverage)
  ‚úÖ Cache hit for KGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KGRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KIDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for KJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for KKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KLXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for KMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for KN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNF (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for KNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for KO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for KODK (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOF (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOKU (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KORP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KOSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KPTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KREF (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KROS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRP (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KRYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KSPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KSTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KT (312 rows, 100% coverage)
  ‚úÖ Cache hit for KTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for KTOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for KURA (312 rows, 100% coverage)
  ‚úÖ Cache hit for KURE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KVUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for KVYO (312 rows, 100% coverage)
  ‚úÖ Cache hit for KW (312 rows, 100% coverage)
  ‚úÖ Cache hit for KWEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for KWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for KYIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for KYMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for KZR (312 rows, 100% coverage)
  ‚úÖ Cache hit for L (312 rows, 100% coverage)
  ‚úÖ Cache hit for LABD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LABU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LADR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAES (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAND (312 rows, 100% coverage)
  ‚úÖ Cache hit for LANV (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LASR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAW (312 rows, 100% coverage)
  ‚úÖ Cache hit for LAZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBRDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBRDK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBRDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBTYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LBTYK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LCID (312 rows, 100% coverage)
  ‚úÖ Cache hit for LCII (312 rows, 100% coverage)
  ‚úÖ Cache hit for LCTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LCTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LDEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for LDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LDOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for LDSF (312 rows, 100% coverage)
  ‚úÖ Cache hit for LDUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LECO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEN-B (312 rows, 100% coverage)
  ‚úÖ Cache hit for LENZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LESL (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LEVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFEQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFST (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for LFVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGIH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGND (312 rows, 100% coverage)
  ‚úÖ Cache hit for LGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LHX (312 rows, 100% coverage)
  ‚úÖ Cache hit for LI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LIF (312 rows, 100% coverage)
  ‚úÖ Cache hit for LII (312 rows, 100% coverage)
  ‚úÖ Cache hit for LILA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LILAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LINC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LIND (312 rows, 100% coverage)
  ‚úÖ Cache hit for LINE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LION (312 rows, 100% coverage)
  ‚úÖ Cache hit for LIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LITE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LIVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LKFN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LKOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LKQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for LLYVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LLYVK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMND (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LND (312 rows, 100% coverage)
  ‚úÖ Cache hit for LNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LNTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LONZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQDH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LQDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRGF (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LRNZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSAF (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LSTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTPZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LTRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for LULU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUMN (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUV (312 rows, 100% coverage)
  ‚úÖ Cache hit for LUXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for LVHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for LVHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for LVLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for LVWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LW (312 rows, 100% coverage)
  ‚úÖ Cache hit for LWAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for LWLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LX (312 rows, 100% coverage)
  ‚úÖ Cache hit for LXEH (312 rows, 100% coverage)
  ‚úÖ Cache hit for LXFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for LXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for LXRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for LXU (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for LYV (312 rows, 100% coverage)
  ‚úÖ Cache hit for LZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for LZB (312 rows, 100% coverage)
  ‚úÖ Cache hit for LZM (312 rows, 100% coverage)
  ‚úÖ Cache hit for M (312 rows, 100% coverage)
  ‚úÖ Cache hit for MA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MANH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MANU (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MARA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MARB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MARM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MART (312 rows, 100% coverage)
  ‚úÖ Cache hit for MARW (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MASI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MASS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MATV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MATW (312 rows, 100% coverage)
  ‚úÖ Cache hit for MATX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MAXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBUU (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCW (312 rows, 100% coverage)
  ‚úÖ Cache hit for MCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDLZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDXG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDXH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MDYV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MED (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MELI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEOH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MERC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MESO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MET (312 rows, 100% coverage)
  ‚úÖ Cache hit for META (312 rows, 100% coverage)
  ‚úÖ Cache hit for METC (312 rows, 100% coverage)
  ‚úÖ Cache hit for METCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for METV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MEXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MFUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MHK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MID (312 rows, 100% coverage)
  ‚úÖ Cache hit for MIDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MILN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MINT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MIR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MIRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MITK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MITT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for MKC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MKC-V (312 rows, 100% coverage)
  ‚úÖ Cache hit for MKL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MKSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MKTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLKN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLPA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MLYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MMYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNKD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNSO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNST (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MODG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MODL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOG-A (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MORN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MORT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MP (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPU (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPW (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MREO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRP (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSFO (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MSTZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for MT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MTZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for MU (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUC (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUST (312 rows, 100% coverage)
  ‚úÖ Cache hit for MUX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MVBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for MVIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for MWA (312 rows, 100% coverage)
  ‚úÖ Cache hit for MX (312 rows, 100% coverage)
  ‚úÖ Cache hit for MXCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for MXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for MXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for MYE (312 rows, 100% coverage)
  ‚úÖ Cache hit for MYGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for MYPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for MYRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for MYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for MZTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NABL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NANR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NATL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NATR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NAVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for NBTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NCDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NCEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NCLH (312 rows, 100% coverage)
  ‚úÖ Cache hit for NCMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NCNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NDAQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for NDLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NDSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NECB (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NERD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NERV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NESR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NET (312 rows, 100% coverage)
  ‚úÖ Cache hit for NETL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEWT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEXA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NEXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for NFRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGVC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NGVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NHTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NICE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NIPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NIU (312 rows, 100% coverage)
  ‚úÖ Cache hit for NJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NJR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NKTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NLOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMIH (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NMRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NNDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NNOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOAH (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NODK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NORW (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOTV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for NPCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NPK (312 rows, 100% coverage)
  ‚úÖ Cache hit for NPKI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NPWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NRP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NSIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NSSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTES (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTRSO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTST (312 rows, 100% coverage)
  ‚úÖ Cache hit for NTSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NU (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for NULG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NULV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for NURE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NUWE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVD (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVST (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWN (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for NWSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for NX (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXP (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXST (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NXTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for NYF (312 rows, 100% coverage)
  ‚úÖ Cache hit for NYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for NYXH (312 rows, 100% coverage)
  ‚úÖ Cache hit for NZAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for O (312 rows, 100% coverage)
  ‚úÖ Cache hit for OABI (312 rows, 100% coverage)
  ‚úÖ Cache hit for OACP (312 rows, 100% coverage)
  ‚úÖ Cache hit for OAEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for OAIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for OALC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OASC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OBDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for OC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for OCUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ODC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ODD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ODFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for OEFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for OFIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for OFLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for OGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for OGIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for OGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for OGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for OI (312 rows, 100% coverage)
  ‚úÖ Cache hit for OIH (312 rows, 100% coverage)
  ‚úÖ Cache hit for OII (312 rows, 100% coverage)
  ‚úÖ Cache hit for OILD (312 rows, 100% coverage)
  ‚úÖ Cache hit for OILK (312 rows, 100% coverage)
  ‚úÖ Cache hit for OIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for OKLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for OKTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OKUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLED (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for OLPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for OM (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMER (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ON (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONBPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONEQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONEW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONEY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONOF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONON (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONTF (312 rows, 100% coverage)
  ‚úÖ Cache hit for ONTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for OOMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPER (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPK (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPTU (312 rows, 100% coverage)
  ‚úÖ Cache hit for OPY (312 rows, 100% coverage)
  ‚úÖ Cache hit for OR (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORKA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ORRF (312 rows, 100% coverage)
  ‚úÖ Cache hit for OS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSCV (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for OSW (312 rows, 100% coverage)
  ‚úÖ Cache hit for OTEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for OTIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OTTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for OUNZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for OUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for OUSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for OUST (312 rows, 100% coverage)
  ‚úÖ Cache hit for OUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for OVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for OVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for OWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for OWNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for OXLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for OXM (312 rows, 100% coverage)
  ‚úÖ Cache hit for OXSQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for OXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for OZK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAAS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PABU (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PACB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PACK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PACS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PALC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PALL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PANL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PANW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PARR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PATH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PATK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAWZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAYC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAYO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PAYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBR-A (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBTP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PBYI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PCYO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PDSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PECO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PENG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PENN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PERI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PETS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PETZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PEZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFFV (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFUT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PFXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PGY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHYL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PHYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PICB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PICK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PID (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PII (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PINK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PINS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PIZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PJP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PJT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKOH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKST (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PKX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PLYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PMVP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNFP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for POCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PODD (312 rows, 100% coverage)
  ‚úÖ Cache hit for POET (312 rows, 100% coverage)
  ‚úÖ Cache hit for PONY (312 rows, 100% coverage)
  ‚úÖ Cache hit for POOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for POR (312 rows, 100% coverage)
  ‚úÖ Cache hit for POST (312 rows, 100% coverage)
  ‚úÖ Cache hit for POWA (312 rows, 100% coverage)
  ‚úÖ Cache hit for POWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for POWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for POWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for POWW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PPTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRDO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PREF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRFZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRME (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRSU (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRU (312 rows, 100% coverage)
  ‚úÖ Cache hit for PRVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSCU (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSET (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSFE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSKY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSTG (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PSX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTNQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTON (312 rows, 100% coverage)
  ‚úÖ Cache hit for PTRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PUBM (312 rows, 100% coverage)
  ‚úÖ Cache hit for PUK (312 rows, 100% coverage)
  ‚úÖ Cache hit for PULS (312 rows, 100% coverage)
  ‚úÖ Cache hit for PUMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PVH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PW (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWP (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWV (312 rows, 100% coverage)
  ‚úÖ Cache hit for PWZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PX (312 rows, 100% coverage)
  ‚úÖ Cache hit for PXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for PXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for PXH (312 rows, 100% coverage)
  ‚úÖ Cache hit for PXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for PY (312 rows, 100% coverage)
  ‚úÖ Cache hit for PYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for PYPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for PYZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for PZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for PZT (312 rows, 100% coverage)
  ‚úÖ Cache hit for PZZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for QABA (312 rows, 100% coverage)
  ‚úÖ Cache hit for QAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for QAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for QBTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QCLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for QCLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QCOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QCRH (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDF (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for QDTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for QEFA (312 rows, 100% coverage)
  ‚úÖ Cache hit for QEMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QETH (312 rows, 100% coverage)
  ‚úÖ Cache hit for QFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for QFLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for QGEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for QGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for QGRW (312 rows, 100% coverage)
  ‚úÖ Cache hit for QHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for QID (312 rows, 100% coverage)
  ‚úÖ Cache hit for QIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for QINT (312 rows, 100% coverage)
  ‚úÖ Cache hit for QJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLVD (312 rows, 100% coverage)
  ‚úÖ Cache hit for QLYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for QMCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for QMOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QNST (312 rows, 100% coverage)
  ‚úÖ Cache hit for QNTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQEW (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQH (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQE (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQH (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQX (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQQY (312 rows, 100% coverage)
  ‚úÖ Cache hit for QQXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for QRHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for QRVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for QS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for QSIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for QSPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for QSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for QTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for QTRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for QTTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for QTUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QTWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for QUAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for QUAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for QUBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for QUIK (312 rows, 100% coverage)
  ‚úÖ Cache hit for QURE (312 rows, 100% coverage)
  ‚úÖ Cache hit for QUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for QVML (312 rows, 100% coverage)
  ‚úÖ Cache hit for QVMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for QVMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for QWLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for QXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for QYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for QYLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for R (312 rows, 100% coverage)
  ‚úÖ Cache hit for RA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RACE (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for RANI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RARE (312 rows, 100% coverage)
  ‚úÖ Cache hit for RAVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBCAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RBRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for RC (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCKY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RCUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDDT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDVT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for REAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for REAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RECS (312 rows, 100% coverage)
  ‚úÖ Cache hit for REET (312 rows, 100% coverage)
  ‚úÖ Cache hit for REFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for REFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for REG (312 rows, 100% coverage)
  ‚úÖ Cache hit for REGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for REGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for REI (312 rows, 100% coverage)
  ‚úÖ Cache hit for REK (312 rows, 100% coverage)
  ‚úÖ Cache hit for RELI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RELX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RELY (312 rows, 100% coverage)
  ‚úÖ Cache hit for REM (312 rows, 100% coverage)
  ‚úÖ Cache hit for REMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RENT (312 rows, 100% coverage)
  ‚úÖ Cache hit for REPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for REPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RERE (312 rows, 100% coverage)
  ‚úÖ Cache hit for RES (312 rows, 100% coverage)
  ‚úÖ Cache hit for RETL (312 rows, 100% coverage)
  ‚úÖ Cache hit for REVB (312 rows, 100% coverage)
  ‚úÖ Cache hit for REVG (312 rows, 100% coverage)
  ‚úÖ Cache hit for REVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for REX (312 rows, 100% coverage)
  ‚úÖ Cache hit for REXR (312 rows, 100% coverage)
  ‚úÖ Cache hit for REYN (312 rows, 100% coverage)
  ‚úÖ Cache hit for REZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for REZI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RF (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RFV (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RGTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RH (312 rows, 100% coverage)
  ‚úÖ Cache hit for RHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for RHRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RHTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RICK (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RILY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RING (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RISN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RITM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RIVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RJF (312 rows, 100% coverage)
  ‚úÖ Cache hit for RKLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RMAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RMNI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNAZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNST (312 rows, 100% coverage)
  ‚úÖ Cache hit for RNW (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROCK (312 rows, 100% coverage)
  ‚úÖ Cache hit for RODM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROKU (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROST (312 rows, 100% coverage)
  ‚úÖ Cache hit for ROUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPID (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RPV (312 rows, 100% coverage)
  ‚úÖ Cache hit for RQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for RRGB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RRR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSKD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPF (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSPU (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for RSVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for RTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for RTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RUNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RUSHA (312 rows, 100% coverage)
  ‚úÖ Cache hit for RUSHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RVLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for RVMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RVNU (312 rows, 100% coverage)
  ‚úÖ Cache hit for RVTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWR (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXST (312 rows, 100% coverage)
  ‚úÖ Cache hit for RXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RY (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYI (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYN (312 rows, 100% coverage)
  ‚úÖ Cache hit for RYTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for RZB (312 rows, 100% coverage)
  ‚úÖ Cache hit for RZG (312 rows, 100% coverage)
  ‚úÖ Cache hit for RZLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for RZLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for RZV (312 rows, 100% coverage)
  ‚úÖ Cache hit for S (312 rows, 100% coverage)
  ‚úÖ Cache hit for SA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SABR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAFE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAFT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SANA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SANM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SARK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SARO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SATS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SAVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBCF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBET (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBFM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBLK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBSW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SBUX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCHZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SCZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDVY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SECT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEER (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEIQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SELV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SENEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SERV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SEZL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFGV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFST (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFWL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SFYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGDJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGML (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SGU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SHYL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SID (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SII (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SILA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SILC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SILJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SINT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SITC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SITE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SITM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SIZE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SJB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SJM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SJNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKLZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKYH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKYW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SKYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLDP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLMBP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLNH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLQD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLQT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLVO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLVP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SLYV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMMU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMWB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNCY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNDK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNDL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNFCA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNPE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SNY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOHU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOJC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOJD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOJE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SON (312 rows, 100% coverage)
  ‚úÖ Cache hit for SONO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SONY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOXQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SOYB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPBO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPGM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPIB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPIR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPRU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPUC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPUU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPVU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPWH (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPXV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SPYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SQM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SQNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SQQQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRAD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRCE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRET (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SRVR (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSO (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSPY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SST (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SSYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ST (312 rows, 100% coverage)
  ‚úÖ Cache hit for STAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for STAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for STBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for STC (312 rows, 100% coverage)
  ‚úÖ Cache hit for STE (312 rows, 100% coverage)
  ‚úÖ Cache hit for STEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for STEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for STGW (312 rows, 100% coverage)
  ‚úÖ Cache hit for STHO (312 rows, 100% coverage)
  ‚úÖ Cache hit for STIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for STKL (312 rows, 100% coverage)
  ‚úÖ Cache hit for STKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for STLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for STLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for STM (312 rows, 100% coverage)
  ‚úÖ Cache hit for STN (312 rows, 100% coverage)
  ‚úÖ Cache hit for STNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for STNE (312 rows, 100% coverage)
  ‚úÖ Cache hit for STNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for STOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for STOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for STPZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for STRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for STRK (312 rows, 100% coverage)
  ‚úÖ Cache hit for STRL (312 rows, 100% coverage)
  ‚úÖ Cache hit for STRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for STRZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for STT (312 rows, 100% coverage)
  ‚úÖ Cache hit for STUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for STVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for STWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for STX (312 rows, 100% coverage)
  ‚úÖ Cache hit for STXS (312 rows, 100% coverage)
  ‚úÖ Cache hit for STZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SU (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUPN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUPV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SURE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUSA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SUZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for SVXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for SW (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWKS (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWVL (312 rows, 100% coverage)
  ‚úÖ Cache hit for SWX (312 rows, 100% coverage)
  ‚úÖ Cache hit for SXC (312 rows, 100% coverage)
  ‚úÖ Cache hit for SXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for SXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYF (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYK (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for SYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for T (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TACK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TALK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TALO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TARK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TARS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TASK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TATT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TAXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBBB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBUX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCAF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCHP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TCX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDOC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDTF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDTT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDV (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDVG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDW (312 rows, 100% coverage)
  ‚úÖ Cache hit for TDY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEAM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TECX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TENB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TENX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TER (312 rows, 100% coverage)
  ‚úÖ Cache hit for TERN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TESL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TETH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFII (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFJL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TFX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGRW (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TGTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TH (312 rows, 100% coverage)
  ‚úÖ Cache hit for THC (312 rows, 100% coverage)
  ‚úÖ Cache hit for THCH (312 rows, 100% coverage)
  ‚úÖ Cache hit for THD (312 rows, 100% coverage)
  ‚úÖ Cache hit for THFF (312 rows, 100% coverage)
  ‚úÖ Cache hit for THG (312 rows, 100% coverage)
  ‚úÖ Cache hit for THNQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for THO (312 rows, 100% coverage)
  ‚úÖ Cache hit for THR (312 rows, 100% coverage)
  ‚úÖ Cache hit for THRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for THRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for THS (312 rows, 100% coverage)
  ‚úÖ Cache hit for THY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TILE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TILT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TIPZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for TITN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TJX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TKC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TKNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TKO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TKR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLRY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for TLYS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMDX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TME (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMFM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMHC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNDM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNET (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TNYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOLZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for TONX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TORO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOST (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOWN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TOYO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPIF (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPVG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TPYP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TQQQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for TR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRAK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TREE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TREX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRMK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRND (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRNR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TROW (312 rows, 100% coverage)
  ‚úÖ Cache hit for TROX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRPA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRST (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRU (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRV (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRVG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TRVI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TS (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSLL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSLX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSPA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TSSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTEK (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TTWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TU (312 rows, 100% coverage)
  ‚úÖ Cache hit for TUA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TUR (312 rows, 100% coverage)
  ‚úÖ Cache hit for TUYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TV (312 rows, 100% coverage)
  ‚úÖ Cache hit for TVTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TW (312 rows, 100% coverage)
  ‚úÖ Cache hit for TWFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TWI (312 rows, 100% coverage)
  ‚úÖ Cache hit for TWLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TWST (312 rows, 100% coverage)
  ‚úÖ Cache hit for TX (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXG (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXN (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXO (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXRH (312 rows, 100% coverage)
  ‚úÖ Cache hit for TXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for TY (312 rows, 100% coverage)
  ‚úÖ Cache hit for TYD (312 rows, 100% coverage)
  ‚úÖ Cache hit for TYL (312 rows, 100% coverage)
  ‚úÖ Cache hit for TYRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for TZA (312 rows, 100% coverage)
  ‚úÖ Cache hit for U (312 rows, 100% coverage)
  ‚úÖ Cache hit for UA (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAA (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAE (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAPR (312 rows, 100% coverage)
  ‚úÖ Cache hit for UAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBER (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UBT (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCON (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UCTT (312 rows, 100% coverage)
  ‚úÖ Cache hit for UDMY (312 rows, 100% coverage)
  ‚úÖ Cache hit for UDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for UDOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for UDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for UE (312 rows, 100% coverage)
  ‚úÖ Cache hit for UEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for UEIC (312 rows, 100% coverage)
  ‚úÖ Cache hit for UEVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UFCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for UFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UFO (312 rows, 100% coverage)
  ‚úÖ Cache hit for UFPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UFPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGE (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGL (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGP (312 rows, 100% coverage)
  ‚úÖ Cache hit for UGRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for UHAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for UHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for UHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for UI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for UITB (312 rows, 100% coverage)
  ‚úÖ Cache hit for UIVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for UJB (312 rows, 100% coverage)
  ‚úÖ Cache hit for UK (312 rows, 100% coverage)
  ‚úÖ Cache hit for UL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULST (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ULVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMDD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMH (312 rows, 100% coverage)
  ‚úÖ Cache hit for UMMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNB (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNF (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNFI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNG (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNH (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNIY (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNL (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNP (312 rows, 100% coverage)
  ‚úÖ Cache hit for UNTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for UP (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPB (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPGD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPST (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPV (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPW (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for UPXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for URA (312 rows, 100% coverage)
  ‚úÖ Cache hit for URBN (312 rows, 100% coverage)
  ‚úÖ Cache hit for URE (312 rows, 100% coverage)
  ‚úÖ Cache hit for URGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for URI (312 rows, 100% coverage)
  ‚úÖ Cache hit for URNJ (312 rows, 100% coverage)
  ‚úÖ Cache hit for URNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UROY (312 rows, 100% coverage)
  ‚úÖ Cache hit for URTH (312 rows, 100% coverage)
  ‚úÖ Cache hit for URTY (312 rows, 100% coverage)
  ‚úÖ Cache hit for USA (312 rows, 100% coverage)
  ‚úÖ Cache hit for USAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for USB (312 rows, 100% coverage)
  ‚úÖ Cache hit for USCB (312 rows, 100% coverage)
  ‚úÖ Cache hit for USCI (312 rows, 100% coverage)
  ‚úÖ Cache hit for USCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for USD (312 rows, 100% coverage)
  ‚úÖ Cache hit for USDU (312 rows, 100% coverage)
  ‚úÖ Cache hit for USFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for USFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for USHY (312 rows, 100% coverage)
  ‚úÖ Cache hit for USIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for USL (312 rows, 100% coverage)
  ‚úÖ Cache hit for USLM (312 rows, 100% coverage)
  ‚úÖ Cache hit for USMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for USMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for USMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for USNA (312 rows, 100% coverage)
  ‚úÖ Cache hit for USO (312 rows, 100% coverage)
  ‚úÖ Cache hit for USOI (312 rows, 100% coverage)
  ‚úÖ Cache hit for USPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for USPX (312 rows, 100% coverage)
  ‚úÖ Cache hit for USRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for USSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for USSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for UST (312 rows, 100% coverage)
  ‚úÖ Cache hit for USTB (312 rows, 100% coverage)
  ‚úÖ Cache hit for USVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for USXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTSI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for UTZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for UUP (312 rows, 100% coverage)
  ‚úÖ Cache hit for UUUU (312 rows, 100% coverage)
  ‚úÖ Cache hit for UVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for UVIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for UVSP (312 rows, 100% coverage)
  ‚úÖ Cache hit for UVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for UVXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for UWM (312 rows, 100% coverage)
  ‚úÖ Cache hit for UWMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for UXI (312 rows, 100% coverage)
  ‚úÖ Cache hit for UXIN (312 rows, 100% coverage)
  ‚úÖ Cache hit for UYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for V (312 rows, 100% coverage)
  ‚úÖ Cache hit for VAC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VACH (312 rows, 100% coverage)
  ‚úÖ Cache hit for VAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VALE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VALN (312 rows, 100% coverage)
  ‚úÖ Cache hit for VALQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for VAW (312 rows, 100% coverage)
  ‚úÖ Cache hit for VB (312 rows, 100% coverage)
  ‚úÖ Cache hit for VBK (312 rows, 100% coverage)
  ‚úÖ Cache hit for VBND (312 rows, 100% coverage)
  ‚úÖ Cache hit for VBNK (312 rows, 100% coverage)
  ‚úÖ Cache hit for VBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VCYT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VDE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VECO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEGA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VELO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEON (312 rows, 100% coverage)
  ‚úÖ Cache hit for VERA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VERI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VERU (312 rows, 100% coverage)
  ‚úÖ Cache hit for VERX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VET (312 rows, 100% coverage)
  ‚úÖ Cache hit for VEU (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFH (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFQY (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VFVA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VGIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VGK (312 rows, 100% coverage)
  ‚úÖ Cache hit for VGLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VGSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for VGT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VHI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VHT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIAV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VICI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VICR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIDI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIK (312 rows, 100% coverage)
  ‚úÖ Cache hit for VINP (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIPS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIST (312 rows, 100% coverage)
  ‚úÖ Cache hit for VITL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIVS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIXM (312 rows, 100% coverage)
  ‚úÖ Cache hit for VIXY (312 rows, 100% coverage)
  ‚úÖ Cache hit for VKTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLGEA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLN (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLUE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for VMBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for VMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNET (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNLA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNM (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNQI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VNT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VONE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VONG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VONV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOOG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VOYA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VPG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VPL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VPU (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VREX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRIG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRNS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRP (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRRM (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRSK (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRSN (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VRTX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSCO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSDA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSGX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSH (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSMV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VST (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSTA (312 rows, 100% coverage)
  ‚úÖ Cache hit for VSTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VT (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTMX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTOL (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTRS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTWG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTWV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VTYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for VUSB (312 rows, 100% coverage)
  ‚úÖ Cache hit for VUSE (312 rows, 100% coverage)
  ‚úÖ Cache hit for VUZI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VVOS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for VVX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for VWOB (312 rows, 100% coverage)
  ‚úÖ Cache hit for VXF (312 rows, 100% coverage)
  ‚úÖ Cache hit for VXUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for VXX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VYGR (312 rows, 100% coverage)
  ‚úÖ Cache hit for VYM (312 rows, 100% coverage)
  ‚úÖ Cache hit for VYMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for VYX (312 rows, 100% coverage)
  ‚úÖ Cache hit for VZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for W (312 rows, 100% coverage)
  ‚úÖ Cache hit for WAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for WABC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WAFD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WAL (312 rows, 100% coverage)
  ‚úÖ Cache hit for WALD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WANT (312 rows, 100% coverage)
  ‚úÖ Cache hit for WASH (312 rows, 100% coverage)
  ‚úÖ Cache hit for WAT (312 rows, 100% coverage)
  ‚úÖ Cache hit for WAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for WB (312 rows, 100% coverage)
  ‚úÖ Cache hit for WBD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WBTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WBX (312 rows, 100% coverage)
  ‚úÖ Cache hit for WCBR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WCLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WCN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDIV (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WDTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEAV (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEBL (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEBS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WELL (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WERN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WES (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEST (312 rows, 100% coverage)
  ‚úÖ Cache hit for WEX (312 rows, 100% coverage)
  ‚úÖ Cache hit for WF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WFG (312 rows, 100% coverage)
  ‚úÖ Cache hit for WFRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WGMI (312 rows, 100% coverage)
  ‚úÖ Cache hit for WGO (312 rows, 100% coverage)
  ‚úÖ Cache hit for WGS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WH (312 rows, 100% coverage)
  ‚úÖ Cache hit for WHD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WHLR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WHWK (312 rows, 100% coverage)
  ‚úÖ Cache hit for WINA (312 rows, 100% coverage)
  ‚úÖ Cache hit for WING (312 rows, 100% coverage)
  ‚úÖ Cache hit for WINN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for WIT (312 rows, 100% coverage)
  ‚úÖ Cache hit for WIX (312 rows, 100% coverage)
  ‚úÖ Cache hit for WK (312 rows, 100% coverage)
  ‚úÖ Cache hit for WKC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLDN (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLDR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLK (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLKP (312 rows, 100% coverage)
  ‚úÖ Cache hit for WLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for WM (312 rows, 100% coverage)
  ‚úÖ Cache hit for WMB (312 rows, 100% coverage)
  ‚úÖ Cache hit for WMG (312 rows, 100% coverage)
  ‚úÖ Cache hit for WMK (312 rows, 100% coverage)
  ‚úÖ Cache hit for WMS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WMT (312 rows, 100% coverage)
  ‚úÖ Cache hit for WNC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WOLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WOOD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WOOF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WOR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WOW (312 rows, 100% coverage)
  ‚úÖ Cache hit for WPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WPM (312 rows, 100% coverage)
  ‚úÖ Cache hit for WPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for WRAP (312 rows, 100% coverage)
  ‚úÖ Cache hit for WRB (312 rows, 100% coverage)
  ‚úÖ Cache hit for WRBY (312 rows, 100% coverage)
  ‚úÖ Cache hit for WRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WRLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSBC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSBF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSFS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSM (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSO (312 rows, 100% coverage)
  ‚úÖ Cache hit for WSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WST (312 rows, 100% coverage)
  ‚úÖ Cache hit for WT (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTBA (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTFC (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTI (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTM (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTMF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTPI (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTRG (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTV (312 rows, 100% coverage)
  ‚úÖ Cache hit for WTW (312 rows, 100% coverage)
  ‚úÖ Cache hit for WU (312 rows, 100% coverage)
  ‚úÖ Cache hit for WUGI (312 rows, 100% coverage)
  ‚úÖ Cache hit for WULF (312 rows, 100% coverage)
  ‚úÖ Cache hit for WVE (312 rows, 100% coverage)
  ‚úÖ Cache hit for WWD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WWJD (312 rows, 100% coverage)
  ‚úÖ Cache hit for WWW (312 rows, 100% coverage)
  ‚úÖ Cache hit for WY (312 rows, 100% coverage)
  ‚úÖ Cache hit for WYNN (312 rows, 100% coverage)
  ‚úÖ Cache hit for XAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XBI (312 rows, 100% coverage)
  ‚úÖ Cache hit for XBIL (312 rows, 100% coverage)
  ‚úÖ Cache hit for XCCC (312 rows, 100% coverage)
  ‚úÖ Cache hit for XCEM (312 rows, 100% coverage)
  ‚úÖ Cache hit for XDTE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for XEMD (312 rows, 100% coverage)
  ‚úÖ Cache hit for XENE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XERS (312 rows, 100% coverage)
  ‚úÖ Cache hit for XES (312 rows, 100% coverage)
  ‚úÖ Cache hit for XFLT (312 rows, 100% coverage)
  ‚úÖ Cache hit for XHB (312 rows, 100% coverage)
  ‚úÖ Cache hit for XHE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XHLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for XHR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XHS (312 rows, 100% coverage)
  ‚úÖ Cache hit for XIFR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XITK (312 rows, 100% coverage)
  ‚úÖ Cache hit for XJH (312 rows, 100% coverage)
  ‚úÖ Cache hit for XJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLB (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLC (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLF (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLI (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLK (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLP (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLRE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLSR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLU (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for XLY (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XME (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMPT (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMTR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XMVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for XNCR (312 rows, 100% coverage)
  ‚úÖ Cache hit for XNTK (312 rows, 100% coverage)
  ‚úÖ Cache hit for XOM (312 rows, 100% coverage)
  ‚úÖ Cache hit for XOMA (312 rows, 100% coverage)
  ‚úÖ Cache hit for XOP (312 rows, 100% coverage)
  ‚úÖ Cache hit for XP (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPEL (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPER (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPEV (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPH (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPO (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPOF (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPON (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPP (312 rows, 100% coverage)
  ‚úÖ Cache hit for XPRO (312 rows, 100% coverage)
  ‚úÖ Cache hit for XRAY (312 rows, 100% coverage)
  ‚úÖ Cache hit for XRT (312 rows, 100% coverage)
  ‚úÖ Cache hit for XRX (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSD (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSHQ (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSLV (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSMO (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSOE (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSVM (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSVN (312 rows, 100% coverage)
  ‚úÖ Cache hit for XSW (312 rows, 100% coverage)
  ‚úÖ Cache hit for XT (312 rows, 100% coverage)
  ‚úÖ Cache hit for XTEN (312 rows, 100% coverage)
  ‚úÖ Cache hit for XTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for XTN (312 rows, 100% coverage)
  ‚úÖ Cache hit for XTWO (312 rows, 100% coverage)
  ‚úÖ Cache hit for XVV (312 rows, 100% coverage)
  ‚úÖ Cache hit for XYF (312 rows, 100% coverage)
  ‚úÖ Cache hit for XYL (312 rows, 100% coverage)
  ‚úÖ Cache hit for XYLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for XYLG (312 rows, 100% coverage)
  ‚úÖ Cache hit for XYZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for YALA (312 rows, 100% coverage)
  ‚úÖ Cache hit for YANG (312 rows, 100% coverage)
  ‚úÖ Cache hit for YBTC (312 rows, 100% coverage)
  ‚úÖ Cache hit for YCL (312 rows, 100% coverage)
  ‚úÖ Cache hit for YCS (312 rows, 100% coverage)
  ‚úÖ Cache hit for YEAR (312 rows, 100% coverage)
  ‚úÖ Cache hit for YELP (312 rows, 100% coverage)
  ‚úÖ Cache hit for YETI (312 rows, 100% coverage)
  ‚úÖ Cache hit for YEXT (312 rows, 100% coverage)
  ‚úÖ Cache hit for YINN (312 rows, 100% coverage)
  ‚úÖ Cache hit for YJUN (312 rows, 100% coverage)
  ‚úÖ Cache hit for YLD (312 rows, 100% coverage)
  ‚úÖ Cache hit for YMAG (312 rows, 100% coverage)
  ‚úÖ Cache hit for YMAX (312 rows, 100% coverage)
  ‚úÖ Cache hit for YMM (312 rows, 100% coverage)
  ‚úÖ Cache hit for YOLO (312 rows, 100% coverage)
  ‚úÖ Cache hit for YORW (312 rows, 100% coverage)
  ‚úÖ Cache hit for YOU (312 rows, 100% coverage)
  ‚úÖ Cache hit for YPF (312 rows, 100% coverage)
  ‚úÖ Cache hit for YRD (312 rows, 100% coverage)
  ‚úÖ Cache hit for YSG (312 rows, 100% coverage)
  ‚úÖ Cache hit for YUM (312 rows, 100% coverage)
  ‚úÖ Cache hit for YUMC (312 rows, 100% coverage)
  ‚úÖ Cache hit for YYAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for YYY (312 rows, 100% coverage)
  ‚úÖ Cache hit for Z (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZALT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZAUG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZBAI (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZBH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZBIO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZBRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZD (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZDEK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZECP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZETA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZEUS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZFEB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZG (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZGN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZIM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZION (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZIONP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZIP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZJAN (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZJK (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZJUL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZKH (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZLAB (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZM (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZNOV (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZNTL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZOCT (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZROZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZSEP (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZSL (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZSPC (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZTO (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZTS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZUMZ (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZVIA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZVRA (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZWS (312 rows, 100% coverage)
  ‚úÖ Cache hit for ZYME (312 rows, 100% coverage)
  üîÑ Downloading data for 2 tickers (2 full, 0 incremental)...
  üì• Downloading FULL range for 2 tickers...
  ‚úÖ Successfully downloaded full range for 2 tickers
      ‚ö†Ô∏è Cache append failed for QQQ, overwrote: Cannot compare tz-naive and tz-aware timestamps
      ‚ö†Ô∏è Cache append failed for SPY, overwrote: Cannot compare tz-naive and tz-aware timestamps

  üìä Data Summary for 5647 Tickers
  ===============================================================================================
  Ticker   Status          Rows     Start Date   End Date     Info                
  -----------------------------------------------------------------------------------------------
  A        Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AA       Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAAU     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAL      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAM      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAMI     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAOI     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAON     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAP      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAPG     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAPL     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAT      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AAXJ     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  AB       Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABBV     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABCB     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABCL     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABEO     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABEV     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABFL     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABG      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABL      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABM      Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABNB     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ABOT     Cache Hit       312      2024-09-30   2025-12-26   From cache          
  ... and 5622 more tickers
  ===============================================================================================
  ‚úÖ All 5647 tickers loaded from cache (no downloads needed)

  üìä Combined data: 5647 cached + 1 fresh = 28245 total columns
üîÑ Converting data from wide format to long format...
   ‚úÖ Converted to long format: 1762488 rows, 5649 tickers
üîç Filtering out delisted stocks (no recent data)...
   ‚úÖ All 5649 tickers have recent data
‚úÖ Comprehensive data download complete.
‚ÑπÔ∏è Capping backtest end date from 2025-12-29 to last available data 2025-12-26
üìÖ Data available until: 2025-12-26
üìÖ Prediction horizon: 63 days
üìÖ Backtest will end: 2025-10-24 (ensuring 63 days of future data for validation)
üîç Fetching SPY data for Market Momentum feature...
  Updating SPY (+2 days)...
‚úÖ SPY Market Momentum data fetched and merged.
üîç Fetching intermarket data...
‚ö†Ô∏è Could not fetch intermarket data. Intermarket features will be 0.

üîç Validating data structure...
‚úÖ Data validation passed:
   - Shape: (1762488, 15)
   - Tickers: 5649
   - Date range: 2024-09-30 00:00:00+00:00 to 2025-12-26 00:00:00+00:00
   - Columns: ['date', 'ticker', 'Close', 'High', 'Low', 'Open', 'Volume', 'Market_Momentum_SPY', 'Intermarket_VIX_Index_Returns', 'Intermarket_DXY_Index_Returns', 'Intermarket_Gold_Futures_Returns', 'Intermarket_Oil_Futures_Returns', 'Intermarket_US10Y_Yield_Returns', 'Intermarket_Oil_Price_Returns', 'Intermarket_Gold_Price_Returns']

üîç Analyzing data quality for each ticker...
   üìä Fast validation for 5649 tickers...

   ‚úÖ Validation complete: 5649 OK, 0 insufficient, 0 empty
   üìä Overall data: 1762488 rows, 5649 tickers

üöÄ AI-Powered Momentum & Trend Strategy
==================================================

üîç Step 2: Identifying stocks outperforming market benchmarks...
  üìÖ Using performance data up to 2025-07-26 (purchase date) to avoid look-ahead bias
- Calculating 1-Year Performance Benchmarks...
  üîÑ Fetching data for QQQ using multi-provider fallback...
  ‚ÑπÔ∏è Alpaca (free tier) does not provide recent data for QQQ. Attempting fallback provider.
  ‚ÑπÔ∏è Alpaca fetch failed for QQQ. Trying TwelveData...
  ‚úÖ Successfully fetched data for QQQ from TwelveData.
  ‚úÖ QQQ 1-Year Performance: 22.33%
  üîÑ Fetching data for SPY using multi-provider fallback...
  ‚ÑπÔ∏è Alpaca (free tier) does not provide recent data for SPY. Attempting fallback provider.
  ‚ÑπÔ∏è Alpaca fetch failed for SPY. Trying TwelveData...
  ‚úÖ Successfully fetched data for SPY from TwelveData.
  ‚úÖ SPY 1-Year Performance: 17.02%
  üìà Using final 1-Year performance benchmark of 22.33%
üîç Calculating 1-Year performance from pre-fetched data...
...performance calculation complete.
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility

‚úÖ Selected top 5638 tickers based on 1-Year performance.
üîç Applying performance benchmarks for selected tickers in parallel...
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility

‚úÖ Found 1055 stocks that passed the performance benchmarks.
  üîç Screening 1055 strong performers for fundamental metrics in parallel...
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
  ‚úÖ Found 966 stocks passing the fundamental screens.

‚úÖ Identified 966 stocks for backtesting: OKLO, PRCH, LAES, RYM, AMPX, TMC, ARQQ, AEVA, QNTM, ABVX, KC, DAVE, SEZL, BMNR, IONQ, SRRK, HOOD, SMR, UAMY, PSIX, LEU, PLTR, OUST, BKSY, GRRR, TSSI, ASPI, RCAT, ACHR, NVTS, KOD, GRPN, CLS, CRDO, ROOT, BE, SLDP, SGHC, HIMS, ATAI, QURE, ZVIA, PGY, INOD, GRAL, CRNC, NNE, KOPN, CANG, APP, ONDS, SOFI, AKBA, MPU, RBRK, ALLT, ORLA, LX, SEI, ETON, KTOS, NAGE, QMCO, GEV, AMLX, SOUN, TTMI, SBET, AISP, OPFI, NET, LGCY, BITU, LMND, MSTR, NEXT, BITX, AS, CAR, SRAD, AGX, TPR, INVZ, CRCL, DPRO, EYE, ESLT, RDDT, OPRT, JFIN, CONL, HTZ, SBSW, GRNQ, PAYS, XYF, TRVG, AMBR, COIN, GOGO, SSRM, QSI, OSS, AAPG, GHRS, LUXE, EVEX, EOSE, MASS, AVXL, TESL, DOMO, DAO, FOA, ALNT, IREN, UPST, ASTS, AENT, TARK, IHS, APEI, NN, DOYU, APYX, LFMD, CTMX, GEO, DXPE, KEN, VNET, UI, TWLO, VVOS, ATOM, MGTX, RCL, EAT, WLDN, MESO, VS, DJTWW, DB, MIR, TSAT, TIGR, RBOT, METC, MSTY, ABCL, TBRG, LQDA, RIOT, MYRG, LIF, TLN, VUZI, UTI, AU, LINC, CVNA, AAOI, CCB, MGIC, HWM, CYD, IBKR, WGS, MNY, SPOT, CRK, JBL, BROS, CVAC, GHM, ARKW, STRL, BTCW, HODL, AXON, EZBC, BITB, BRRR, IBIT, FBTC, ARKB, BTCO, REAL, SAN, AMSC, SKE, DEFI, OM, UUUU, FRHC, TPB, GBTC, BBIO, ARKF, LASR, KEP, IRTC, HIMX, GDS, ETHZ, IESC, TME, PARR, APPS, VEON, XMTR, ATRO, FIX, LMB, CIB, TPC, ORGO, INDV, EXEL, PBI, ICCM, DASH, FUTU, TATT, CPS, TENX, BITO, NRG, CRS, ODC, PPTA, KGC, SNEX, CMPX, CTOS, DRS, NXT, TOST, REVG, HNRG, BLOK, MNMD, DNA, SLNO, YALA, JNUG, KD, NBP, MGNI, IVA, AVGO, CMP, AEHR, EVLV, HRTG, ODD, OMAB, UNFI, LLYVA, ZS, NIU, SE, VIK, CIFR, SHLD, CCJ, CRWD, SII, NFLX, HUT, LLYVK, API, VSEC, WEBL, DOUG, RYTM, LITE, CAKE, ARKX, LTM, AFRM, BCS, MPAA, SLI, CXW, ICL, LPLA, GDXU, GFI, WBD, CTRN, SENEA, CUK, MSB, VST, DLTR, TIGO, YBTC, APH, PCT, UFO, ZVRA, MRX, SMLR, ARKQ, ARKK, VRSN, WF, DAPP, PEGA, ORN, BITQ, CCL, NUGT, PRIM, IDCC, GTX, ITRN, APG, UAL, AEM, LUNR, HIPO, PRA, DAN, TGTX, AVPT, RDVT, VSTA, AAMI, ENVX, ASA, C, FNGO, FIVE, YPF, NWG, PW, UPXI, CD, ADTN, RL, SHOP, WPM, AAP, ZLAB, RRGB, GRND, BA, BBAR, BELFB, NGD, HTT, CAE, BTF, ATGE, GREK, LRN, AVAL, ESQ, OR, WFC, BBW, SCHW, MTZ, QTUM, ACAD, ROAD, HSBC, TGS, BTI, MFG, LFCR, WWD, DGP, CG, WOR, SGDM, BRNS, FHN, NFLY, URA, IDT, FINV, ECG, BWMN, PCRX, CTLP, IPI, FLEX, GS, WRBY, BB, UGI, CALM, OSIS, RMBS, CELH, UNTY, ESE, SYF, NTES, CHEF, CNM, SSP, CBRL, VIRT, KMDA, EME, CW, NTRS, CORT, TFPM, SUPV, DRD, ACMR, INBX, FSM, HUYA, OPLN, JOYY, MUFG, GILT, PFIX, MRP, AHR, SERV, SGDJ, IRS, UGL, TE, TTWO, SIL, FGM, NVDU, FDIG, SFM, NLR, JCI, EXK, BFC, NERD, ORCL, EPOL, SLM, LB, GE, OCS, TITN, MCK, CAH, GTES, LOMA, NVDL, KODK, PAYC, SANM, IZRL, EQT, CIEN, BK, OPY, ULS, IFS, COF, UEC, METCB, SXT, EIS, AG, ATLC, MRCY, FROG, LZ, PAY, TSM, NVDA, GOAU, LYG, GDXJ, ATRA, UAN, CRMD, NAMS, HGV, DEC, PLTM, PPLT, DLO, ATI, ESPO, VLN, AMPL, GILD, CTOR, TNL, PWR, EWO, PAAS, JBTM, SLVP, NPK, CENX, GRAB, MOS, CASY, XAR, NIC, QXO, STX, ETH, BKCH, OCUL, ETHW, ETHV, POET, EZET, RSI, FETH, PERI, TETH, ETHA, PTON, LAUR, WGMI, QETH, PLMR, BTSG, RING, LYV, HMY, FTI, USD, TEL, RJF, MAIN, DOCS, ATAT, BYRN, CX, UNM, EUFN, PLNT, CSV, TMV, MCB, UAE, PJT, HNST, MBI, EMR, ING, TMAT, VEEV, GTLS, VRT, TKO, PUK, SPNT, WT, DDS, IAI, PGNY, SGI, STN, ETHE, FAS, BAP, OLLI, NRGV, KLTR, MTW, DCO, TRMB, KT, FHI, GDX, BWXT, ETR, CGNT, SAH, FPX, TBPH, RBA, TCBX, CHAT, WWW, METV, BAM, PM, AVAV, OKTA, EFXT, FFIV, FAST, FBIO, GENI, LGND, IGIC, NGVC, HLF, MTA, GRMN, OSW, EMBJ, ROK, ISRA, FTNT, MEXX, FOX, TOYO, BBCP, CDE, WRLD, STRK, AX, IIIV, TR, BLX, CBNK, FOXA, LANV, BFH, BKNG, IAG, ELPC, LOUP, CFLT, FDD, GLXY, MEDP, ARLO, COCO, BYD, AEIS, AZZ, ECNS, USFD, OMF, MRUS, TXNM, XPP, EWP, WDC, PAC, T, ITA, QUAD, LENZ, PRLB, SHAK, FNGS, MTRX, MCRI, STVN, CNP, MIRM, CUBI, DXCM, RAMP, FWONK, CSCO, CTRI, RPRX, COLO, ULTA, FNV, GSAT, SKYW, TASK, DSP, STT, LIND, CME, AB, SEIC, WBS, PAHC, DAX, HERO, RTX, EWI, BMO, ECVT, EETH, EYPT, CMCM, NDAQ, OGIG, CEPU, PAR, LTL, THFF, SMMT, IMAX, SMCI, BGM, NG, FN, UFCS, NOTV, DG, SMFG, CCEP, MT, V, COMM, AGQ, GOLF, HCI, DTM, KBWB, APLD, NPKI, OSK, DDOG, VCTR, SFST, AGI, HAFC, FEP, WMB, ATEN, RZLT, AL, COR, ONC, NEU, YINN, RBC, PRDO, CSGS, EWS, FWONA, BKR, NMG, MKL, NTR, SONY, BETZ, DUOL, AWI, BEN, DAKT, DBD, NPO, VNM, NVDY, BMBL, NPCE, IVZ, SILJ, PPA, GL, XPO, DOCU, PBYI, WCC, EWG, FDN, DRTS, GLAD, FLUT, NTGR, AFK, WAY, DRI, ALV, SYBT, UBS, SHCO, FSCO, KMI, SPPP, OPPE, WELL, YMM, RDWR, BNT, EGAN, BN, SPMO, FMS, SKWD, BSVN, EWBC, PWRD, CIO, BBUC, FET, NGS, DIS, INTU, GVAL, PAX, ALTG, ITRI, ORLY, VALN, FFTY, R, EXPE, Z, EQH, TDY, OPEN, FDT, SAP, TEO, RELY, IAUM, GLDM, BAR, DK, BULZ, SGOL, EBAY, BSX, OUNZ, AAAU, KR, HACK, FINX, IAU, ASTE, CLSK, UROY, ARGT, GLD, CPER, CIBR, DCTH, BJ, FITE, ENVA, ZG, LNG, TARS, FER, KCE, SOCL, IGV, TBT, NTNX, IETC, ARTY, KALV, CTVA, SPWH, SIXG, OIS, ESAB, IYG, TS, PHYS, UPWK, VMI, YMAX, PRM, DE, GLTR, PBW, HLI, PIZ, FXI, MVBF, ETOR, NEM, CRBG, IDMO, GEOS, DBP, RIVN, SLVO, META, HWKN, GLW, ASLE, CLM, PODD, WUGI, RIGL, OZK, HEI-A, NTB, FSK, PSKY, NI, HDB, TG, TD, NBN, IDV, AMZY, NRIM, JTEK, AMZN, CFG, CRMT, DTST, NVMI, SNX, BRW, GNE, CHWY, WRB, IQM, HEI, TRMK, EVC, PAM, COWG, FDP, PSLV, MO, TQQQ, TRS, MTSI, WCBR, MAMA, FRSH, SOBO, GGAL, AXS, MASI, RSG, IRON, CPA, GEL, SIMO, GT, PEJ, FEDU, SPRY, CFR, CSGP, CPNG, UIVM, KEMQ, WTFC, FDTS, STEL, XITK, VSAT, EPR, SPOK, TRIP, IXG, PDLB, FTDR, QLD, HURN, CECO, WTS, ALG, CMPO, SMBK, AIT, FDNI, CDNS, BBSI, PCOR, FGD, UXI, FBNC, ONLN

üìÖ Backtest configured for 90 days (~3-Month)

üîç Step 3: Training AI models for 3-Month backtest...
    üìä OKLO 3- Training: Horizon=60d, Target=136.97% (B&H: 833.25%, Scale=0.164)
‚úÖ OKLO: Training data validated - 205 rows over 365 days
    üìä PRCH 3- Training: Horizon=60d, Target=127.28% (B&H: 774.27%, Scale=0.164)
‚úÖ PRCH: Training data validated - 205 rows over 365 days
    üìä LAES 3- Training: Horizon=60d, Target=107.60% (B&H: 654.55%, Scale=0.164)
‚úÖ LAES: Training data validated - 205 rows over 365 days
    üìä RYM 3- Training: Horizon=60d, Target=106.32% (B&H: 646.75%, Scale=0.164)
‚úÖ RYM: Training data validated - 205 rows over 365 days
    üìä AMPX 3- Training: Horizon=60d, Target=105.00% (B&H: 638.74%, Scale=0.164)
‚úÖ AMPX: Training data validated - 205 rows over 365 days
    üìä TMC 3- Training: Horizon=60d, Target=104.37% (B&H: 634.91%, Scale=0.164)
‚úÖ TMC: Training data validated - 205 rows over 365 days
    üìä ARQQ 3- Training: Horizon=60d, Target=94.10% (B&H: 572.46%, Scale=0.164)
‚úÖ ARQQ: Training data validated - 205 rows over 365 days
    üìä AEVA 3- Training: Horizon=60d, Target=90.94% (B&H: 553.19%, Scale=0.164)
‚úÖ AEVA: Training data validated - 205 rows over 365 days
    üìä QNTM 3- Training: Horizon=60d, Target=86.92% (B&H: 528.75%, Scale=0.164)
‚úÖ QNTM: Training data validated - 205 rows over 365 days
    üìä ABVX 3- Training: Horizon=60d, Target=78.44% (B&H: 477.17%, Scale=0.164)
‚úÖ ABVX: Training data validated - 205 rows over 365 days
    üìä KC 3- Training: Horizon=60d, Target=71.90% (B&H: 437.41%, Scale=0.164)
‚úÖ KC: Training data validated - 205 rows over 365 days
    üìä DAVE 3- Training: Horizon=60d, Target=70.20% (B&H: 427.05%, Scale=0.164)
‚úÖ DAVE: Training data validated - 205 rows over 365 days
    üìä SEZL 3- Training: Horizon=60d, Target=69.68% (B&H: 423.89%, Scale=0.164)
‚úÖ SEZL: Training data validated - 205 rows over 365 days
    üìä BMNR 3- Training: Horizon=60d, Target=67.98% (B&H: 413.55%, Scale=0.164)
  ‚ùå BMNR: Too many NaN values in Close price: 170 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä IONQ 3- Training: Horizon=60d, Target=64.76% (B&H: 393.94%, Scale=0.164)
‚úÖ IONQ: Training data validated - 205 rows over 365 days
    üìä SRRK 3- Training: Horizon=60d, Target=63.50% (B&H: 386.27%, Scale=0.164)
‚úÖ SRRK: Training data validated - 205 rows over 365 days
    üìä HOOD 3- Training: Horizon=60d, Target=57.16% (B&H: 347.69%, Scale=0.164)
‚úÖ HOOD: Training data validated - 205 rows over 365 days
    üìä SMR 3- Training: Horizon=60d, Target=56.91% (B&H: 346.20%, Scale=0.164)
‚úÖ SMR: Training data validated - 205 rows over 365 days
    üìä UAMY 3- Training: Horizon=60d, Target=56.02% (B&H: 340.79%, Scale=0.164)
‚úÖ UAMY: Training data validated - 205 rows over 365 days
    üìä PSIX 3- Training: Horizon=60d, Target=55.97% (B&H: 340.49%, Scale=0.164)
‚úÖ PSIX: Training data validated - 205 rows over 365 days
    üìä LEU 3- Training: Horizon=60d, Target=55.79% (B&H: 339.38%, Scale=0.164)
‚úÖ LEU: Training data validated - 205 rows over 365 days
    üìä PLTR 3- Training: Horizon=60d, Target=53.73% (B&H: 326.88%, Scale=0.164)
‚úÖ PLTR: Training data validated - 205 rows over 365 days
    üìä OUST 3- Training: Horizon=60d, Target=52.24% (B&H: 317.78%, Scale=0.164)
‚úÖ OUST: Training data validated - 205 rows over 365 days
    üìä BKSY 3- Training: Horizon=60d, Target=52.16% (B&H: 317.30%, Scale=0.164)
‚úÖ BKSY: Training data validated - 205 rows over 365 days
    üìä GRRR 3- Training: Horizon=60d, Target=51.64% (B&H: 314.14%, Scale=0.164)
‚úÖ GRRR: Training data validated - 205 rows over 365 days
    üìä TSSI 3- Training: Horizon=60d, Target=50.31% (B&H: 306.03%, Scale=0.164)
‚úÖ TSSI: Training data validated - 205 rows over 365 days
    üìä ASPI 3- Training: Horizon=60d, Target=45.47% (B&H: 276.62%, Scale=0.164)
‚úÖ ASPI: Training data validated - 205 rows over 365 days
    üìä RCAT 3- Training: Horizon=60d, Target=44.78% (B&H: 272.44%, Scale=0.164)
‚úÖ RCAT: Training data validated - 205 rows over 365 days
    üìä ACHR 3- Training: Horizon=60d, Target=44.38% (B&H: 269.97%, Scale=0.164)
‚úÖ ACHR: Training data validated - 205 rows over 365 days
    üìä NVTS 3- Training: Horizon=60d, Target=42.54% (B&H: 258.78%, Scale=0.164)
‚úÖ NVTS: Training data validated - 205 rows over 365 days
    üìä KOD 3- Training: Horizon=60d, Target=39.24% (B&H: 238.70%, Scale=0.164)
‚úÖ KOD: Training data validated - 205 rows over 365 days
    üìä GRPN 3- Training: Horizon=60d, Target=38.94% (B&H: 236.91%, Scale=0.164)
‚úÖ GRPN: Training data validated - 205 rows over 365 days
    üìä CLS 3- Training: Horizon=60d, Target=38.30% (B&H: 232.98%, Scale=0.164)
‚úÖ CLS: Training data validated - 205 rows over 365 days
    üìä CRDO 3- Training: Horizon=60d, Target=37.58% (B&H: 228.64%, Scale=0.164)
‚úÖ CRDO: Training data validated - 205 rows over 365 days
    üìä ROOT 3- Training: Horizon=60d, Target=37.34% (B&H: 227.16%, Scale=0.164)
‚úÖ ROOT: Training data validated - 205 rows over 365 days
    üìä BE 3- Training: Horizon=60d, Target=37.02% (B&H: 225.19%, Scale=0.164)
‚úÖ BE: Training data validated - 205 rows over 365 days
    üìä SLDP 3- Training: Horizon=60d, Target=36.04% (B&H: 219.26%, Scale=0.164)
‚úÖ SLDP: Training data validated - 205 rows over 365 days
    üìä SGHC 3- Training: Horizon=60d, Target=35.06% (B&H: 213.25%, Scale=0.164)
‚úÖ SGHC: Training data validated - 205 rows over 365 days
    üìä HIMS 3- Training: Horizon=60d, Target=35.01% (B&H: 212.98%, Scale=0.164)
‚úÖ HIMS: Training data validated - 205 rows over 365 days
    üìä ATAI 3- Training: Horizon=60d, Target=34.15% (B&H: 207.76%, Scale=0.164)
‚úÖ ATAI: Training data validated - 205 rows over 365 days
    üìä QURE 3- Training: Horizon=60d, Target=33.94% (B&H: 206.49%, Scale=0.164)
‚úÖ QURE: Training data validated - 205 rows over 365 days
    üìä ZVIA 3- Training: Horizon=60d, Target=33.64% (B&H: 204.63%, Scale=0.164)
‚úÖ ZVIA: Training data validated - 205 rows over 365 days
    üìä PGY 3- Training: Horizon=60d, Target=32.50% (B&H: 197.73%, Scale=0.164)
‚úÖ PGY: Training data validated - 205 rows over 365 days
    üìä INOD 3- Training: Horizon=60d, Target=31.79% (B&H: 193.38%, Scale=0.164)
‚úÖ INOD: Training data validated - 205 rows over 365 days
    üìä GRAL 3- Training: Horizon=60d, Target=31.69% (B&H: 192.81%, Scale=0.164)
‚úÖ GRAL: Training data validated - 205 rows over 365 days
    üìä CRNC 3- Training: Horizon=60d, Target=31.62% (B&H: 192.38%, Scale=0.164)
‚úÖ CRNC: Training data validated - 205 rows over 365 days
    üìä NNE 3- Training: Horizon=60d, Target=31.30% (B&H: 190.42%, Scale=0.164)
‚úÖ NNE: Training data validated - 205 rows over 365 days
    üìä KOPN 3- Training: Horizon=60d, Target=29.72% (B&H: 180.82%, Scale=0.164)
‚úÖ KOPN: Training data validated - 205 rows over 365 days
    üìä CANG 3- Training: Horizon=60d, Target=29.55% (B&H: 179.78%, Scale=0.164)
‚úÖ CANG: Training data validated - 205 rows over 365 days
    üìä APP 3- Training: Horizon=60d, Target=29.41% (B&H: 178.93%, Scale=0.164)
‚úÖ APP: Training data validated - 205 rows over 365 days
    üìä ONDS 3- Training: Horizon=60d, Target=28.98% (B&H: 176.26%, Scale=0.164)
‚úÖ ONDS: Training data validated - 205 rows over 365 days
    üìä SOFI 3- Training: Horizon=60d, Target=27.90% (B&H: 169.72%, Scale=0.164)
‚úÖ SOFI: Training data validated - 205 rows over 365 days
    üìä AKBA 3- Training: Horizon=60d, Target=27.90% (B&H: 169.70%, Scale=0.164)
‚úÖ AKBA: Training data validated - 205 rows over 365 days
    üìä MPU 3- Training: Horizon=60d, Target=27.18% (B&H: 165.35%, Scale=0.164)
‚úÖ MPU: Training data validated - 205 rows over 365 days
    üìä RBRK 3- Training: Horizon=60d, Target=27.02% (B&H: 164.39%, Scale=0.164)
‚úÖ RBRK: Training data validated - 205 rows over 365 days
    üìä ALLT 3- Training: Horizon=60d, Target=26.84% (B&H: 163.30%, Scale=0.164)
‚úÖ ALLT: Training data validated - 205 rows over 365 days
    üìä ORLA 3- Training: Horizon=60d, Target=26.47% (B&H: 161.00%, Scale=0.164)
‚úÖ ORLA: Training data validated - 205 rows over 365 days
    üìä LX 3- Training: Horizon=60d, Target=26.29% (B&H: 159.91%, Scale=0.164)
‚úÖ LX: Training data validated - 205 rows over 365 days
    üìä SEI 3- Training: Horizon=60d, Target=26.03% (B&H: 158.35%, Scale=0.164)
‚úÖ SEI: Training data validated - 205 rows over 365 days
    üìä ETON 3- Training: Horizon=60d, Target=25.75% (B&H: 156.67%, Scale=0.164)
‚úÖ ETON: Training data validated - 205 rows over 365 days
    üìä KTOS 3- Training: Horizon=60d, Target=25.73% (B&H: 156.52%, Scale=0.164)
‚úÖ KTOS: Training data validated - 205 rows over 365 days
    üìä NAGE 3- Training: Horizon=60d, Target=25.54% (B&H: 155.34%, Scale=0.164)
‚úÖ NAGE: Training data validated - 205 rows over 365 days
    üìä QMCO 3- Training: Horizon=60d, Target=25.27% (B&H: 153.71%, Scale=0.164)
‚úÖ QMCO: Training data validated - 205 rows over 365 days
    üìä GEV 3- Training: Horizon=60d, Target=25.20% (B&H: 153.29%, Scale=0.164)
‚úÖ GEV: Training data validated - 205 rows over 365 days
    üìä AMLX 3- Training: Horizon=60d, Target=24.91% (B&H: 151.54%, Scale=0.164)
‚úÖ AMLX: Training data validated - 205 rows over 365 days
    üìä SOUN 3- Training: Horizon=60d, Target=24.87% (B&H: 151.29%, Scale=0.164)
‚úÖ SOUN: Training data validated - 205 rows over 365 days
    üìä TTMI 3- Training: Horizon=60d, Target=24.36% (B&H: 148.16%, Scale=0.164)
‚úÖ TTMI: Training data validated - 205 rows over 365 days
    üìä SBET 3- Training: Horizon=60d, Target=24.27% (B&H: 147.64%, Scale=0.164)
‚úÖ SBET: Training data validated - 205 rows over 365 days
    üìä AISP 3- Training: Horizon=60d, Target=24.09% (B&H: 146.52%, Scale=0.164)
‚úÖ AISP: Training data validated - 205 rows over 365 days
    üìä OPFI 3- Training: Horizon=60d, Target=23.97% (B&H: 145.80%, Scale=0.164)
‚úÖ OPFI: Training data validated - 205 rows over 365 days
    üìä NET 3- Training: Horizon=60d, Target=23.88% (B&H: 145.30%, Scale=0.164)
‚úÖ NET: Training data validated - 205 rows over 365 days
    üìä LGCY 3- Training: Horizon=60d, Target=23.80% (B&H: 144.78%, Scale=0.164)
‚úÖ LGCY: Training data validated - 205 rows over 365 days
    üìä BITU 3- Training: Horizon=60d, Target=23.78% (B&H: 144.68%, Scale=0.164)
‚úÖ BITU: Training data validated - 205 rows over 365 days
    üìä LMND 3- Training: Horizon=60d, Target=23.42% (B&H: 142.45%, Scale=0.164)
‚úÖ LMND: Training data validated - 205 rows over 365 days
    üìä MSTR 3- Training: Horizon=60d, Target=23.14% (B&H: 140.74%, Scale=0.164)
‚úÖ MSTR: Training data validated - 205 rows over 365 days
    üìä NEXT 3- Training: Horizon=60d, Target=23.07% (B&H: 140.34%, Scale=0.164)
‚úÖ NEXT: Training data validated - 205 rows over 365 days
    üìä BITX 3- Training: Horizon=60d, Target=22.89% (B&H: 139.25%, Scale=0.164)
‚úÖ BITX: Training data validated - 205 rows over 365 days
    üìä AS 3- Training: Horizon=60d, Target=22.37% (B&H: 136.11%, Scale=0.164)
‚úÖ AS: Training data validated - 205 rows over 365 days
    üìä CAR 3- Training: Horizon=60d, Target=22.37% (B&H: 136.09%, Scale=0.164)
‚úÖ CAR: Training data validated - 205 rows over 365 days
    üìä SRAD 3- Training: Horizon=60d, Target=22.26% (B&H: 135.43%, Scale=0.164)
‚úÖ SRAD: Training data validated - 205 rows over 365 days
    üìä AGX 3- Training: Horizon=60d, Target=22.17% (B&H: 134.86%, Scale=0.164)
‚úÖ AGX: Training data validated - 205 rows over 365 days
    üìä TPR 3- Training: Horizon=60d, Target=22.01% (B&H: 133.92%, Scale=0.164)
‚úÖ TPR: Training data validated - 205 rows over 365 days
    üìä INVZ 3- Training: Horizon=60d, Target=21.68% (B&H: 131.88%, Scale=0.164)
‚úÖ INVZ: Training data validated - 205 rows over 365 days
    üìä CRCL 3- Training: Horizon=60d, Target=21.65% (B&H: 131.72%, Scale=0.164)
  ‚ùå CRCL: Too many NaN values in Close price: 170 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä DPRO 3- Training: Horizon=60d, Target=21.60% (B&H: 131.37%, Scale=0.164)
‚úÖ DPRO: Training data validated - 205 rows over 365 days
    üìä EYE 3- Training: Horizon=60d, Target=21.26% (B&H: 129.33%, Scale=0.164)
‚úÖ EYE: Training data validated - 205 rows over 365 days
    üìä ESLT 3- Training: Horizon=60d, Target=21.03% (B&H: 127.94%, Scale=0.164)
‚úÖ ESLT: Training data validated - 205 rows over 365 days
    üìä RDDT 3- Training: Horizon=60d, Target=20.88% (B&H: 127.03%, Scale=0.164)
‚úÖ RDDT: Training data validated - 205 rows over 365 days
    üìä OPRT 3- Training: Horizon=60d, Target=20.77% (B&H: 126.33%, Scale=0.164)
‚úÖ OPRT: Training data validated - 205 rows over 365 days
    üìä JFIN 3- Training: Horizon=60d, Target=20.71% (B&H: 125.98%, Scale=0.164)
‚úÖ JFIN: Training data validated - 205 rows over 365 days
    üìä CONL 3- Training: Horizon=60d, Target=20.70% (B&H: 125.93%, Scale=0.164)
‚úÖ CONL: Training data validated - 205 rows over 365 days
    üìä HTZ 3- Training: Horizon=60d, Target=20.62% (B&H: 125.45%, Scale=0.164)
‚úÖ HTZ: Training data validated - 205 rows over 365 days
    üìä SBSW 3- Training: Horizon=60d, Target=20.52% (B&H: 124.82%, Scale=0.164)
‚úÖ SBSW: Training data validated - 205 rows over 365 days
    üìä GRNQ 3- Training: Horizon=60d, Target=20.34% (B&H: 123.71%, Scale=0.164)
‚úÖ GRNQ: Training data validated - 205 rows over 365 days
    üìä PAYS 3- Training: Horizon=60d, Target=20.07% (B&H: 122.07%, Scale=0.164)
‚úÖ PAYS: Training data validated - 205 rows over 365 days
    üìä XYF 3- Training: Horizon=60d, Target=19.88% (B&H: 120.92%, Scale=0.164)
‚úÖ XYF: Training data validated - 205 rows over 365 days
    üìä TRVG 3- Training: Horizon=60d, Target=19.76% (B&H: 120.23%, Scale=0.164)
‚úÖ TRVG: Training data validated - 205 rows over 365 days
    üìä AMBR 3- Training: Horizon=60d, Target=19.73% (B&H: 120.00%, Scale=0.164)
‚úÖ AMBR: Training data validated - 205 rows over 365 days
    üìä COIN 3- Training: Horizon=60d, Target=19.70% (B&H: 119.82%, Scale=0.164)
‚úÖ COIN: Training data validated - 205 rows over 365 days
    üìä GOGO 3- Training: Horizon=60d, Target=19.69% (B&H: 119.78%, Scale=0.164)
‚úÖ GOGO: Training data validated - 205 rows over 365 days
    üìä SSRM 3- Training: Horizon=60d, Target=19.68% (B&H: 119.72%, Scale=0.164)
‚úÖ SSRM: Training data validated - 205 rows over 365 days
    üìä QSI 3- Training: Horizon=60d, Target=19.53% (B&H: 118.82%, Scale=0.164)
‚úÖ QSI: Training data validated - 205 rows over 365 days
    üìä OSS 3- Training: Horizon=60d, Target=19.46% (B&H: 118.41%, Scale=0.164)
‚úÖ OSS: Training data validated - 205 rows over 365 days
    üìä AAPG 3- Training: Horizon=60d, Target=19.31% (B&H: 117.49%, Scale=0.164)
  ‚ùå AAPG: Too many NaN values in Close price: 79 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä GHRS 3- Training: Horizon=60d, Target=19.19% (B&H: 116.74%, Scale=0.164)
‚úÖ GHRS: Training data validated - 205 rows over 365 days
    üìä LUXE 3- Training: Horizon=60d, Target=19.07% (B&H: 116.01%, Scale=0.164)
‚úÖ LUXE: Training data validated - 205 rows over 365 days
    üìä EVEX 3- Training: Horizon=60d, Target=18.87% (B&H: 114.81%, Scale=0.164)
‚úÖ EVEX: Training data validated - 205 rows over 365 days
    üìä EOSE 3- Training: Horizon=60d, Target=18.82% (B&H: 114.48%, Scale=0.164)
‚úÖ EOSE: Training data validated - 205 rows over 365 days
    üìä MASS 3- Training: Horizon=60d, Target=18.71% (B&H: 113.83%, Scale=0.164)
‚úÖ MASS: Training data validated - 205 rows over 365 days
    üìä AVXL 3- Training: Horizon=60d, Target=18.67% (B&H: 113.56%, Scale=0.164)
‚úÖ AVXL: Training data validated - 205 rows over 365 days
    üìä TESL 3- Training: Horizon=60d, Target=18.54% (B&H: 112.80%, Scale=0.164)
‚úÖ TESL: Training data validated - 205 rows over 365 days
    üìä DOMO 3- Training: Horizon=60d, Target=18.50% (B&H: 112.52%, Scale=0.164)
‚úÖ DOMO: Training data validated - 205 rows over 365 days
    üìä DAO 3- Training: Horizon=60d, Target=18.10% (B&H: 110.12%, Scale=0.164)
‚úÖ DAO: Training data validated - 205 rows over 365 days
    üìä FOA 3- Training: Horizon=60d, Target=18.09% (B&H: 110.03%, Scale=0.164)
‚úÖ FOA: Training data validated - 205 rows over 365 days
    üìä ALNT 3- Training: Horizon=60d, Target=18.08% (B&H: 109.98%, Scale=0.164)
‚úÖ ALNT: Training data validated - 205 rows over 365 days
    üìä IREN 3- Training: Horizon=60d, Target=18.07% (B&H: 109.95%, Scale=0.164)
‚úÖ IREN: Training data validated - 205 rows over 365 days
    üìä UPST 3- Training: Horizon=60d, Target=17.88% (B&H: 108.77%, Scale=0.164)
‚úÖ UPST: Training data validated - 205 rows over 365 days
    üìä ASTS 3- Training: Horizon=60d, Target=17.72% (B&H: 107.78%, Scale=0.164)
‚úÖ ASTS: Training data validated - 205 rows over 365 days
    üìä AENT 3- Training: Horizon=60d, Target=17.56% (B&H: 106.80%, Scale=0.164)
‚úÖ AENT: Training data validated - 205 rows over 365 days
    üìä TARK 3- Training: Horizon=60d, Target=17.44% (B&H: 106.09%, Scale=0.164)
‚úÖ TARK: Training data validated - 205 rows over 365 days
    üìä IHS 3- Training: Horizon=60d, Target=17.43% (B&H: 106.02%, Scale=0.164)
‚úÖ IHS: Training data validated - 205 rows over 365 days
    üìä APEI 3- Training: Horizon=60d, Target=17.36% (B&H: 105.63%, Scale=0.164)
‚úÖ APEI: Training data validated - 205 rows over 365 days
    üìä NN 3- Training: Horizon=60d, Target=17.36% (B&H: 105.61%, Scale=0.164)
‚úÖ NN: Training data validated - 205 rows over 365 days
    üìä DOYU 3- Training: Horizon=60d, Target=17.28% (B&H: 105.10%, Scale=0.164)
‚úÖ DOYU: Training data validated - 205 rows over 365 days
    üìä APYX 3- Training: Horizon=60d, Target=17.23% (B&H: 104.84%, Scale=0.164)
‚úÖ APYX: Training data validated - 205 rows over 365 days
    üìä LFMD 3- Training: Horizon=60d, Target=17.10% (B&H: 104.01%, Scale=0.164)
‚úÖ LFMD: Training data validated - 205 rows over 365 days
    üìä CTMX 3- Training: Horizon=60d, Target=17.00% (B&H: 103.39%, Scale=0.164)
‚úÖ CTMX: Training data validated - 205 rows over 365 days
    üìä GEO 3- Training: Horizon=60d, Target=16.95% (B&H: 103.11%, Scale=0.164)
‚úÖ GEO: Training data validated - 205 rows over 365 days
    üìä DXPE 3- Training: Horizon=60d, Target=16.89% (B&H: 102.72%, Scale=0.164)
‚úÖ DXPE: Training data validated - 205 rows over 365 days
    üìä KEN 3- Training: Horizon=60d, Target=16.84% (B&H: 102.45%, Scale=0.164)
‚úÖ KEN: Training data validated - 205 rows over 365 days
    üìä VNET 3- Training: Horizon=60d, Target=16.84% (B&H: 102.45%, Scale=0.164)
‚úÖ VNET: Training data validated - 205 rows over 365 days
    üìä UI 3- Training: Horizon=60d, Target=16.83% (B&H: 102.36%, Scale=0.164)
‚úÖ UI: Training data validated - 205 rows over 365 days
    üìä TWLO 3- Training: Horizon=60d, Target=16.59% (B&H: 100.95%, Scale=0.164)
‚úÖ TWLO: Training data validated - 205 rows over 365 days
    üìä VVOS 3- Training: Horizon=60d, Target=16.56% (B&H: 100.77%, Scale=0.164)
‚úÖ VVOS: Training data validated - 205 rows over 365 days
    üìä ATOM 3- Training: Horizon=60d, Target=16.56% (B&H: 100.76%, Scale=0.164)
‚úÖ ATOM: Training data validated - 205 rows over 365 days
    üìä MGTX 3- Training: Horizon=60d, Target=16.56% (B&H: 100.72%, Scale=0.164)
‚úÖ MGTX: Training data validated - 205 rows over 365 days
    üìä RCL 3- Training: Horizon=60d, Target=16.54% (B&H: 100.64%, Scale=0.164)
‚úÖ RCL: Training data validated - 205 rows over 365 days
    üìä EAT 3- Training: Horizon=60d, Target=16.53% (B&H: 100.57%, Scale=0.164)
‚úÖ EAT: Training data validated - 205 rows over 365 days
    üìä WLDN 3- Training: Horizon=60d, Target=16.49% (B&H: 100.34%, Scale=0.164)
‚úÖ WLDN: Training data validated - 205 rows over 365 days
    üìä MESO 3- Training: Horizon=60d, Target=16.48% (B&H: 100.24%, Scale=0.164)
‚úÖ MESO: Training data validated - 205 rows over 365 days
    üìä VS 3- Training: Horizon=60d, Target=16.30% (B&H: 99.16%, Scale=0.164)
‚úÖ VS: Training data validated - 205 rows over 365 days
    üìä DJTWW 3- Training: Horizon=60d, Target=16.30% (B&H: 99.13%, Scale=0.164)
‚úÖ DJTWW: Training data validated - 205 rows over 365 days
    üìä DB 3- Training: Horizon=60d, Target=16.24% (B&H: 98.77%, Scale=0.164)
‚úÖ DB: Training data validated - 205 rows over 365 days
    üìä MIR 3- Training: Horizon=60d, Target=16.14% (B&H: 98.19%, Scale=0.164)
‚úÖ MIR: Training data validated - 205 rows over 365 days
    üìä TSAT 3- Training: Horizon=60d, Target=16.14% (B&H: 98.18%, Scale=0.164)
‚úÖ TSAT: Training data validated - 205 rows over 365 days
    üìä TIGR 3- Training: Horizon=60d, Target=16.10% (B&H: 97.94%, Scale=0.164)
‚úÖ TIGR: Training data validated - 205 rows over 365 days
    üìä RBOT 3- Training: Horizon=60d, Target=16.03% (B&H: 97.54%, Scale=0.164)
‚úÖ RBOT: Training data validated - 205 rows over 365 days
    üìä METC 3- Training: Horizon=60d, Target=15.99% (B&H: 97.30%, Scale=0.164)
‚úÖ METC: Training data validated - 205 rows over 365 days
    üìä MSTY 3- Training: Horizon=60d, Target=15.95% (B&H: 97.00%, Scale=0.164)
‚úÖ MSTY: Training data validated - 205 rows over 365 days
    üìä ABCL 3- Training: Horizon=60d, Target=15.87% (B&H: 96.54%, Scale=0.164)
‚úÖ ABCL: Training data validated - 205 rows over 365 days
    üìä TBRG 3- Training: Horizon=60d, Target=15.86% (B&H: 96.49%, Scale=0.164)
‚úÖ TBRG: Training data validated - 205 rows over 365 days
    üìä LQDA 3- Training: Horizon=60d, Target=15.78% (B&H: 96.00%, Scale=0.164)
‚úÖ LQDA: Training data validated - 205 rows over 365 days
    üìä RIOT 3- Training: Horizon=60d, Target=15.77% (B&H: 95.96%, Scale=0.164)
‚úÖ RIOT: Training data validated - 205 rows over 365 days
    üìä MYRG 3- Training: Horizon=60d, Target=15.77% (B&H: 95.93%, Scale=0.164)
‚úÖ MYRG: Training data validated - 205 rows over 365 days
    üìä LIF 3- Training: Horizon=60d, Target=15.65% (B&H: 95.20%, Scale=0.164)
‚úÖ LIF: Training data validated - 205 rows over 365 days
    üìä TLN 3- Training: Horizon=60d, Target=15.53% (B&H: 94.47%, Scale=0.164)
‚úÖ TLN: Training data validated - 205 rows over 365 days
    üìä VUZI 3- Training: Horizon=60d, Target=15.45% (B&H: 94.02%, Scale=0.164)
‚úÖ VUZI: Training data validated - 205 rows over 365 days
    üìä UTI 3- Training: Horizon=60d, Target=15.45% (B&H: 93.97%, Scale=0.164)
‚úÖ UTI: Training data validated - 205 rows over 365 days
    üìä AU 3- Training: Horizon=60d, Target=15.29% (B&H: 93.00%, Scale=0.164)
‚úÖ AU: Training data validated - 205 rows over 365 days
    üìä LINC 3- Training: Horizon=60d, Target=15.03% (B&H: 91.46%, Scale=0.164)
‚úÖ LINC: Training data validated - 205 rows over 365 days
    üìä CVNA 3- Training: Horizon=60d, Target=14.92% (B&H: 90.75%, Scale=0.164)
‚úÖ CVNA: Training data validated - 205 rows over 365 days
    üìä AAOI 3- Training: Horizon=60d, Target=14.73% (B&H: 89.59%, Scale=0.164)
‚úÖ AAOI: Training data validated - 205 rows over 365 days
    üìä CCB 3- Training: Horizon=60d, Target=14.72% (B&H: 89.52%, Scale=0.164)
‚úÖ CCB: Training data validated - 205 rows over 365 days
    üìä MGIC 3- Training: Horizon=60d, Target=14.71% (B&H: 89.48%, Scale=0.164)
‚úÖ MGIC: Training data validated - 205 rows over 365 days
    üìä HWM 3- Training: Horizon=60d, Target=14.70% (B&H: 89.45%, Scale=0.164)
‚úÖ HWM: Training data validated - 205 rows over 365 days
    üìä CYD 3- Training: Horizon=60d, Target=14.64% (B&H: 89.08%, Scale=0.164)
‚úÖ CYD: Training data validated - 205 rows over 365 days
    üìä IBKR 3- Training: Horizon=60d, Target=14.59% (B&H: 88.77%, Scale=0.164)
‚úÖ IBKR: Training data validated - 205 rows over 365 days
    üìä WGS 3- Training: Horizon=60d, Target=14.48% (B&H: 88.10%, Scale=0.164)
‚úÖ WGS: Training data validated - 205 rows over 365 days
    üìä MNY 3- Training: Horizon=60d, Target=14.48% (B&H: 88.07%, Scale=0.164)
‚úÖ MNY: Training data validated - 205 rows over 365 days
    üìä SPOT 3- Training: Horizon=60d, Target=14.48% (B&H: 88.07%, Scale=0.164)
‚úÖ SPOT: Training data validated - 205 rows over 365 days
    üìä CRK 3- Training: Horizon=60d, Target=14.19% (B&H: 86.34%, Scale=0.164)
‚úÖ CRK: Training data validated - 205 rows over 365 days
    üìä JBL 3- Training: Horizon=60d, Target=14.14% (B&H: 86.01%, Scale=0.164)
‚úÖ JBL: Training data validated - 205 rows over 365 days
    üìä BROS 3- Training: Horizon=60d, Target=14.10% (B&H: 85.76%, Scale=0.164)
‚úÖ BROS: Training data validated - 205 rows over 365 days
    üìä CVAC 3- Training: Horizon=60d, Target=14.09% (B&H: 85.71%, Scale=0.164)
‚úÖ CVAC: Training data validated - 205 rows over 365 days
    üìä GHM 3- Training: Horizon=60d, Target=14.07% (B&H: 85.60%, Scale=0.164)
‚úÖ GHM: Training data validated - 205 rows over 365 days
    üìä ARKW 3- Training: Horizon=60d, Target=13.99% (B&H: 85.11%, Scale=0.164)
‚úÖ ARKW: Training data validated - 205 rows over 365 days
    üìä STRL 3- Training: Horizon=60d, Target=13.96% (B&H: 84.90%, Scale=0.164)
‚úÖ STRL: Training data validated - 205 rows over 365 days
    üìä BTCW 3- Training: Horizon=60d, Target=13.83% (B&H: 84.14%, Scale=0.164)
‚úÖ BTCW: Training data validated - 205 rows over 365 days
    üìä HODL 3- Training: Horizon=60d, Target=13.82% (B&H: 84.10%, Scale=0.164)
‚úÖ HODL: Training data validated - 205 rows over 365 days
    üìä AXON 3- Training: Horizon=60d, Target=13.80% (B&H: 83.94%, Scale=0.164)
‚úÖ AXON: Training data validated - 205 rows over 365 days
    üìä EZBC 3- Training: Horizon=60d, Target=13.80% (B&H: 83.92%, Scale=0.164)
‚úÖ EZBC: Training data validated - 205 rows over 365 days
    üìä BITB 3- Training: Horizon=60d, Target=13.77% (B&H: 83.78%, Scale=0.164)
‚úÖ BITB: Training data validated - 205 rows over 365 days
    üìä BRRR 3- Training: Horizon=60d, Target=13.76% (B&H: 83.70%, Scale=0.164)
‚úÖ BRRR: Training data validated - 205 rows over 365 days
    üìä IBIT 3- Training: Horizon=60d, Target=13.75% (B&H: 83.67%, Scale=0.164)
‚úÖ IBIT: Training data validated - 205 rows over 365 days
    üìä FBTC 3- Training: Horizon=60d, Target=13.75% (B&H: 83.63%, Scale=0.164)
‚úÖ FBTC: Training data validated - 205 rows over 365 days
    üìä ARKB 3- Training: Horizon=60d, Target=13.73% (B&H: 83.54%, Scale=0.164)
‚úÖ ARKB: Training data validated - 205 rows over 365 days
    üìä BTCO 3- Training: Horizon=60d, Target=13.73% (B&H: 83.52%, Scale=0.164)
‚úÖ BTCO: Training data validated - 205 rows over 365 days
    üìä REAL 3- Training: Horizon=60d, Target=13.72% (B&H: 83.44%, Scale=0.164)
‚úÖ REAL: Training data validated - 205 rows over 365 days
    üìä SAN 3- Training: Horizon=60d, Target=13.66% (B&H: 83.08%, Scale=0.164)
‚úÖ SAN: Training data validated - 205 rows over 365 days
    üìä AMSC 3- Training: Horizon=60d, Target=13.65% (B&H: 83.05%, Scale=0.164)
‚úÖ AMSC: Training data validated - 205 rows over 365 days
    üìä SKE 3- Training: Horizon=60d, Target=13.60% (B&H: 82.74%, Scale=0.164)
‚úÖ SKE: Training data validated - 205 rows over 365 days
    üìä DEFI 3- Training: Horizon=60d, Target=13.57% (B&H: 82.55%, Scale=0.164)
‚úÖ DEFI: Training data validated - 205 rows over 365 days
    üìä OM 3- Training: Horizon=60d, Target=13.56% (B&H: 82.47%, Scale=0.164)
‚úÖ OM: Training data validated - 205 rows over 365 days
    üìä UUUU 3- Training: Horizon=60d, Target=13.53% (B&H: 82.33%, Scale=0.164)
‚úÖ UUUU: Training data validated - 205 rows over 365 days
    üìä FRHC 3- Training: Horizon=60d, Target=13.53% (B&H: 82.28%, Scale=0.164)
‚úÖ FRHC: Training data validated - 205 rows over 365 days
    üìä TPB 3- Training: Horizon=60d, Target=13.49% (B&H: 82.07%, Scale=0.164)
‚úÖ TPB: Training data validated - 205 rows over 365 days
    üìä GBTC 3- Training: Horizon=60d, Target=13.47% (B&H: 81.92%, Scale=0.164)
‚úÖ GBTC: Training data validated - 205 rows over 365 days
    üìä BBIO 3- Training: Horizon=60d, Target=13.46% (B&H: 81.85%, Scale=0.164)
‚úÖ BBIO: Training data validated - 205 rows over 365 days
    üìä ARKF 3- Training: Horizon=60d, Target=13.38% (B&H: 81.39%, Scale=0.164)
‚úÖ ARKF: Training data validated - 205 rows over 365 days
    üìä LASR 3- Training: Horizon=60d, Target=13.38% (B&H: 81.38%, Scale=0.164)
‚úÖ LASR: Training data validated - 205 rows over 365 days
    üìä KEP 3- Training: Horizon=60d, Target=13.33% (B&H: 81.12%, Scale=0.164)
‚úÖ KEP: Training data validated - 205 rows over 365 days
    üìä IRTC 3- Training: Horizon=60d, Target=13.27% (B&H: 80.74%, Scale=0.164)
‚úÖ IRTC: Training data validated - 205 rows over 365 days
    üìä HIMX 3- Training: Horizon=60d, Target=13.24% (B&H: 80.53%, Scale=0.164)
‚úÖ HIMX: Training data validated - 205 rows over 365 days
    üìä GDS 3- Training: Horizon=60d, Target=13.20% (B&H: 80.29%, Scale=0.164)
‚úÖ GDS: Training data validated - 205 rows over 365 days
    üìä ETHZ 3- Training: Horizon=60d, Target=13.09% (B&H: 79.64%, Scale=0.164)
‚úÖ ETHZ: Training data validated - 205 rows over 365 days
    üìä IESC 3- Training: Horizon=60d, Target=13.01% (B&H: 79.17%, Scale=0.164)
‚úÖ IESC: Training data validated - 205 rows over 365 days
    üìä TME 3- Training: Horizon=60d, Target=12.98% (B&H: 78.99%, Scale=0.164)
‚úÖ TME: Training data validated - 205 rows over 365 days
    üìä PARR 3- Training: Horizon=60d, Target=12.92% (B&H: 78.58%, Scale=0.164)
‚úÖ PARR: Training data validated - 205 rows over 365 days
    üìä APPS 3- Training: Horizon=60d, Target=12.80% (B&H: 77.85%, Scale=0.164)
‚úÖ APPS: Training data validated - 205 rows over 365 days
    üìä VEON 3- Training: Horizon=60d, Target=12.79% (B&H: 77.78%, Scale=0.164)
‚úÖ VEON: Training data validated - 205 rows over 365 days
    üìä XMTR 3- Training: Horizon=60d, Target=12.75% (B&H: 77.57%, Scale=0.164)
‚úÖ XMTR: Training data validated - 205 rows over 365 days
    üìä ATRO 3- Training: Horizon=60d, Target=12.67% (B&H: 77.05%, Scale=0.164)
‚úÖ ATRO: Training data validated - 205 rows over 365 days
    üìä FIX 3- Training: Horizon=60d, Target=12.65% (B&H: 76.97%, Scale=0.164)
‚úÖ FIX: Training data validated - 205 rows over 365 days
    üìä LMB 3- Training: Horizon=60d, Target=12.60% (B&H: 76.66%, Scale=0.164)
‚úÖ LMB: Training data validated - 205 rows over 365 days
    üìä CIB 3- Training: Horizon=60d, Target=12.56% (B&H: 76.43%, Scale=0.164)
‚úÖ CIB: Training data validated - 205 rows over 365 days
    üìä TPC 3- Training: Horizon=60d, Target=12.56% (B&H: 76.40%, Scale=0.164)
‚úÖ TPC: Training data validated - 205 rows over 365 days
    üìä ORGO 3- Training: Horizon=60d, Target=12.47% (B&H: 75.87%, Scale=0.164)
‚úÖ ORGO: Training data validated - 205 rows over 365 days
    üìä INDV 3- Training: Horizon=60d, Target=12.46% (B&H: 75.79%, Scale=0.164)
‚úÖ INDV: Training data validated - 205 rows over 365 days
    üìä EXEL 3- Training: Horizon=60d, Target=12.45% (B&H: 75.72%, Scale=0.164)
‚úÖ EXEL: Training data validated - 205 rows over 365 days
    üìä PBI 3- Training: Horizon=60d, Target=12.45% (B&H: 75.71%, Scale=0.164)
‚úÖ PBI: Training data validated - 205 rows over 365 days
    üìä ICCM 3- Training: Horizon=60d, Target=12.40% (B&H: 75.41%, Scale=0.164)
‚úÖ ICCM: Training data validated - 205 rows over 365 days
    üìä DASH 3- Training: Horizon=60d, Target=12.35% (B&H: 75.10%, Scale=0.164)
‚úÖ DASH: Training data validated - 205 rows over 365 days
    üìä FUTU 3- Training: Horizon=60d, Target=12.34% (B&H: 75.08%, Scale=0.164)
‚úÖ FUTU: Training data validated - 205 rows over 365 days
    üìä TATT 3- Training: Horizon=60d, Target=12.33% (B&H: 75.03%, Scale=0.164)
‚úÖ TATT: Training data validated - 205 rows over 365 days
    üìä CPS 3- Training: Horizon=60d, Target=12.31% (B&H: 74.91%, Scale=0.164)
‚úÖ CPS: Training data validated - 205 rows over 365 days
    üìä TENX 3- Training: Horizon=60d, Target=12.31% (B&H: 74.86%, Scale=0.164)
‚úÖ TENX: Training data validated - 205 rows over 365 days
    üìä BITO 3- Training: Horizon=60d, Target=12.29% (B&H: 74.74%, Scale=0.164)
‚úÖ BITO: Training data validated - 205 rows over 365 days
    üìä NRG 3- Training: Horizon=60d, Target=12.18% (B&H: 74.11%, Scale=0.164)
‚úÖ NRG: Training data validated - 205 rows over 365 days
    üìä CRS 3- Training: Horizon=60d, Target=12.14% (B&H: 73.83%, Scale=0.164)
‚úÖ CRS: Training data validated - 205 rows over 365 days
    üìä ODC 3- Training: Horizon=60d, Target=12.10% (B&H: 73.62%, Scale=0.164)
‚úÖ ODC: Training data validated - 205 rows over 365 days
    üìä PPTA 3- Training: Horizon=60d, Target=12.06% (B&H: 73.37%, Scale=0.164)
‚úÖ PPTA: Training data validated - 205 rows over 365 days
    üìä KGC 3- Training: Horizon=60d, Target=12.04% (B&H: 73.24%, Scale=0.164)
‚úÖ KGC: Training data validated - 205 rows over 365 days
    üìä SNEX 3- Training: Horizon=60d, Target=12.03% (B&H: 73.16%, Scale=0.164)
‚úÖ SNEX: Training data validated - 205 rows over 365 days
    üìä CMPX 3- Training: Horizon=60d, Target=11.97% (B&H: 72.83%, Scale=0.164)
‚úÖ CMPX: Training data validated - 205 rows over 365 days
    üìä CTOS 3- Training: Horizon=60d, Target=11.96% (B&H: 72.75%, Scale=0.164)
‚úÖ CTOS: Training data validated - 205 rows over 365 days
    üìä DRS 3- Training: Horizon=60d, Target=11.92% (B&H: 72.53%, Scale=0.164)
‚úÖ DRS: Training data validated - 205 rows over 365 days
    üìä NXT 3- Training: Horizon=60d, Target=11.86% (B&H: 72.17%, Scale=0.164)
‚úÖ NXT: Training data validated - 205 rows over 365 days
    üìä TOST 3- Training: Horizon=60d, Target=11.84% (B&H: 72.02%, Scale=0.164)
‚úÖ TOST: Training data validated - 205 rows over 365 days
    üìä REVG 3- Training: Horizon=60d, Target=11.79% (B&H: 71.71%, Scale=0.164)
‚úÖ REVG: Training data validated - 205 rows over 365 days
    üìä HNRG 3- Training: Horizon=60d, Target=11.78% (B&H: 71.69%, Scale=0.164)
‚úÖ HNRG: Training data validated - 205 rows over 365 days
    üìä BLOK 3- Training: Horizon=60d, Target=11.77% (B&H: 71.62%, Scale=0.164)
‚úÖ BLOK: Training data validated - 205 rows over 365 days
    üìä MNMD 3- Training: Horizon=60d, Target=11.76% (B&H: 71.53%, Scale=0.164)
‚úÖ MNMD: Training data validated - 205 rows over 365 days
    üìä DNA 3- Training: Horizon=60d, Target=11.72% (B&H: 71.29%, Scale=0.164)
‚úÖ DNA: Training data validated - 205 rows over 365 days
    üìä SLNO 3- Training: Horizon=60d, Target=11.71% (B&H: 71.26%, Scale=0.164)
‚úÖ SLNO: Training data validated - 205 rows over 365 days
    üìä YALA 3- Training: Horizon=60d, Target=11.62% (B&H: 70.70%, Scale=0.164)
‚úÖ YALA: Training data validated - 205 rows over 365 days
    üìä JNUG 3- Training: Horizon=60d, Target=11.55% (B&H: 70.24%, Scale=0.164)
‚úÖ JNUG: Training data validated - 205 rows over 365 days
    üìä KD 3- Training: Horizon=60d, Target=11.55% (B&H: 70.23%, Scale=0.164)
‚úÖ KD: Training data validated - 205 rows over 365 days
    üìä NBP 3- Training: Horizon=60d, Target=11.53% (B&H: 70.16%, Scale=0.164)
‚úÖ NBP: Training data validated - 205 rows over 365 days
    üìä MGNI 3- Training: Horizon=60d, Target=11.48% (B&H: 69.82%, Scale=0.164)
‚úÖ MGNI: Training data validated - 205 rows over 365 days
    üìä IVA 3- Training: Horizon=60d, Target=11.48% (B&H: 69.81%, Scale=0.164)
‚úÖ IVA: Training data validated - 205 rows over 365 days
    üìä AVGO 3- Training: Horizon=60d, Target=11.44% (B&H: 69.58%, Scale=0.164)
‚úÖ AVGO: Training data validated - 205 rows over 365 days
    üìä CMP 3- Training: Horizon=60d, Target=11.42% (B&H: 69.47%, Scale=0.164)
‚úÖ CMP: Training data validated - 205 rows over 365 days
    üìä AEHR 3- Training: Horizon=60d, Target=11.37% (B&H: 69.18%, Scale=0.164)
‚úÖ AEHR: Training data validated - 205 rows over 365 days
    üìä EVLV 3- Training: Horizon=60d, Target=11.24% (B&H: 68.40%, Scale=0.164)
‚úÖ EVLV: Training data validated - 205 rows over 365 days
    üìä HRTG 3- Training: Horizon=60d, Target=11.20% (B&H: 68.14%, Scale=0.164)
‚úÖ HRTG: Training data validated - 205 rows over 365 days
    üìä ODD 3- Training: Horizon=60d, Target=11.20% (B&H: 68.13%, Scale=0.164)
‚úÖ ODD: Training data validated - 205 rows over 365 days
    üìä OMAB 3- Training: Horizon=60d, Target=11.17% (B&H: 67.98%, Scale=0.164)
‚úÖ OMAB: Training data validated - 205 rows over 365 days
    üìä UNFI 3- Training: Horizon=60d, Target=11.13% (B&H: 67.72%, Scale=0.164)
‚úÖ UNFI: Training data validated - 205 rows over 365 days
    üìä LLYVA 3- Training: Horizon=60d, Target=11.10% (B&H: 67.50%, Scale=0.164)
‚úÖ LLYVA: Training data validated - 205 rows over 365 days
    üìä ZS 3- Training: Horizon=60d, Target=11.08% (B&H: 67.42%, Scale=0.164)
‚úÖ ZS: Training data validated - 205 rows over 365 days
    üìä NIU 3- Training: Horizon=60d, Target=11.08% (B&H: 67.39%, Scale=0.164)
‚úÖ NIU: Training data validated - 205 rows over 365 days
    üìä SE 3- Training: Horizon=60d, Target=11.06% (B&H: 67.28%, Scale=0.164)
‚úÖ SE: Training data validated - 205 rows over 365 days
    üìä VIK 3- Training: Horizon=60d, Target=11.05% (B&H: 67.21%, Scale=0.164)
‚úÖ VIK: Training data validated - 205 rows over 365 days
    üìä CIFR 3- Training: Horizon=60d, Target=11.04% (B&H: 67.18%, Scale=0.164)
‚úÖ CIFR: Training data validated - 205 rows over 365 days
    üìä SHLD 3- Training: Horizon=60d, Target=11.02% (B&H: 67.07%, Scale=0.164)
‚úÖ SHLD: Training data validated - 205 rows over 365 days
    üìä CCJ 3- Training: Horizon=60d, Target=11.01% (B&H: 66.99%, Scale=0.164)
‚úÖ CCJ: Training data validated - 205 rows over 365 days
    üìä CRWD 3- Training: Horizon=60d, Target=10.99% (B&H: 66.83%, Scale=0.164)
‚úÖ CRWD: Training data validated - 205 rows over 365 days
    üìä SII 3- Training: Horizon=60d, Target=10.97% (B&H: 66.74%, Scale=0.164)
‚úÖ SII: Training data validated - 205 rows over 365 days
    üìä NFLX 3- Training: Horizon=60d, Target=10.92% (B&H: 66.44%, Scale=0.164)
‚úÖ NFLX: Training data validated - 205 rows over 365 days
    üìä HUT 3- Training: Horizon=60d, Target=10.89% (B&H: 66.23%, Scale=0.164)
‚úÖ HUT: Training data validated - 205 rows over 365 days
    üìä LLYVK 3- Training: Horizon=60d, Target=10.86% (B&H: 66.08%, Scale=0.164)
‚úÖ LLYVK: Training data validated - 205 rows over 365 days
    üìä API 3- Training: Horizon=60d, Target=10.85% (B&H: 65.98%, Scale=0.164)
‚úÖ API: Training data validated - 205 rows over 365 days
    üìä VSEC 3- Training: Horizon=60d, Target=10.81% (B&H: 65.79%, Scale=0.164)
‚úÖ VSEC: Training data validated - 205 rows over 365 days
    üìä WEBL 3- Training: Horizon=60d, Target=10.81% (B&H: 65.78%, Scale=0.164)
‚úÖ WEBL: Training data validated - 205 rows over 365 days
    üìä DOUG 3- Training: Horizon=60d, Target=10.78% (B&H: 65.57%, Scale=0.164)
‚úÖ DOUG: Training data validated - 205 rows over 365 days
    üìä RYTM 3- Training: Horizon=60d, Target=10.69% (B&H: 65.01%, Scale=0.164)
‚úÖ RYTM: Training data validated - 205 rows over 365 days
    üìä LITE 3- Training: Horizon=60d, Target=10.67% (B&H: 64.91%, Scale=0.164)
‚úÖ LITE: Training data validated - 205 rows over 365 days
    üìä CAKE 3- Training: Horizon=60d, Target=10.66% (B&H: 64.85%, Scale=0.164)
‚úÖ CAKE: Training data validated - 205 rows over 365 days
    üìä ARKX 3- Training: Horizon=60d, Target=10.62% (B&H: 64.59%, Scale=0.164)
‚úÖ ARKX: Training data validated - 205 rows over 365 days
    üìä LTM 3- Training: Horizon=60d, Target=10.61% (B&H: 64.53%, Scale=0.164)
‚úÖ LTM: Training data validated - 205 rows over 365 days
    üìä AFRM 3- Training: Horizon=60d, Target=10.58% (B&H: 64.36%, Scale=0.164)
‚úÖ AFRM: Training data validated - 205 rows over 365 days
    üìä BCS 3- Training: Horizon=60d, Target=10.58% (B&H: 64.35%, Scale=0.164)
‚úÖ BCS: Training data validated - 205 rows over 365 days
    üìä MPAA 3- Training: Horizon=60d, Target=10.57% (B&H: 64.28%, Scale=0.164)
‚úÖ MPAA: Training data validated - 205 rows over 365 days
    üìä SLI 3- Training: Horizon=60d, Target=10.52% (B&H: 63.98%, Scale=0.164)
‚úÖ SLI: Training data validated - 205 rows over 365 days
    üìä CXW 3- Training: Horizon=60d, Target=10.50% (B&H: 63.87%, Scale=0.164)
‚úÖ CXW: Training data validated - 205 rows over 365 days
    üìä ICL 3- Training: Horizon=60d, Target=10.50% (B&H: 63.87%, Scale=0.164)
‚úÖ ICL: Training data validated - 205 rows over 365 days
    üìä LPLA 3- Training: Horizon=60d, Target=10.49% (B&H: 63.81%, Scale=0.164)
‚úÖ LPLA: Training data validated - 205 rows over 365 days
    üìä GDXU 3- Training: Horizon=60d, Target=10.48% (B&H: 63.77%, Scale=0.164)
‚úÖ GDXU: Training data validated - 205 rows over 365 days
    üìä GFI 3- Training: Horizon=60d, Target=10.48% (B&H: 63.74%, Scale=0.164)
‚úÖ GFI: Training data validated - 205 rows over 365 days
    üìä WBD 3- Training: Horizon=60d, Target=10.44% (B&H: 63.52%, Scale=0.164)
‚úÖ WBD: Training data validated - 205 rows over 365 days
    üìä CTRN 3- Training: Horizon=60d, Target=10.40% (B&H: 63.26%, Scale=0.164)
‚úÖ CTRN: Training data validated - 205 rows over 365 days
    üìä SENEA 3- Training: Horizon=60d, Target=10.40% (B&H: 63.24%, Scale=0.164)
‚úÖ SENEA: Training data validated - 205 rows over 365 days
    üìä CUK 3- Training: Horizon=60d, Target=10.35% (B&H: 62.99%, Scale=0.164)
‚úÖ CUK: Training data validated - 205 rows over 365 days
    üìä MSB 3- Training: Horizon=60d, Target=10.34% (B&H: 62.91%, Scale=0.164)
‚úÖ MSB: Training data validated - 205 rows over 365 days
    üìä VST 3- Training: Horizon=60d, Target=10.34% (B&H: 62.89%, Scale=0.164)
‚úÖ VST: Training data validated - 205 rows over 365 days
    üìä DLTR 3- Training: Horizon=60d, Target=10.31% (B&H: 62.70%, Scale=0.164)
‚úÖ DLTR: Training data validated - 205 rows over 365 days
    üìä TIGO 3- Training: Horizon=60d, Target=10.29% (B&H: 62.60%, Scale=0.164)
‚úÖ TIGO: Training data validated - 205 rows over 365 days
    üìä YBTC 3- Training: Horizon=60d, Target=10.27% (B&H: 62.45%, Scale=0.164)
‚úÖ YBTC: Training data validated - 205 rows over 365 days
    üìä APH 3- Training: Horizon=60d, Target=10.23% (B&H: 62.23%, Scale=0.164)
‚úÖ APH: Training data validated - 205 rows over 365 days
    üìä PCT 3- Training: Horizon=60d, Target=10.21% (B&H: 62.11%, Scale=0.164)
‚úÖ PCT: Training data validated - 205 rows over 365 days
    üìä UFO 3- Training: Horizon=60d, Target=10.19% (B&H: 61.98%, Scale=0.164)
‚úÖ UFO: Training data validated - 205 rows over 365 days
    üìä ZVRA 3- Training: Horizon=60d, Target=10.16% (B&H: 61.82%, Scale=0.164)
‚úÖ ZVRA: Training data validated - 205 rows over 365 days
    üìä MRX 3- Training: Horizon=60d, Target=10.16% (B&H: 61.78%, Scale=0.164)
‚úÖ MRX: Training data validated - 205 rows over 365 days
    üìä SMLR 3- Training: Horizon=60d, Target=10.14% (B&H: 61.70%, Scale=0.164)
‚úÖ SMLR: Training data validated - 205 rows over 365 days
    üìä ARKQ 3- Training: Horizon=60d, Target=10.13% (B&H: 61.64%, Scale=0.164)
‚úÖ ARKQ: Training data validated - 205 rows over 365 days
    üìä ARKK 3- Training: Horizon=60d, Target=10.12% (B&H: 61.56%, Scale=0.164)
‚úÖ ARKK: Training data validated - 205 rows over 365 days
    üìä VRSN 3- Training: Horizon=60d, Target=10.10% (B&H: 61.41%, Scale=0.164)
‚úÖ VRSN: Training data validated - 205 rows over 365 days
    üìä WF 3- Training: Horizon=60d, Target=10.07% (B&H: 61.28%, Scale=0.164)
‚úÖ WF: Training data validated - 205 rows over 365 days
    üìä DAPP 3- Training: Horizon=60d, Target=10.07% (B&H: 61.27%, Scale=0.164)
‚úÖ DAPP: Training data validated - 205 rows over 365 days
    üìä PEGA 3- Training: Horizon=60d, Target=10.07% (B&H: 61.26%, Scale=0.164)
‚úÖ PEGA: Training data validated - 205 rows over 365 days
    üìä ORN 3- Training: Horizon=60d, Target=10.06% (B&H: 61.18%, Scale=0.164)
‚úÖ ORN: Training data validated - 205 rows over 365 days
    üìä BITQ 3- Training: Horizon=60d, Target=10.04% (B&H: 61.07%, Scale=0.164)
‚úÖ BITQ: Training data validated - 205 rows over 365 days
    üìä CCL 3- Training: Horizon=60d, Target=10.02% (B&H: 60.98%, Scale=0.164)
‚úÖ CCL: Training data validated - 205 rows over 365 days
    üìä NUGT 3- Training: Horizon=60d, Target=9.98% (B&H: 60.69%, Scale=0.164)
‚úÖ NUGT: Training data validated - 205 rows over 365 days
    üìä PRIM 3- Training: Horizon=60d, Target=9.93% (B&H: 60.38%, Scale=0.164)
‚úÖ PRIM: Training data validated - 205 rows over 365 days
    üìä IDCC 3- Training: Horizon=60d, Target=9.90% (B&H: 60.23%, Scale=0.164)
‚úÖ IDCC: Training data validated - 205 rows over 365 days
    üìä GTX 3- Training: Horizon=60d, Target=9.88% (B&H: 60.12%, Scale=0.164)
‚úÖ GTX: Training data validated - 205 rows over 365 days
    üìä ITRN 3- Training: Horizon=60d, Target=9.81% (B&H: 59.69%, Scale=0.164)
‚úÖ ITRN: Training data validated - 205 rows over 365 days
    üìä APG 3- Training: Horizon=60d, Target=9.81% (B&H: 59.68%, Scale=0.164)
‚úÖ APG: Training data validated - 205 rows over 365 days
    üìä UAL 3- Training: Horizon=60d, Target=9.81% (B&H: 59.67%, Scale=0.164)
‚úÖ UAL: Training data validated - 205 rows over 365 days
    üìä AEM 3- Training: Horizon=60d, Target=9.77% (B&H: 59.43%, Scale=0.164)
‚úÖ AEM: Training data validated - 205 rows over 365 days
    üìä LUNR 3- Training: Horizon=60d, Target=9.74% (B&H: 59.25%, Scale=0.164)
‚úÖ LUNR: Training data validated - 205 rows over 365 days
    üìä HIPO 3- Training: Horizon=60d, Target=9.70% (B&H: 59.00%, Scale=0.164)
‚úÖ HIPO: Training data validated - 205 rows over 365 days
    üìä PRA 3- Training: Horizon=60d, Target=9.57% (B&H: 58.24%, Scale=0.164)
‚úÖ PRA: Training data validated - 205 rows over 365 days
    üìä DAN 3- Training: Horizon=60d, Target=9.52% (B&H: 57.93%, Scale=0.164)
‚úÖ DAN: Training data validated - 205 rows over 365 days
    üìä TGTX 3- Training: Horizon=60d, Target=9.49% (B&H: 57.76%, Scale=0.164)
‚úÖ TGTX: Training data validated - 205 rows over 365 days
    üìä AVPT 3- Training: Horizon=60d, Target=9.48% (B&H: 57.69%, Scale=0.164)
‚úÖ AVPT: Training data validated - 205 rows over 365 days
    üìä RDVT 3- Training: Horizon=60d, Target=9.46% (B&H: 57.57%, Scale=0.164)
‚úÖ RDVT: Training data validated - 205 rows over 365 days
    üìä VSTA 3- Training: Horizon=60d, Target=9.45% (B&H: 57.49%, Scale=0.164)
‚úÖ VSTA: Training data validated - 205 rows over 365 days
    üìä AAMI 3- Training: Horizon=60d, Target=9.42% (B&H: 57.33%, Scale=0.164)
‚úÖ AAMI: Training data validated - 205 rows over 365 days
    üìä ENVX 3- Training: Horizon=60d, Target=9.42% (B&H: 57.28%, Scale=0.164)
‚úÖ ENVX: Training data validated - 205 rows over 365 days
    üìä ASA 3- Training: Horizon=60d, Target=9.41% (B&H: 57.25%, Scale=0.164)
‚úÖ ASA: Training data validated - 205 rows over 365 days
    üìä C 3- Training: Horizon=60d, Target=9.39% (B&H: 57.15%, Scale=0.164)
‚úÖ C: Training data validated - 205 rows over 365 days
    üìä FNGO 3- Training: Horizon=60d, Target=9.38% (B&H: 57.07%, Scale=0.164)
‚úÖ FNGO: Training data validated - 205 rows over 365 days
    üìä FIVE 3- Training: Horizon=60d, Target=9.36% (B&H: 56.96%, Scale=0.164)
‚úÖ FIVE: Training data validated - 205 rows over 365 days
    üìä YPF 3- Training: Horizon=60d, Target=9.26% (B&H: 56.34%, Scale=0.164)
‚úÖ YPF: Training data validated - 205 rows over 365 days
    üìä NWG 3- Training: Horizon=60d, Target=9.25% (B&H: 56.29%, Scale=0.164)
‚úÖ NWG: Training data validated - 205 rows over 365 days
    üìä PW 3- Training: Horizon=60d, Target=9.21% (B&H: 56.00%, Scale=0.164)
‚úÖ PW: Training data validated - 205 rows over 365 days
    üìä UPXI 3- Training: Horizon=60d, Target=9.18% (B&H: 55.87%, Scale=0.164)
‚úÖ UPXI: Training data validated - 205 rows over 365 days
    üìä CD 3- Training: Horizon=60d, Target=9.16% (B&H: 55.75%, Scale=0.164)
‚úÖ CD: Training data validated - 205 rows over 365 days
    üìä ADTN 3- Training: Horizon=60d, Target=9.15% (B&H: 55.65%, Scale=0.164)
‚úÖ ADTN: Training data validated - 205 rows over 365 days
    üìä RL 3- Training: Horizon=60d, Target=9.12% (B&H: 55.46%, Scale=0.164)
‚úÖ RL: Training data validated - 205 rows over 365 days
    üìä SHOP 3- Training: Horizon=60d, Target=9.08% (B&H: 55.27%, Scale=0.164)
‚úÖ SHOP: Training data validated - 205 rows over 365 days
    üìä WPM 3- Training: Horizon=60d, Target=9.02% (B&H: 54.87%, Scale=0.164)
‚úÖ WPM: Training data validated - 205 rows over 365 days
    üìä AAP 3- Training: Horizon=60d, Target=8.91% (B&H: 54.19%, Scale=0.164)
‚úÖ AAP: Training data validated - 205 rows over 365 days
    üìä ZLAB 3- Training: Horizon=60d, Target=8.87% (B&H: 53.98%, Scale=0.164)
‚úÖ ZLAB: Training data validated - 205 rows over 365 days
    üìä RRGB 3- Training: Horizon=60d, Target=8.87% (B&H: 53.97%, Scale=0.164)
‚úÖ RRGB: Training data validated - 205 rows over 365 days
    üìä GRND 3- Training: Horizon=60d, Target=8.76% (B&H: 53.31%, Scale=0.164)
‚úÖ GRND: Training data validated - 205 rows over 365 days
    üìä BA 3- Training: Horizon=60d, Target=8.76% (B&H: 53.29%, Scale=0.164)
‚úÖ BA: Training data validated - 205 rows over 365 days
    üìä BBAR 3- Training: Horizon=60d, Target=8.75% (B&H: 53.23%, Scale=0.164)
‚úÖ BBAR: Training data validated - 205 rows over 365 days
    üìä BELFB 3- Training: Horizon=60d, Target=8.74% (B&H: 53.16%, Scale=0.164)
‚úÖ BELFB: Training data validated - 205 rows over 365 days
    üìä NGD 3- Training: Horizon=60d, Target=8.73% (B&H: 53.12%, Scale=0.164)
‚úÖ NGD: Training data validated - 205 rows over 365 days
    üìä HTT 3- Training: Horizon=60d, Target=8.72% (B&H: 53.05%, Scale=0.164)
‚úÖ HTT: Training data validated - 205 rows over 365 days
    üìä CAE 3- Training: Horizon=60d, Target=8.71% (B&H: 52.96%, Scale=0.164)
‚úÖ CAE: Training data validated - 205 rows over 365 days
    üìä BTF 3- Training: Horizon=60d, Target=8.66% (B&H: 52.68%, Scale=0.164)
‚úÖ BTF: Training data validated - 205 rows over 365 days
    üìä ATGE 3- Training: Horizon=60d, Target=8.63% (B&H: 52.52%, Scale=0.164)
‚úÖ ATGE: Training data validated - 205 rows over 365 days
    üìä GREK 3- Training: Horizon=60d, Target=8.61% (B&H: 52.35%, Scale=0.164)
‚úÖ GREK: Training data validated - 205 rows over 365 days
    üìä LRN 3- Training: Horizon=60d, Target=8.60% (B&H: 52.32%, Scale=0.164)
‚úÖ LRN: Training data validated - 205 rows over 365 days
    üìä AVAL 3- Training: Horizon=60d, Target=8.60% (B&H: 52.32%, Scale=0.164)
‚úÖ AVAL: Training data validated - 205 rows over 365 days
    üìä ESQ 3- Training: Horizon=60d, Target=8.58% (B&H: 52.20%, Scale=0.164)
‚úÖ ESQ: Training data validated - 205 rows over 365 days
    üìä OR 3- Training: Horizon=60d, Target=8.53% (B&H: 51.87%, Scale=0.164)
‚úÖ OR: Training data validated - 205 rows over 365 days
    üìä WFC 3- Training: Horizon=60d, Target=8.50% (B&H: 51.69%, Scale=0.164)
‚úÖ WFC: Training data validated - 205 rows over 365 days
    üìä BBW 3- Training: Horizon=60d, Target=8.49% (B&H: 51.66%, Scale=0.164)
‚úÖ BBW: Training data validated - 205 rows over 365 days
    üìä SCHW 3- Training: Horizon=60d, Target=8.43% (B&H: 51.26%, Scale=0.164)
‚úÖ SCHW: Training data validated - 205 rows over 365 days
    üìä MTZ 3- Training: Horizon=60d, Target=8.42% (B&H: 51.25%, Scale=0.164)
‚úÖ MTZ: Training data validated - 205 rows over 365 days
    üìä QTUM 3- Training: Horizon=60d, Target=8.42% (B&H: 51.20%, Scale=0.164)
‚úÖ QTUM: Training data validated - 205 rows over 365 days
    üìä ACAD 3- Training: Horizon=60d, Target=8.41% (B&H: 51.17%, Scale=0.164)
‚úÖ ACAD: Training data validated - 205 rows over 365 days
    üìä ROAD 3- Training: Horizon=60d, Target=8.39% (B&H: 51.06%, Scale=0.164)
‚úÖ ROAD: Training data validated - 205 rows over 365 days
    üìä HSBC 3- Training: Horizon=60d, Target=8.39% (B&H: 51.05%, Scale=0.164)
‚úÖ HSBC: Training data validated - 205 rows over 365 days
    üìä TGS 3- Training: Horizon=60d, Target=8.37% (B&H: 50.93%, Scale=0.164)
‚úÖ TGS: Training data validated - 205 rows over 365 days
    üìä BTI 3- Training: Horizon=60d, Target=8.35% (B&H: 50.82%, Scale=0.164)
‚úÖ BTI: Training data validated - 205 rows over 365 days
    üìä MFG 3- Training: Horizon=60d, Target=8.35% (B&H: 50.82%, Scale=0.164)
‚úÖ MFG: Training data validated - 205 rows over 365 days
    üìä LFCR 3- Training: Horizon=60d, Target=8.34% (B&H: 50.71%, Scale=0.164)
‚úÖ LFCR: Training data validated - 205 rows over 365 days
    üìä WWD 3- Training: Horizon=60d, Target=8.32% (B&H: 50.64%, Scale=0.164)
‚úÖ WWD: Training data validated - 205 rows over 365 days
    üìä DGP 3- Training: Horizon=60d, Target=8.32% (B&H: 50.60%, Scale=0.164)
‚úÖ DGP: Training data validated - 205 rows over 365 days
    üìä CG 3- Training: Horizon=60d, Target=8.30% (B&H: 50.50%, Scale=0.164)
‚úÖ CG: Training data validated - 205 rows over 365 days
    üìä WOR 3- Training: Horizon=60d, Target=8.30% (B&H: 50.46%, Scale=0.164)
‚úÖ WOR: Training data validated - 205 rows over 365 days
    üìä SGDM 3- Training: Horizon=60d, Target=8.29% (B&H: 50.46%, Scale=0.164)
‚úÖ SGDM: Training data validated - 205 rows over 365 days
    üìä BRNS 3- Training: Horizon=60d, Target=8.29% (B&H: 50.42%, Scale=0.164)
‚úÖ BRNS: Training data validated - 205 rows over 365 days
    üìä FHN 3- Training: Horizon=60d, Target=8.27% (B&H: 50.31%, Scale=0.164)
‚úÖ FHN: Training data validated - 205 rows over 365 days
    üìä NFLY 3- Training: Horizon=60d, Target=8.26% (B&H: 50.26%, Scale=0.164)
‚úÖ NFLY: Training data validated - 205 rows over 365 days
    üìä URA 3- Training: Horizon=60d, Target=8.25% (B&H: 50.17%, Scale=0.164)
‚úÖ URA: Training data validated - 205 rows over 365 days
    üìä IDT 3- Training: Horizon=60d, Target=8.24% (B&H: 50.15%, Scale=0.164)
‚úÖ IDT: Training data validated - 205 rows over 365 days
    üìä FINV 3- Training: Horizon=60d, Target=8.24% (B&H: 50.12%, Scale=0.164)
‚úÖ FINV: Training data validated - 205 rows over 365 days
    üìä ECG 3- Training: Horizon=60d, Target=8.23% (B&H: 50.06%, Scale=0.164)
‚úÖ ECG: Training data validated - 205 rows over 365 days
    üìä BWMN 3- Training: Horizon=60d, Target=8.23% (B&H: 50.04%, Scale=0.164)
‚úÖ BWMN: Training data validated - 205 rows over 365 days
    üìä PCRX 3- Training: Horizon=60d, Target=8.19% (B&H: 49.83%, Scale=0.164)
‚úÖ PCRX: Training data validated - 205 rows over 365 days
    üìä CTLP 3- Training: Horizon=60d, Target=8.17% (B&H: 49.73%, Scale=0.164)
‚úÖ CTLP: Training data validated - 205 rows over 365 days
    üìä IPI 3- Training: Horizon=60d, Target=8.14% (B&H: 49.54%, Scale=0.164)
‚úÖ IPI: Training data validated - 205 rows over 365 days
    üìä FLEX 3- Training: Horizon=60d, Target=8.14% (B&H: 49.54%, Scale=0.164)
‚úÖ FLEX: Training data validated - 205 rows over 365 days
    üìä GS 3- Training: Horizon=60d, Target=8.13% (B&H: 49.44%, Scale=0.164)
‚úÖ GS: Training data validated - 205 rows over 365 days
    üìä WRBY 3- Training: Horizon=60d, Target=8.11% (B&H: 49.36%, Scale=0.164)
‚úÖ WRBY: Training data validated - 205 rows over 365 days
    üìä BB 3- Training: Horizon=60d, Target=8.09% (B&H: 49.24%, Scale=0.164)
‚úÖ BB: Training data validated - 205 rows over 365 days
    üìä UGI 3- Training: Horizon=60d, Target=8.08% (B&H: 49.14%, Scale=0.164)
‚úÖ UGI: Training data validated - 205 rows over 365 days
    üìä CALM 3- Training: Horizon=60d, Target=8.06% (B&H: 49.04%, Scale=0.164)
‚úÖ CALM: Training data validated - 205 rows over 365 days
    üìä OSIS 3- Training: Horizon=60d, Target=8.04% (B&H: 48.90%, Scale=0.164)
‚úÖ OSIS: Training data validated - 205 rows over 365 days
    üìä RMBS 3- Training: Horizon=60d, Target=8.03% (B&H: 48.86%, Scale=0.164)
‚úÖ RMBS: Training data validated - 205 rows over 365 days
    üìä CELH 3- Training: Horizon=60d, Target=8.01% (B&H: 48.72%, Scale=0.164)
‚úÖ CELH: Training data validated - 205 rows over 365 days
    üìä UNTY 3- Training: Horizon=60d, Target=7.99% (B&H: 48.64%, Scale=0.164)
‚úÖ UNTY: Training data validated - 205 rows over 365 days
    üìä ESE 3- Training: Horizon=60d, Target=7.99% (B&H: 48.63%, Scale=0.164)
‚úÖ ESE: Training data validated - 205 rows over 365 days
    üìä SYF 3- Training: Horizon=60d, Target=7.92% (B&H: 48.17%, Scale=0.164)
‚úÖ SYF: Training data validated - 205 rows over 365 days
    üìä NTES 3- Training: Horizon=60d, Target=7.92% (B&H: 48.16%, Scale=0.164)
‚úÖ NTES: Training data validated - 205 rows over 365 days
    üìä CHEF 3- Training: Horizon=60d, Target=7.91% (B&H: 48.13%, Scale=0.164)
‚úÖ CHEF: Training data validated - 205 rows over 365 days
    üìä CNM 3- Training: Horizon=60d, Target=7.91% (B&H: 48.13%, Scale=0.164)
‚úÖ CNM: Training data validated - 205 rows over 365 days
    üìä SSP 3- Training: Horizon=60d, Target=7.89% (B&H: 48.00%, Scale=0.164)
‚úÖ SSP: Training data validated - 205 rows over 365 days
    üìä CBRL 3- Training: Horizon=60d, Target=7.88% (B&H: 47.93%, Scale=0.164)
‚úÖ CBRL: Training data validated - 205 rows over 365 days
    üìä VIRT 3- Training: Horizon=60d, Target=7.88% (B&H: 47.92%, Scale=0.164)
‚úÖ VIRT: Training data validated - 205 rows over 365 days
    üìä KMDA 3- Training: Horizon=60d, Target=7.87% (B&H: 47.88%, Scale=0.164)
‚úÖ KMDA: Training data validated - 205 rows over 365 days
    üìä EME 3- Training: Horizon=60d, Target=7.86% (B&H: 47.82%, Scale=0.164)
‚úÖ EME: Training data validated - 205 rows over 365 days
    üìä CW 3- Training: Horizon=60d, Target=7.85% (B&H: 47.78%, Scale=0.164)
‚úÖ CW: Training data validated - 205 rows over 365 days
    üìä NTRS 3- Training: Horizon=60d, Target=7.83% (B&H: 47.61%, Scale=0.164)
‚úÖ NTRS: Training data validated - 205 rows over 365 days
    üìä CORT 3- Training: Horizon=60d, Target=7.82% (B&H: 47.56%, Scale=0.164)
‚úÖ CORT: Training data validated - 205 rows over 365 days
    üìä TFPM 3- Training: Horizon=60d, Target=7.81% (B&H: 47.52%, Scale=0.164)
‚úÖ TFPM: Training data validated - 205 rows over 365 days
    üìä SUPV 3- Training: Horizon=60d, Target=7.80% (B&H: 47.48%, Scale=0.164)
‚úÖ SUPV: Training data validated - 205 rows over 365 days
    üìä DRD 3- Training: Horizon=60d, Target=7.80% (B&H: 47.45%, Scale=0.164)
‚úÖ DRD: Training data validated - 205 rows over 365 days
    üìä ACMR 3- Training: Horizon=60d, Target=7.80% (B&H: 47.44%, Scale=0.164)
‚úÖ ACMR: Training data validated - 205 rows over 365 days
    üìä INBX 3- Training: Horizon=60d, Target=7.79% (B&H: 47.38%, Scale=0.164)
‚úÖ INBX: Training data validated - 205 rows over 365 days
    üìä FSM 3- Training: Horizon=60d, Target=7.78% (B&H: 47.30%, Scale=0.164)
‚úÖ FSM: Training data validated - 205 rows over 365 days
    üìä HUYA 3- Training: Horizon=60d, Target=7.77% (B&H: 47.29%, Scale=0.164)
‚úÖ HUYA: Training data validated - 205 rows over 365 days
    üìä OPLN 3- Training: Horizon=60d, Target=7.75% (B&H: 47.16%, Scale=0.164)
‚úÖ OPLN: Training data validated - 205 rows over 365 days
    üìä JOYY 3- Training: Horizon=60d, Target=7.73% (B&H: 47.03%, Scale=0.164)
‚úÖ JOYY: Training data validated - 205 rows over 365 days
    üìä MUFG 3- Training: Horizon=60d, Target=7.72% (B&H: 46.93%, Scale=0.164)
‚úÖ MUFG: Training data validated - 205 rows over 365 days
    üìä GILT 3- Training: Horizon=60d, Target=7.70% (B&H: 46.87%, Scale=0.164)
‚úÖ GILT: Training data validated - 205 rows over 365 days
    üìä PFIX 3- Training: Horizon=60d, Target=7.69% (B&H: 46.80%, Scale=0.164)
‚úÖ PFIX: Training data validated - 205 rows over 365 days
    üìä MRP 3- Training: Horizon=60d, Target=7.67% (B&H: 46.64%, Scale=0.164)
  ‚ùå MRP: Too many NaN values in Close price: 87 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä AHR 3- Training: Horizon=60d, Target=7.66% (B&H: 46.59%, Scale=0.164)
‚úÖ AHR: Training data validated - 205 rows over 365 days
    üìä SERV 3- Training: Horizon=60d, Target=7.65% (B&H: 46.54%, Scale=0.164)
‚úÖ SERV: Training data validated - 205 rows over 365 days
    üìä SGDJ 3- Training: Horizon=60d, Target=7.65% (B&H: 46.54%, Scale=0.164)
‚úÖ SGDJ: Training data validated - 205 rows over 365 days
    üìä IRS 3- Training: Horizon=60d, Target=7.65% (B&H: 46.53%, Scale=0.164)
‚úÖ IRS: Training data validated - 205 rows over 365 days
    üìä UGL 3- Training: Horizon=60d, Target=7.63% (B&H: 46.42%, Scale=0.164)
‚úÖ UGL: Training data validated - 205 rows over 365 days
    üìä TE 3- Training: Horizon=60d, Target=7.63% (B&H: 46.39%, Scale=0.164)
‚úÖ TE: Training data validated - 205 rows over 365 days
    üìä TTWO 3- Training: Horizon=60d, Target=7.60% (B&H: 46.22%, Scale=0.164)
‚úÖ TTWO: Training data validated - 205 rows over 365 days
    üìä SIL 3- Training: Horizon=60d, Target=7.60% (B&H: 46.21%, Scale=0.164)
‚úÖ SIL: Training data validated - 205 rows over 365 days
    üìä FGM 3- Training: Horizon=60d, Target=7.58% (B&H: 46.12%, Scale=0.164)
‚úÖ FGM: Training data validated - 205 rows over 365 days
    üìä NVDU 3- Training: Horizon=60d, Target=7.57% (B&H: 46.04%, Scale=0.164)
‚úÖ NVDU: Training data validated - 205 rows over 365 days
    üìä FDIG 3- Training: Horizon=60d, Target=7.55% (B&H: 45.90%, Scale=0.164)
‚úÖ FDIG: Training data validated - 205 rows over 365 days
    üìä SFM 3- Training: Horizon=60d, Target=7.54% (B&H: 45.88%, Scale=0.164)
‚úÖ SFM: Training data validated - 205 rows over 365 days
    üìä NLR 3- Training: Horizon=60d, Target=7.54% (B&H: 45.85%, Scale=0.164)
‚úÖ NLR: Training data validated - 205 rows over 365 days
    üìä JCI 3- Training: Horizon=60d, Target=7.53% (B&H: 45.80%, Scale=0.164)
‚úÖ JCI: Training data validated - 205 rows over 365 days
    üìä EXK 3- Training: Horizon=60d, Target=7.47% (B&H: 45.43%, Scale=0.164)
‚úÖ EXK: Training data validated - 205 rows over 365 days
    üìä BFC 3- Training: Horizon=60d, Target=7.46% (B&H: 45.41%, Scale=0.164)
‚úÖ BFC: Training data validated - 205 rows over 365 days
    üìä NERD 3- Training: Horizon=60d, Target=7.46% (B&H: 45.38%, Scale=0.164)
‚úÖ NERD: Training data validated - 205 rows over 365 days
    üìä ORCL 3- Training: Horizon=60d, Target=7.46% (B&H: 45.35%, Scale=0.164)
‚úÖ ORCL: Training data validated - 205 rows over 365 days
    üìä EPOL 3- Training: Horizon=60d, Target=7.44% (B&H: 45.27%, Scale=0.164)
‚úÖ EPOL: Training data validated - 205 rows over 365 days
    üìä SLM 3- Training: Horizon=60d, Target=7.41% (B&H: 45.08%, Scale=0.164)
‚úÖ SLM: Training data validated - 205 rows over 365 days
    üìä LB 3- Training: Horizon=60d, Target=7.37% (B&H: 44.85%, Scale=0.164)
‚úÖ LB: Training data validated - 205 rows over 365 days
    üìä GE 3- Training: Horizon=60d, Target=7.35% (B&H: 44.73%, Scale=0.164)
‚úÖ GE: Training data validated - 205 rows over 365 days
    üìä OCS 3- Training: Horizon=60d, Target=7.31% (B&H: 44.45%, Scale=0.164)
‚úÖ OCS: Training data validated - 205 rows over 365 days
    üìä TITN 3- Training: Horizon=60d, Target=7.30% (B&H: 44.44%, Scale=0.164)
‚úÖ TITN: Training data validated - 205 rows over 365 days
    üìä MCK 3- Training: Horizon=60d, Target=7.30% (B&H: 44.44%, Scale=0.164)
‚úÖ MCK: Training data validated - 205 rows over 365 days
    üìä CAH 3- Training: Horizon=60d, Target=7.30% (B&H: 44.42%, Scale=0.164)
‚úÖ CAH: Training data validated - 205 rows over 365 days
    üìä GTES 3- Training: Horizon=60d, Target=7.27% (B&H: 44.22%, Scale=0.164)
‚úÖ GTES: Training data validated - 205 rows over 365 days
    üìä LOMA 3- Training: Horizon=60d, Target=7.26% (B&H: 44.17%, Scale=0.164)
‚úÖ LOMA: Training data validated - 205 rows over 365 days
    üìä NVDL 3- Training: Horizon=60d, Target=7.25% (B&H: 44.10%, Scale=0.164)
‚úÖ NVDL: Training data validated - 205 rows over 365 days
    üìä KODK 3- Training: Horizon=60d, Target=7.24% (B&H: 44.07%, Scale=0.164)
‚úÖ KODK: Training data validated - 205 rows over 365 days
    üìä PAYC 3- Training: Horizon=60d, Target=7.24% (B&H: 44.06%, Scale=0.164)
‚úÖ PAYC: Training data validated - 205 rows over 365 days
    üìä SANM 3- Training: Horizon=60d, Target=7.24% (B&H: 44.02%, Scale=0.164)
‚úÖ SANM: Training data validated - 205 rows over 365 days
    üìä IZRL 3- Training: Horizon=60d, Target=7.23% (B&H: 44.01%, Scale=0.164)
‚úÖ IZRL: Training data validated - 205 rows over 365 days
    üìä EQT 3- Training: Horizon=60d, Target=7.22% (B&H: 43.89%, Scale=0.164)
‚úÖ EQT: Training data validated - 205 rows over 365 days
    üìä CIEN 3- Training: Horizon=60d, Target=7.21% (B&H: 43.85%, Scale=0.164)
‚úÖ CIEN: Training data validated - 205 rows over 365 days
    üìä BK 3- Training: Horizon=60d, Target=7.19% (B&H: 43.73%, Scale=0.164)
‚úÖ BK: Training data validated - 205 rows over 365 days
    üìä OPY 3- Training: Horizon=60d, Target=7.17% (B&H: 43.62%, Scale=0.164)
‚úÖ OPY: Training data validated - 205 rows over 365 days
    üìä ULS 3- Training: Horizon=60d, Target=7.17% (B&H: 43.60%, Scale=0.164)
‚úÖ ULS: Training data validated - 205 rows over 365 days
    üìä IFS 3- Training: Horizon=60d, Target=7.16% (B&H: 43.57%, Scale=0.164)
‚úÖ IFS: Training data validated - 205 rows over 365 days
    üìä COF 3- Training: Horizon=60d, Target=7.15% (B&H: 43.50%, Scale=0.164)
‚úÖ COF: Training data validated - 205 rows over 365 days
    üìä UEC 3- Training: Horizon=60d, Target=7.15% (B&H: 43.48%, Scale=0.164)
‚úÖ UEC: Training data validated - 205 rows over 365 days
    üìä METCB 3- Training: Horizon=60d, Target=7.13% (B&H: 43.40%, Scale=0.164)
‚úÖ METCB: Training data validated - 205 rows over 365 days
    üìä SXT 3- Training: Horizon=60d, Target=7.13% (B&H: 43.39%, Scale=0.164)
‚úÖ SXT: Training data validated - 205 rows over 365 days
    üìä EIS 3- Training: Horizon=60d, Target=7.11% (B&H: 43.27%, Scale=0.164)
‚úÖ EIS: Training data validated - 205 rows over 365 days
    üìä AG 3- Training: Horizon=60d, Target=7.11% (B&H: 43.23%, Scale=0.164)
‚úÖ AG: Training data validated - 205 rows over 365 days
    üìä ATLC 3- Training: Horizon=60d, Target=7.10% (B&H: 43.22%, Scale=0.164)
‚úÖ ATLC: Training data validated - 205 rows over 365 days
    üìä MRCY 3- Training: Horizon=60d, Target=7.10% (B&H: 43.19%, Scale=0.164)
‚úÖ MRCY: Training data validated - 205 rows over 365 days
    üìä FROG 3- Training: Horizon=60d, Target=7.08% (B&H: 43.04%, Scale=0.164)
‚úÖ FROG: Training data validated - 205 rows over 365 days
    üìä LZ 3- Training: Horizon=60d, Target=7.07% (B&H: 42.99%, Scale=0.164)
‚úÖ LZ: Training data validated - 205 rows over 365 days
    üìä PAY 3- Training: Horizon=60d, Target=7.06% (B&H: 42.96%, Scale=0.164)
‚úÖ PAY: Training data validated - 205 rows over 365 days
    üìä TSM 3- Training: Horizon=60d, Target=7.06% (B&H: 42.94%, Scale=0.164)
‚úÖ TSM: Training data validated - 205 rows over 365 days
    üìä NVDA 3- Training: Horizon=60d, Target=7.05% (B&H: 42.90%, Scale=0.164)
‚úÖ NVDA: Training data validated - 205 rows over 365 days
    üìä GOAU 3- Training: Horizon=60d, Target=7.05% (B&H: 42.87%, Scale=0.164)
‚úÖ GOAU: Training data validated - 205 rows over 365 days
    üìä LYG 3- Training: Horizon=60d, Target=7.00% (B&H: 42.60%, Scale=0.164)
‚úÖ LYG: Training data validated - 205 rows over 365 days
    üìä GDXJ 3- Training: Horizon=60d, Target=6.99% (B&H: 42.53%, Scale=0.164)
‚úÖ GDXJ: Training data validated - 205 rows over 365 days
    üìä ATRA 3- Training: Horizon=60d, Target=6.98% (B&H: 42.44%, Scale=0.164)
‚úÖ ATRA: Training data validated - 205 rows over 365 days
    üìä UAN 3- Training: Horizon=60d, Target=6.97% (B&H: 42.43%, Scale=0.164)
‚úÖ UAN: Training data validated - 205 rows over 365 days
    üìä CRMD 3- Training: Horizon=60d, Target=6.94% (B&H: 42.20%, Scale=0.164)
‚úÖ CRMD: Training data validated - 205 rows over 365 days
    üìä NAMS 3- Training: Horizon=60d, Target=6.93% (B&H: 42.17%, Scale=0.164)
‚úÖ NAMS: Training data validated - 205 rows over 365 days
    üìä HGV 3- Training: Horizon=60d, Target=6.93% (B&H: 42.15%, Scale=0.164)
‚úÖ HGV: Training data validated - 205 rows over 365 days
    üìä DEC 3- Training: Horizon=60d, Target=6.93% (B&H: 42.15%, Scale=0.164)
‚úÖ DEC: Training data validated - 205 rows over 365 days
    üìä PLTM 3- Training: Horizon=60d, Target=6.92% (B&H: 42.12%, Scale=0.164)
‚úÖ PLTM: Training data validated - 205 rows over 365 days
    üìä PPLT 3- Training: Horizon=60d, Target=6.92% (B&H: 42.12%, Scale=0.164)
‚úÖ PPLT: Training data validated - 205 rows over 365 days
    üìä DLO 3- Training: Horizon=60d, Target=6.90% (B&H: 42.00%, Scale=0.164)
‚úÖ DLO: Training data validated - 205 rows over 365 days
    üìä ATI 3- Training: Horizon=60d, Target=6.90% (B&H: 41.97%, Scale=0.164)
‚úÖ ATI: Training data validated - 205 rows over 365 days
    üìä ESPO 3- Training: Horizon=60d, Target=6.89% (B&H: 41.91%, Scale=0.164)
‚úÖ ESPO: Training data validated - 205 rows over 365 days
    üìä VLN 3- Training: Horizon=60d, Target=6.87% (B&H: 41.82%, Scale=0.164)
‚úÖ VLN: Training data validated - 205 rows over 365 days
    üìä AMPL 3- Training: Horizon=60d, Target=6.87% (B&H: 41.81%, Scale=0.164)
‚úÖ AMPL: Training data validated - 205 rows over 365 days
    üìä GILD 3- Training: Horizon=60d, Target=6.86% (B&H: 41.74%, Scale=0.164)
‚úÖ GILD: Training data validated - 205 rows over 365 days
    üìä CTOR 3- Training: Horizon=60d, Target=6.86% (B&H: 41.73%, Scale=0.164)
‚úÖ CTOR: Training data validated - 205 rows over 365 days
    üìä TNL 3- Training: Horizon=60d, Target=6.84% (B&H: 41.61%, Scale=0.164)
‚úÖ TNL: Training data validated - 205 rows over 365 days
    üìä PWR 3- Training: Horizon=60d, Target=6.84% (B&H: 41.61%, Scale=0.164)
‚úÖ PWR: Training data validated - 205 rows over 365 days
    üìä EWO 3- Training: Horizon=60d, Target=6.84% (B&H: 41.58%, Scale=0.164)
‚úÖ EWO: Training data validated - 205 rows over 365 days
    üìä PAAS 3- Training: Horizon=60d, Target=6.82% (B&H: 41.48%, Scale=0.164)
‚úÖ PAAS: Training data validated - 205 rows over 365 days
    üìä JBTM 3- Training: Horizon=60d, Target=6.82% (B&H: 41.46%, Scale=0.164)
‚úÖ JBTM: Training data validated - 205 rows over 365 days
    üìä SLVP 3- Training: Horizon=60d, Target=6.78% (B&H: 41.22%, Scale=0.164)
‚úÖ SLVP: Training data validated - 205 rows over 365 days
    üìä NPK 3- Training: Horizon=60d, Target=6.77% (B&H: 41.19%, Scale=0.164)
‚úÖ NPK: Training data validated - 205 rows over 365 days
    üìä CENX 3- Training: Horizon=60d, Target=6.76% (B&H: 41.10%, Scale=0.164)
‚úÖ CENX: Training data validated - 205 rows over 365 days
    üìä GRAB 3- Training: Horizon=60d, Target=6.75% (B&H: 41.05%, Scale=0.164)
‚úÖ GRAB: Training data validated - 205 rows over 365 days
    üìä MOS 3- Training: Horizon=60d, Target=6.74% (B&H: 41.00%, Scale=0.164)
‚úÖ MOS: Training data validated - 205 rows over 365 days
    üìä CASY 3- Training: Horizon=60d, Target=6.72% (B&H: 40.90%, Scale=0.164)
‚úÖ CASY: Training data validated - 205 rows over 365 days
    üìä XAR 3- Training: Horizon=60d, Target=6.72% (B&H: 40.87%, Scale=0.164)
‚úÖ XAR: Training data validated - 205 rows over 365 days
    üìä NIC 3- Training: Horizon=60d, Target=6.70% (B&H: 40.73%, Scale=0.164)
‚úÖ NIC: Training data validated - 205 rows over 365 days
    üìä QXO 3- Training: Horizon=60d, Target=6.69% (B&H: 40.71%, Scale=0.164)
‚úÖ QXO: Training data validated - 205 rows over 365 days
    üìä STX 3- Training: Horizon=60d, Target=6.68% (B&H: 40.66%, Scale=0.164)
‚úÖ STX: Training data validated - 205 rows over 365 days
    üìä ETH 3- Training: Horizon=60d, Target=6.68% (B&H: 40.66%, Scale=0.164)
‚úÖ ETH: Training data validated - 205 rows over 365 days
    üìä BKCH 3- Training: Horizon=60d, Target=6.64% (B&H: 40.39%, Scale=0.164)
‚úÖ BKCH: Training data validated - 205 rows over 365 days
    üìä OCUL 3- Training: Horizon=60d, Target=6.63% (B&H: 40.34%, Scale=0.164)
‚úÖ OCUL: Training data validated - 205 rows over 365 days
    üìä ETHW 3- Training: Horizon=60d, Target=6.63% (B&H: 40.33%, Scale=0.164)
‚úÖ ETHW: Training data validated - 205 rows over 365 days
    üìä ETHV 3- Training: Horizon=60d, Target=6.63% (B&H: 40.32%, Scale=0.164)
‚úÖ ETHV: Training data validated - 205 rows over 365 days
    üìä POET 3- Training: Horizon=60d, Target=6.63% (B&H: 40.32%, Scale=0.164)
‚úÖ POET: Training data validated - 205 rows over 365 days
    üìä EZET 3- Training: Horizon=60d, Target=6.62% (B&H: 40.29%, Scale=0.164)
‚úÖ EZET: Training data validated - 205 rows over 365 days
    üìä RSI 3- Training: Horizon=60d, Target=6.62% (B&H: 40.28%, Scale=0.164)
‚úÖ RSI: Training data validated - 205 rows over 365 days
    üìä FETH 3- Training: Horizon=60d, Target=6.61% (B&H: 40.24%, Scale=0.164)
‚úÖ FETH: Training data validated - 205 rows over 365 days
    üìä PERI 3- Training: Horizon=60d, Target=6.61% (B&H: 40.23%, Scale=0.164)
‚úÖ PERI: Training data validated - 205 rows over 365 days
    üìä TETH 3- Training: Horizon=60d, Target=6.61% (B&H: 40.22%, Scale=0.164)
‚úÖ TETH: Training data validated - 205 rows over 365 days
    üìä ETHA 3- Training: Horizon=60d, Target=6.61% (B&H: 40.21%, Scale=0.164)
‚úÖ ETHA: Training data validated - 205 rows over 365 days
    üìä PTON 3- Training: Horizon=60d, Target=6.60% (B&H: 40.17%, Scale=0.164)
‚úÖ PTON: Training data validated - 205 rows over 365 days
    üìä LAUR 3- Training: Horizon=60d, Target=6.59% (B&H: 40.10%, Scale=0.164)
‚úÖ LAUR: Training data validated - 205 rows over 365 days
    üìä WGMI 3- Training: Horizon=60d, Target=6.58% (B&H: 40.01%, Scale=0.164)
‚úÖ WGMI: Training data validated - 205 rows over 365 days
    üìä QETH 3- Training: Horizon=60d, Target=6.57% (B&H: 39.98%, Scale=0.164)
‚úÖ QETH: Training data validated - 205 rows over 365 days
    üìä PLMR 3- Training: Horizon=60d, Target=6.57% (B&H: 39.98%, Scale=0.164)
‚úÖ PLMR: Training data validated - 205 rows over 365 days
    üìä BTSG 3- Training: Horizon=60d, Target=6.55% (B&H: 39.85%, Scale=0.164)
‚úÖ BTSG: Training data validated - 205 rows over 365 days
    üìä RING 3- Training: Horizon=60d, Target=6.55% (B&H: 39.84%, Scale=0.164)
‚úÖ RING: Training data validated - 205 rows over 365 days
    üìä LYV 3- Training: Horizon=60d, Target=6.54% (B&H: 39.79%, Scale=0.164)
‚úÖ LYV: Training data validated - 205 rows over 365 days
    üìä HMY 3- Training: Horizon=60d, Target=6.53% (B&H: 39.71%, Scale=0.164)
‚úÖ HMY: Training data validated - 205 rows over 365 days
    üìä FTI 3- Training: Horizon=60d, Target=6.52% (B&H: 39.68%, Scale=0.164)
‚úÖ FTI: Training data validated - 205 rows over 365 days
    üìä USD 3- Training: Horizon=60d, Target=6.51% (B&H: 39.60%, Scale=0.164)
‚úÖ USD: Training data validated - 205 rows over 365 days
    üìä TEL 3- Training: Horizon=60d, Target=6.51% (B&H: 39.58%, Scale=0.164)
‚úÖ TEL: Training data validated - 205 rows over 365 days
    üìä RJF 3- Training: Horizon=60d, Target=6.50% (B&H: 39.53%, Scale=0.164)
‚úÖ RJF: Training data validated - 205 rows over 365 days
    üìä MAIN 3- Training: Horizon=60d, Target=6.50% (B&H: 39.52%, Scale=0.164)
‚úÖ MAIN: Training data validated - 205 rows over 365 days
    üìä DOCS 3- Training: Horizon=60d, Target=6.49% (B&H: 39.48%, Scale=0.164)
‚úÖ DOCS: Training data validated - 205 rows over 365 days
    üìä ATAT 3- Training: Horizon=60d, Target=6.49% (B&H: 39.46%, Scale=0.164)
‚úÖ ATAT: Training data validated - 205 rows over 365 days
    üìä BYRN 3- Training: Horizon=60d, Target=6.47% (B&H: 39.36%, Scale=0.164)
‚úÖ BYRN: Training data validated - 205 rows over 365 days
    üìä CX 3- Training: Horizon=60d, Target=6.46% (B&H: 39.29%, Scale=0.164)
‚úÖ CX: Training data validated - 205 rows over 365 days
    üìä UNM 3- Training: Horizon=60d, Target=6.42% (B&H: 39.07%, Scale=0.164)
‚úÖ UNM: Training data validated - 205 rows over 365 days
    üìä EUFN 3- Training: Horizon=60d, Target=6.39% (B&H: 38.84%, Scale=0.164)
‚úÖ EUFN: Training data validated - 205 rows over 365 days
    üìä PLNT 3- Training: Horizon=60d, Target=6.38% (B&H: 38.82%, Scale=0.164)
‚úÖ PLNT: Training data validated - 205 rows over 365 days
    üìä CSV 3- Training: Horizon=60d, Target=6.38% (B&H: 38.81%, Scale=0.164)
‚úÖ CSV: Training data validated - 205 rows over 365 days
    üìä TMV 3- Training: Horizon=60d, Target=6.38% (B&H: 38.78%, Scale=0.164)
‚úÖ TMV: Training data validated - 205 rows over 365 days
    üìä MCB 3- Training: Horizon=60d, Target=6.37% (B&H: 38.78%, Scale=0.164)
‚úÖ MCB: Training data validated - 205 rows over 365 days
    üìä UAE 3- Training: Horizon=60d, Target=6.37% (B&H: 38.75%, Scale=0.164)
‚úÖ UAE: Training data validated - 205 rows over 365 days
    üìä PJT 3- Training: Horizon=60d, Target=6.37% (B&H: 38.74%, Scale=0.164)
‚úÖ PJT: Training data validated - 205 rows over 365 days
    üìä HNST 3- Training: Horizon=60d, Target=6.35% (B&H: 38.66%, Scale=0.164)
‚úÖ HNST: Training data validated - 205 rows over 365 days
    üìä MBI 3- Training: Horizon=60d, Target=6.35% (B&H: 38.66%, Scale=0.164)
‚úÖ MBI: Training data validated - 205 rows over 365 days
    üìä EMR 3- Training: Horizon=60d, Target=6.34% (B&H: 38.56%, Scale=0.164)
‚úÖ EMR: Training data validated - 205 rows over 365 days
    üìä ING 3- Training: Horizon=60d, Target=6.34% (B&H: 38.55%, Scale=0.164)
‚úÖ ING: Training data validated - 205 rows over 365 days
    üìä TMAT 3- Training: Horizon=60d, Target=6.31% (B&H: 38.39%, Scale=0.164)
‚úÖ TMAT: Training data validated - 205 rows over 365 days
    üìä VEEV 3- Training: Horizon=60d, Target=6.31% (B&H: 38.38%, Scale=0.164)
‚úÖ VEEV: Training data validated - 205 rows over 365 days
    üìä GTLS 3- Training: Horizon=60d, Target=6.31% (B&H: 38.38%, Scale=0.164)
‚úÖ GTLS: Training data validated - 205 rows over 365 days
    üìä VRT 3- Training: Horizon=60d, Target=6.30% (B&H: 38.32%, Scale=0.164)
‚úÖ VRT: Training data validated - 205 rows over 365 days
    üìä TKO 3- Training: Horizon=60d, Target=6.27% (B&H: 38.15%, Scale=0.164)
‚úÖ TKO: Training data validated - 205 rows over 365 days
    üìä PUK 3- Training: Horizon=60d, Target=6.27% (B&H: 38.14%, Scale=0.164)
‚úÖ PUK: Training data validated - 205 rows over 365 days
    üìä SPNT 3- Training: Horizon=60d, Target=6.25% (B&H: 38.01%, Scale=0.164)
‚úÖ SPNT: Training data validated - 205 rows over 365 days
    üìä WT 3- Training: Horizon=60d, Target=6.25% (B&H: 38.00%, Scale=0.164)
‚úÖ WT: Training data validated - 205 rows over 365 days
    üìä DDS 3- Training: Horizon=60d, Target=6.24% (B&H: 37.93%, Scale=0.164)
‚úÖ DDS: Training data validated - 205 rows over 365 days
    üìä IAI 3- Training: Horizon=60d, Target=6.24% (B&H: 37.93%, Scale=0.164)
‚úÖ IAI: Training data validated - 205 rows over 365 days
    üìä PGNY 3- Training: Horizon=60d, Target=6.23% (B&H: 37.89%, Scale=0.164)
‚úÖ PGNY: Training data validated - 205 rows over 365 days
    üìä SGI 3- Training: Horizon=60d, Target=6.22% (B&H: 37.83%, Scale=0.164)
‚úÖ SGI: Training data validated - 205 rows over 365 days
    üìä STN 3- Training: Horizon=60d, Target=6.21% (B&H: 37.77%, Scale=0.164)
‚úÖ STN: Training data validated - 205 rows over 365 days
    üìä ETHE 3- Training: Horizon=60d, Target=6.20% (B&H: 37.73%, Scale=0.164)
‚úÖ ETHE: Training data validated - 205 rows over 365 days
    üìä FAS 3- Training: Horizon=60d, Target=6.20% (B&H: 37.69%, Scale=0.164)
‚úÖ FAS: Training data validated - 205 rows over 365 days
    üìä BAP 3- Training: Horizon=60d, Target=6.19% (B&H: 37.66%, Scale=0.164)
‚úÖ BAP: Training data validated - 205 rows over 365 days
    üìä OLLI 3- Training: Horizon=60d, Target=6.18% (B&H: 37.58%, Scale=0.164)
‚úÖ OLLI: Training data validated - 205 rows over 365 days
    üìä NRGV 3- Training: Horizon=60d, Target=6.16% (B&H: 37.50%, Scale=0.164)
‚úÖ NRGV: Training data validated - 205 rows over 365 days
    üìä KLTR 3- Training: Horizon=60d, Target=6.16% (B&H: 37.50%, Scale=0.164)
‚úÖ KLTR: Training data validated - 205 rows over 365 days
    üìä MTW 3- Training: Horizon=60d, Target=6.15% (B&H: 37.42%, Scale=0.164)
‚úÖ MTW: Training data validated - 205 rows over 365 days
    üìä DCO 3- Training: Horizon=60d, Target=6.15% (B&H: 37.40%, Scale=0.164)
‚úÖ DCO: Training data validated - 205 rows over 365 days
    üìä TRMB 3- Training: Horizon=60d, Target=6.13% (B&H: 37.28%, Scale=0.164)
‚úÖ TRMB: Training data validated - 205 rows over 365 days
    üìä KT 3- Training: Horizon=60d, Target=6.13% (B&H: 37.28%, Scale=0.164)
‚úÖ KT: Training data validated - 205 rows over 365 days
    üìä FHI 3- Training: Horizon=60d, Target=6.11% (B&H: 37.18%, Scale=0.164)
‚úÖ FHI: Training data validated - 205 rows over 365 days
    üìä GDX 3- Training: Horizon=60d, Target=6.10% (B&H: 37.10%, Scale=0.164)
‚úÖ GDX: Training data validated - 205 rows over 365 days
    üìä BWXT 3- Training: Horizon=60d, Target=6.09% (B&H: 37.04%, Scale=0.164)
‚úÖ BWXT: Training data validated - 205 rows over 365 days
    üìä ETR 3- Training: Horizon=60d, Target=6.09% (B&H: 37.03%, Scale=0.164)
‚úÖ ETR: Training data validated - 205 rows over 365 days
    üìä CGNT 3- Training: Horizon=60d, Target=6.08% (B&H: 36.97%, Scale=0.164)
‚úÖ CGNT: Training data validated - 205 rows over 365 days
    üìä SAH 3- Training: Horizon=60d, Target=6.06% (B&H: 36.89%, Scale=0.164)
‚úÖ SAH: Training data validated - 205 rows over 365 days
    üìä FPX 3- Training: Horizon=60d, Target=6.06% (B&H: 36.86%, Scale=0.164)
‚úÖ FPX: Training data validated - 205 rows over 365 days
    üìä TBPH 3- Training: Horizon=60d, Target=6.06% (B&H: 36.85%, Scale=0.164)
‚úÖ TBPH: Training data validated - 205 rows over 365 days
    üìä RBA 3- Training: Horizon=60d, Target=6.04% (B&H: 36.73%, Scale=0.164)
‚úÖ RBA: Training data validated - 205 rows over 365 days
    üìä TCBX 3- Training: Horizon=60d, Target=6.04% (B&H: 36.72%, Scale=0.164)
‚úÖ TCBX: Training data validated - 205 rows over 365 days
    üìä CHAT 3- Training: Horizon=60d, Target=6.03% (B&H: 36.69%, Scale=0.164)
‚úÖ CHAT: Training data validated - 205 rows over 365 days
    üìä WWW 3- Training: Horizon=60d, Target=6.00% (B&H: 36.52%, Scale=0.164)
‚úÖ WWW: Training data validated - 205 rows over 365 days
    üìä METV 3- Training: Horizon=60d, Target=5.98% (B&H: 36.40%, Scale=0.164)
‚úÖ METV: Training data validated - 205 rows over 365 days
    üìä BAM 3- Training: Horizon=60d, Target=5.96% (B&H: 36.28%, Scale=0.164)
‚úÖ BAM: Training data validated - 205 rows over 365 days
    üìä PM 3- Training: Horizon=60d, Target=5.96% (B&H: 36.23%, Scale=0.164)
‚úÖ PM: Training data validated - 205 rows over 365 days
    üìä AVAV 3- Training: Horizon=60d, Target=5.93% (B&H: 36.10%, Scale=0.164)
‚úÖ AVAV: Training data validated - 205 rows over 365 days
    üìä OKTA 3- Training: Horizon=60d, Target=5.92% (B&H: 36.00%, Scale=0.164)
‚úÖ OKTA: Training data validated - 205 rows over 365 days
    üìä EFXT 3- Training: Horizon=60d, Target=5.91% (B&H: 35.96%, Scale=0.164)
‚úÖ EFXT: Training data validated - 205 rows over 365 days
    üìä FFIV 3- Training: Horizon=60d, Target=5.91% (B&H: 35.95%, Scale=0.164)
‚úÖ FFIV: Training data validated - 205 rows over 365 days
    üìä FAST 3- Training: Horizon=60d, Target=5.90% (B&H: 35.91%, Scale=0.164)
‚úÖ FAST: Training data validated - 205 rows over 365 days
    üìä FBIO 3- Training: Horizon=60d, Target=5.90% (B&H: 35.86%, Scale=0.164)
‚úÖ FBIO: Training data validated - 205 rows over 365 days
    üìä GENI 3- Training: Horizon=60d, Target=5.87% (B&H: 35.71%, Scale=0.164)
‚úÖ GENI: Training data validated - 205 rows over 365 days
    üìä LGND 3- Training: Horizon=60d, Target=5.87% (B&H: 35.71%, Scale=0.164)
‚úÖ LGND: Training data validated - 205 rows over 365 days
    üìä IGIC 3- Training: Horizon=60d, Target=5.87% (B&H: 35.71%, Scale=0.164)
‚úÖ IGIC: Training data validated - 205 rows over 365 days
    üìä NGVC 3- Training: Horizon=60d, Target=5.87% (B&H: 35.69%, Scale=0.164)
‚úÖ NGVC: Training data validated - 205 rows over 365 days
    üìä HLF 3- Training: Horizon=60d, Target=5.85% (B&H: 35.61%, Scale=0.164)
‚úÖ HLF: Training data validated - 205 rows over 365 days
    üìä MTA 3- Training: Horizon=60d, Target=5.85% (B&H: 35.60%, Scale=0.164)
‚úÖ MTA: Training data validated - 205 rows over 365 days
    üìä GRMN 3- Training: Horizon=60d, Target=5.82% (B&H: 35.41%, Scale=0.164)
‚úÖ GRMN: Training data validated - 205 rows over 365 days
    üìä OSW 3- Training: Horizon=60d, Target=5.82% (B&H: 35.41%, Scale=0.164)
‚úÖ OSW: Training data validated - 205 rows over 365 days
    üìä EMBJ 3- Training: Horizon=60d, Target=5.82% (B&H: 35.39%, Scale=0.164)
‚úÖ EMBJ: Training data validated - 205 rows over 365 days
    üìä ROK 3- Training: Horizon=60d, Target=5.81% (B&H: 35.33%, Scale=0.164)
‚úÖ ROK: Training data validated - 205 rows over 365 days
    üìä ISRA 3- Training: Horizon=60d, Target=5.79% (B&H: 35.24%, Scale=0.164)
‚úÖ ISRA: Training data validated - 205 rows over 365 days
    üìä FTNT 3- Training: Horizon=60d, Target=5.78% (B&H: 35.16%, Scale=0.164)
‚úÖ FTNT: Training data validated - 205 rows over 365 days
    üìä MEXX 3- Training: Horizon=60d, Target=5.77% (B&H: 35.09%, Scale=0.164)
‚úÖ MEXX: Training data validated - 205 rows over 365 days
    üìä FOX 3- Training: Horizon=60d, Target=5.76% (B&H: 35.02%, Scale=0.164)
‚úÖ FOX: Training data validated - 205 rows over 365 days
    üìä TOYO 3- Training: Horizon=60d, Target=5.76% (B&H: 35.02%, Scale=0.164)
‚úÖ TOYO: Training data validated - 205 rows over 365 days
    üìä BBCP 3- Training: Horizon=60d, Target=5.74% (B&H: 34.90%, Scale=0.164)
‚úÖ BBCP: Training data validated - 205 rows over 365 days
    üìä CDE 3- Training: Horizon=60d, Target=5.73% (B&H: 34.88%, Scale=0.164)
‚úÖ CDE: Training data validated - 205 rows over 365 days
    üìä WRLD 3- Training: Horizon=60d, Target=5.73% (B&H: 34.84%, Scale=0.164)
‚úÖ WRLD: Training data validated - 205 rows over 365 days
    üìä STRK 3- Training: Horizon=60d, Target=5.73% (B&H: 34.83%, Scale=0.164)
  ‚ùå STRK: Too many NaN values in Close price: 85 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä AX 3- Training: Horizon=60d, Target=5.71% (B&H: 34.73%, Scale=0.164)
‚úÖ AX: Training data validated - 205 rows over 365 days
    üìä IIIV 3- Training: Horizon=60d, Target=5.69% (B&H: 34.63%, Scale=0.164)
‚úÖ IIIV: Training data validated - 205 rows over 365 days
    üìä TR 3- Training: Horizon=60d, Target=5.68% (B&H: 34.55%, Scale=0.164)
‚úÖ TR: Training data validated - 205 rows over 365 days
    üìä BLX 3- Training: Horizon=60d, Target=5.68% (B&H: 34.55%, Scale=0.164)
‚úÖ BLX: Training data validated - 205 rows over 365 days
    üìä CBNK 3- Training: Horizon=60d, Target=5.68% (B&H: 34.53%, Scale=0.164)
‚úÖ CBNK: Training data validated - 205 rows over 365 days
    üìä FOXA 3- Training: Horizon=60d, Target=5.67% (B&H: 34.52%, Scale=0.164)
‚úÖ FOXA: Training data validated - 205 rows over 365 days
    üìä LANV 3- Training: Horizon=60d, Target=5.67% (B&H: 34.50%, Scale=0.164)
‚úÖ LANV: Training data validated - 205 rows over 365 days
    üìä BFH 3- Training: Horizon=60d, Target=5.66% (B&H: 34.46%, Scale=0.164)
‚úÖ BFH: Training data validated - 205 rows over 365 days
    üìä BKNG 3- Training: Horizon=60d, Target=5.66% (B&H: 34.44%, Scale=0.164)
‚úÖ BKNG: Training data validated - 205 rows over 365 days
    üìä IAG 3- Training: Horizon=60d, Target=5.66% (B&H: 34.42%, Scale=0.164)
‚úÖ IAG: Training data validated - 205 rows over 365 days
    üìä ELPC 3- Training: Horizon=60d, Target=5.65% (B&H: 34.36%, Scale=0.164)
‚úÖ ELPC: Training data validated - 205 rows over 365 days
    üìä LOUP 3- Training: Horizon=60d, Target=5.64% (B&H: 34.32%, Scale=0.164)
‚úÖ LOUP: Training data validated - 205 rows over 365 days
    üìä CFLT 3- Training: Horizon=60d, Target=5.62% (B&H: 34.20%, Scale=0.164)
‚úÖ CFLT: Training data validated - 205 rows over 365 days
    üìä FDD 3- Training: Horizon=60d, Target=5.62% (B&H: 34.18%, Scale=0.164)
‚úÖ FDD: Training data validated - 205 rows over 365 days
    üìä GLXY 3- Training: Horizon=60d, Target=5.62% (B&H: 34.17%, Scale=0.164)
  ‚ùå GLXY: Too many NaN values in Close price: 157 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä MEDP 3- Training: Horizon=60d, Target=5.60% (B&H: 34.06%, Scale=0.164)
‚úÖ MEDP: Training data validated - 205 rows over 365 days
    üìä ARLO 3- Training: Horizon=60d, Target=5.59% (B&H: 34.02%, Scale=0.164)
‚úÖ ARLO: Training data validated - 205 rows over 365 days
    üìä COCO 3- Training: Horizon=60d, Target=5.58% (B&H: 33.95%, Scale=0.164)
‚úÖ COCO: Training data validated - 205 rows over 365 days
    üìä BYD 3- Training: Horizon=60d, Target=5.57% (B&H: 33.91%, Scale=0.164)
‚úÖ BYD: Training data validated - 205 rows over 365 days
    üìä AEIS 3- Training: Horizon=60d, Target=5.57% (B&H: 33.88%, Scale=0.164)
‚úÖ AEIS: Training data validated - 205 rows over 365 days
    üìä AZZ 3- Training: Horizon=60d, Target=5.57% (B&H: 33.87%, Scale=0.164)
‚úÖ AZZ: Training data validated - 205 rows over 365 days
    üìä ECNS 3- Training: Horizon=60d, Target=5.55% (B&H: 33.78%, Scale=0.164)
‚úÖ ECNS: Training data validated - 205 rows over 365 days
    üìä USFD 3- Training: Horizon=60d, Target=5.55% (B&H: 33.76%, Scale=0.164)
‚úÖ USFD: Training data validated - 205 rows over 365 days
    üìä OMF 3- Training: Horizon=60d, Target=5.55% (B&H: 33.75%, Scale=0.164)
‚úÖ OMF: Training data validated - 205 rows over 365 days
    üìä MRUS 3- Training: Horizon=60d, Target=5.55% (B&H: 33.75%, Scale=0.164)
‚úÖ MRUS: Training data validated - 205 rows over 365 days
    üìä TXNM 3- Training: Horizon=60d, Target=5.55% (B&H: 33.74%, Scale=0.164)
‚úÖ TXNM: Training data validated - 205 rows over 365 days
    üìä XPP 3- Training: Horizon=60d, Target=5.54% (B&H: 33.69%, Scale=0.164)
‚úÖ XPP: Training data validated - 205 rows over 365 days
    üìä EWP 3- Training: Horizon=60d, Target=5.53% (B&H: 33.64%, Scale=0.164)
‚úÖ EWP: Training data validated - 205 rows over 365 days
    üìä WDC 3- Training: Horizon=60d, Target=5.52% (B&H: 33.58%, Scale=0.164)
‚úÖ WDC: Training data validated - 205 rows over 365 days
    üìä PAC 3- Training: Horizon=60d, Target=5.51% (B&H: 33.55%, Scale=0.164)
‚úÖ PAC: Training data validated - 205 rows over 365 days
    üìä T 3- Training: Horizon=60d, Target=5.47% (B&H: 33.25%, Scale=0.164)
‚úÖ T: Training data validated - 205 rows over 365 days
    üìä ITA 3- Training: Horizon=60d, Target=5.46% (B&H: 33.23%, Scale=0.164)
‚úÖ ITA: Training data validated - 205 rows over 365 days
    üìä QUAD 3- Training: Horizon=60d, Target=5.46% (B&H: 33.20%, Scale=0.164)
‚úÖ QUAD: Training data validated - 205 rows over 365 days
    üìä LENZ 3- Training: Horizon=60d, Target=5.46% (B&H: 33.19%, Scale=0.164)
‚úÖ LENZ: Training data validated - 205 rows over 365 days
    üìä PRLB 3- Training: Horizon=60d, Target=5.45% (B&H: 33.16%, Scale=0.164)
‚úÖ PRLB: Training data validated - 205 rows over 365 days
    üìä SHAK 3- Training: Horizon=60d, Target=5.44% (B&H: 33.12%, Scale=0.164)
‚úÖ SHAK: Training data validated - 205 rows over 365 days
    üìä FNGS 3- Training: Horizon=60d, Target=5.43% (B&H: 33.05%, Scale=0.164)
‚úÖ FNGS: Training data validated - 205 rows over 365 days
    üìä MTRX 3- Training: Horizon=60d, Target=5.43% (B&H: 33.04%, Scale=0.164)
‚úÖ MTRX: Training data validated - 205 rows over 365 days
    üìä MCRI 3- Training: Horizon=60d, Target=5.43% (B&H: 33.01%, Scale=0.164)
‚úÖ MCRI: Training data validated - 205 rows over 365 days
    üìä STVN 3- Training: Horizon=60d, Target=5.41% (B&H: 32.92%, Scale=0.164)
‚úÖ STVN: Training data validated - 205 rows over 365 days
    üìä CNP 3- Training: Horizon=60d, Target=5.41% (B&H: 32.91%, Scale=0.164)
‚úÖ CNP: Training data validated - 205 rows over 365 days
    üìä MIRM 3- Training: Horizon=60d, Target=5.41% (B&H: 32.90%, Scale=0.164)
‚úÖ MIRM: Training data validated - 205 rows over 365 days
    üìä CUBI 3- Training: Horizon=60d, Target=5.39% (B&H: 32.81%, Scale=0.164)
‚úÖ CUBI: Training data validated - 205 rows over 365 days
    üìä DXCM 3- Training: Horizon=60d, Target=5.38% (B&H: 32.74%, Scale=0.164)
‚úÖ DXCM: Training data validated - 205 rows over 365 days
    üìä RAMP 3- Training: Horizon=60d, Target=5.37% (B&H: 32.65%, Scale=0.164)
‚úÖ RAMP: Training data validated - 205 rows over 365 days
    üìä FWONK 3- Training: Horizon=60d, Target=5.36% (B&H: 32.62%, Scale=0.164)
‚úÖ FWONK: Training data validated - 205 rows over 365 days
    üìä CSCO 3- Training: Horizon=60d, Target=5.36% (B&H: 32.61%, Scale=0.164)
‚úÖ CSCO: Training data validated - 205 rows over 365 days
    üìä CTRI 3- Training: Horizon=60d, Target=5.35% (B&H: 32.57%, Scale=0.164)
‚úÖ CTRI: Training data validated - 205 rows over 365 days
    üìä RPRX 3- Training: Horizon=60d, Target=5.33% (B&H: 32.44%, Scale=0.164)
‚úÖ RPRX: Training data validated - 205 rows over 365 days
    üìä COLO 3- Training: Horizon=60d, Target=5.33% (B&H: 32.42%, Scale=0.164)
‚úÖ COLO: Training data validated - 205 rows over 365 days
    üìä ULTA 3- Training: Horizon=60d, Target=5.32% (B&H: 32.34%, Scale=0.164)
‚úÖ ULTA: Training data validated - 205 rows over 365 days
    üìä FNV 3- Training: Horizon=60d, Target=5.28% (B&H: 32.10%, Scale=0.164)
‚úÖ FNV: Training data validated - 205 rows over 365 days
    üìä GSAT 3- Training: Horizon=60d, Target=5.28% (B&H: 32.10%, Scale=0.164)
‚úÖ GSAT: Training data validated - 205 rows over 365 days
    üìä SKYW 3- Training: Horizon=60d, Target=5.27% (B&H: 32.09%, Scale=0.164)
‚úÖ SKYW: Training data validated - 205 rows over 365 days
    üìä TASK 3- Training: Horizon=60d, Target=5.27% (B&H: 32.04%, Scale=0.164)
‚úÖ TASK: Training data validated - 205 rows over 365 days
    üìä DSP 3- Training: Horizon=60d, Target=5.24% (B&H: 31.89%, Scale=0.164)
‚úÖ DSP: Training data validated - 205 rows over 365 days
    üìä STT 3- Training: Horizon=60d, Target=5.23% (B&H: 31.81%, Scale=0.164)
‚úÖ STT: Training data validated - 205 rows over 365 days
    üìä LIND 3- Training: Horizon=60d, Target=5.22% (B&H: 31.78%, Scale=0.164)
‚úÖ LIND: Training data validated - 205 rows over 365 days
    üìä CME 3- Training: Horizon=60d, Target=5.21% (B&H: 31.71%, Scale=0.164)
‚úÖ CME: Training data validated - 205 rows over 365 days
    üìä AB 3- Training: Horizon=60d, Target=5.20% (B&H: 31.66%, Scale=0.164)
‚úÖ AB: Training data validated - 205 rows over 365 days
    üìä SEIC 3- Training: Horizon=60d, Target=5.19% (B&H: 31.57%, Scale=0.164)
‚úÖ SEIC: Training data validated - 205 rows over 365 days
    üìä WBS 3- Training: Horizon=60d, Target=5.19% (B&H: 31.56%, Scale=0.164)
‚úÖ WBS: Training data validated - 205 rows over 365 days
    üìä PAHC 3- Training: Horizon=60d, Target=5.18% (B&H: 31.53%, Scale=0.164)
‚úÖ PAHC: Training data validated - 205 rows over 365 days
    üìä DAX 3- Training: Horizon=60d, Target=5.18% (B&H: 31.52%, Scale=0.164)
‚úÖ DAX: Training data validated - 205 rows over 365 days
    üìä HERO 3- Training: Horizon=60d, Target=5.18% (B&H: 31.51%, Scale=0.164)
‚úÖ HERO: Training data validated - 205 rows over 365 days
    üìä RTX 3- Training: Horizon=60d, Target=5.18% (B&H: 31.50%, Scale=0.164)
‚úÖ RTX: Training data validated - 205 rows over 365 days
    üìä EWI 3- Training: Horizon=60d, Target=5.16% (B&H: 31.41%, Scale=0.164)
‚úÖ EWI: Training data validated - 205 rows over 365 days
    üìä BMO 3- Training: Horizon=60d, Target=5.16% (B&H: 31.41%, Scale=0.164)
‚úÖ BMO: Training data validated - 205 rows over 365 days
    üìä ECVT 3- Training: Horizon=60d, Target=5.14% (B&H: 31.24%, Scale=0.164)
‚úÖ ECVT: Training data validated - 205 rows over 365 days
    üìä EETH 3- Training: Horizon=60d, Target=5.13% (B&H: 31.18%, Scale=0.164)
‚úÖ EETH: Training data validated - 205 rows over 365 days
    üìä EYPT 3- Training: Horizon=60d, Target=5.12% (B&H: 31.16%, Scale=0.164)
‚úÖ EYPT: Training data validated - 205 rows over 365 days
    üìä CMCM 3- Training: Horizon=60d, Target=5.12% (B&H: 31.15%, Scale=0.164)
‚úÖ CMCM: Training data validated - 205 rows over 365 days
    üìä NDAQ 3- Training: Horizon=60d, Target=5.12% (B&H: 31.13%, Scale=0.164)
‚úÖ NDAQ: Training data validated - 205 rows over 365 days
    üìä OGIG 3- Training: Horizon=60d, Target=5.11% (B&H: 31.11%, Scale=0.164)
‚úÖ OGIG: Training data validated - 205 rows over 365 days
    üìä CEPU 3- Training: Horizon=60d, Target=5.11% (B&H: 31.10%, Scale=0.164)
‚úÖ CEPU: Training data validated - 205 rows over 365 days
    üìä PAR 3- Training: Horizon=60d, Target=5.11% (B&H: 31.07%, Scale=0.164)
‚úÖ PAR: Training data validated - 205 rows over 365 days
    üìä LTL 3- Training: Horizon=60d, Target=5.10% (B&H: 31.05%, Scale=0.164)
‚úÖ LTL: Training data validated - 205 rows over 365 days
    üìä THFF 3- Training: Horizon=60d, Target=5.08% (B&H: 30.92%, Scale=0.164)
‚úÖ THFF: Training data validated - 205 rows over 365 days
    üìä SMMT 3- Training: Horizon=60d, Target=5.08% (B&H: 30.91%, Scale=0.164)
‚úÖ SMMT: Training data validated - 205 rows over 365 days
    üìä IMAX 3- Training: Horizon=60d, Target=5.08% (B&H: 30.91%, Scale=0.164)
‚úÖ IMAX: Training data validated - 205 rows over 365 days
    üìä SMCI 3- Training: Horizon=60d, Target=5.06% (B&H: 30.81%, Scale=0.164)
‚úÖ SMCI: Training data validated - 205 rows over 365 days
    üìä BGM 3- Training: Horizon=60d, Target=5.06% (B&H: 30.79%, Scale=0.164)
‚úÖ BGM: Training data validated - 205 rows over 365 days
    üìä NG 3- Training: Horizon=60d, Target=5.05% (B&H: 30.73%, Scale=0.164)
‚úÖ NG: Training data validated - 205 rows over 365 days
    üìä FN 3- Training: Horizon=60d, Target=5.04% (B&H: 30.64%, Scale=0.164)
‚úÖ FN: Training data validated - 205 rows over 365 days
    üìä UFCS 3- Training: Horizon=60d, Target=5.04% (B&H: 30.63%, Scale=0.164)
‚úÖ UFCS: Training data validated - 205 rows over 365 days
    üìä NOTV 3- Training: Horizon=60d, Target=5.03% (B&H: 30.59%, Scale=0.164)
‚úÖ NOTV: Training data validated - 205 rows over 365 days
    üìä DG 3- Training: Horizon=60d, Target=5.03% (B&H: 30.58%, Scale=0.164)
‚úÖ DG: Training data validated - 205 rows over 365 days
    üìä SMFG 3- Training: Horizon=60d, Target=5.03% (B&H: 30.58%, Scale=0.164)
‚úÖ SMFG: Training data validated - 205 rows over 365 days
    üìä CCEP 3- Training: Horizon=60d, Target=5.03% (B&H: 30.57%, Scale=0.164)
‚úÖ CCEP: Training data validated - 205 rows over 365 days
    üìä MT 3- Training: Horizon=60d, Target=5.02% (B&H: 30.56%, Scale=0.164)
‚úÖ MT: Training data validated - 205 rows over 365 days
    üìä V 3- Training: Horizon=60d, Target=5.02% (B&H: 30.54%, Scale=0.164)
‚úÖ V: Training data validated - 205 rows over 365 days
    üìä COMM 3- Training: Horizon=60d, Target=5.00% (B&H: 30.44%, Scale=0.164)
‚úÖ COMM: Training data validated - 205 rows over 365 days
    üìä AGQ 3- Training: Horizon=60d, Target=5.00% (B&H: 30.42%, Scale=0.164)
‚úÖ AGQ: Training data validated - 205 rows over 365 days
    üìä GOLF 3- Training: Horizon=60d, Target=5.00% (B&H: 30.41%, Scale=0.164)
‚úÖ GOLF: Training data validated - 205 rows over 365 days
    üìä HCI 3- Training: Horizon=60d, Target=5.00% (B&H: 30.39%, Scale=0.164)
‚úÖ HCI: Training data validated - 205 rows over 365 days
    üìä DTM 3- Training: Horizon=60d, Target=4.99% (B&H: 30.38%, Scale=0.164)
‚úÖ DTM: Training data validated - 205 rows over 365 days
    üìä KBWB 3- Training: Horizon=60d, Target=4.99% (B&H: 30.33%, Scale=0.164)
‚úÖ KBWB: Training data validated - 205 rows over 365 days
    üìä APLD 3- Training: Horizon=60d, Target=4.98% (B&H: 30.30%, Scale=0.164)
‚úÖ APLD: Training data validated - 205 rows over 365 days
    üìä NPKI 3- Training: Horizon=60d, Target=4.98% (B&H: 30.30%, Scale=0.164)
‚úÖ NPKI: Training data validated - 205 rows over 365 days
    üìä OSK 3- Training: Horizon=60d, Target=4.97% (B&H: 30.24%, Scale=0.164)
‚úÖ OSK: Training data validated - 205 rows over 365 days
    üìä DDOG 3- Training: Horizon=60d, Target=4.97% (B&H: 30.23%, Scale=0.164)
‚úÖ DDOG: Training data validated - 205 rows over 365 days
    üìä VCTR 3- Training: Horizon=60d, Target=4.97% (B&H: 30.22%, Scale=0.164)
‚úÖ VCTR: Training data validated - 205 rows over 365 days
    üìä SFST 3- Training: Horizon=60d, Target=4.96% (B&H: 30.19%, Scale=0.164)
‚úÖ SFST: Training data validated - 205 rows over 365 days
    üìä AGI 3- Training: Horizon=60d, Target=4.96% (B&H: 30.16%, Scale=0.164)
‚úÖ AGI: Training data validated - 205 rows over 365 days
    üìä HAFC 3- Training: Horizon=60d, Target=4.95% (B&H: 30.10%, Scale=0.164)
‚úÖ HAFC: Training data validated - 205 rows over 365 days
    üìä FEP 3- Training: Horizon=60d, Target=4.94% (B&H: 30.05%, Scale=0.164)
‚úÖ FEP: Training data validated - 205 rows over 365 days
    üìä WMB 3- Training: Horizon=60d, Target=4.93% (B&H: 29.99%, Scale=0.164)
‚úÖ WMB: Training data validated - 205 rows over 365 days
    üìä ATEN 3- Training: Horizon=60d, Target=4.92% (B&H: 29.94%, Scale=0.164)
‚úÖ ATEN: Training data validated - 205 rows over 365 days
    üìä RZLT 3- Training: Horizon=60d, Target=4.91% (B&H: 29.90%, Scale=0.164)
‚úÖ RZLT: Training data validated - 205 rows over 365 days
    üìä AL 3- Training: Horizon=60d, Target=4.91% (B&H: 29.87%, Scale=0.164)
‚úÖ AL: Training data validated - 205 rows over 365 days
    üìä COR 3- Training: Horizon=60d, Target=4.90% (B&H: 29.84%, Scale=0.164)
‚úÖ COR: Training data validated - 205 rows over 365 days
    üìä ONC 3- Training: Horizon=60d, Target=4.90% (B&H: 29.82%, Scale=0.164)
‚úÖ ONC: Training data validated - 205 rows over 365 days
    üìä NEU 3- Training: Horizon=60d, Target=4.89% (B&H: 29.76%, Scale=0.164)
‚úÖ NEU: Training data validated - 205 rows over 365 days
    üìä YINN 3- Training: Horizon=60d, Target=4.89% (B&H: 29.73%, Scale=0.164)
‚úÖ YINN: Training data validated - 205 rows over 365 days
    üìä RBC 3- Training: Horizon=60d, Target=4.89% (B&H: 29.72%, Scale=0.164)
‚úÖ RBC: Training data validated - 205 rows over 365 days
    üìä PRDO 3- Training: Horizon=60d, Target=4.88% (B&H: 29.71%, Scale=0.164)
‚úÖ PRDO: Training data validated - 205 rows over 365 days
    üìä CSGS 3- Training: Horizon=60d, Target=4.88% (B&H: 29.69%, Scale=0.164)
‚úÖ CSGS: Training data validated - 205 rows over 365 days
    üìä EWS 3- Training: Horizon=60d, Target=4.87% (B&H: 29.62%, Scale=0.164)
‚úÖ EWS: Training data validated - 205 rows over 365 days
    üìä FWONA 3- Training: Horizon=60d, Target=4.87% (B&H: 29.60%, Scale=0.164)
‚úÖ FWONA: Training data validated - 205 rows over 365 days
    üìä BKR 3- Training: Horizon=60d, Target=4.85% (B&H: 29.53%, Scale=0.164)
‚úÖ BKR: Training data validated - 205 rows over 365 days
    üìä NMG 3- Training: Horizon=60d, Target=4.82% (B&H: 29.30%, Scale=0.164)
‚úÖ NMG: Training data validated - 205 rows over 365 days
    üìä MKL 3- Training: Horizon=60d, Target=4.80% (B&H: 29.21%, Scale=0.164)
‚úÖ MKL: Training data validated - 205 rows over 365 days
    üìä NTR 3- Training: Horizon=60d, Target=4.80% (B&H: 29.18%, Scale=0.164)
‚úÖ NTR: Training data validated - 205 rows over 365 days
    üìä SONY 3- Training: Horizon=60d, Target=4.80% (B&H: 29.18%, Scale=0.164)
‚úÖ SONY: Training data validated - 205 rows over 365 days
    üìä BETZ 3- Training: Horizon=60d, Target=4.79% (B&H: 29.11%, Scale=0.164)
‚úÖ BETZ: Training data validated - 205 rows over 365 days
    üìä DUOL 3- Training: Horizon=60d, Target=4.78% (B&H: 29.10%, Scale=0.164)
‚úÖ DUOL: Training data validated - 205 rows over 365 days
    üìä AWI 3- Training: Horizon=60d, Target=4.77% (B&H: 29.02%, Scale=0.164)
‚úÖ AWI: Training data validated - 205 rows over 365 days
    üìä BEN 3- Training: Horizon=60d, Target=4.76% (B&H: 28.98%, Scale=0.164)
‚úÖ BEN: Training data validated - 205 rows over 365 days
    üìä DAKT 3- Training: Horizon=60d, Target=4.76% (B&H: 28.97%, Scale=0.164)
‚úÖ DAKT: Training data validated - 205 rows over 365 days
    üìä DBD 3- Training: Horizon=60d, Target=4.76% (B&H: 28.95%, Scale=0.164)
‚úÖ DBD: Training data validated - 205 rows over 365 days
    üìä NPO 3- Training: Horizon=60d, Target=4.75% (B&H: 28.88%, Scale=0.164)
‚úÖ NPO: Training data validated - 205 rows over 365 days
    üìä VNM 3- Training: Horizon=60d, Target=4.75% (B&H: 28.87%, Scale=0.164)
‚úÖ VNM: Training data validated - 205 rows over 365 days
    üìä NVDY 3- Training: Horizon=60d, Target=4.75% (B&H: 28.87%, Scale=0.164)
‚úÖ NVDY: Training data validated - 205 rows over 365 days
    üìä BMBL 3- Training: Horizon=60d, Target=4.74% (B&H: 28.84%, Scale=0.164)
‚úÖ BMBL: Training data validated - 205 rows over 365 days
    üìä NPCE 3- Training: Horizon=60d, Target=4.74% (B&H: 28.84%, Scale=0.164)
‚úÖ NPCE: Training data validated - 205 rows over 365 days
    üìä IVZ 3- Training: Horizon=60d, Target=4.74% (B&H: 28.80%, Scale=0.164)
‚úÖ IVZ: Training data validated - 205 rows over 365 days
    üìä SILJ 3- Training: Horizon=60d, Target=4.73% (B&H: 28.76%, Scale=0.164)
‚úÖ SILJ: Training data validated - 205 rows over 365 days
    üìä PPA 3- Training: Horizon=60d, Target=4.72% (B&H: 28.73%, Scale=0.164)
‚úÖ PPA: Training data validated - 205 rows over 365 days
    üìä GL 3- Training: Horizon=60d, Target=4.72% (B&H: 28.71%, Scale=0.164)
‚úÖ GL: Training data validated - 205 rows over 365 days
    üìä XPO 3- Training: Horizon=60d, Target=4.71% (B&H: 28.65%, Scale=0.164)
‚úÖ XPO: Training data validated - 205 rows over 365 days
    üìä DOCU 3- Training: Horizon=60d, Target=4.71% (B&H: 28.64%, Scale=0.164)
‚úÖ DOCU: Training data validated - 205 rows over 365 days
    üìä PBYI 3- Training: Horizon=60d, Target=4.71% (B&H: 28.63%, Scale=0.164)
‚úÖ PBYI: Training data validated - 205 rows over 365 days
    üìä WCC 3- Training: Horizon=60d, Target=4.71% (B&H: 28.62%, Scale=0.164)
‚úÖ WCC: Training data validated - 205 rows over 365 days
    üìä EWG 3- Training: Horizon=60d, Target=4.70% (B&H: 28.61%, Scale=0.164)
‚úÖ EWG: Training data validated - 205 rows over 365 days
    üìä FDN 3- Training: Horizon=60d, Target=4.70% (B&H: 28.60%, Scale=0.164)
‚úÖ FDN: Training data validated - 205 rows over 365 days
    üìä DRTS 3- Training: Horizon=60d, Target=4.70% (B&H: 28.57%, Scale=0.164)
‚úÖ DRTS: Training data validated - 205 rows over 365 days
    üìä GLAD 3- Training: Horizon=60d, Target=4.70% (B&H: 28.57%, Scale=0.164)
‚úÖ GLAD: Training data validated - 205 rows over 365 days
    üìä FLUT 3- Training: Horizon=60d, Target=4.69% (B&H: 28.53%, Scale=0.164)
‚úÖ FLUT: Training data validated - 205 rows over 365 days
    üìä NTGR 3- Training: Horizon=60d, Target=4.69% (B&H: 28.51%, Scale=0.164)
‚úÖ NTGR: Training data validated - 205 rows over 365 days
    üìä AFK 3- Training: Horizon=60d, Target=4.68% (B&H: 28.47%, Scale=0.164)
‚úÖ AFK: Training data validated - 205 rows over 365 days
    üìä WAY 3- Training: Horizon=60d, Target=4.68% (B&H: 28.47%, Scale=0.164)
‚úÖ WAY: Training data validated - 205 rows over 365 days
    üìä DRI 3- Training: Horizon=60d, Target=4.68% (B&H: 28.46%, Scale=0.164)
‚úÖ DRI: Training data validated - 205 rows over 365 days
    üìä ALV 3- Training: Horizon=60d, Target=4.66% (B&H: 28.37%, Scale=0.164)
‚úÖ ALV: Training data validated - 205 rows over 365 days
    üìä SYBT 3- Training: Horizon=60d, Target=4.66% (B&H: 28.35%, Scale=0.164)
‚úÖ SYBT: Training data validated - 205 rows over 365 days
    üìä UBS 3- Training: Horizon=60d, Target=4.65% (B&H: 28.31%, Scale=0.164)
‚úÖ UBS: Training data validated - 205 rows over 365 days
    üìä SHCO 3- Training: Horizon=60d, Target=4.64% (B&H: 28.24%, Scale=0.164)
‚úÖ SHCO: Training data validated - 205 rows over 365 days
    üìä FSCO 3- Training: Horizon=60d, Target=4.63% (B&H: 28.18%, Scale=0.164)
‚úÖ FSCO: Training data validated - 205 rows over 365 days
    üìä KMI 3- Training: Horizon=60d, Target=4.63% (B&H: 28.16%, Scale=0.164)
‚úÖ KMI: Training data validated - 205 rows over 365 days
    üìä SPPP 3- Training: Horizon=60d, Target=4.62% (B&H: 28.11%, Scale=0.164)
‚úÖ SPPP: Training data validated - 205 rows over 365 days
    üìä OPPE 3- Training: Horizon=60d, Target=4.61% (B&H: 28.03%, Scale=0.164)
‚úÖ OPPE: Training data validated - 205 rows over 365 days
    üìä WELL 3- Training: Horizon=60d, Target=4.60% (B&H: 27.97%, Scale=0.164)
‚úÖ WELL: Training data validated - 205 rows over 365 days
    üìä YMM 3- Training: Horizon=60d, Target=4.60% (B&H: 27.96%, Scale=0.164)
‚úÖ YMM: Training data validated - 205 rows over 365 days
    üìä RDWR 3- Training: Horizon=60d, Target=4.60% (B&H: 27.96%, Scale=0.164)
‚úÖ RDWR: Training data validated - 205 rows over 365 days
    üìä BNT 3- Training: Horizon=60d, Target=4.59% (B&H: 27.92%, Scale=0.164)
‚úÖ BNT: Training data validated - 205 rows over 365 days
    üìä EGAN 3- Training: Horizon=60d, Target=4.58% (B&H: 27.84%, Scale=0.164)
‚úÖ EGAN: Training data validated - 205 rows over 365 days
    üìä BN 3- Training: Horizon=60d, Target=4.57% (B&H: 27.79%, Scale=0.164)
‚úÖ BN: Training data validated - 205 rows over 365 days
    üìä SPMO 3- Training: Horizon=60d, Target=4.55% (B&H: 27.66%, Scale=0.164)
‚úÖ SPMO: Training data validated - 205 rows over 365 days
    üìä FMS 3- Training: Horizon=60d, Target=4.54% (B&H: 27.63%, Scale=0.164)
‚úÖ FMS: Training data validated - 205 rows over 365 days
    üìä SKWD 3- Training: Horizon=60d, Target=4.54% (B&H: 27.62%, Scale=0.164)
‚úÖ SKWD: Training data validated - 205 rows over 365 days
    üìä BSVN 3- Training: Horizon=60d, Target=4.54% (B&H: 27.60%, Scale=0.164)
‚úÖ BSVN: Training data validated - 205 rows over 365 days
    üìä EWBC 3- Training: Horizon=60d, Target=4.53% (B&H: 27.55%, Scale=0.164)
‚úÖ EWBC: Training data validated - 205 rows over 365 days
    üìä PWRD 3- Training: Horizon=60d, Target=4.53% (B&H: 27.54%, Scale=0.164)
‚úÖ PWRD: Training data validated - 205 rows over 365 days
    üìä CIO 3- Training: Horizon=60d, Target=4.53% (B&H: 27.53%, Scale=0.164)
‚úÖ CIO: Training data validated - 205 rows over 365 days
    üìä BBUC 3- Training: Horizon=60d, Target=4.52% (B&H: 27.51%, Scale=0.164)
‚úÖ BBUC: Training data validated - 205 rows over 365 days
    üìä FET 3- Training: Horizon=60d, Target=4.52% (B&H: 27.49%, Scale=0.164)
‚úÖ FET: Training data validated - 205 rows over 365 days
    üìä NGS 3- Training: Horizon=60d, Target=4.51% (B&H: 27.42%, Scale=0.164)
‚úÖ NGS: Training data validated - 205 rows over 365 days
    üìä DIS 3- Training: Horizon=60d, Target=4.50% (B&H: 27.38%, Scale=0.164)
‚úÖ DIS: Training data validated - 205 rows over 365 days
    üìä INTU 3- Training: Horizon=60d, Target=4.50% (B&H: 27.38%, Scale=0.164)
‚úÖ INTU: Training data validated - 205 rows over 365 days
    üìä GVAL 3- Training: Horizon=60d, Target=4.50% (B&H: 27.35%, Scale=0.164)
‚úÖ GVAL: Training data validated - 205 rows over 365 days
    üìä PAX 3- Training: Horizon=60d, Target=4.49% (B&H: 27.34%, Scale=0.164)
‚úÖ PAX: Training data validated - 205 rows over 365 days
    üìä ALTG 3- Training: Horizon=60d, Target=4.49% (B&H: 27.32%, Scale=0.164)
‚úÖ ALTG: Training data validated - 205 rows over 365 days
    üìä ITRI 3- Training: Horizon=60d, Target=4.49% (B&H: 27.30%, Scale=0.164)
‚úÖ ITRI: Training data validated - 205 rows over 365 days
    üìä ORLY 3- Training: Horizon=60d, Target=4.47% (B&H: 27.21%, Scale=0.164)
‚úÖ ORLY: Training data validated - 205 rows over 365 days
    üìä VALN 3- Training: Horizon=60d, Target=4.46% (B&H: 27.15%, Scale=0.164)
‚úÖ VALN: Training data validated - 205 rows over 365 days
    üìä FFTY 3- Training: Horizon=60d, Target=4.46% (B&H: 27.14%, Scale=0.164)
‚úÖ FFTY: Training data validated - 205 rows over 365 days
    üìä R 3- Training: Horizon=60d, Target=4.46% (B&H: 27.13%, Scale=0.164)
‚úÖ R: Training data validated - 205 rows over 365 days
    üìä EXPE 3- Training: Horizon=60d, Target=4.46% (B&H: 27.13%, Scale=0.164)
‚úÖ EXPE: Training data validated - 205 rows over 365 days
    üìä Z 3- Training: Horizon=60d, Target=4.46% (B&H: 27.11%, Scale=0.164)
‚úÖ Z: Training data validated - 205 rows over 365 days
    üìä EQH 3- Training: Horizon=60d, Target=4.45% (B&H: 27.08%, Scale=0.164)
‚úÖ EQH: Training data validated - 205 rows over 365 days
    üìä TDY 3- Training: Horizon=60d, Target=4.44% (B&H: 27.01%, Scale=0.164)
‚úÖ TDY: Training data validated - 205 rows over 365 days
    üìä OPEN 3- Training: Horizon=60d, Target=4.44% (B&H: 27.00%, Scale=0.164)
‚úÖ OPEN: Training data validated - 205 rows over 365 days
    üìä FDT 3- Training: Horizon=60d, Target=4.43% (B&H: 26.92%, Scale=0.164)
‚úÖ FDT: Training data validated - 205 rows over 365 days
    üìä SAP 3- Training: Horizon=60d, Target=4.42% (B&H: 26.86%, Scale=0.164)
‚úÖ SAP: Training data validated - 205 rows over 365 days
    üìä TEO 3- Training: Horizon=60d, Target=4.41% (B&H: 26.83%, Scale=0.164)
‚úÖ TEO: Training data validated - 205 rows over 365 days
    üìä RELY 3- Training: Horizon=60d, Target=4.41% (B&H: 26.81%, Scale=0.164)
‚úÖ RELY: Training data validated - 205 rows over 365 days
    üìä IAUM 3- Training: Horizon=60d, Target=4.40% (B&H: 26.78%, Scale=0.164)
‚úÖ IAUM: Training data validated - 205 rows over 365 days
    üìä GLDM 3- Training: Horizon=60d, Target=4.40% (B&H: 26.78%, Scale=0.164)
‚úÖ GLDM: Training data validated - 205 rows over 365 days
    üìä BAR 3- Training: Horizon=60d, Target=4.40% (B&H: 26.76%, Scale=0.164)
‚úÖ BAR: Training data validated - 205 rows over 365 days
    üìä DK 3- Training: Horizon=60d, Target=4.40% (B&H: 26.75%, Scale=0.164)
‚úÖ DK: Training data validated - 205 rows over 365 days
    üìä BULZ 3- Training: Horizon=60d, Target=4.39% (B&H: 26.72%, Scale=0.164)
‚úÖ BULZ: Training data validated - 205 rows over 365 days
    üìä SGOL 3- Training: Horizon=60d, Target=4.39% (B&H: 26.70%, Scale=0.164)
‚úÖ SGOL: Training data validated - 205 rows over 365 days
    üìä EBAY 3- Training: Horizon=60d, Target=4.38% (B&H: 26.67%, Scale=0.164)
‚úÖ EBAY: Training data validated - 205 rows over 365 days
    üìä BSX 3- Training: Horizon=60d, Target=4.38% (B&H: 26.66%, Scale=0.164)
‚úÖ BSX: Training data validated - 205 rows over 365 days
    üìä OUNZ 3- Training: Horizon=60d, Target=4.38% (B&H: 26.62%, Scale=0.164)
‚úÖ OUNZ: Training data validated - 205 rows over 365 days
    üìä AAAU 3- Training: Horizon=60d, Target=4.38% (B&H: 26.62%, Scale=0.164)
‚úÖ AAAU: Training data validated - 205 rows over 365 days
    üìä KR 3- Training: Horizon=60d, Target=4.38% (B&H: 26.62%, Scale=0.164)
‚úÖ KR: Training data validated - 205 rows over 365 days
    üìä HACK 3- Training: Horizon=60d, Target=4.37% (B&H: 26.61%, Scale=0.164)
‚úÖ HACK: Training data validated - 205 rows over 365 days
    üìä FINX 3- Training: Horizon=60d, Target=4.37% (B&H: 26.59%, Scale=0.164)
‚úÖ FINX: Training data validated - 205 rows over 365 days
    üìä IAU 3- Training: Horizon=60d, Target=4.37% (B&H: 26.58%, Scale=0.164)
‚úÖ IAU: Training data validated - 205 rows over 365 days
    üìä ASTE 3- Training: Horizon=60d, Target=4.37% (B&H: 26.58%, Scale=0.164)
‚úÖ ASTE: Training data validated - 205 rows over 365 days
    üìä CLSK 3- Training: Horizon=60d, Target=4.36% (B&H: 26.55%, Scale=0.164)
‚úÖ CLSK: Training data validated - 205 rows over 365 days
    üìä UROY 3- Training: Horizon=60d, Target=4.36% (B&H: 26.53%, Scale=0.164)
‚úÖ UROY: Training data validated - 205 rows over 365 days
    üìä ARGT 3- Training: Horizon=60d, Target=4.35% (B&H: 26.48%, Scale=0.164)
‚úÖ ARGT: Training data validated - 205 rows over 365 days
    üìä GLD 3- Training: Horizon=60d, Target=4.35% (B&H: 26.47%, Scale=0.164)
‚úÖ GLD: Training data validated - 205 rows over 365 days
    üìä CPER 3- Training: Horizon=60d, Target=4.34% (B&H: 26.42%, Scale=0.164)
‚úÖ CPER: Training data validated - 205 rows over 365 days
    üìä CIBR 3- Training: Horizon=60d, Target=4.33% (B&H: 26.37%, Scale=0.164)
‚úÖ CIBR: Training data validated - 205 rows over 365 days
    üìä DCTH 3- Training: Horizon=60d, Target=4.33% (B&H: 26.36%, Scale=0.164)
‚úÖ DCTH: Training data validated - 205 rows over 365 days
    üìä BJ 3- Training: Horizon=60d, Target=4.33% (B&H: 26.33%, Scale=0.164)
‚úÖ BJ: Training data validated - 205 rows over 365 days
    üìä FITE 3- Training: Horizon=60d, Target=4.32% (B&H: 26.30%, Scale=0.164)
‚úÖ FITE: Training data validated - 205 rows over 365 days
    üìä ENVA 3- Training: Horizon=60d, Target=4.32% (B&H: 26.29%, Scale=0.164)
‚úÖ ENVA: Training data validated - 205 rows over 365 days
    üìä ZG 3- Training: Horizon=60d, Target=4.31% (B&H: 26.21%, Scale=0.164)
‚úÖ ZG: Training data validated - 205 rows over 365 days
    üìä LNG 3- Training: Horizon=60d, Target=4.30% (B&H: 26.18%, Scale=0.164)
‚úÖ LNG: Training data validated - 205 rows over 365 days
    üìä TARS 3- Training: Horizon=60d, Target=4.30% (B&H: 26.18%, Scale=0.164)
‚úÖ TARS: Training data validated - 205 rows over 365 days
    üìä FER 3- Training: Horizon=60d, Target=4.30% (B&H: 26.18%, Scale=0.164)
‚úÖ FER: Training data validated - 205 rows over 365 days
    üìä KCE 3- Training: Horizon=60d, Target=4.30% (B&H: 26.14%, Scale=0.164)
‚úÖ KCE: Training data validated - 205 rows over 365 days
    üìä SOCL 3- Training: Horizon=60d, Target=4.29% (B&H: 26.11%, Scale=0.164)
‚úÖ SOCL: Training data validated - 205 rows over 365 days
    üìä IGV 3- Training: Horizon=60d, Target=4.29% (B&H: 26.09%, Scale=0.164)
‚úÖ IGV: Training data validated - 205 rows over 365 days
    üìä TBT 3- Training: Horizon=60d, Target=4.29% (B&H: 26.09%, Scale=0.164)
‚úÖ TBT: Training data validated - 205 rows over 365 days
    üìä NTNX 3- Training: Horizon=60d, Target=4.28% (B&H: 26.06%, Scale=0.164)
‚úÖ NTNX: Training data validated - 205 rows over 365 days
    üìä IETC 3- Training: Horizon=60d, Target=4.27% (B&H: 25.97%, Scale=0.164)
‚úÖ IETC: Training data validated - 205 rows over 365 days
    üìä ARTY 3- Training: Horizon=60d, Target=4.26% (B&H: 25.93%, Scale=0.164)
‚úÖ ARTY: Training data validated - 205 rows over 365 days
    üìä KALV 3- Training: Horizon=60d, Target=4.26% (B&H: 25.91%, Scale=0.164)
‚úÖ KALV: Training data validated - 205 rows over 365 days
    üìä CTVA 3- Training: Horizon=60d, Target=4.26% (B&H: 25.89%, Scale=0.164)
‚úÖ CTVA: Training data validated - 205 rows over 365 days
    üìä SPWH 3- Training: Horizon=60d, Target=4.25% (B&H: 25.83%, Scale=0.164)
‚úÖ SPWH: Training data validated - 205 rows over 365 days
    üìä SIXG 3- Training: Horizon=60d, Target=4.22% (B&H: 25.67%, Scale=0.164)
‚úÖ SIXG: Training data validated - 205 rows over 365 days
    üìä OIS 3- Training: Horizon=60d, Target=4.22% (B&H: 25.65%, Scale=0.164)
‚úÖ OIS: Training data validated - 205 rows over 365 days
    üìä ESAB 3- Training: Horizon=60d, Target=4.22% (B&H: 25.64%, Scale=0.164)
‚úÖ ESAB: Training data validated - 205 rows over 365 days
    üìä IYG 3- Training: Horizon=60d, Target=4.21% (B&H: 25.64%, Scale=0.164)
‚úÖ IYG: Training data validated - 205 rows over 365 days
    üìä TS 3- Training: Horizon=60d, Target=4.21% (B&H: 25.63%, Scale=0.164)
‚úÖ TS: Training data validated - 205 rows over 365 days
    üìä PHYS 3- Training: Horizon=60d, Target=4.21% (B&H: 25.61%, Scale=0.164)
‚úÖ PHYS: Training data validated - 205 rows over 365 days
    üìä UPWK 3- Training: Horizon=60d, Target=4.18% (B&H: 25.45%, Scale=0.164)
‚úÖ UPWK: Training data validated - 205 rows over 365 days
    üìä VMI 3- Training: Horizon=60d, Target=4.18% (B&H: 25.43%, Scale=0.164)
‚úÖ VMI: Training data validated - 205 rows over 365 days
    üìä YMAX 3- Training: Horizon=60d, Target=4.17% (B&H: 25.39%, Scale=0.164)
‚úÖ YMAX: Training data validated - 205 rows over 365 days
    üìä PRM 3- Training: Horizon=60d, Target=4.17% (B&H: 25.35%, Scale=0.164)
‚úÖ PRM: Training data validated - 205 rows over 365 days
    üìä DE 3- Training: Horizon=60d, Target=4.16% (B&H: 25.28%, Scale=0.164)
‚úÖ DE: Training data validated - 205 rows over 365 days
    üìä GLTR 3- Training: Horizon=60d, Target=4.15% (B&H: 25.26%, Scale=0.164)
‚úÖ GLTR: Training data validated - 205 rows over 365 days
    üìä PBW 3- Training: Horizon=60d, Target=4.15% (B&H: 25.22%, Scale=0.164)
‚úÖ PBW: Training data validated - 205 rows over 365 days
    üìä HLI 3- Training: Horizon=60d, Target=4.14% (B&H: 25.21%, Scale=0.164)
‚úÖ HLI: Training data validated - 205 rows over 365 days
    üìä PIZ 3- Training: Horizon=60d, Target=4.14% (B&H: 25.19%, Scale=0.164)
‚úÖ PIZ: Training data validated - 205 rows over 365 days
    üìä FXI 3- Training: Horizon=60d, Target=4.14% (B&H: 25.18%, Scale=0.164)
‚úÖ FXI: Training data validated - 205 rows over 365 days
    üìä MVBF 3- Training: Horizon=60d, Target=4.14% (B&H: 25.18%, Scale=0.164)
‚úÖ MVBF: Training data validated - 205 rows over 365 days
    üìä ETOR 3- Training: Horizon=60d, Target=4.13% (B&H: 25.15%, Scale=0.164)
  ‚ùå ETOR: Too many NaN values in Close price: 154 / 205 rows
   üí° Data quality issue - try different data source or date range
    üìä NEM 3- Training: Horizon=60d, Target=4.12% (B&H: 25.06%, Scale=0.164)
‚úÖ NEM: Training data validated - 205 rows over 365 days
    üìä CRBG 3- Training: Horizon=60d, Target=4.11% (B&H: 24.98%, Scale=0.164)
‚úÖ CRBG: Training data validated - 205 rows over 365 days
    üìä IDMO 3- Training: Horizon=60d, Target=4.10% (B&H: 24.97%, Scale=0.164)
‚úÖ IDMO: Training data validated - 205 rows over 365 days
    üìä GEOS 3- Training: Horizon=60d, Target=4.10% (B&H: 24.95%, Scale=0.164)
‚úÖ GEOS: Training data validated - 205 rows over 365 days
    üìä DBP 3- Training: Horizon=60d, Target=4.10% (B&H: 24.94%, Scale=0.164)
‚úÖ DBP: Training data validated - 205 rows over 365 days
    üìä RIVN 3- Training: Horizon=60d, Target=4.09% (B&H: 24.87%, Scale=0.164)
‚úÖ RIVN: Training data validated - 205 rows over 365 days
    üìä SLVO 3- Training: Horizon=60d, Target=4.08% (B&H: 24.82%, Scale=0.164)
‚úÖ SLVO: Training data validated - 205 rows over 365 days
    üìä META 3- Training: Horizon=60d, Target=4.08% (B&H: 24.81%, Scale=0.164)
‚úÖ META: Training data validated - 205 rows over 365 days
    üìä HWKN 3- Training: Horizon=60d, Target=4.08% (B&H: 24.81%, Scale=0.164)
‚úÖ HWKN: Training data validated - 205 rows over 365 days
    üìä GLW 3- Training: Horizon=60d, Target=4.07% (B&H: 24.76%, Scale=0.164)
‚úÖ GLW: Training data validated - 205 rows over 365 days
    üìä ASLE 3- Training: Horizon=60d, Target=4.07% (B&H: 24.75%, Scale=0.164)
‚úÖ ASLE: Training data validated - 205 rows over 365 days
    üìä CLM 3- Training: Horizon=60d, Target=4.07% (B&H: 24.73%, Scale=0.164)
‚úÖ CLM: Training data validated - 205 rows over 365 days
    üìä PODD 3- Training: Horizon=60d, Target=4.06% (B&H: 24.69%, Scale=0.164)
‚úÖ PODD: Training data validated - 205 rows over 365 days
    üìä WUGI 3- Training: Horizon=60d, Target=4.06% (B&H: 24.68%, Scale=0.164)
‚úÖ WUGI: Training data validated - 205 rows over 365 days
    üìä RIGL 3- Training: Horizon=60d, Target=4.05% (B&H: 24.66%, Scale=0.164)
‚úÖ RIGL: Training data validated - 205 rows over 365 days
    üìä OZK 3- Training: Horizon=60d, Target=4.05% (B&H: 24.64%, Scale=0.164)
‚úÖ OZK: Training data validated - 205 rows over 365 days
    üìä HEI-A 3- Training: Horizon=60d, Target=4.04% (B&H: 24.60%, Scale=0.164)
‚úÖ HEI-A: Training data validated - 205 rows over 365 days
    üìä NTB 3- Training: Horizon=60d, Target=4.03% (B&H: 24.51%, Scale=0.164)
‚úÖ NTB: Training data validated - 205 rows over 365 days
    üìä FSK 3- Training: Horizon=60d, Target=4.03% (B&H: 24.50%, Scale=0.164)
‚úÖ FSK: Training data validated - 205 rows over 365 days
    üìä PSKY 3- Training: Horizon=60d, Target=4.02% (B&H: 24.49%, Scale=0.164)
‚úÖ PSKY: Training data validated - 205 rows over 365 days
    üìä NI 3- Training: Horizon=60d, Target=4.02% (B&H: 24.46%, Scale=0.164)
‚úÖ NI: Training data validated - 205 rows over 365 days
    üìä HDB 3- Training: Horizon=60d, Target=4.01% (B&H: 24.42%, Scale=0.164)
‚úÖ HDB: Training data validated - 205 rows over 365 days
    üìä TG 3- Training: Horizon=60d, Target=4.01% (B&H: 24.42%, Scale=0.164)
‚úÖ TG: Training data validated - 205 rows over 365 days
    üìä TD 3- Training: Horizon=60d, Target=4.00% (B&H: 24.36%, Scale=0.164)
‚úÖ TD: Training data validated - 205 rows over 365 days
    üìä NBN 3- Training: Horizon=60d, Target=4.00% (B&H: 24.35%, Scale=0.164)
‚úÖ NBN: Training data validated - 205 rows over 365 days
    üìä IDV 3- Training: Horizon=60d, Target=4.00% (B&H: 24.34%, Scale=0.164)
‚úÖ IDV: Training data validated - 205 rows over 365 days
    üìä AMZY 3- Training: Horizon=60d, Target=4.00% (B&H: 24.32%, Scale=0.164)
‚úÖ AMZY: Training data validated - 205 rows over 365 days
    üìä NRIM 3- Training: Horizon=60d, Target=3.99% (B&H: 24.30%, Scale=0.164)
‚úÖ NRIM: Training data validated - 205 rows over 365 days
    üìä JTEK 3- Training: Horizon=60d, Target=3.99% (B&H: 24.28%, Scale=0.164)
‚úÖ JTEK: Training data validated - 205 rows over 365 days
    üìä AMZN 3- Training: Horizon=60d, Target=3.98% (B&H: 24.21%, Scale=0.164)
‚úÖ AMZN: Training data validated - 205 rows over 365 days
    üìä CFG 3- Training: Horizon=60d, Target=3.97% (B&H: 24.17%, Scale=0.164)
‚úÖ CFG: Training data validated - 205 rows over 365 days
    üìä CRMT 3- Training: Horizon=60d, Target=3.97% (B&H: 24.14%, Scale=0.164)
‚úÖ CRMT: Training data validated - 205 rows over 365 days
    üìä DTST 3- Training: Horizon=60d, Target=3.97% (B&H: 24.14%, Scale=0.164)
‚úÖ DTST: Training data validated - 205 rows over 365 days
    üìä NVMI 3- Training: Horizon=60d, Target=3.97% (B&H: 24.13%, Scale=0.164)
‚úÖ NVMI: Training data validated - 205 rows over 365 days
    üìä SNX 3- Training: Horizon=60d, Target=3.96% (B&H: 24.09%, Scale=0.164)
‚úÖ SNX: Training data validated - 205 rows over 365 days
    üìä BRW 3- Training: Horizon=60d, Target=3.96% (B&H: 24.09%, Scale=0.164)
‚úÖ BRW: Training data validated - 205 rows over 365 days
    üìä GNE 3- Training: Horizon=60d, Target=3.94% (B&H: 23.99%, Scale=0.164)
‚úÖ GNE: Training data validated - 205 rows over 365 days
    üìä CHWY 3- Training: Horizon=60d, Target=3.94% (B&H: 23.97%, Scale=0.164)
‚úÖ CHWY: Training data validated - 205 rows over 365 days
    üìä WRB 3- Training: Horizon=60d, Target=3.93% (B&H: 23.93%, Scale=0.164)
‚úÖ WRB: Training data validated - 205 rows over 365 days
    üìä IQM 3- Training: Horizon=60d, Target=3.93% (B&H: 23.93%, Scale=0.164)
‚úÖ IQM: Training data validated - 205 rows over 365 days
    üìä HEI 3- Training: Horizon=60d, Target=3.93% (B&H: 23.91%, Scale=0.164)
‚úÖ HEI: Training data validated - 205 rows over 365 days
    üìä TRMK 3- Training: Horizon=60d, Target=3.93% (B&H: 23.90%, Scale=0.164)
‚úÖ TRMK: Training data validated - 205 rows over 365 days
    üìä EVC 3- Training: Horizon=60d, Target=3.93% (B&H: 23.88%, Scale=0.164)
‚úÖ EVC: Training data validated - 205 rows over 365 days
    üìä PAM 3- Training: Horizon=60d, Target=3.92% (B&H: 23.83%, Scale=0.164)
‚úÖ PAM: Training data validated - 205 rows over 365 days
    üìä COWG 3- Training: Horizon=60d, Target=3.92% (B&H: 23.83%, Scale=0.164)
‚úÖ COWG: Training data validated - 205 rows over 365 days
    üìä FDP 3- Training: Horizon=60d, Target=3.92% (B&H: 23.83%, Scale=0.164)
‚úÖ FDP: Training data validated - 205 rows over 365 days
    üìä PSLV 3- Training: Horizon=60d, Target=3.92% (B&H: 23.83%, Scale=0.164)
‚úÖ PSLV: Training data validated - 205 rows over 365 days
    üìä MO 3- Training: Horizon=60d, Target=3.91% (B&H: 23.79%, Scale=0.164)
‚úÖ MO: Training data validated - 205 rows over 365 days
    üìä TQQQ 3- Training: Horizon=60d, Target=3.91% (B&H: 23.78%, Scale=0.164)
‚úÖ TQQQ: Training data validated - 205 rows over 365 days
    üìä TRS 3- Training: Horizon=60d, Target=3.90% (B&H: 23.75%, Scale=0.164)
‚úÖ TRS: Training data validated - 205 rows over 365 days
    üìä MTSI 3- Training: Horizon=60d, Target=3.90% (B&H: 23.74%, Scale=0.164)
‚úÖ MTSI: Training data validated - 205 rows over 365 days
    üìä WCBR 3- Training: Horizon=60d, Target=3.90% (B&H: 23.71%, Scale=0.164)
‚úÖ WCBR: Training data validated - 205 rows over 365 days
    üìä MAMA 3- Training: Horizon=60d, Target=3.90% (B&H: 23.70%, Scale=0.164)
‚úÖ MAMA: Training data validated - 205 rows over 365 days
    üìä FRSH 3- Training: Horizon=60d, Target=3.88% (B&H: 23.61%, Scale=0.164)
‚úÖ FRSH: Training data validated - 205 rows over 365 days
    üìä SOBO 3- Training: Horizon=60d, Target=3.88% (B&H: 23.60%, Scale=0.164)
‚úÖ SOBO: Training data validated - 205 rows over 365 days
    üìä GGAL 3- Training: Horizon=60d, Target=3.88% (B&H: 23.59%, Scale=0.164)
‚úÖ GGAL: Training data validated - 205 rows over 365 days
    üìä AXS 3- Training: Horizon=60d, Target=3.86% (B&H: 23.50%, Scale=0.164)
‚úÖ AXS: Training data validated - 205 rows over 365 days
    üìä MASI 3- Training: Horizon=60d, Target=3.86% (B&H: 23.47%, Scale=0.164)
‚úÖ MASI: Training data validated - 205 rows over 365 days
    üìä RSG 3- Training: Horizon=60d, Target=3.84% (B&H: 23.36%, Scale=0.164)
‚úÖ RSG: Training data validated - 205 rows over 365 days
    üìä IRON 3- Training: Horizon=60d, Target=3.84% (B&H: 23.34%, Scale=0.164)
‚úÖ IRON: Training data validated - 205 rows over 365 days
    üìä CPA 3- Training: Horizon=60d, Target=3.83% (B&H: 23.33%, Scale=0.164)
‚úÖ CPA: Training data validated - 205 rows over 365 days
    üìä GEL 3- Training: Horizon=60d, Target=3.83% (B&H: 23.29%, Scale=0.164)
‚úÖ GEL: Training data validated - 205 rows over 365 days
    üìä SIMO 3- Training: Horizon=60d, Target=3.83% (B&H: 23.29%, Scale=0.164)
‚úÖ SIMO: Training data validated - 205 rows over 365 days
    üìä GT 3- Training: Horizon=60d, Target=3.83% (B&H: 23.28%, Scale=0.164)
‚úÖ GT: Training data validated - 205 rows over 365 days
    üìä PEJ 3- Training: Horizon=60d, Target=3.83% (B&H: 23.27%, Scale=0.164)
‚úÖ PEJ: Training data validated - 205 rows over 365 days
    üìä FEDU 3- Training: Horizon=60d, Target=3.82% (B&H: 23.24%, Scale=0.164)
‚úÖ FEDU: Training data validated - 205 rows over 365 days
    üìä SPRY 3- Training: Horizon=60d, Target=3.82% (B&H: 23.24%, Scale=0.164)
‚úÖ SPRY: Training data validated - 205 rows over 365 days
    üìä CFR 3- Training: Horizon=60d, Target=3.82% (B&H: 23.24%, Scale=0.164)
‚úÖ CFR: Training data validated - 205 rows over 365 days
    üìä CSGP 3- Training: Horizon=60d, Target=3.82% (B&H: 23.24%, Scale=0.164)
‚úÖ CSGP: Training data validated - 205 rows over 365 days
    üìä CPNG 3- Training: Horizon=60d, Target=3.82% (B&H: 23.22%, Scale=0.164)
‚úÖ CPNG: Training data validated - 205 rows over 365 days
    üìä UIVM 3- Training: Horizon=60d, Target=3.81% (B&H: 23.20%, Scale=0.164)
‚úÖ UIVM: Training data validated - 205 rows over 365 days
    üìä KEMQ 3- Training: Horizon=60d, Target=3.81% (B&H: 23.16%, Scale=0.164)
‚úÖ KEMQ: Training data validated - 205 rows over 365 days
    üìä WTFC 3- Training: Horizon=60d, Target=3.81% (B&H: 23.16%, Scale=0.164)
‚úÖ WTFC: Training data validated - 205 rows over 365 days
    üìä FDTS 3- Training: Horizon=60d, Target=3.80% (B&H: 23.09%, Scale=0.164)
‚úÖ FDTS: Training data validated - 205 rows over 365 days
    üìä STEL 3- Training: Horizon=60d, Target=3.79% (B&H: 23.06%, Scale=0.164)
‚úÖ STEL: Training data validated - 205 rows over 365 days
    üìä XITK 3- Training: Horizon=60d, Target=3.79% (B&H: 23.03%, Scale=0.164)
‚úÖ XITK: Training data validated - 205 rows over 365 days
    üìä VSAT 3- Training: Horizon=60d, Target=3.79% (B&H: 23.03%, Scale=0.164)
‚úÖ VSAT: Training data validated - 205 rows over 365 days
    üìä EPR 3- Training: Horizon=60d, Target=3.78% (B&H: 22.99%, Scale=0.164)
‚úÖ EPR: Training data validated - 205 rows over 365 days
    üìä SPOK 3- Training: Horizon=60d, Target=3.78% (B&H: 22.98%, Scale=0.164)
‚úÖ SPOK: Training data validated - 205 rows over 365 days
    üìä TRIP 3- Training: Horizon=60d, Target=3.78% (B&H: 22.98%, Scale=0.164)
‚úÖ TRIP: Training data validated - 205 rows over 365 days
    üìä IXG 3- Training: Horizon=60d, Target=3.78% (B&H: 22.97%, Scale=0.164)
‚úÖ IXG: Training data validated - 205 rows over 365 days
    üìä PDLB 3- Training: Horizon=60d, Target=3.77% (B&H: 22.93%, Scale=0.164)
‚úÖ PDLB: Training data validated - 205 rows over 365 days
    üìä FTDR 3- Training: Horizon=60d, Target=3.76% (B&H: 22.90%, Scale=0.164)
‚úÖ FTDR: Training data validated - 205 rows over 365 days
    üìä QLD 3- Training: Horizon=60d, Target=3.75% (B&H: 22.83%, Scale=0.164)
‚úÖ QLD: Training data validated - 205 rows over 365 days
    üìä HURN 3- Training: Horizon=60d, Target=3.75% (B&H: 22.81%, Scale=0.164)
‚úÖ HURN: Training data validated - 205 rows over 365 days
    üìä CECO 3- Training: Horizon=60d, Target=3.75% (B&H: 22.80%, Scale=0.164)
‚úÖ CECO: Training data validated - 205 rows over 365 days
    üìä WTS 3- Training: Horizon=60d, Target=3.74% (B&H: 22.76%, Scale=0.164)
‚úÖ WTS: Training data validated - 205 rows over 365 days
    üìä ALG 3- Training: Horizon=60d, Target=3.72% (B&H: 22.65%, Scale=0.164)
‚úÖ ALG: Training data validated - 205 rows over 365 days
    üìä CMPO 3- Training: Horizon=60d, Target=3.72% (B&H: 22.63%, Scale=0.164)
‚úÖ CMPO: Training data validated - 205 rows over 365 days
    üìä SMBK 3- Training: Horizon=60d, Target=3.72% (B&H: 22.62%, Scale=0.164)
‚úÖ SMBK: Training data validated - 205 rows over 365 days
    üìä AIT 3- Training: Horizon=60d, Target=3.71% (B&H: 22.59%, Scale=0.164)
‚úÖ AIT: Training data validated - 205 rows over 365 days
    üìä FDNI 3- Training: Horizon=60d, Target=3.71% (B&H: 22.57%, Scale=0.164)
‚úÖ FDNI: Training data validated - 205 rows over 365 days
    üìä CDNS 3- Training: Horizon=60d, Target=3.71% (B&H: 22.57%, Scale=0.164)
‚úÖ CDNS: Training data validated - 205 rows over 365 days
    üìä BBSI 3- Training: Horizon=60d, Target=3.71% (B&H: 22.55%, Scale=0.164)
‚úÖ BBSI: Training data validated - 205 rows over 365 days
    üìä PCOR 3- Training: Horizon=60d, Target=3.70% (B&H: 22.50%, Scale=0.164)
‚úÖ PCOR: Training data validated - 205 rows over 365 days
    üìä FGD 3- Training: Horizon=60d, Target=3.70% (B&H: 22.48%, Scale=0.164)
‚úÖ FGD: Training data validated - 205 rows over 365 days
    üìä UXI 3- Training: Horizon=60d, Target=3.69% (B&H: 22.46%, Scale=0.164)
‚úÖ UXI: Training data validated - 205 rows over 365 days
    üìä FBNC 3- Training: Horizon=60d, Target=3.69% (B&H: 22.44%, Scale=0.164)
‚úÖ FBNC: Training data validated - 205 rows over 365 days
    üìä ONLN 3- Training: Horizon=60d, Target=3.67% (B&H: 22.35%, Scale=0.164)
‚úÖ ONLN: Training data validated - 205 rows over 365 days
ü§ñ Training 3-Month models in parallel for 966 tickers using 15 processes...
üêõ DEBUG: Using torch.multiprocessing.Pool with spawn for GPU support
üêõ DEBUG: Pool started with 15 worker processes. PIDs=[195112, 195113, 195114, 195115, 195116, 195117, 195118, 195119, 195120, 195121, 195122, 195123, 195124, 195125, 195126]
üêõ DEBUG: Submitting 959 training tasks...
üêõ DEBUG: Submitted 959 tasks, waiting for results with 600s timeout per ticker...
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for OKLO
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for PRCH
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for LAES
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for RYM
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for AMPX
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for TMC
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for ARQQ
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for AEVA
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for QNTM
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for ABVX
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for KC
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for DAVE
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for SEZL
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for IONQ
[GPU] torch.cuda.is_available(): True
[GPU] XGBoost version: 3.1.2
[GPU] LightGBM version: 4.6.0
DEBUG: Script execution initiated.
üîß Using torch.multiprocessing with 'spawn' for CUDA compatibility
üêõ DEBUG: train_worker started for SRRK
  ‚öôÔ∏è Training models for OKLO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - OKLO: Initiating feature extraction for training.
  [DIAGNOSTIC] OKLO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OKLO: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ OKLO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OKLO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OKLO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OKLO: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for PRCH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - PRCH: Initiating feature extraction for training.
  [DIAGNOSTIC] PRCH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRCH: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ PRCH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRCH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRCH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRCH: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for LAES (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - LAES: Initiating feature extraction for training.
  [DIAGNOSTIC] LAES: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LAES: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ LAES: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LAES: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LAES: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LAES: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for RYM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - RYM: Initiating feature extraction for training.
  [DIAGNOSTIC] RYM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RYM: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ RYM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RYM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RYM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RYM: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for AMPX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - AMPX: Initiating feature extraction for training.
  [DIAGNOSTIC] AMPX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMPX: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ AMPX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMPX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMPX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMPX: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for TMC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TMC: Initiating feature extraction for training.
  [DIAGNOSTIC] TMC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TMC: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ TMC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TMC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TMC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TMC: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for ARQQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - ARQQ: Initiating feature extraction for training.
  [DIAGNOSTIC] ARQQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARQQ: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ ARQQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARQQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARQQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARQQ: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for AEVA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - AEVA: Initiating feature extraction for training.
  [DIAGNOSTIC] AEVA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AEVA: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ AEVA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AEVA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AEVA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AEVA: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for QNTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - QNTM: Initiating feature extraction for training.
  [DIAGNOSTIC] QNTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QNTM: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ QNTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for KC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - KC: Initiating feature extraction for training.
  [DIAGNOSTIC] KC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KC: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ KC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for ABVX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - ABVX: Initiating feature extraction for training.
  [DIAGNOSTIC] ABVX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ABVX: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ ABVX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for SEZL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - SEZL: Initiating feature extraction for training.
  [DIAGNOSTIC] SEZL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SEZL: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ SEZL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QNTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QNTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QNTM: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for DAVE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - DAVE: Initiating feature extraction for training.
  [DIAGNOSTIC] DAVE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAVE: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ DAVE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KC: Training LSTM (50 epochs)...
  [DIAGNOSTIC] ABVX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ABVX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
  [DIAGNOSTIC] SEZL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SEZL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SEZL: Training LSTM (50 epochs)...
      üîπ ABVX: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for SRRK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - SRRK: Initiating feature extraction for training.
  [DIAGNOSTIC] SRRK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SRRK: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ SRRK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for IONQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - IONQ: Initiating feature extraction for training.
  [DIAGNOSTIC] IONQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IONQ: rows after features available: 126
‚úÖ CUDA is available and working. GPU acceleration enabled with deterministic algorithms.
‚ÑπÔ∏è LightGBM: using CPU (OpenCL not compatible with NVIDIA/PoCL in WSL2).
‚úÖ XGBoostRegressor found. Configured for GPU (gpu_hist tree_method).
üéØ IONQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAVE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAVE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAVE: Training LSTM (50 epochs)...
  [DIAGNOSTIC] SRRK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SRRK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SRRK: Training LSTM (50 epochs)...
  [DIAGNOSTIC] IONQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IONQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IONQ: Training LSTM (50 epochs)...
      ‚è≥ PRCH LSTM: Epoch 10/50 (20%)
      ‚è≥ LAES LSTM: Epoch 10/50 (20%)
      ‚è≥ OKLO LSTM: Epoch 10/50 (20%)
      ‚è≥ RYM LSTM: Epoch 10/50 (20%)
      ‚è≥ ARQQ LSTM: Epoch 10/50 (20%)
      ‚è≥ AMPX LSTM: Epoch 10/50 (20%)
      ‚è≥ ABVX LSTM: Epoch 10/50 (20%)
      ‚è≥ AEVA LSTM: Epoch 10/50 (20%)
      ‚è≥ SRRK LSTM: Epoch 10/50 (20%)
      ‚è≥ KC LSTM: Epoch 10/50 (20%)
      ‚è≥ IONQ LSTM: Epoch 10/50 (20%)
      ‚è≥ TMC LSTM: Epoch 10/50 (20%)
      ‚è≥ DAVE LSTM: Epoch 10/50 (20%)
      ‚è≥ QNTM LSTM: Epoch 10/50 (20%)
      ‚è≥ SEZL LSTM: Epoch 10/50 (20%)
      ‚è≥ LAES LSTM: Epoch 20/50 (40%)
      ‚è≥ PRCH LSTM: Epoch 20/50 (40%)
      ‚è≥ OKLO LSTM: Epoch 20/50 (40%)
      ‚è≥ RYM LSTM: Epoch 20/50 (40%)
      ‚è≥ ARQQ LSTM: Epoch 20/50 (40%)
      ‚è≥ ABVX LSTM: Epoch 20/50 (40%)
      ‚è≥ AEVA LSTM: Epoch 20/50 (40%)
      ‚è≥ SRRK LSTM: Epoch 20/50 (40%)
      ‚è≥ DAVE LSTM: Epoch 20/50 (40%)
      ‚è≥ KC LSTM: Epoch 20/50 (40%)
      ‚è≥ IONQ LSTM: Epoch 20/50 (40%)
      ‚è≥ AMPX LSTM: Epoch 20/50 (40%)
      ‚è≥ TMC LSTM: Epoch 20/50 (40%)
      ‚è≥ OKLO LSTM: Epoch 30/50 (60%)
      ‚è≥ LAES LSTM: Epoch 30/50 (60%)
      ‚è≥ PRCH LSTM: Epoch 30/50 (60%)
      ‚è≥ SEZL LSTM: Epoch 20/50 (40%)
      ‚è≥ RYM LSTM: Epoch 30/50 (60%)
      ‚è≥ QNTM LSTM: Epoch 20/50 (40%)
      ‚è≥ ARQQ LSTM: Epoch 30/50 (60%)
      ‚è≥ AEVA LSTM: Epoch 30/50 (60%)
      ‚è≥ ABVX LSTM: Epoch 30/50 (60%)
      ‚è≥ SRRK LSTM: Epoch 30/50 (60%)
      ‚è≥ KC LSTM: Epoch 30/50 (60%)
      ‚è≥ IONQ LSTM: Epoch 30/50 (60%)
      ‚è≥ DAVE LSTM: Epoch 30/50 (60%)
      ‚è≥ OKLO LSTM: Epoch 40/50 (80%)
      ‚è≥ AMPX LSTM: Epoch 30/50 (60%)
      ‚è≥ PRCH LSTM: Epoch 40/50 (80%)
      ‚è≥ LAES LSTM: Epoch 40/50 (80%)
      ‚è≥ RYM LSTM: Epoch 40/50 (80%)
      ‚è≥ TMC LSTM: Epoch 30/50 (60%)
      ‚è≥ QNTM LSTM: Epoch 30/50 (60%)
      ‚è≥ SEZL LSTM: Epoch 30/50 (60%)
      ‚è≥ ARQQ LSTM: Epoch 40/50 (80%)
      ‚è≥ SRRK LSTM: Epoch 40/50 (80%)
      ‚è≥ AEVA LSTM: Epoch 40/50 (80%)
      ‚è≥ ABVX LSTM: Epoch 40/50 (80%)
      ‚è≥ KC LSTM: Epoch 40/50 (80%)
      ‚è≥ IONQ LSTM: Epoch 40/50 (80%)
      ‚è≥ DAVE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.693452
         RMSE: 0.832738
         R¬≤ Score: -0.7627 (Poor - 76.3% variance explained)
      üîπ OKLO: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.289160
         RMSE: 0.537736
         R¬≤ Score: -1.0000 (Poor - 100.0% variance explained)
      üîπ PRCH: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.022852
         RMSE: 0.151167
         R¬≤ Score: -6.4328 (Poor - 643.3% variance explained)
      üîπ LAES: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.029095
         RMSE: 0.170571
         R¬≤ Score: -3.2726 (Poor - 327.3% variance explained)
      üîπ RYM: Training TCN (50 epochs)...
      ‚è≥ AMPX LSTM: Epoch 40/50 (80%)
      ‚è≥ OKLO TCN: Epoch 10/50 (20%)
      ‚è≥ TMC LSTM: Epoch 40/50 (80%)
      ‚è≥ PRCH TCN: Epoch 10/50 (20%)
      ‚è≥ SEZL LSTM: Epoch 40/50 (80%)
      ‚è≥ RYM TCN: Epoch 10/50 (20%)
      ‚è≥ LAES TCN: Epoch 10/50 (20%)
      ‚è≥ QNTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.358639
         RMSE: 0.598865
         R¬≤ Score: -0.9192 (Poor - 91.9% variance explained)
      üîπ ARQQ: Training TCN (50 epochs)...
      ‚è≥ OKLO TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.491893
         RMSE: 0.701351
         R¬≤ Score: -1.0297 (Poor - 103.0% variance explained)
      üîπ SRRK: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.203441
         RMSE: 0.451045
         R¬≤ Score: -0.4098 (Poor - 41.0% variance explained)
      üîπ ABVX: Training TCN (50 epochs)...
      ‚è≥ PRCH TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.330640
         RMSE: 0.575013
         R¬≤ Score: -1.2487 (Poor - 124.9% variance explained)
      üîπ IONQ: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.011326
         RMSE: 0.106422
         R¬≤ Score: -0.6143 (Poor - 61.4% variance explained)
      üîπ KC: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.450363
         RMSE: 0.671091
         R¬≤ Score: -1.2631 (Poor - 126.3% variance explained)
      üîπ AEVA: Training TCN (50 epochs)...
      ‚è≥ RYM TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.439928
         RMSE: 0.663271
         R¬≤ Score: -0.8467 (Poor - 84.7% variance explained)
      üîπ DAVE: Training TCN (50 epochs)...
      ‚è≥ LAES TCN: Epoch 20/50 (40%)
      ‚è≥ ARQQ TCN: Epoch 10/50 (20%)
      ‚è≥ OKLO TCN: Epoch 30/50 (60%)
      ‚è≥ ABVX TCN: Epoch 10/50 (20%)
      ‚è≥ PRCH TCN: Epoch 30/50 (60%)
      ‚è≥ SRRK TCN: Epoch 10/50 (20%)
      ‚è≥ KC TCN: Epoch 10/50 (20%)
      ‚è≥ AEVA TCN: Epoch 10/50 (20%)
      ‚è≥ IONQ TCN: Epoch 10/50 (20%)
      ‚è≥ RYM TCN: Epoch 30/50 (60%)
      ‚è≥ DAVE TCN: Epoch 10/50 (20%)
      ‚è≥ LAES TCN: Epoch 30/50 (60%)
      ‚è≥ ARQQ TCN: Epoch 20/50 (40%)
      ‚è≥ OKLO TCN: Epoch 40/50 (80%)
      ‚è≥ ABVX TCN: Epoch 20/50 (40%)
      ‚è≥ PRCH TCN: Epoch 40/50 (80%)
      ‚è≥ SRRK TCN: Epoch 20/50 (40%)
      ‚è≥ KC TCN: Epoch 20/50 (40%)
      ‚è≥ RYM TCN: Epoch 40/50 (80%)
      ‚è≥ AEVA TCN: Epoch 20/50 (40%)
      ‚è≥ IONQ TCN: Epoch 20/50 (40%)
      ‚è≥ DAVE TCN: Epoch 20/50 (40%)
      ‚è≥ LAES TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.615701
         RMSE: 0.784666
         R¬≤ Score: -1.0413 (Poor - 104.1% variance explained)
      üîπ AMPX: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.688855
         RMSE: 0.829973
         R¬≤ Score: -0.7510
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OKLO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OKLO Random Forest: Starting GridSearchCV fit...
      ‚è≥ ARQQ TCN: Epoch 30/50 (60%)
      ‚è≥ ABVX TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.141342
         RMSE: 0.375955
         R¬≤ Score: 0.0224
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRCH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRCH Random Forest: Starting GridSearchCV fit...
      ‚è≥ SRRK TCN: Epoch 30/50 (60%)
      ‚è≥ KC TCN: Epoch 30/50 (60%)
      ‚è≥ IONQ TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.009525
         RMSE: 0.097598
         R¬≤ Score: -0.3988
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RYM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RYM Random Forest: Starting GridSearchCV fit...
      ‚è≥ AEVA TCN: Epoch 30/50 (60%)
      ‚è≥ DAVE TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.005278
         RMSE: 0.072650
         R¬≤ Score: -0.7168
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LAES: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LAES Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.499078
         RMSE: 0.706454
         R¬≤ Score: -0.7982 (Poor - 79.8% variance explained)
      üîπ TMC: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.456880
         RMSE: 0.675929
         R¬≤ Score: -1.0473 (Poor - 104.7% variance explained)
      üîπ SEZL: Training TCN (50 epochs)...
      ‚è≥ AMPX TCN: Epoch 10/50 (20%)
      ‚è≥ ARQQ TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.261637
         RMSE: 0.511505
         R¬≤ Score: -0.7610 (Poor - 76.1% variance explained)
      üîπ QNTM: Training TCN (50 epochs)...
      ‚è≥ SRRK TCN: Epoch 40/50 (80%)
      ‚è≥ ABVX TCN: Epoch 40/50 (80%)
      ‚è≥ KC TCN: Epoch 40/50 (80%)
      ‚è≥ IONQ TCN: Epoch 40/50 (80%)
      ‚è≥ AEVA TCN: Epoch 40/50 (80%)
      ‚è≥ DAVE TCN: Epoch 40/50 (80%)
      ‚è≥ TMC TCN: Epoch 10/50 (20%)
      ‚è≥ SEZL TCN: Epoch 10/50 (20%)
      ‚è≥ AMPX TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.263691
         RMSE: 0.513508
         R¬≤ Score: -0.4111
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARQQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARQQ Random Forest: Starting GridSearchCV fit...
      ‚è≥ QNTM TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.319374
         RMSE: 0.565132
         R¬≤ Score: -0.3178
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SRRK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SRRK Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.146203
         RMSE: 0.382366
         R¬≤ Score: -0.0132
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ABVX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ABVX Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.197083
         RMSE: 0.443940
         R¬≤ Score: -0.3404
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IONQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IONQ Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.200922
         RMSE: 0.448243
         R¬≤ Score: -0.0096
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AEVA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AEVA Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.011840
         RMSE: 0.108811
         R¬≤ Score: -0.6876
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KC Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.367139
         RMSE: 0.605920
         R¬≤ Score: -0.5411
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAVE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAVE Random Forest: Starting GridSearchCV fit...
      ‚è≥ TMC TCN: Epoch 20/50 (40%)
      ‚è≥ SEZL TCN: Epoch 20/50 (40%)
      ‚è≥ AMPX TCN: Epoch 30/50 (60%)
      ‚è≥ QNTM TCN: Epoch 20/50 (40%)
      ‚è≥ SEZL TCN: Epoch 30/50 (60%)
      ‚è≥ TMC TCN: Epoch 30/50 (60%)
      ‚è≥ AMPX TCN: Epoch 40/50 (80%)
      ‚è≥ QNTM TCN: Epoch 30/50 (60%)
      ‚è≥ SEZL TCN: Epoch 40/50 (80%)
      ‚è≥ TMC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.302027
         RMSE: 0.549570
         R¬≤ Score: -0.0013
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMPX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMPX Random Forest: Starting GridSearchCV fit...
      ‚è≥ QNTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.280986
         RMSE: 0.530081
         R¬≤ Score: -0.2591
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SEZL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SEZL Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.287598
         RMSE: 0.536282
         R¬≤ Score: -0.0362
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TMC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TMC Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.184478
         RMSE: 0.429508
         R¬≤ Score: -0.2417
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QNTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QNTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ LAES Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3377.6165 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LAES LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OKLO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=438.1104 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OKLO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARQQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=954.3916 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARQQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=323.0237 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SRRK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.1956 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SRRK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RYM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=681.1591 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RYM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PRCH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=951.5244 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRCH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AEVA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2915.4027 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AEVA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DAVE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=423.2286 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAVE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IONQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=146.9234 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IONQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AMPX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=359.0904 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMPX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ABVX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14247.0553 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ABVX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SEZL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=748.7164 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SEZL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TMC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=453.7278 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TMC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LAES LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4238.7491 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LAES XGBoost: Starting GridSearchCV fit...
       ‚úÖ QNTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=1446.4740 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QNTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OKLO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=542.2383 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OKLO XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARQQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=918.6564 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARQQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ SRRK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.1031 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SRRK XGBoost: Starting GridSearchCV fit...
       ‚úÖ RYM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=2110.5706 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RYM XGBoost: Starting GridSearchCV fit...
       ‚úÖ KC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=700.2891 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KC XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRCH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=826.8349 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRCH XGBoost: Starting GridSearchCV fit...
       ‚úÖ AEVA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=3879.0384 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AEVA XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAVE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=397.2072 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAVE XGBoost: Starting GridSearchCV fit...
       ‚úÖ IONQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=221.2162 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IONQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ AMPX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=766.4788 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMPX XGBoost: Starting GridSearchCV fit...
       ‚úÖ ABVX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13377.0101 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ABVX XGBoost: Starting GridSearchCV fit...
       ‚úÖ SEZL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=1011.8016 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SEZL XGBoost: Starting GridSearchCV fit...
       ‚úÖ TMC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=695.7269 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TMC XGBoost: Starting GridSearchCV fit...
       ‚úÖ QNTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=2025.1683 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QNTM XGBoost: Starting GridSearchCV fit...
       ‚úÖ LAES XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3083.5140 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 130.5s
    - LSTM: MSE=0.0229
    - TCN: MSE=0.0053
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0053
        ‚Ä¢ LSTM: MSE=0.0229
        ‚Ä¢ XGBoost: MSE=3083.5140
        ‚Ä¢ Random Forest: MSE=3377.6165
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4238.7491
   ‚úÖ LAES: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LAES (TargetReturn): TCN with MSE=0.0053
üêõ DEBUG: LAES - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LAES.
üêõ DEBUG: LAES - Moving model to CPU before return...
üêõ DEBUG [22:19:41.338]: LAES - Returning result metadata...
üêõ DEBUG: train_worker started for HOOD
  ‚öôÔ∏è Training models for HOOD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - HOOD: Initiating feature extraction for training.
  [DIAGNOSTIC] HOOD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HOOD: rows after features available: 126
üéØ HOOD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HOOD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HOOD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HOOD: Training LSTM (50 epochs)...
      ‚è≥ HOOD LSTM: Epoch 10/50 (20%)
       ‚úÖ RYM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=745.6468 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 131.0s
    - LSTM: MSE=0.0291
    - TCN: MSE=0.0095
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0095
        ‚Ä¢ LSTM: MSE=0.0291
        ‚Ä¢ Random Forest: MSE=681.1591
        ‚Ä¢ XGBoost: MSE=745.6468
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2110.5706
   ‚úÖ RYM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RYM (TargetReturn): TCN with MSE=0.0095
üêõ DEBUG: RYM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RYM.
üêõ DEBUG: RYM - Moving model to CPU before return...
üêõ DEBUG [22:19:42.187]: RYM - Returning result metadata...
üêõ DEBUG: train_worker started for SMR
  ‚öôÔ∏è Training models for SMR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SMR: Initiating feature extraction for training.
  [DIAGNOSTIC] SMR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMR: rows after features available: 126
üéØ SMR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SMR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMR: Training LSTM (50 epochs)...
      ‚è≥ HOOD LSTM: Epoch 20/50 (40%)
      ‚è≥ SMR LSTM: Epoch 10/50 (20%)
      ‚è≥ HOOD LSTM: Epoch 30/50 (60%)
       ‚úÖ OKLO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=611.2795 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.9s
    - LSTM: MSE=0.6935
    - TCN: MSE=0.6889
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6889
        ‚Ä¢ LSTM: MSE=0.6935
        ‚Ä¢ Random Forest: MSE=438.1104
        ‚Ä¢ LightGBM Regressor (CPU): MSE=542.2383
        ‚Ä¢ XGBoost: MSE=611.2795
   ‚úÖ OKLO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OKLO (TargetReturn): TCN with MSE=0.6889
üêõ DEBUG: OKLO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OKLO.
üêõ DEBUG: OKLO - Moving model to CPU before return...
üêõ DEBUG [22:19:42.907]: OKLO - Returning result metadata...
üêõ DEBUG: train_worker started for UAMY
üêõ DEBUG [22:19:42.908]: Main received result for OKLO
  ‚öôÔ∏è Training models for UAMY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - UAMY: Initiating feature extraction for training.
  [DIAGNOSTIC] UAMY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UAMY: rows after features available: 126
üéØ UAMY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UAMY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UAMY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UAMY: Training LSTM (50 epochs)...
      ‚è≥ SMR LSTM: Epoch 20/50 (40%)
      ‚è≥ HOOD LSTM: Epoch 40/50 (80%)
      ‚è≥ UAMY LSTM: Epoch 10/50 (20%)
       ‚úÖ ARQQ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1499.0862 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 132.6s
    - LSTM: MSE=0.3586
    - TCN: MSE=0.2637
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2637
        ‚Ä¢ LSTM: MSE=0.3586
        ‚Ä¢ LightGBM Regressor (CPU): MSE=918.6564
        ‚Ä¢ Random Forest: MSE=954.3916
        ‚Ä¢ XGBoost: MSE=1499.0862
   ‚úÖ ARQQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARQQ (TargetReturn): TCN with MSE=0.2637
üêõ DEBUG: ARQQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARQQ.
üêõ DEBUG: ARQQ - Moving model to CPU before return...
üêõ DEBUG [22:19:43.622]: ARQQ - Returning result metadata...
üêõ DEBUG: train_worker started for PSIX
  ‚öôÔ∏è Training models for PSIX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - PSIX: Initiating feature extraction for training.
  [DIAGNOSTIC] PSIX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PSIX: rows after features available: 126
üéØ PSIX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PSIX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PSIX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PSIX: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.796117
         RMSE: 0.892254
         R¬≤ Score: -1.4697 (Poor - 147.0% variance explained)
      üîπ HOOD: Training TCN (50 epochs)...
      ‚è≥ SMR LSTM: Epoch 30/50 (60%)
       ‚úÖ KC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=485.6149 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 132.7s
    - LSTM: MSE=0.0113
    - TCN: MSE=0.0118
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.6 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.0113
        ‚Ä¢ TCN: MSE=0.0118
        ‚Ä¢ Random Forest: MSE=323.0237
        ‚Ä¢ XGBoost: MSE=485.6149
        ‚Ä¢ LightGBM Regressor (CPU): MSE=700.2891
   ‚úÖ KC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KC (TargetReturn): LSTM with MSE=0.0113
üêõ DEBUG: KC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KC.
üêõ DEBUG: KC - Moving model to CPU before return...
üêõ DEBUG [22:19:43.879]: KC - Returning result metadata...
üêõ DEBUG: train_worker started for LEU
      ‚è≥ HOOD TCN: Epoch 10/50 (20%)
      ‚è≥ UAMY LSTM: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for LEU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - LEU: Initiating feature extraction for training.
  [DIAGNOSTIC] LEU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LEU: rows after features available: 126
üéØ LEU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LEU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LEU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LEU: Training LSTM (50 epochs)...
      ‚è≥ HOOD TCN: Epoch 20/50 (40%)
      ‚è≥ HOOD TCN: Epoch 30/50 (60%)
      ‚è≥ PSIX LSTM: Epoch 10/50 (20%)
      ‚è≥ HOOD TCN: Epoch 40/50 (80%)
      ‚è≥ SMR LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.771732
         RMSE: 0.878483
         R¬≤ Score: -1.3940
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HOOD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HOOD Random Forest: Starting GridSearchCV fit...
      ‚è≥ UAMY LSTM: Epoch 30/50 (60%)
      ‚è≥ LEU LSTM: Epoch 10/50 (20%)
      ‚è≥ PSIX LSTM: Epoch 20/50 (40%)
       ‚úÖ PRCH XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=644.4748 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.6s
    - LSTM: MSE=0.2892
    - TCN: MSE=0.1413
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.9 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1413
        ‚Ä¢ LSTM: MSE=0.2892
        ‚Ä¢ XGBoost: MSE=644.4748
        ‚Ä¢ LightGBM Regressor (CPU): MSE=826.8349
        ‚Ä¢ Random Forest: MSE=951.5244
   ‚úÖ PRCH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRCH (TargetReturn): TCN with MSE=0.1413
üêõ DEBUG: PRCH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRCH.
üêõ DEBUG: PRCH - Moving model to CPU before return...
üêõ DEBUG [22:19:44.855]: PRCH - Returning result metadata...
üêõ DEBUG: train_worker started for PLTR
üêõ DEBUG [22:19:44.858]: Main received result for PRCH
üêõ DEBUG [22:19:44.858]: Main received result for LAES
üêõ DEBUG [22:19:44.858]: Main received result for RYM
üêõ DEBUG: Training progress: 4/959 done
  ‚öôÔ∏è Training models for PLTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - PLTR: Initiating feature extraction for training.
  [DIAGNOSTIC] PLTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PLTR: rows after features available: 126
üéØ PLTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PLTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PLTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PLTR: Training LSTM (50 epochs)...
      ‚è≥ UAMY LSTM: Epoch 40/50 (80%)
       ‚úÖ ABVX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15526.7949 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 133.4s
    - LSTM: MSE=0.2034
    - TCN: MSE=0.1462
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.7 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1462
        ‚Ä¢ LSTM: MSE=0.2034
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13377.0101
        ‚Ä¢ Random Forest: MSE=14247.0553
        ‚Ä¢ XGBoost: MSE=15526.7949
   ‚úÖ ABVX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ABVX (TargetReturn): TCN with MSE=0.1462
üêõ DEBUG: ABVX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ABVX.
üêõ DEBUG: ABVX - Moving model to CPU before return...
üêõ DEBUG [22:19:44.945]: ABVX - Returning result metadata...
üêõ DEBUG: train_worker started for OUST
      üìä LSTM Regression Metrics:
         MSE: 0.783067
         RMSE: 0.884911
         R¬≤ Score: -1.4678 (Poor - 146.8% variance explained)
      üîπ SMR: Training TCN (50 epochs)...
  ‚öôÔ∏è Training models for OUST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - OUST: Initiating feature extraction for training.
  [DIAGNOSTIC] OUST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OUST: rows after features available: 126
üéØ OUST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OUST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OUST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OUST: Training LSTM (50 epochs)...
      ‚è≥ SMR TCN: Epoch 10/50 (20%)
      ‚è≥ LEU LSTM: Epoch 20/50 (40%)
      ‚è≥ SMR TCN: Epoch 20/50 (40%)
       ‚úÖ AEVA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3856.6504 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 134.0s
    - LSTM: MSE=0.4504
    - TCN: MSE=0.2009
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.9 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2009
        ‚Ä¢ LSTM: MSE=0.4504
        ‚Ä¢ Random Forest: MSE=2915.4027
        ‚Ä¢ XGBoost: MSE=3856.6504
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3879.0384
   ‚úÖ AEVA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AEVA (TargetReturn): TCN with MSE=0.2009
üêõ DEBUG: AEVA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AEVA.
üêõ DEBUG: AEVA - Moving model to CPU before return...
üêõ DEBUG [22:19:45.242]: AEVA - Returning result metadata...
üêõ DEBUG: train_worker started for BKSY
  ‚öôÔ∏è Training models for BKSY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - BKSY: Initiating feature extraction for training.
  [DIAGNOSTIC] BKSY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BKSY: rows after features available: 126
üéØ BKSY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BKSY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BKSY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BKSY: Training LSTM (50 epochs)...
      ‚è≥ SMR TCN: Epoch 30/50 (60%)
      ‚è≥ PSIX LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.171477
         RMSE: 0.414097
         R¬≤ Score: -1.7020 (Poor - 170.2% variance explained)
      üîπ UAMY: Training TCN (50 epochs)...
      ‚è≥ SMR TCN: Epoch 40/50 (80%)
      ‚è≥ PLTR LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.601371
         RMSE: 0.775481
         R¬≤ Score: -0.8952
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMR Random Forest: Starting GridSearchCV fit...
      ‚è≥ OUST LSTM: Epoch 10/50 (20%)
      ‚è≥ UAMY TCN: Epoch 10/50 (20%)
       ‚úÖ AMPX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=371.0217 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 134.2s
    - LSTM: MSE=0.6157
    - TCN: MSE=0.3020
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.1 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3020
        ‚Ä¢ LSTM: MSE=0.6157
        ‚Ä¢ Random Forest: MSE=359.0904
        ‚Ä¢ XGBoost: MSE=371.0217
        ‚Ä¢ LightGBM Regressor (CPU): MSE=766.4788
   ‚úÖ AMPX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMPX (TargetReturn): TCN with MSE=0.3020
üêõ DEBUG: AMPX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMPX.
üêõ DEBUG: AMPX - Moving model to CPU before return...
üêõ DEBUG [22:19:45.699]: AMPX - Returning result metadata...
üêõ DEBUG [22:19:45.700]: Main received result for AMPX
üêõ DEBUG: train_worker started for GRRR
  ‚öôÔ∏è Training models for GRRR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - GRRR: Initiating feature extraction for training.
  [DIAGNOSTIC] GRRR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRRR: rows after features available: 126
üéØ GRRR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRRR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRRR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRRR: Training LSTM (50 epochs)...
      ‚è≥ UAMY TCN: Epoch 20/50 (40%)
      ‚è≥ LEU LSTM: Epoch 30/50 (60%)
      ‚è≥ BKSY LSTM: Epoch 10/50 (20%)
      ‚è≥ UAMY TCN: Epoch 30/50 (60%)
      ‚è≥ PSIX LSTM: Epoch 40/50 (80%)
       ‚úÖ TMC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=439.7524 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 134.5s
    - LSTM: MSE=0.4991
    - TCN: MSE=0.2876
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2876
        ‚Ä¢ LSTM: MSE=0.4991
        ‚Ä¢ XGBoost: MSE=439.7524
        ‚Ä¢ Random Forest: MSE=453.7278
        ‚Ä¢ LightGBM Regressor (CPU): MSE=695.7269
   ‚úÖ TMC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TMC (TargetReturn): TCN with MSE=0.2876
üêõ DEBUG: TMC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TMC.
üêõ DEBUG: TMC - Moving model to CPU before return...
üêõ DEBUG [22:19:46.093]: TMC - Returning result metadata...
üêõ DEBUG: train_worker started for TSSI
üêõ DEBUG [22:19:46.096]: Main received result for TMC
üêõ DEBUG [22:19:46.096]: Main received result for ARQQ
üêõ DEBUG [22:19:46.096]: Main received result for AEVA
üêõ DEBUG: Training progress: 8/959 done
      ‚è≥ UAMY TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for TSSI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TSSI: Initiating feature extraction for training.
  [DIAGNOSTIC] TSSI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TSSI: rows after features available: 126
üéØ TSSI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TSSI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TSSI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TSSI: Training LSTM (50 epochs)...
      ‚è≥ PLTR LSTM: Epoch 20/50 (40%)
      ‚è≥ OUST LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.078412
         RMSE: 0.280021
         R¬≤ Score: -0.2355
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UAMY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UAMY Random Forest: Starting GridSearchCV fit...
       ‚úÖ SRRK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.9797 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 135.3s
    - LSTM: MSE=0.4919
    - TCN: MSE=0.3194
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3194
        ‚Ä¢ LSTM: MSE=0.4919
        ‚Ä¢ Random Forest: MSE=20.1956
        ‚Ä¢ XGBoost: MSE=24.9797
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.1031
   ‚úÖ SRRK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SRRK (TargetReturn): TCN with MSE=0.3194
üêõ DEBUG: SRRK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SRRK.
üêõ DEBUG: SRRK - Moving model to CPU before return...
üêõ DEBUG [22:19:46.448]: SRRK - Returning result metadata...
üêõ DEBUG: train_worker started for ASPI
  ‚öôÔ∏è Training models for ASPI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - ASPI: Initiating feature extraction for training.
  [DIAGNOSTIC] ASPI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ASPI: rows after features available: 126
üéØ ASPI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ASPI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ASPI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ASPI: Training LSTM (50 epochs)...
      ‚è≥ LEU LSTM: Epoch 40/50 (80%)
      ‚è≥ GRRR LSTM: Epoch 10/50 (20%)
      ‚è≥ BKSY LSTM: Epoch 20/50 (40%)
       ‚úÖ SEZL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1023.0949 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 135.2s
    - LSTM: MSE=0.4569
    - TCN: MSE=0.2810
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.1 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2810
        ‚Ä¢ LSTM: MSE=0.4569
        ‚Ä¢ Random Forest: MSE=748.7164
        ‚Ä¢ LightGBM Regressor (CPU): MSE=1011.8016
        ‚Ä¢ XGBoost: MSE=1023.0949
   ‚úÖ SEZL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SEZL (TargetReturn): TCN with MSE=0.2810
üêõ DEBUG: SEZL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SEZL.
üêõ DEBUG: SEZL - Moving model to CPU before return...
üêõ DEBUG [22:19:46.757]: SEZL - Returning result metadata...
üêõ DEBUG: train_worker started for RCAT
      üìä LSTM Regression Metrics:
         MSE: 0.765974
         RMSE: 0.875199
         R¬≤ Score: -1.3904 (Poor - 139.0% variance explained)
      üîπ PSIX: Training TCN (50 epochs)...
  ‚öôÔ∏è Training models for RCAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - RCAT: Initiating feature extraction for training.
  [DIAGNOSTIC] RCAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RCAT: rows after features available: 126
üéØ RCAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RCAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RCAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RCAT: Training LSTM (50 epochs)...
      ‚è≥ TSSI LSTM: Epoch 10/50 (20%)
       ‚úÖ QNTM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1601.5973 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 135.1s
    - LSTM: MSE=0.2616
    - TCN: MSE=0.1845
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1845
        ‚Ä¢ LSTM: MSE=0.2616
        ‚Ä¢ Random Forest: MSE=1446.4740
        ‚Ä¢ XGBoost: MSE=1601.5973
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2025.1683
   ‚úÖ QNTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QNTM (TargetReturn): TCN with MSE=0.1845
üêõ DEBUG: QNTM - train_and_evaluate_models completed
       ‚úÖ IONQ XGBoost: GridSearchCV complete
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QNTM.
üêõ DEBUG: QNTM - Moving model to CPU before return...
üêõ DEBUG [22:19:46.921]: QNTM - Returning result metadata...
    ‚úÖ üéØ BEST! XGBoost: MSE=121.4363 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 135.5s
    - LSTM: MSE=0.3306
    - TCN: MSE=0.1971
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.6 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1971
        ‚Ä¢ LSTM: MSE=0.3306
        ‚Ä¢ XGBoost: MSE=121.4363
        ‚Ä¢ Random Forest: MSE=146.9234
        ‚Ä¢ LightGBM Regressor (CPU): MSE=221.2162
   ‚úÖ IONQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IONQ (TargetReturn): TCN with MSE=0.1971
üêõ DEBUG: IONQ - train_and_evaluate_models completed
      ‚è≥ PSIX TCN: Epoch 10/50 (20%)
üêõ DEBUG [22:19:46.924]: Main received result for QNTM
üêõ DEBUG: train_worker started for ACHR
üêõ DEBUG [22:19:46.924]: Main received result for ABVX
üêõ DEBUG [22:19:46.924]: Main received result for KC
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IONQ.
üêõ DEBUG: IONQ - Moving model to CPU before return...
üêõ DEBUG [22:19:46.930]: IONQ - Returning result metadata...
üêõ DEBUG: train_worker started for NVTS
  ‚öôÔ∏è Training models for NVTS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - NVTS: Initiating feature extraction for training.
  [DIAGNOSTIC] NVTS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVTS: rows after features available: 126
üéØ NVTS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for ACHR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ACHR: Initiating feature extraction for training.
  [DIAGNOSTIC] ACHR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ACHR: rows after features available: 126
üéØ ACHR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVTS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVTS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVTS: Training LSTM (50 epochs)...
  [DIAGNOSTIC] ACHR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ACHR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ACHR: Training LSTM (50 epochs)...
      ‚è≥ PLTR LSTM: Epoch 30/50 (60%)
      ‚è≥ OUST LSTM: Epoch 30/50 (60%)
      ‚è≥ PSIX TCN: Epoch 20/50 (40%)
       ‚úÖ DAVE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=419.2261 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 135.8s
    - LSTM: MSE=0.4399
    - TCN: MSE=0.3671
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.8 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3671
        ‚Ä¢ LSTM: MSE=0.4399
        ‚Ä¢ LightGBM Regressor (CPU): MSE=397.2072
        ‚Ä¢ XGBoost: MSE=419.2261
        ‚Ä¢ Random Forest: MSE=423.2286
   ‚úÖ DAVE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAVE (TargetReturn): TCN with MSE=0.3671
üêõ DEBUG: DAVE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAVE.
üêõ DEBUG: DAVE - Moving model to CPU before return...
üêõ DEBUG [22:19:47.134]: DAVE - Returning result metadata...
üêõ DEBUG [22:19:47.135]: Main received result for DAVE
üêõ DEBUG: Training progress: 12/959 done
üêõ DEBUG: train_worker started for KOD
üêõ DEBUG [22:19:47.136]: Main received result for SEZL
üêõ DEBUG [22:19:47.136]: Main received result for IONQ
üêõ DEBUG [22:19:47.136]: Main received result for SRRK
  ‚öôÔ∏è Training models for KOD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - KOD: Initiating feature extraction for training.
  [DIAGNOSTIC] KOD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KOD: rows after features available: 126
üéØ KOD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KOD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KOD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KOD: Training LSTM (50 epochs)...
      ‚è≥ ASPI LSTM: Epoch 10/50 (20%)
      ‚è≥ PSIX TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 1.065716
         RMSE: 1.032335
         R¬≤ Score: -1.6960 (Poor - 169.6% variance explained)
      üîπ LEU: Training TCN (50 epochs)...
      ‚è≥ GRRR LSTM: Epoch 20/50 (40%)
      ‚è≥ BKSY LSTM: Epoch 30/50 (60%)
      ‚è≥ PSIX TCN: Epoch 40/50 (80%)
      ‚è≥ LEU TCN: Epoch 10/50 (20%)
      ‚è≥ TSSI LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.484742
         RMSE: 0.696234
         R¬≤ Score: -0.5128
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PSIX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PSIX Random Forest: Starting GridSearchCV fit...
      ‚è≥ RCAT LSTM: Epoch 10/50 (20%)
      ‚è≥ LEU TCN: Epoch 20/50 (40%)
      ‚è≥ PLTR LSTM: Epoch 40/50 (80%)
      ‚è≥ NVTS LSTM: Epoch 10/50 (20%)
      ‚è≥ OUST LSTM: Epoch 40/50 (80%)
      ‚è≥ ACHR LSTM: Epoch 10/50 (20%)
      ‚è≥ LEU TCN: Epoch 30/50 (60%)
      ‚è≥ ASPI LSTM: Epoch 20/50 (40%)
      ‚è≥ KOD LSTM: Epoch 10/50 (20%)
      ‚è≥ LEU TCN: Epoch 40/50 (80%)
      ‚è≥ GRRR LSTM: Epoch 30/50 (60%)
      ‚è≥ BKSY LSTM: Epoch 40/50 (80%)
       ‚úÖ HOOD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=266.7814 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HOOD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.418402
         RMSE: 0.646840
         R¬≤ Score: -0.0585
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LEU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LEU Random Forest: Starting GridSearchCV fit...
      ‚è≥ TSSI LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.360167
         RMSE: 0.600139
         R¬≤ Score: -1.4262 (Poor - 142.6% variance explained)
      üîπ PLTR: Training TCN (50 epochs)...
      ‚è≥ RCAT LSTM: Epoch 20/50 (40%)
      ‚è≥ NVTS LSTM: Epoch 20/50 (40%)
      ‚è≥ ACHR LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.959024
         RMSE: 0.979298
         R¬≤ Score: -2.1157 (Poor - 211.6% variance explained)
      üîπ OUST: Training TCN (50 epochs)...
      ‚è≥ PLTR TCN: Epoch 10/50 (20%)
      ‚è≥ ASPI LSTM: Epoch 30/50 (60%)
      ‚è≥ OUST TCN: Epoch 10/50 (20%)
      ‚è≥ KOD LSTM: Epoch 20/50 (40%)
      ‚è≥ PLTR TCN: Epoch 20/50 (40%)
      ‚è≥ OUST TCN: Epoch 20/50 (40%)
      ‚è≥ GRRR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.762231
         RMSE: 0.873059
         R¬≤ Score: -1.5151 (Poor - 151.5% variance explained)
      üîπ BKSY: Training TCN (50 epochs)...
      ‚è≥ PLTR TCN: Epoch 30/50 (60%)
      ‚è≥ TSSI LSTM: Epoch 40/50 (80%)
      ‚è≥ OUST TCN: Epoch 30/50 (60%)
      ‚è≥ BKSY TCN: Epoch 10/50 (20%)
      ‚è≥ PLTR TCN: Epoch 40/50 (80%)
      ‚è≥ NVTS LSTM: Epoch 30/50 (60%)
      ‚è≥ OUST TCN: Epoch 40/50 (80%)
       ‚úÖ HOOD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=207.5341 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HOOD XGBoost: Starting GridSearchCV fit...
      ‚è≥ RCAT LSTM: Epoch 30/50 (60%)
      ‚è≥ BKSY TCN: Epoch 20/50 (40%)
      ‚è≥ ACHR LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.304059
         RMSE: 0.551416
         R¬≤ Score: -1.0483
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PLTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PLTR Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.477584
         RMSE: 0.691075
         R¬≤ Score: -0.5516
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OUST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OUST Random Forest: Starting GridSearchCV fit...
      ‚è≥ ASPI LSTM: Epoch 40/50 (80%)
      ‚è≥ BKSY TCN: Epoch 30/50 (60%)
      ‚è≥ KOD LSTM: Epoch 30/50 (60%)
      ‚è≥ BKSY TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.011203
         RMSE: 0.105845
         R¬≤ Score: -1.0062 (Poor - 100.6% variance explained)
      üîπ GRRR: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.470413
         RMSE: 0.685867
         R¬≤ Score: -0.5522
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BKSY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BKSY Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 1.042517
         RMSE: 1.021037
         R¬≤ Score: -1.6830 (Poor - 168.3% variance explained)
      üîπ TSSI: Training TCN (50 epochs)...
       ‚úÖ SMR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=385.3084 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GRRR TCN: Epoch 10/50 (20%)
      ‚è≥ TSSI TCN: Epoch 10/50 (20%)
      ‚è≥ NVTS LSTM: Epoch 40/50 (80%)
      ‚è≥ ACHR LSTM: Epoch 40/50 (80%)
      ‚è≥ GRRR TCN: Epoch 20/50 (40%)
      ‚è≥ TSSI TCN: Epoch 20/50 (40%)
      ‚è≥ RCAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.246710
         RMSE: 0.496699
         R¬≤ Score: -0.4471 (Poor - 44.7% variance explained)
      üîπ ASPI: Training TCN (50 epochs)...
      ‚è≥ GRRR TCN: Epoch 30/50 (60%)
      ‚è≥ TSSI TCN: Epoch 30/50 (60%)
      ‚è≥ ASPI TCN: Epoch 10/50 (20%)
      ‚è≥ KOD LSTM: Epoch 40/50 (80%)
      ‚è≥ GRRR TCN: Epoch 40/50 (80%)
      ‚è≥ TSSI TCN: Epoch 40/50 (80%)
      ‚è≥ ASPI TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.009157
         RMSE: 0.095690
         R¬≤ Score: -0.6397
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRRR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRRR Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.770795
         RMSE: 0.877950
         R¬≤ Score: -0.9837
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TSSI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TSSI Random Forest: Starting GridSearchCV fit...
       ‚úÖ UAMY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=263.7356 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UAMY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ASPI TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.797583
         RMSE: 0.893075
         R¬≤ Score: -0.9521 (Poor - 95.2% variance explained)
      üîπ NVTS: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.063537
         RMSE: 0.252066
         R¬≤ Score: -0.8729 (Poor - 87.3% variance explained)
      üîπ ACHR: Training TCN (50 epochs)...
      ‚è≥ ASPI TCN: Epoch 40/50 (80%)
      ‚è≥ NVTS TCN: Epoch 10/50 (20%)
      ‚è≥ ACHR TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.253998
         RMSE: 0.503982
         R¬≤ Score: -0.4898
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ASPI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ASPI Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.304704
         RMSE: 0.552000
         R¬≤ Score: -1.3668 (Poor - 136.7% variance explained)
      üîπ RCAT: Training TCN (50 epochs)...
       ‚úÖ SMR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=261.5929 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMR XGBoost: Starting GridSearchCV fit...
      ‚è≥ NVTS TCN: Epoch 20/50 (40%)
      ‚è≥ ACHR TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.519629
         RMSE: 0.720853
         R¬≤ Score: -1.5292 (Poor - 152.9% variance explained)
      üîπ KOD: Training TCN (50 epochs)...
      ‚è≥ RCAT TCN: Epoch 10/50 (20%)
      ‚è≥ NVTS TCN: Epoch 30/50 (60%)
      ‚è≥ ACHR TCN: Epoch 30/50 (60%)
      ‚è≥ KOD TCN: Epoch 10/50 (20%)
      ‚è≥ RCAT TCN: Epoch 20/50 (40%)
      ‚è≥ NVTS TCN: Epoch 40/50 (80%)
      ‚è≥ ACHR TCN: Epoch 40/50 (80%)
      ‚è≥ KOD TCN: Epoch 20/50 (40%)
      ‚è≥ RCAT TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.070860
         RMSE: 0.266195
         R¬≤ Score: -1.0887
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ACHR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ACHR Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.906359
         RMSE: 0.952029
         R¬≤ Score: -1.2184
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVTS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVTS Random Forest: Starting GridSearchCV fit...
      ‚è≥ KOD TCN: Epoch 30/50 (60%)
       ‚úÖ UAMY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=342.0728 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UAMY XGBoost: Starting GridSearchCV fit...
      ‚è≥ RCAT TCN: Epoch 40/50 (80%)
      ‚è≥ KOD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.448399
         RMSE: 0.669626
         R¬≤ Score: -1.1825
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KOD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KOD Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.199448
         RMSE: 0.446596
         R¬≤ Score: -0.5492
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RCAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RCAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ PSIX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=572.2533 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PSIX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LEU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=898.3984 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LEU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PSIX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=780.2259 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PSIX XGBoost: Starting GridSearchCV fit...
       ‚úÖ LEU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=678.8349 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LEU XGBoost: Starting GridSearchCV fit...
       ‚úÖ OUST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=1030.6599 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OUST LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PLTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=95.4295 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PLTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BKSY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=520.0179 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BKSY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OUST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=729.2499 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OUST XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRRR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2915.8157 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRRR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TSSI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=570.7483 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TSSI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BKSY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=285.7639 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BKSY XGBoost: Starting GridSearchCV fit...
       ‚úÖ PLTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=136.2601 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PLTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ ASPI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=212.3672 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ASPI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TSSI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=613.4824 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TSSI XGBoost: Starting GridSearchCV fit...
       ‚úÖ ACHR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=133.2526 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ACHR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GRRR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=3272.0158 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRRR XGBoost: Starting GridSearchCV fit...
       ‚úÖ NVTS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=541.8545 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVTS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KOD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=117.6751 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KOD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RCAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=317.5232 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RCAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ASPI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=200.2992 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ASPI XGBoost: Starting GridSearchCV fit...
       ‚úÖ ACHR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=234.4106 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ACHR XGBoost: Starting GridSearchCV fit...
       ‚úÖ NVTS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=797.0439 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVTS XGBoost: Starting GridSearchCV fit...
       ‚úÖ KOD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=232.3084 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KOD XGBoost: Starting GridSearchCV fit...
       ‚úÖ RCAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=405.9426 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RCAT XGBoost: Starting GridSearchCV fit...
       ‚úÖ HOOD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=296.7064 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.7961
    - TCN: MSE=0.7717
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7717
        ‚Ä¢ LSTM: MSE=0.7961
        ‚Ä¢ LightGBM Regressor (CPU): MSE=207.5341
        ‚Ä¢ Random Forest: MSE=266.7814
        ‚Ä¢ XGBoost: MSE=296.7064
   ‚úÖ HOOD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HOOD (TargetReturn): TCN with MSE=0.7717
üêõ DEBUG: HOOD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HOOD.
üêõ DEBUG: HOOD - Moving model to CPU before return...
üêõ DEBUG [22:21:49.420]: HOOD - Returning result metadata...
üêõ DEBUG [22:21:49.421]: Main received result for HOODüêõ DEBUG: train_worker started for GRPN

üêõ DEBUG: Training progress: 16/959 done
  ‚öôÔ∏è Training models for GRPN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - GRPN: Initiating feature extraction for training.
  [DIAGNOSTIC] GRPN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRPN: rows after features available: 126
üéØ GRPN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRPN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRPN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRPN: Training LSTM (50 epochs)...
      ‚è≥ GRPN LSTM: Epoch 10/50 (20%)
      ‚è≥ GRPN LSTM: Epoch 20/50 (40%)
      ‚è≥ GRPN LSTM: Epoch 30/50 (60%)
      ‚è≥ GRPN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.165687
         RMSE: 0.407047
         R¬≤ Score: -0.5925 (Poor - 59.3% variance explained)
      üîπ GRPN: Training TCN (50 epochs)...
      ‚è≥ GRPN TCN: Epoch 10/50 (20%)
      ‚è≥ GRPN TCN: Epoch 20/50 (40%)
      ‚è≥ GRPN TCN: Epoch 30/50 (60%)
      ‚è≥ GRPN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.105116
         RMSE: 0.324217
         R¬≤ Score: -0.0103
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRPN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRPN Random Forest: Starting GridSearchCV fit...
       ‚úÖ GRPN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=258.8612 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRPN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GRPN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=238.6251 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRPN XGBoost: Starting GridSearchCV fit...
       ‚úÖ UAMY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=338.7838 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 126.8s
    - LSTM: MSE=0.1715
    - TCN: MSE=0.0784
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0784
        ‚Ä¢ LSTM: MSE=0.1715
        ‚Ä¢ Random Forest: MSE=263.7356
        ‚Ä¢ XGBoost: MSE=338.7838
        ‚Ä¢ LightGBM Regressor (CPU): MSE=342.0728
   ‚úÖ UAMY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UAMY (TargetReturn): TCN with MSE=0.0784
üêõ DEBUG: UAMY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UAMY.
üêõ DEBUG: UAMY - Moving model to CPU before return...
üêõ DEBUG [22:21:58.661]: UAMY - Returning result metadata...
üêõ DEBUG: train_worker started for CLS
  ‚öôÔ∏è Training models for CLS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - CLS: Initiating feature extraction for training.
  [DIAGNOSTIC] CLS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CLS: rows after features available: 126
üéØ CLS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CLS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CLS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CLS: Training LSTM (50 epochs)...
      ‚è≥ CLS LSTM: Epoch 10/50 (20%)
       ‚úÖ SMR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=358.3666 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.2s
    - LSTM: MSE=0.7831
    - TCN: MSE=0.6014
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6014
        ‚Ä¢ LSTM: MSE=0.7831
        ‚Ä¢ LightGBM Regressor (CPU): MSE=261.5929
        ‚Ä¢ XGBoost: MSE=358.3666
        ‚Ä¢ Random Forest: MSE=385.3084
   ‚úÖ SMR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMR (TargetReturn): TCN with MSE=0.6014
üêõ DEBUG: SMR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMR.
üêõ DEBUG: SMR - Moving model to CPU before return...
üêõ DEBUG [22:21:59.467]: SMR - Returning result metadata...
üêõ DEBUG: train_worker started for CRDO
üêõ DEBUG [22:21:59.469]: Main received result for SMR
üêõ DEBUG [22:21:59.469]: Main received result for UAMY
  ‚öôÔ∏è Training models for CRDO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - CRDO: Initiating feature extraction for training.
  [DIAGNOSTIC] CRDO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRDO: rows after features available: 126
üéØ CRDO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRDO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRDO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRDO: Training LSTM (50 epochs)...
      ‚è≥ CLS LSTM: Epoch 20/50 (40%)
      ‚è≥ CRDO LSTM: Epoch 10/50 (20%)
      ‚è≥ CLS LSTM: Epoch 30/50 (60%)
      ‚è≥ CRDO LSTM: Epoch 20/50 (40%)
      ‚è≥ CLS LSTM: Epoch 40/50 (80%)
      ‚è≥ CRDO LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.607385
         RMSE: 0.779349
         R¬≤ Score: -0.7534 (Poor - 75.3% variance explained)
      üîπ CLS: Training TCN (50 epochs)...
      ‚è≥ CRDO LSTM: Epoch 40/50 (80%)
      ‚è≥ CLS TCN: Epoch 10/50 (20%)
      ‚è≥ CLS TCN: Epoch 20/50 (40%)
      ‚è≥ CLS TCN: Epoch 30/50 (60%)
      ‚è≥ CLS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.869887
         RMSE: 0.932677
         R¬≤ Score: -1.5112
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CLS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CLS Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.568953
         RMSE: 0.754290
         R¬≤ Score: -0.5353 (Poor - 53.5% variance explained)
      üîπ CRDO: Training TCN (50 epochs)...
      ‚è≥ CRDO TCN: Epoch 10/50 (20%)
      ‚è≥ CRDO TCN: Epoch 20/50 (40%)
      ‚è≥ CRDO TCN: Epoch 30/50 (60%)
      ‚è≥ CRDO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.929761
         RMSE: 0.964241
         R¬≤ Score: -1.5090
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRDO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRDO Random Forest: Starting GridSearchCV fit...
       ‚úÖ CLS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=251.8098 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CLS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRDO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=258.4516 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRDO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CLS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=134.1547 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CLS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRDO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=165.5983 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRDO XGBoost: Starting GridSearchCV fit...
       ‚úÖ PSIX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1024.4189 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 135.4s
    - LSTM: MSE=0.7660
    - TCN: MSE=0.4847
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 140.7 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4847
        ‚Ä¢ LSTM: MSE=0.7660
        ‚Ä¢ Random Forest: MSE=572.2533
        ‚Ä¢ LightGBM Regressor (CPU): MSE=780.2259
        ‚Ä¢ XGBoost: MSE=1024.4189
   ‚úÖ PSIX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PSIX (TargetReturn): TCN with MSE=0.4847
üêõ DEBUG: PSIX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PSIX.
üêõ DEBUG: PSIX - Moving model to CPU before return...
üêõ DEBUG [22:22:08.433]: PSIX - Returning result metadata...
üêõ DEBUG: train_worker started for ROOT
üêõ DEBUG [22:22:08.433]: Main received result for PSIX
  ‚öôÔ∏è Training models for ROOT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - ROOT: Initiating feature extraction for training.
  [DIAGNOSTIC] ROOT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ROOT: rows after features available: 126
üéØ ROOT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ROOT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ROOT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ROOT: Training LSTM (50 epochs)...
      ‚è≥ ROOT LSTM: Epoch 10/50 (20%)
      ‚è≥ ROOT LSTM: Epoch 20/50 (40%)
       ‚úÖ BKSY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=648.6837 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 134.9s
    - LSTM: MSE=0.7622
    - TCN: MSE=0.4704
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.7 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4704
        ‚Ä¢ LSTM: MSE=0.7622
        ‚Ä¢ LightGBM Regressor (CPU): MSE=285.7639
        ‚Ä¢ Random Forest: MSE=520.0179
        ‚Ä¢ XGBoost: MSE=648.6837
   ‚úÖ BKSY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BKSY (TargetReturn): TCN with MSE=0.4704
üêõ DEBUG: BKSY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BKSY.
üêõ DEBUG: BKSY - Moving model to CPU before return...
üêõ DEBUG [22:22:09.675]: BKSY - Returning result metadata...
üêõ DEBUG: train_worker started for BE
  ‚öôÔ∏è Training models for BE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - BE: Initiating feature extraction for training.
  [DIAGNOSTIC] BE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BE: rows after features available: 126
üéØ BE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BE: Training LSTM (50 epochs)...
      ‚è≥ ROOT LSTM: Epoch 30/50 (60%)
      ‚è≥ BE LSTM: Epoch 10/50 (20%)
      ‚è≥ ROOT LSTM: Epoch 40/50 (80%)
      ‚è≥ BE LSTM: Epoch 20/50 (40%)
       ‚úÖ PLTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=116.8214 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 136.3s
    - LSTM: MSE=0.3602
    - TCN: MSE=0.3041
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 141.5 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3041
        ‚Ä¢ LSTM: MSE=0.3602
        ‚Ä¢ Random Forest: MSE=95.4295
        ‚Ä¢ XGBoost: MSE=116.8214
        ‚Ä¢ LightGBM Regressor (CPU): MSE=136.2601
   ‚úÖ PLTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PLTR (TargetReturn): TCN with MSE=0.3041
üêõ DEBUG: PLTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PLTR.
üêõ DEBUG: PLTR - Moving model to CPU before return...
üêõ DEBUG [22:22:11.057]: PLTR - Returning result metadata...
üêõ DEBUG: train_worker started for SLDP
      üìä LSTM Regression Metrics:
         MSE: 0.136834
         RMSE: 0.369910
         R¬≤ Score: -0.7613 (Poor - 76.1% variance explained)
      üîπ ROOT: Training TCN (50 epochs)...
  ‚öôÔ∏è Training models for SLDP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SLDP: Initiating feature extraction for training.
  [DIAGNOSTIC] SLDP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLDP: rows after features available: 126
üéØ SLDP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLDP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLDP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLDP: Training LSTM (50 epochs)...
      ‚è≥ BE LSTM: Epoch 30/50 (60%)
      ‚è≥ ROOT TCN: Epoch 10/50 (20%)
      ‚è≥ ROOT TCN: Epoch 20/50 (40%)
      ‚è≥ ROOT TCN: Epoch 30/50 (60%)
      ‚è≥ ROOT TCN: Epoch 40/50 (80%)
      ‚è≥ BE LSTM: Epoch 40/50 (80%)
      ‚è≥ SLDP LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.093781
         RMSE: 0.306236
         R¬≤ Score: -0.2072
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ROOT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ROOT Random Forest: Starting GridSearchCV fit...
      ‚è≥ SLDP LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.112057
         RMSE: 0.334749
         R¬≤ Score: -0.9794 (Poor - 97.9% variance explained)
      üîπ BE: Training TCN (50 epochs)...
      ‚è≥ BE TCN: Epoch 10/50 (20%)
       ‚úÖ GRRR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=2344.9219 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 137.0s
    - LSTM: MSE=0.0112
    - TCN: MSE=0.0092
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 141.7 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0092
        ‚Ä¢ LSTM: MSE=0.0112
        ‚Ä¢ XGBoost: MSE=2344.9219
        ‚Ä¢ Random Forest: MSE=2915.8157
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3272.0158
   ‚úÖ GRRR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRRR (TargetReturn): TCN with MSE=0.0092
üêõ DEBUG: GRRR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRRR.
üêõ DEBUG: GRRR - Moving model to CPU before return...
üêõ DEBUG [22:22:12.447]: GRRR - Returning result metadata...
üêõ DEBUG: train_worker started for SGHC
  ‚öôÔ∏è Training models for SGHC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - SGHC: Initiating feature extraction for training.
  [DIAGNOSTIC] SGHC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SGHC: rows after features available: 126
üéØ SGHC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ BE TCN: Epoch 20/50 (40%)
  [DIAGNOSTIC] SGHC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SGHC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SGHC: Training LSTM (50 epochs)...
      ‚è≥ BE TCN: Epoch 30/50 (60%)
       ‚úÖ LEU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1001.4411 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 139.1s
    - LSTM: MSE=1.0657
    - TCN: MSE=0.4184
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 144.3 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4184
        ‚Ä¢ LSTM: MSE=1.0657
        ‚Ä¢ LightGBM Regressor (CPU): MSE=678.8349
        ‚Ä¢ Random Forest: MSE=898.3984
        ‚Ä¢ XGBoost: MSE=1001.4411
   ‚úÖ LEU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LEU (TargetReturn): TCN with MSE=0.4184
üêõ DEBUG: LEU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LEU.
üêõ DEBUG: LEU - Moving model to CPU before return...
üêõ DEBUG [22:22:12.644]: LEU - Returning result metadata...
üêõ DEBUG [22:22:12.644]: Main received result for LEU
üêõ DEBUG: Training progress: 20/959 done
üêõ DEBUG [22:22:12.644]: Main received result for PLTR
üêõ DEBUG: train_worker started for HIMS
  ‚öôÔ∏è Training models for HIMS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - HIMS: Initiating feature extraction for training.
  [DIAGNOSTIC] HIMS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HIMS: rows after features available: 126
üéØ HIMS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HIMS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HIMS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HIMS: Training LSTM (50 epochs)...
      ‚è≥ SLDP LSTM: Epoch 30/50 (60%)
      ‚è≥ BE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.077703
         RMSE: 0.278753
         R¬≤ Score: -0.3725
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BE Random Forest: Starting GridSearchCV fit...
      ‚è≥ SGHC LSTM: Epoch 10/50 (20%)
       ‚úÖ ACHR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=148.4294 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 137.0s
    - LSTM: MSE=0.0635
    - TCN: MSE=0.0709
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 141.4 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.0635
        ‚Ä¢ TCN: MSE=0.0709
        ‚Ä¢ Random Forest: MSE=133.2526
        ‚Ä¢ XGBoost: MSE=148.4294
        ‚Ä¢ LightGBM Regressor (CPU): MSE=234.4106
   ‚úÖ ACHR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ACHR (TargetReturn): LSTM with MSE=0.0635
üêõ DEBUG: ACHR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ACHR.
üêõ DEBUG: ACHR - Moving model to CPU before return...
üêõ DEBUG [22:22:13.146]: ACHR - Returning result metadata...
üêõ DEBUG: train_worker started for ATAI
  ‚öôÔ∏è Training models for ATAI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ATAI: Initiating feature extraction for training.
  [DIAGNOSTIC] ATAI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATAI: rows after features available: 126
üéØ ATAI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATAI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATAI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATAI: Training LSTM (50 epochs)...
      ‚è≥ HIMS LSTM: Epoch 10/50 (20%)
      ‚è≥ SLDP LSTM: Epoch 40/50 (80%)
      ‚è≥ SGHC LSTM: Epoch 20/50 (40%)
      ‚è≥ ATAI LSTM: Epoch 10/50 (20%)
       ‚úÖ OUST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=995.2932 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 139.4s
    - LSTM: MSE=0.9590
    - TCN: MSE=0.4776
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 144.2 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4776
        ‚Ä¢ LSTM: MSE=0.9590
        ‚Ä¢ LightGBM Regressor (CPU): MSE=729.2499
        ‚Ä¢ XGBoost: MSE=995.2932
        ‚Ä¢ Random Forest: MSE=1030.6599
   ‚úÖ OUST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OUST (TargetReturn): TCN with MSE=0.4776
üêõ DEBUG: OUST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OUST.
üêõ DEBUG: OUST - Moving model to CPU before return...
üêõ DEBUG [22:22:13.848]: OUST - Returning result metadata...
üêõ DEBUG: train_worker started for QURE
üêõ DEBUG [22:22:13.849]: Main received result for OUST
üêõ DEBUG [22:22:13.849]: Main received result for BKSY
üêõ DEBUG [22:22:13.849]: Main received result for GRRR
üêõ DEBUG: Training progress: 24/959 done
       ‚úÖ KOD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=99.9767 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 137.4s
    - LSTM: MSE=0.5196
    - TCN: MSE=0.4484
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 141.8 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4484
        ‚Ä¢ LSTM: MSE=0.5196
        ‚Ä¢ XGBoost: MSE=99.9767
        ‚Ä¢ Random Forest: MSE=117.6751
        ‚Ä¢ LightGBM Regressor (CPU): MSE=232.3084
   ‚úÖ KOD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KOD (TargetReturn): TCN with MSE=0.4484
üêõ DEBUG: KOD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KOD.
üêõ DEBUG: KOD - Moving model to CPU before return...
üêõ DEBUG [22:22:13.860]: KOD - Returning result metadata...
üêõ DEBUG: train_worker started for ZVIA
  ‚öôÔ∏è Training models for QURE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - QURE: Initiating feature extraction for training.
  [DIAGNOSTIC] QURE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QURE: rows after features available: 126
üéØ QURE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QURE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QURE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QURE: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for ZVIA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - ZVIA: Initiating feature extraction for training.
  [DIAGNOSTIC] ZVIA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ZVIA: rows after features available: 126
üéØ ZVIA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ZVIA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ZVIA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ZVIA: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.527642
         RMSE: 0.726390
         R¬≤ Score: -1.1508 (Poor - 115.1% variance explained)
      üîπ SLDP: Training TCN (50 epochs)...
      ‚è≥ HIMS LSTM: Epoch 20/50 (40%)
      ‚è≥ SLDP TCN: Epoch 10/50 (20%)
      ‚è≥ SLDP TCN: Epoch 20/50 (40%)
      ‚è≥ SGHC LSTM: Epoch 30/50 (60%)
      ‚è≥ SLDP TCN: Epoch 30/50 (60%)
      ‚è≥ ATAI LSTM: Epoch 20/50 (40%)
      ‚è≥ ZVIA LSTM: Epoch 10/50 (20%)
      ‚è≥ QURE LSTM: Epoch 10/50 (20%)
       ‚úÖ TSSI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=505.9753 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 139.2s
    - LSTM: MSE=1.0425
    - TCN: MSE=0.7708
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 143.8 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7708
        ‚Ä¢ LSTM: MSE=1.0425
        ‚Ä¢ XGBoost: MSE=505.9753
        ‚Ä¢ Random Forest: MSE=570.7483
        ‚Ä¢ LightGBM Regressor (CPU): MSE=613.4824
   ‚úÖ TSSI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TSSI (TargetReturn): TCN with MSE=0.7708
üêõ DEBUG: TSSI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TSSI.
üêõ DEBUG: TSSI - Moving model to CPU before return...
üêõ DEBUG [22:22:14.641]: TSSI - Returning result metadata...
üêõ DEBUG: train_worker started for PGY
üêõ DEBUG [22:22:14.643]: Main received result for TSSI
      ‚è≥ HIMS LSTM: Epoch 30/50 (60%)
      ‚è≥ SLDP TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for PGY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - PGY: Initiating feature extraction for training.
  [DIAGNOSTIC] PGY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PGY: rows after features available: 126
üéØ PGY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PGY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PGY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PGY: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.600474
         RMSE: 0.774903
         R¬≤ Score: -1.4477
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLDP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLDP Random Forest: Starting GridSearchCV fit...
      ‚è≥ SGHC LSTM: Epoch 40/50 (80%)
      ‚è≥ ATAI LSTM: Epoch 30/50 (60%)
      ‚è≥ QURE LSTM: Epoch 20/50 (40%)
      ‚è≥ ZVIA LSTM: Epoch 20/50 (40%)
      ‚è≥ PGY LSTM: Epoch 10/50 (20%)
      ‚è≥ HIMS LSTM: Epoch 40/50 (80%)
       ‚úÖ NVTS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=617.0351 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 139.1s
    - LSTM: MSE=0.7976
    - TCN: MSE=0.9064
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 143.7 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7976
        ‚Ä¢ TCN: MSE=0.9064
        ‚Ä¢ Random Forest: MSE=541.8545
        ‚Ä¢ XGBoost: MSE=617.0351
        ‚Ä¢ LightGBM Regressor (CPU): MSE=797.0439
   ‚úÖ NVTS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVTS (TargetReturn): LSTM with MSE=0.7976
üêõ DEBUG: NVTS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVTS.
üêõ DEBUG: NVTS - Moving model to CPU before return...
üêõ DEBUG [22:22:15.456]: NVTS - Returning result metadata...
üêõ DEBUG: train_worker started for INOD
  ‚öôÔ∏è Training models for INOD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - INOD: Initiating feature extraction for training.
  [DIAGNOSTIC] INOD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ INOD: rows after features available: 126
üéØ INOD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] INOD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö INOD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ INOD: Training LSTM (50 epochs)...
       ‚úÖ ROOT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=128.6593 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ROOT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RCAT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=291.9062 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 139.5s
    - LSTM: MSE=0.3047
    - TCN: MSE=0.1994
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 143.8 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1994
        ‚Ä¢ LSTM: MSE=0.3047
        ‚Ä¢ XGBoost: MSE=291.9062
        ‚Ä¢ Random Forest: MSE=317.5232
        ‚Ä¢ LightGBM Regressor (CPU): MSE=405.9426
   ‚úÖ RCAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RCAT (TargetReturn): TCN with MSE=0.1994
üêõ DEBUG: RCAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RCAT.
üêõ DEBUG: RCAT - Moving model to CPU before return...
üêõ DEBUG [22:22:15.911]: RCAT - Returning result metadata...
üêõ DEBUG: train_worker started for GRAL
  ‚öôÔ∏è Training models for GRAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - GRAL: Initiating feature extraction for training.
  [DIAGNOSTIC] GRAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRAL: rows after features available: 126
üéØ GRAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRAL: Training LSTM (50 epochs)...
      ‚è≥ ZVIA LSTM: Epoch 30/50 (60%)
      ‚è≥ QURE LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.321206
         RMSE: 0.566751
         R¬≤ Score: -0.6115 (Poor - 61.1% variance explained)
      üîπ SGHC: Training TCN (50 epochs)...
      ‚è≥ ATAI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.190100
         RMSE: 0.436004
         R¬≤ Score: -0.9291 (Poor - 92.9% variance explained)
      üîπ HIMS: Training TCN (50 epochs)...
      ‚è≥ PGY LSTM: Epoch 20/50 (40%)
      ‚è≥ SGHC TCN: Epoch 10/50 (20%)
      ‚è≥ HIMS TCN: Epoch 10/50 (20%)
      ‚è≥ SGHC TCN: Epoch 20/50 (40%)
      ‚è≥ INOD LSTM: Epoch 10/50 (20%)
      ‚è≥ HIMS TCN: Epoch 20/50 (40%)
      ‚è≥ SGHC TCN: Epoch 30/50 (60%)
       ‚úÖ ASPI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=271.7473 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 140.8s
    - LSTM: MSE=0.2467
    - TCN: MSE=0.2540
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 145.4 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2467
        ‚Ä¢ TCN: MSE=0.2540
        ‚Ä¢ LightGBM Regressor (CPU): MSE=200.2992
        ‚Ä¢ Random Forest: MSE=212.3672
        ‚Ä¢ XGBoost: MSE=271.7473
   ‚úÖ ASPI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ASPI (TargetReturn): LSTM with MSE=0.2467
üêõ DEBUG: ASPI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ASPI.
üêõ DEBUG: ASPI - Moving model to CPU before return...
üêõ DEBUG [22:22:16.624]: ASPI - Returning result metadata...
üêõ DEBUG: train_worker started for CRNC
üêõ DEBUG [22:22:16.626]: Main received result for ASPI
üêõ DEBUG [22:22:16.626]: Main received result for RCAT
üêõ DEBUG [22:22:16.626]: Main received result for ACHR
üêõ DEBUG: Training progress: 28/959 done
üêõ DEBUG [22:22:16.626]: Main received result for NVTS
üêõ DEBUG [22:22:16.626]: Main received result for KOD
       ‚úÖ ROOT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=136.4446 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.1s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ROOT XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for CRNC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - CRNC: Initiating feature extraction for training.
  [DIAGNOSTIC] CRNC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRNC: rows after features available: 126
üéØ CRNC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRNC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRNC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRNC: Training LSTM (50 epochs)...
      ‚è≥ HIMS TCN: Epoch 30/50 (60%)
      ‚è≥ SGHC TCN: Epoch 40/50 (80%)
      ‚è≥ GRAL LSTM: Epoch 10/50 (20%)
      ‚è≥ HIMS TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.553557
         RMSE: 0.744014
         R¬≤ Score: -0.9519 (Poor - 95.2% variance explained)
      üîπ ATAI: Training TCN (50 epochs)...
      ‚è≥ QURE LSTM: Epoch 40/50 (80%)
      ‚è≥ PGY LSTM: Epoch 30/50 (60%)
      ‚è≥ ZVIA LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.358857
         RMSE: 0.599047
         R¬≤ Score: -0.8003
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SGHC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SGHC Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.179231
         RMSE: 0.423357
         R¬≤ Score: -0.8188
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HIMS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HIMS Random Forest: Starting GridSearchCV fit...
      ‚è≥ ATAI TCN: Epoch 10/50 (20%)
      ‚è≥ ATAI TCN: Epoch 20/50 (40%)
      ‚è≥ INOD LSTM: Epoch 20/50 (40%)
       ‚úÖ BE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=162.5681 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ATAI TCN: Epoch 30/50 (60%)
      ‚è≥ CRNC LSTM: Epoch 10/50 (20%)
      ‚è≥ ATAI TCN: Epoch 40/50 (80%)
      ‚è≥ GRAL LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.075550
         RMSE: 0.274863
         R¬≤ Score: -0.8294 (Poor - 82.9% variance explained)
      üîπ ZVIA: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.092158
         RMSE: 0.303576
         R¬≤ Score: -1.2372 (Poor - 123.7% variance explained)
      üîπ QURE: Training TCN (50 epochs)...
      ‚è≥ PGY LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.646634
         RMSE: 0.804136
         R¬≤ Score: -1.2801
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATAI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATAI Random Forest: Starting GridSearchCV fit...
      ‚è≥ QURE TCN: Epoch 10/50 (20%)
      ‚è≥ ZVIA TCN: Epoch 10/50 (20%)
      ‚è≥ QURE TCN: Epoch 20/50 (40%)
      ‚è≥ INOD LSTM: Epoch 30/50 (60%)
      ‚è≥ ZVIA TCN: Epoch 20/50 (40%)
      ‚è≥ QURE TCN: Epoch 30/50 (60%)
      ‚è≥ ZVIA TCN: Epoch 30/50 (60%)
      ‚è≥ CRNC LSTM: Epoch 20/50 (40%)
      ‚è≥ ZVIA TCN: Epoch 40/50 (80%)
      ‚è≥ QURE TCN: Epoch 40/50 (80%)
      ‚è≥ GRAL LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.563557
         RMSE: 0.750704
         R¬≤ Score: -1.1642 (Poor - 116.4% variance explained)
      üîπ PGY: Training TCN (50 epochs)...
       ‚úÖ BE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=245.8882 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BE XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.063600
         RMSE: 0.252190
         R¬≤ Score: -0.5400
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ZVIA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ZVIA Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.059095
         RMSE: 0.243095
         R¬≤ Score: -0.4345
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QURE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QURE Random Forest: Starting GridSearchCV fit...
      ‚è≥ PGY TCN: Epoch 10/50 (20%)
      ‚è≥ PGY TCN: Epoch 20/50 (40%)
      ‚è≥ INOD LSTM: Epoch 40/50 (80%)
      ‚è≥ PGY TCN: Epoch 30/50 (60%)
      ‚è≥ CRNC LSTM: Epoch 30/50 (60%)
      ‚è≥ PGY TCN: Epoch 40/50 (80%)
      ‚è≥ GRAL LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.587261
         RMSE: 0.766329
         R¬≤ Score: -1.2552
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PGY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PGY Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLDP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=417.7357 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLDP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.297306
         RMSE: 0.545257
         R¬≤ Score: -0.7178 (Poor - 71.8% variance explained)
      üîπ INOD: Training TCN (50 epochs)...
      ‚è≥ INOD TCN: Epoch 10/50 (20%)
      ‚è≥ CRNC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.104298
         RMSE: 0.322952
         R¬≤ Score: -0.8407 (Poor - 84.1% variance explained)
      üîπ GRAL: Training TCN (50 epochs)...
      ‚è≥ INOD TCN: Epoch 20/50 (40%)
      ‚è≥ GRAL TCN: Epoch 10/50 (20%)
      ‚è≥ INOD TCN: Epoch 30/50 (60%)
      ‚è≥ GRAL TCN: Epoch 20/50 (40%)
      ‚è≥ INOD TCN: Epoch 40/50 (80%)
      ‚è≥ GRAL TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.270916
         RMSE: 0.520496
         R¬≤ Score: -0.5653
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä INOD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ INOD Random Forest: Starting GridSearchCV fit...
      ‚è≥ GRAL TCN: Epoch 40/50 (80%)
       ‚úÖ SLDP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=507.0025 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLDP XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.033965
         RMSE: 0.184296
         R¬≤ Score: -1.5776 (Poor - 157.8% variance explained)
      üîπ CRNC: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.068152
         RMSE: 0.261060
         R¬≤ Score: -0.2028
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRAL Random Forest: Starting GridSearchCV fit...
      ‚è≥ CRNC TCN: Epoch 10/50 (20%)
      ‚è≥ CRNC TCN: Epoch 20/50 (40%)
      ‚è≥ CRNC TCN: Epoch 30/50 (60%)
      ‚è≥ CRNC TCN: Epoch 40/50 (80%)
       ‚úÖ SGHC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=117.3415 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SGHC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.019083
         RMSE: 0.138142
         R¬≤ Score: -0.4482
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRNC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRNC Random Forest: Starting GridSearchCV fit...
       ‚úÖ HIMS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=309.5122 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HIMS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SGHC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=129.0095 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SGHC XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATAI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=249.3573 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATAI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HIMS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=320.5228 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HIMS XGBoost: Starting GridSearchCV fit...
       ‚úÖ QURE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=139.6773 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QURE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ZVIA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=158.1405 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ZVIA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ATAI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=249.9630 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATAI XGBoost: Starting GridSearchCV fit...
       ‚úÖ PGY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=895.8462 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PGY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QURE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=300.4418 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QURE XGBoost: Starting GridSearchCV fit...
       ‚úÖ ZVIA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=437.5204 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ZVIA XGBoost: Starting GridSearchCV fit...
       ‚úÖ INOD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=126.7237 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ INOD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PGY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=753.2332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PGY XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=373.0307 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRNC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=350.5879 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRNC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ INOD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=144.0420 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ INOD XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=483.8855 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRAL XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRNC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=1121.1279 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRNC XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRPN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=265.7002 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.1657
    - TCN: MSE=0.1051
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1051
        ‚Ä¢ LSTM: MSE=0.1657
        ‚Ä¢ LightGBM Regressor (CPU): MSE=238.6251
        ‚Ä¢ Random Forest: MSE=258.8612
        ‚Ä¢ XGBoost: MSE=265.7002
   ‚úÖ GRPN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRPN (TargetReturn): TCN with MSE=0.1051
üêõ DEBUG: GRPN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRPN.
üêõ DEBUG: GRPN - Moving model to CPU before return...
üêõ DEBUG [22:23:55.143]: GRPN - Returning result metadata...
üêõ DEBUG: train_worker started for NNE
üêõ DEBUG [22:23:55.145]: Main received result for GRPN
  ‚öôÔ∏è Training models for NNE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NNE: Initiating feature extraction for training.
  [DIAGNOSTIC] NNE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NNE: rows after features available: 126
üéØ NNE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NNE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NNE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NNE: Training LSTM (50 epochs)...
      ‚è≥ NNE LSTM: Epoch 10/50 (20%)
      ‚è≥ NNE LSTM: Epoch 20/50 (40%)
      ‚è≥ NNE LSTM: Epoch 30/50 (60%)
      ‚è≥ NNE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.391682
         RMSE: 0.625845
         R¬≤ Score: -1.4771 (Poor - 147.7% variance explained)
      üîπ NNE: Training TCN (50 epochs)...
      ‚è≥ NNE TCN: Epoch 10/50 (20%)
      ‚è≥ NNE TCN: Epoch 20/50 (40%)
      ‚è≥ NNE TCN: Epoch 30/50 (60%)
      ‚è≥ NNE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.269702
         RMSE: 0.519329
         R¬≤ Score: -0.7057
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NNE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NNE Random Forest: Starting GridSearchCV fit...
       ‚úÖ NNE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=184.3177 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NNE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NNE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=205.3232 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NNE XGBoost: Starting GridSearchCV fit...
       ‚úÖ CLS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=179.8892 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.6074
    - TCN: MSE=0.8699
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6074
        ‚Ä¢ TCN: MSE=0.8699
        ‚Ä¢ LightGBM Regressor (CPU): MSE=134.1547
        ‚Ä¢ XGBoost: MSE=179.8892
        ‚Ä¢ Random Forest: MSE=251.8098
   ‚úÖ CLS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CLS (TargetReturn): LSTM with MSE=0.6074
üêõ DEBUG: CLS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CLS.
üêõ DEBUG: CLS - Moving model to CPU before return...
üêõ DEBUG [22:24:03.885]: CLS - Returning result metadata...
üêõ DEBUG [22:24:03.885]: Main received result for CLS
üêõ DEBUG: Training progress: 32/959 done
üêõ DEBUG: train_worker started for KOPN
  ‚öôÔ∏è Training models for KOPN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - KOPN: Initiating feature extraction for training.
  [DIAGNOSTIC] KOPN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KOPN: rows after features available: 126
üéØ KOPN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KOPN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KOPN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KOPN: Training LSTM (50 epochs)...
      ‚è≥ KOPN LSTM: Epoch 10/50 (20%)
      ‚è≥ KOPN LSTM: Epoch 20/50 (40%)
      ‚è≥ KOPN LSTM: Epoch 30/50 (60%)
      ‚è≥ KOPN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.497386
         RMSE: 0.705256
         R¬≤ Score: -1.4027 (Poor - 140.3% variance explained)
      üîπ KOPN: Training TCN (50 epochs)...
      ‚è≥ KOPN TCN: Epoch 10/50 (20%)
      ‚è≥ KOPN TCN: Epoch 20/50 (40%)
      ‚è≥ KOPN TCN: Epoch 30/50 (60%)
      ‚è≥ KOPN TCN: Epoch 40/50 (80%)
       ‚úÖ CRDO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=220.8067 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.8s
    - LSTM: MSE=0.5690
    - TCN: MSE=0.9298
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5690
        ‚Ä¢ TCN: MSE=0.9298
        ‚Ä¢ LightGBM Regressor (CPU): MSE=165.5983
        ‚Ä¢ XGBoost: MSE=220.8067
        ‚Ä¢ Random Forest: MSE=258.4516
   ‚úÖ CRDO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRDO (TargetReturn): LSTM with MSE=0.5690
üêõ DEBUG: CRDO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRDO.
üêõ DEBUG: CRDO - Moving model to CPU before return...
üêõ DEBUG [22:24:07.133]: CRDO - Returning result metadata...
üêõ DEBUG [22:24:07.134]: Main received result for CRDO
üêõ DEBUG: train_worker started for CANG
  ‚öôÔ∏è Training models for CANG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - CANG: Initiating feature extraction for training.
  [DIAGNOSTIC] CANG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CANG: rows after features available: 126
üéØ CANG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CANG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CANG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CANG: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.386662
         RMSE: 0.621822
         R¬≤ Score: -0.8678
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KOPN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KOPN Random Forest: Starting GridSearchCV fit...
      ‚è≥ CANG LSTM: Epoch 10/50 (20%)
      ‚è≥ CANG LSTM: Epoch 20/50 (40%)
      ‚è≥ CANG LSTM: Epoch 30/50 (60%)
      ‚è≥ CANG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.173052
         RMSE: 0.415996
         R¬≤ Score: -1.3114 (Poor - 131.1% variance explained)
      üîπ CANG: Training TCN (50 epochs)...
      ‚è≥ CANG TCN: Epoch 10/50 (20%)
      ‚è≥ CANG TCN: Epoch 20/50 (40%)
      ‚è≥ CANG TCN: Epoch 30/50 (60%)
      ‚è≥ CANG TCN: Epoch 40/50 (80%)
       ‚úÖ KOPN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=240.6503 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KOPN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.122577
         RMSE: 0.350110
         R¬≤ Score: -0.6372
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CANG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CANG Random Forest: Starting GridSearchCV fit...
       ‚úÖ KOPN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=139.0153 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KOPN XGBoost: Starting GridSearchCV fit...
       ‚úÖ CANG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=105.0344 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CANG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CANG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=129.5871 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CANG XGBoost: Starting GridSearchCV fit...
       ‚úÖ ROOT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=121.5170 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.2s
    - LSTM: MSE=0.1368
    - TCN: MSE=0.0938
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0938
        ‚Ä¢ LSTM: MSE=0.1368
        ‚Ä¢ XGBoost: MSE=121.5170
        ‚Ä¢ Random Forest: MSE=128.6593
        ‚Ä¢ LightGBM Regressor (CPU): MSE=136.4446
   ‚úÖ ROOT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ROOT (TargetReturn): TCN with MSE=0.0938
üêõ DEBUG: ROOT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ROOT.
üêõ DEBUG: ROOT - Moving model to CPU before return...
üêõ DEBUG [22:24:21.863]: ROOT - Returning result metadata...
üêõ DEBUG [22:24:21.863]: Main received result for ROOT
üêõ DEBUG: train_worker started for APP
  ‚öôÔ∏è Training models for APP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - APP: Initiating feature extraction for training.
  [DIAGNOSTIC] APP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APP: rows after features available: 126
üéØ APP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APP: Training LSTM (50 epochs)...
      ‚è≥ APP LSTM: Epoch 10/50 (20%)
      ‚è≥ APP LSTM: Epoch 20/50 (40%)
      ‚è≥ APP LSTM: Epoch 30/50 (60%)
      ‚è≥ APP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.200507
         RMSE: 0.447780
         R¬≤ Score: -0.6395 (Poor - 63.9% variance explained)
      üîπ APP: Training TCN (50 epochs)...
      ‚è≥ APP TCN: Epoch 10/50 (20%)
      ‚è≥ APP TCN: Epoch 20/50 (40%)
      ‚è≥ APP TCN: Epoch 30/50 (60%)
      ‚è≥ APP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.174071
         RMSE: 0.417218
         R¬≤ Score: -0.4233
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APP Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLDP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=618.1293 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.0s
    - LSTM: MSE=0.5276
    - TCN: MSE=0.6005
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5276
        ‚Ä¢ TCN: MSE=0.6005
        ‚Ä¢ Random Forest: MSE=417.7357
        ‚Ä¢ LightGBM Regressor (CPU): MSE=507.0025
        ‚Ä¢ XGBoost: MSE=618.1293
   ‚úÖ SLDP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLDP (TargetReturn): LSTM with MSE=0.5276
üêõ DEBUG: SLDP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLDP.
üêõ DEBUG: SLDP - Moving model to CPU before return...
üêõ DEBUG [22:24:27.251]: SLDP - Returning result metadata...
üêõ DEBUG: train_worker started for ONDS
  ‚öôÔ∏è Training models for ONDS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - ONDS: Initiating feature extraction for training.
  [DIAGNOSTIC] ONDS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ONDS: rows after features available: 126
üéØ ONDS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ONDS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ONDS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ONDS: Training LSTM (50 epochs)...
       ‚úÖ BE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=154.7919 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 128.9s
    - LSTM: MSE=0.1121
    - TCN: MSE=0.0777
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0777
        ‚Ä¢ LSTM: MSE=0.1121
        ‚Ä¢ XGBoost: MSE=154.7919
        ‚Ä¢ Random Forest: MSE=162.5681
        ‚Ä¢ LightGBM Regressor (CPU): MSE=245.8882
   ‚úÖ BE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BE (TargetReturn): TCN with MSE=0.0777
üêõ DEBUG: BE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BE.
üêõ DEBUG: BE - Moving model to CPU before return...
üêõ DEBUG [22:24:27.314]: BE - Returning result metadata...
üêõ DEBUG: train_worker started for SOFI
üêõ DEBUG [22:24:27.314]: Main received result for BE
üêõ DEBUG [22:24:27.315]: Main received result for SLDP
üêõ DEBUG: Training progress: 36/959 done
  ‚öôÔ∏è Training models for SOFI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - SOFI: Initiating feature extraction for training.
  [DIAGNOSTIC] SOFI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SOFI: rows after features available: 126
üéØ SOFI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SOFI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SOFI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SOFI: Training LSTM (50 epochs)...
       ‚úÖ APP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=139.3172 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ONDS LSTM: Epoch 10/50 (20%)
      ‚è≥ SOFI LSTM: Epoch 10/50 (20%)
      ‚è≥ SOFI LSTM: Epoch 20/50 (40%)
      ‚è≥ ONDS LSTM: Epoch 20/50 (40%)
       ‚úÖ APP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=137.4236 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APP XGBoost: Starting GridSearchCV fit...
      ‚è≥ ONDS LSTM: Epoch 30/50 (60%)
      ‚è≥ SOFI LSTM: Epoch 30/50 (60%)
      ‚è≥ ONDS LSTM: Epoch 40/50 (80%)
      ‚è≥ SOFI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.585222
         RMSE: 0.764998
         R¬≤ Score: -0.8342 (Poor - 83.4% variance explained)
      üîπ SOFI: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.793494
         RMSE: 0.890783
         R¬≤ Score: -0.9032 (Poor - 90.3% variance explained)
      üîπ ONDS: Training TCN (50 epochs)...
      ‚è≥ SOFI TCN: Epoch 10/50 (20%)
      ‚è≥ ONDS TCN: Epoch 10/50 (20%)
      ‚è≥ SOFI TCN: Epoch 20/50 (40%)
      ‚è≥ ONDS TCN: Epoch 20/50 (40%)
      ‚è≥ SOFI TCN: Epoch 30/50 (60%)
      ‚è≥ ONDS TCN: Epoch 30/50 (60%)
      ‚è≥ SOFI TCN: Epoch 40/50 (80%)
      ‚è≥ ONDS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.483448
         RMSE: 0.695304
         R¬≤ Score: -0.5152
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SOFI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SOFI Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.582098
         RMSE: 0.762954
         R¬≤ Score: -0.3961
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ONDS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ONDS Random Forest: Starting GridSearchCV fit...
       ‚úÖ SGHC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=105.1179 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 131.0s
    - LSTM: MSE=0.3212
    - TCN: MSE=0.3589
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.8 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3212
        ‚Ä¢ TCN: MSE=0.3589
        ‚Ä¢ XGBoost: MSE=105.1179
        ‚Ä¢ Random Forest: MSE=117.3415
        ‚Ä¢ LightGBM Regressor (CPU): MSE=129.0095
   ‚úÖ SGHC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SGHC (TargetReturn): LSTM with MSE=0.3212
üêõ DEBUG: SGHC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SGHC.
üêõ DEBUG: SGHC - Moving model to CPU before return...
üêõ DEBUG [22:24:32.755]: SGHC - Returning result metadata...
üêõ DEBUG: train_worker started for AKBA
üêõ DEBUG [22:24:32.756]: Main received result for SGHC
  ‚öôÔ∏è Training models for AKBA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - AKBA: Initiating feature extraction for training.
  [DIAGNOSTIC] AKBA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AKBA: rows after features available: 126
üéØ AKBA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AKBA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AKBA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AKBA: Training LSTM (50 epochs)...
       ‚úÖ ONDS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=544.1467 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ONDS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SOFI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=72.7626 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SOFI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AKBA LSTM: Epoch 10/50 (20%)
       ‚úÖ ZVIA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=181.5376 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 130.4s
    - LSTM: MSE=0.0755
    - TCN: MSE=0.0636
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0636
        ‚Ä¢ LSTM: MSE=0.0755
        ‚Ä¢ Random Forest: MSE=158.1405
        ‚Ä¢ XGBoost: MSE=181.5376
        ‚Ä¢ LightGBM Regressor (CPU): MSE=437.5204
   ‚úÖ ZVIA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ZVIA (TargetReturn): TCN with MSE=0.0636
üêõ DEBUG: ZVIA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ZVIA.
üêõ DEBUG: ZVIA - Moving model to CPU before return...
üêõ DEBUG [22:24:33.576]: ZVIA - Returning result metadata...
üêõ DEBUG: train_worker started for MPU
  ‚öôÔ∏è Training models for MPU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - MPU: Initiating feature extraction for training.
  [DIAGNOSTIC] MPU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MPU: rows after features available: 126
üéØ MPU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MPU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MPU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MPU: Training LSTM (50 epochs)...
      ‚è≥ AKBA LSTM: Epoch 20/50 (40%)
       ‚úÖ HIMS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=422.3157 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.1s
    - LSTM: MSE=0.1901
    - TCN: MSE=0.1792
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1792
        ‚Ä¢ LSTM: MSE=0.1901
        ‚Ä¢ Random Forest: MSE=309.5122
        ‚Ä¢ LightGBM Regressor (CPU): MSE=320.5228
        ‚Ä¢ XGBoost: MSE=422.3157
   ‚úÖ HIMS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HIMS (TargetReturn): TCN with MSE=0.1792
üêõ DEBUG: HIMS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HIMS.
üêõ DEBUG: HIMS - Moving model to CPU before return...
üêõ DEBUG [22:24:34.061]: HIMS - Returning result metadata...
üêõ DEBUG: train_worker started for RBRK
üêõ DEBUG [22:24:34.062]: Main received result for HIMS
       ‚úÖ ONDS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=363.2798 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ONDS XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for RBRK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - RBRK: Initiating feature extraction for training.
  [DIAGNOSTIC] RBRK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RBRK: rows after features available: 126
üéØ RBRK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RBRK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RBRK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RBRK: Training LSTM (50 epochs)...
      ‚è≥ MPU LSTM: Epoch 10/50 (20%)
       ‚úÖ ATAI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=299.6049 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.8s
    - LSTM: MSE=0.5536
    - TCN: MSE=0.6466
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.6 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5536
        ‚Ä¢ TCN: MSE=0.6466
        ‚Ä¢ Random Forest: MSE=249.3573
        ‚Ä¢ LightGBM Regressor (CPU): MSE=249.9630
        ‚Ä¢ XGBoost: MSE=299.6049
   ‚úÖ ATAI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATAI (TargetReturn): LSTM with MSE=0.5536
üêõ DEBUG: ATAI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATAI.
üêõ DEBUG: ATAI - Moving model to CPU before return...
üêõ DEBUG [22:24:34.332]: ATAI - Returning result metadata...
üêõ DEBUG: train_worker started for ALLT
üêõ DEBUG [22:24:34.333]: Main received result for ATAI
  ‚öôÔ∏è Training models for ALLT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ALLT: Initiating feature extraction for training.
  [DIAGNOSTIC] ALLT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ALLT: rows after features available: 126
üéØ ALLT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ALLT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ALLT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ALLT: Training LSTM (50 epochs)...
       ‚úÖ SOFI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=81.0129 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.1s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SOFI XGBoost: Starting GridSearchCV fit...
      ‚è≥ AKBA LSTM: Epoch 30/50 (60%)
      ‚è≥ RBRK LSTM: Epoch 10/50 (20%)
      ‚è≥ MPU LSTM: Epoch 20/50 (40%)
       ‚úÖ PGY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1002.9628 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.2s
    - LSTM: MSE=0.5636
    - TCN: MSE=0.5873
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.7 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5636
        ‚Ä¢ TCN: MSE=0.5873
        ‚Ä¢ LightGBM Regressor (CPU): MSE=753.2332
        ‚Ä¢ Random Forest: MSE=895.8462
        ‚Ä¢ XGBoost: MSE=1002.9628
   ‚úÖ PGY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PGY (TargetReturn): LSTM with MSE=0.5636
üêõ DEBUG: PGY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PGY.
üêõ DEBUG: PGY - Moving model to CPU before return...
üêõ DEBUG [22:24:34.837]: PGY - Returning result metadata...
üêõ DEBUG: train_worker started for ORLA
  ‚öôÔ∏è Training models for ORLA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ORLA: Initiating feature extraction for training.
  [DIAGNOSTIC] ORLA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ORLA: rows after features available: 126
üéØ ORLA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ORLA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ORLA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ORLA: Training LSTM (50 epochs)...
      ‚è≥ ALLT LSTM: Epoch 10/50 (20%)
       ‚úÖ GRAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=404.2139 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 130.6s
    - LSTM: MSE=0.1043
    - TCN: MSE=0.0682
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0682
        ‚Ä¢ LSTM: MSE=0.1043
        ‚Ä¢ Random Forest: MSE=373.0307
        ‚Ä¢ XGBoost: MSE=404.2139
        ‚Ä¢ LightGBM Regressor (CPU): MSE=483.8855
   ‚úÖ GRAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRAL (TargetReturn): TCN with MSE=0.0682
üêõ DEBUG: GRAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRAL.
üêõ DEBUG: GRAL - Moving model to CPU before return...
üêõ DEBUG [22:24:35.189]: GRAL - Returning result metadata...
üêõ DEBUG: train_worker started for LX
      ‚è≥ AKBA LSTM: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for LX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - LX: Initiating feature extraction for training.
  [DIAGNOSTIC] LX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LX: rows after features available: 126
üéØ LX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LX: Training LSTM (50 epochs)...
      ‚è≥ RBRK LSTM: Epoch 20/50 (40%)
      ‚è≥ MPU LSTM: Epoch 30/50 (60%)
      ‚è≥ ALLT LSTM: Epoch 20/50 (40%)
      ‚è≥ ORLA LSTM: Epoch 10/50 (20%)
      ‚è≥ RBRK LSTM: Epoch 30/50 (60%)
       ‚úÖ QURE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=101.4229 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.8s
    - LSTM: MSE=0.0922
    - TCN: MSE=0.0591
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0591
        ‚Ä¢ LSTM: MSE=0.0922
        ‚Ä¢ XGBoost: MSE=101.4229
        ‚Ä¢ Random Forest: MSE=139.6773
        ‚Ä¢ LightGBM Regressor (CPU): MSE=300.4418
   ‚úÖ QURE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QURE (TargetReturn): TCN with MSE=0.0591
üêõ DEBUG: QURE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QURE.
üêõ DEBUG: QURE - Moving model to CPU before return...
üêõ DEBUG [22:24:35.895]: QURE - Returning result metadata...
üêõ DEBUG: train_worker started for SEI
üêõ DEBUG [22:24:35.896]: Main received result for QURE
üêõ DEBUG: Training progress: 40/959 done
üêõ DEBUG [22:24:35.896]: Main received result for ZVIA
üêõ DEBUG [22:24:35.896]: Main received result for PGY
      ‚è≥ LX LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for SEI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - SEI: Initiating feature extraction for training.
  [DIAGNOSTIC] SEI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SEI: rows after features available: 126
üéØ SEI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ MPU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.323245
         RMSE: 0.568546
         R¬≤ Score: -0.6530 (Poor - 65.3% variance explained)
      üîπ AKBA: Training TCN (50 epochs)...
  [DIAGNOSTIC] SEI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SEI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SEI: Training LSTM (50 epochs)...
       ‚úÖ CRNC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1305.4981 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 130.9s
    - LSTM: MSE=0.0340
    - TCN: MSE=0.0191
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.1 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0191
        ‚Ä¢ LSTM: MSE=0.0340
        ‚Ä¢ Random Forest: MSE=350.5879
        ‚Ä¢ LightGBM Regressor (CPU): MSE=1121.1279
        ‚Ä¢ XGBoost: MSE=1305.4981
   ‚úÖ CRNC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRNC (TargetReturn): TCN with MSE=0.0191
üêõ DEBUG: CRNC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRNC.
üêõ DEBUG: CRNC - Moving model to CPU before return...
üêõ DEBUG [22:24:35.993]: CRNC - Returning result metadata...
üêõ DEBUG: train_worker started for ETON
  ‚öôÔ∏è Training models for ETON (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - ETON: Initiating feature extraction for training.
  [DIAGNOSTIC] ETON: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETON: rows after features available: 126
üéØ ETON: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETON: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETON: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETON: Training LSTM (50 epochs)...
      ‚è≥ AKBA TCN: Epoch 10/50 (20%)
      ‚è≥ ALLT LSTM: Epoch 30/50 (60%)
      ‚è≥ ORLA LSTM: Epoch 20/50 (40%)
      ‚è≥ AKBA TCN: Epoch 20/50 (40%)
      ‚è≥ AKBA TCN: Epoch 30/50 (60%)
      ‚è≥ RBRK LSTM: Epoch 40/50 (80%)
      ‚è≥ AKBA TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.407770
         RMSE: 0.638569
         R¬≤ Score: -1.1218 (Poor - 112.2% variance explained)
      üîπ MPU: Training TCN (50 epochs)...
      ‚è≥ LX LSTM: Epoch 20/50 (40%)
      ‚è≥ SEI LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.313898
         RMSE: 0.560266
         R¬≤ Score: -0.6052
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AKBA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AKBA Random Forest: Starting GridSearchCV fit...
      ‚è≥ MPU TCN: Epoch 10/50 (20%)
      ‚è≥ ETON LSTM: Epoch 10/50 (20%)
      ‚è≥ ORLA LSTM: Epoch 30/50 (60%)
      ‚è≥ ALLT LSTM: Epoch 40/50 (80%)
      ‚è≥ MPU TCN: Epoch 20/50 (40%)
      ‚è≥ MPU TCN: Epoch 30/50 (60%)
      ‚è≥ MPU TCN: Epoch 40/50 (80%)
      ‚è≥ LX LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.373461
         RMSE: 0.611115
         R¬≤ Score: -0.9987 (Poor - 99.9% variance explained)
      üîπ RBRK: Training TCN (50 epochs)...
      ‚è≥ SEI LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.305081
         RMSE: 0.552341
         R¬≤ Score: -0.5875
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MPU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MPU Random Forest: Starting GridSearchCV fit...
      ‚è≥ ETON LSTM: Epoch 20/50 (40%)
      ‚è≥ RBRK TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.269273
         RMSE: 0.518915
         R¬≤ Score: -0.8188 (Poor - 81.9% variance explained)
      üîπ ALLT: Training TCN (50 epochs)...
      ‚è≥ ORLA LSTM: Epoch 40/50 (80%)
      ‚è≥ RBRK TCN: Epoch 20/50 (40%)
      ‚è≥ ALLT TCN: Epoch 10/50 (20%)
      ‚è≥ RBRK TCN: Epoch 30/50 (60%)
      ‚è≥ ALLT TCN: Epoch 20/50 (40%)
      ‚è≥ RBRK TCN: Epoch 40/50 (80%)
      ‚è≥ ALLT TCN: Epoch 30/50 (60%)
      ‚è≥ LX LSTM: Epoch 40/50 (80%)
       ‚úÖ INOD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=128.6075 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.9s
    - LSTM: MSE=0.2973
    - TCN: MSE=0.2709
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2709
        ‚Ä¢ LSTM: MSE=0.2973
        ‚Ä¢ Random Forest: MSE=126.7237
        ‚Ä¢ XGBoost: MSE=128.6075
        ‚Ä¢ LightGBM Regressor (CPU): MSE=144.0420
   ‚úÖ INOD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for INOD (TargetReturn): TCN with MSE=0.2709
üêõ DEBUG: INOD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for INOD.
üêõ DEBUG: INOD - Moving model to CPU before return...
üêõ DEBUG [22:24:38.134]: INOD - Returning result metadata...
üêõ DEBUG [22:24:38.134]: Main received result for INOD
üêõ DEBUG [22:24:38.134]: Main received result for GRAL
üêõ DEBUG: Training progress: 44/959 done
üêõ DEBUG [22:24:38.134]: Main received result for CRNC
üêõ DEBUG: train_worker started for KTOS
      ‚è≥ SEI LSTM: Epoch 30/50 (60%)
      ‚è≥ ALLT TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for KTOS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - KTOS: Initiating feature extraction for training.
  [DIAGNOSTIC] KTOS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KTOS: rows after features available: 126
üéØ KTOS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      üìä TCN Regression Metrics:
         MSE: 0.255725
         RMSE: 0.505692
         R¬≤ Score: -0.3686
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RBRK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RBRK Random Forest: Starting GridSearchCV fit...
  [DIAGNOSTIC] KTOS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KTOS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KTOS: Training LSTM (50 epochs)...
      ‚è≥ ETON LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.258017
         RMSE: 0.507954
         R¬≤ Score: -0.7428
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ALLT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ALLT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.405910
         RMSE: 0.637111
         R¬≤ Score: -0.7824 (Poor - 78.2% variance explained)
      üîπ ORLA: Training TCN (50 epochs)...
      ‚è≥ ORLA TCN: Epoch 10/50 (20%)
      ‚è≥ ORLA TCN: Epoch 20/50 (40%)
      ‚è≥ ORLA TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.063387
         RMSE: 0.251769
         R¬≤ Score: -2.5568 (Poor - 255.7% variance explained)
      üîπ LX: Training TCN (50 epochs)...
      ‚è≥ SEI LSTM: Epoch 40/50 (80%)
      ‚è≥ ETON LSTM: Epoch 40/50 (80%)
      ‚è≥ ORLA TCN: Epoch 40/50 (80%)
      ‚è≥ KTOS LSTM: Epoch 10/50 (20%)
      ‚è≥ LX TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.386755
         RMSE: 0.621896
         R¬≤ Score: -0.6983
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ORLA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ORLA Random Forest: Starting GridSearchCV fit...
      ‚è≥ LX TCN: Epoch 20/50 (40%)
      ‚è≥ LX TCN: Epoch 30/50 (60%)
      ‚è≥ LX TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.118856
         RMSE: 0.344754
         R¬≤ Score: -1.1654 (Poor - 116.5% variance explained)
      üîπ ETON: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.017276
         RMSE: 0.131440
         R¬≤ Score: 0.0306
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LX Random Forest: Starting GridSearchCV fit...
      ‚è≥ KTOS LSTM: Epoch 20/50 (40%)
      ‚è≥ ETON TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.259320
         RMSE: 0.509235
         R¬≤ Score: -0.9500 (Poor - 95.0% variance explained)
      üîπ SEI: Training TCN (50 epochs)...
      ‚è≥ ETON TCN: Epoch 20/50 (40%)
      ‚è≥ SEI TCN: Epoch 10/50 (20%)
      ‚è≥ ETON TCN: Epoch 30/50 (60%)
      ‚è≥ SEI TCN: Epoch 20/50 (40%)
      ‚è≥ ETON TCN: Epoch 40/50 (80%)
      ‚è≥ SEI TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.054694
         RMSE: 0.233868
         R¬≤ Score: 0.0035
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETON: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETON Random Forest: Starting GridSearchCV fit...
      ‚è≥ SEI TCN: Epoch 40/50 (80%)
      ‚è≥ KTOS LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.191002
         RMSE: 0.437037
         R¬≤ Score: -0.4363
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SEI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SEI Random Forest: Starting GridSearchCV fit...
       ‚úÖ AKBA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=185.8752 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AKBA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ KTOS LSTM: Epoch 40/50 (80%)
       ‚úÖ AKBA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=234.6810 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AKBA XGBoost: Starting GridSearchCV fit...
       ‚úÖ MPU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=194.7452 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MPU LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.499188
         RMSE: 0.706532
         R¬≤ Score: -1.2155 (Poor - 121.6% variance explained)
      üîπ KTOS: Training TCN (50 epochs)...
      ‚è≥ KTOS TCN: Epoch 10/50 (20%)
       ‚úÖ RBRK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=51.6070 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RBRK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ KTOS TCN: Epoch 20/50 (40%)
      ‚è≥ KTOS TCN: Epoch 30/50 (60%)
       ‚úÖ ALLT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=84.8721 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ALLT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ KTOS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.395916
         RMSE: 0.629219
         R¬≤ Score: -0.7572
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KTOS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KTOS Random Forest: Starting GridSearchCV fit...
       ‚úÖ MPU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=483.3389 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MPU XGBoost: Starting GridSearchCV fit...
       ‚úÖ ORLA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=78.6007 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ORLA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RBRK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=55.6822 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RBRK XGBoost: Starting GridSearchCV fit...
       ‚úÖ ALLT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=107.7470 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ALLT XGBoost: Starting GridSearchCV fit...
       ‚úÖ LX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=105.7017 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ORLA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=74.0505 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ORLA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ETON Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.0282 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETON LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SEI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=137.5024 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SEI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=132.7167 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LX XGBoost: Starting GridSearchCV fit...
       ‚úÖ ETON LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=69.9160 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETON XGBoost: Starting GridSearchCV fit...
       ‚úÖ SEI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=137.9801 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SEI XGBoost: Starting GridSearchCV fit...
       ‚úÖ KTOS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=52.4171 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KTOS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KTOS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=45.1987 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KTOS XGBoost: Starting GridSearchCV fit...
       ‚úÖ NNE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=190.0298 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.8s
    - LSTM: MSE=0.3917
    - TCN: MSE=0.2697
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2697
        ‚Ä¢ LSTM: MSE=0.3917
        ‚Ä¢ Random Forest: MSE=184.3177
        ‚Ä¢ XGBoost: MSE=190.0298
        ‚Ä¢ LightGBM Regressor (CPU): MSE=205.3232
   ‚úÖ NNE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NNE (TargetReturn): TCN with MSE=0.2697
üêõ DEBUG: NNE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NNE.
üêõ DEBUG: NNE - Moving model to CPU before return...
üêõ DEBUG [22:26:03.374]: NNE - Returning result metadata...
üêõ DEBUG [22:26:03.374]: Main received result for NNE
üêõ DEBUG: train_worker started for NAGE
  ‚öôÔ∏è Training models for NAGE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NAGE: Initiating feature extraction for training.
  [DIAGNOSTIC] NAGE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NAGE: rows after features available: 126
üéØ NAGE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NAGE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NAGE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NAGE: Training LSTM (50 epochs)...
      ‚è≥ NAGE LSTM: Epoch 10/50 (20%)
      ‚è≥ NAGE LSTM: Epoch 20/50 (40%)
      ‚è≥ NAGE LSTM: Epoch 30/50 (60%)
      ‚è≥ NAGE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.251943
         RMSE: 0.501939
         R¬≤ Score: -1.0427 (Poor - 104.3% variance explained)
      üîπ NAGE: Training TCN (50 epochs)...
      ‚è≥ NAGE TCN: Epoch 10/50 (20%)
      ‚è≥ NAGE TCN: Epoch 20/50 (40%)
      ‚è≥ NAGE TCN: Epoch 30/50 (60%)
      ‚è≥ NAGE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.127022
         RMSE: 0.356401
         R¬≤ Score: -0.0298
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NAGE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NAGE Random Forest: Starting GridSearchCV fit...
       ‚úÖ NAGE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=185.9476 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NAGE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NAGE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=132.1669 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NAGE XGBoost: Starting GridSearchCV fit...
       ‚úÖ KOPN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=164.2276 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.3s
    - LSTM: MSE=0.4974
    - TCN: MSE=0.3867
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3867
        ‚Ä¢ LSTM: MSE=0.4974
        ‚Ä¢ LightGBM Regressor (CPU): MSE=139.0153
        ‚Ä¢ XGBoost: MSE=164.2276
        ‚Ä¢ Random Forest: MSE=240.6503
   ‚úÖ KOPN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KOPN (TargetReturn): TCN with MSE=0.3867
üêõ DEBUG: KOPN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KOPN.
üêõ DEBUG: KOPN - Moving model to CPU before return...
üêõ DEBUG [22:26:14.152]: KOPN - Returning result metadata...
üêõ DEBUG: train_worker started for QMCO
üêõ DEBUG [22:26:14.153]: Main received result for KOPN
  ‚öôÔ∏è Training models for QMCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - QMCO: Initiating feature extraction for training.
  [DIAGNOSTIC] QMCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QMCO: rows after features available: 126
üéØ QMCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QMCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QMCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QMCO: Training LSTM (50 epochs)...
      ‚è≥ QMCO LSTM: Epoch 10/50 (20%)
      ‚è≥ QMCO LSTM: Epoch 20/50 (40%)
       ‚úÖ CANG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=115.3653 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.9s
    - LSTM: MSE=0.1731
    - TCN: MSE=0.1226
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1226
        ‚Ä¢ LSTM: MSE=0.1731
        ‚Ä¢ Random Forest: MSE=105.0344
        ‚Ä¢ XGBoost: MSE=115.3653
        ‚Ä¢ LightGBM Regressor (CPU): MSE=129.5871
   ‚úÖ CANG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CANG (TargetReturn): TCN with MSE=0.1226
üêõ DEBUG: CANG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CANG.
üêõ DEBUG: CANG - Moving model to CPU before return...
üêõ DEBUG [22:26:15.500]: CANG - Returning result metadata...
üêõ DEBUG: train_worker started for GEV
üêõ DEBUG [22:26:15.501]: Main received result for CANG
üêõ DEBUG: Training progress: 48/959 done
  ‚öôÔ∏è Training models for GEV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - GEV: Initiating feature extraction for training.
  [DIAGNOSTIC] GEV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GEV: rows after features available: 126
üéØ GEV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GEV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GEV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GEV: Training LSTM (50 epochs)...
      ‚è≥ QMCO LSTM: Epoch 30/50 (60%)
      ‚è≥ GEV LSTM: Epoch 10/50 (20%)
      ‚è≥ QMCO LSTM: Epoch 40/50 (80%)
      ‚è≥ GEV LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.012893
         RMSE: 0.113548
         R¬≤ Score: -2.2486 (Poor - 224.9% variance explained)
      üîπ QMCO: Training TCN (50 epochs)...
      ‚è≥ QMCO TCN: Epoch 10/50 (20%)
      ‚è≥ QMCO TCN: Epoch 20/50 (40%)
      ‚è≥ GEV LSTM: Epoch 30/50 (60%)
      ‚è≥ QMCO TCN: Epoch 30/50 (60%)
      ‚è≥ QMCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.004507
         RMSE: 0.067135
         R¬≤ Score: -0.1356
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QMCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QMCO Random Forest: Starting GridSearchCV fit...
      ‚è≥ GEV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.352167
         RMSE: 0.593436
         R¬≤ Score: -0.4986 (Poor - 49.9% variance explained)
      üîπ GEV: Training TCN (50 epochs)...
      ‚è≥ GEV TCN: Epoch 10/50 (20%)
      ‚è≥ GEV TCN: Epoch 20/50 (40%)
      ‚è≥ GEV TCN: Epoch 30/50 (60%)
      ‚è≥ GEV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.326399
         RMSE: 0.571314
         R¬≤ Score: -0.3890
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GEV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GEV Random Forest: Starting GridSearchCV fit...
       ‚úÖ QMCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=707.4518 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QMCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QMCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=2205.7179 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QMCO XGBoost: Starting GridSearchCV fit...
       ‚úÖ GEV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=91.1056 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GEV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GEV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=75.2107 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GEV XGBoost: Starting GridSearchCV fit...
       ‚úÖ APP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=158.8256 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.8s
    - LSTM: MSE=0.2005
    - TCN: MSE=0.1741
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1741
        ‚Ä¢ LSTM: MSE=0.2005
        ‚Ä¢ LightGBM Regressor (CPU): MSE=137.4236
        ‚Ä¢ Random Forest: MSE=139.3172
        ‚Ä¢ XGBoost: MSE=158.8256
   ‚úÖ APP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APP (TargetReturn): TCN with MSE=0.1741
üêõ DEBUG: APP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APP.
üêõ DEBUG: APP - Moving model to CPU before return...
üêõ DEBUG [22:26:34.210]: APP - Returning result metadata...
üêõ DEBUG: train_worker started for AMLX
üêõ DEBUG [22:26:34.215]: Main received result for APP
  ‚öôÔ∏è Training models for AMLX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - AMLX: Initiating feature extraction for training.
  [DIAGNOSTIC] AMLX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMLX: rows after features available: 126
üéØ AMLX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMLX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMLX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMLX: Training LSTM (50 epochs)...
      ‚è≥ AMLX LSTM: Epoch 10/50 (20%)
      ‚è≥ AMLX LSTM: Epoch 20/50 (40%)
      ‚è≥ AMLX LSTM: Epoch 30/50 (60%)
      ‚è≥ AMLX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.229792
         RMSE: 0.479367
         R¬≤ Score: -1.4121 (Poor - 141.2% variance explained)
      üîπ AMLX: Training TCN (50 epochs)...
      ‚è≥ AMLX TCN: Epoch 10/50 (20%)
       ‚úÖ ONDS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=726.0720 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.7935
    - TCN: MSE=0.5821
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5821
        ‚Ä¢ LSTM: MSE=0.7935
        ‚Ä¢ LightGBM Regressor (CPU): MSE=363.2798
        ‚Ä¢ Random Forest: MSE=544.1467
        ‚Ä¢ XGBoost: MSE=726.0720
   ‚úÖ ONDS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ONDS (TargetReturn): TCN with MSE=0.5821
üêõ DEBUG: ONDS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ONDS.
üêõ DEBUG: ONDS - Moving model to CPU before return...
üêõ DEBUG [22:26:36.684]: ONDS - Returning result metadata...
üêõ DEBUG: train_worker started for SOUN
üêõ DEBUG [22:26:36.684]: Main received result for ONDS
      ‚è≥ AMLX TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for SOUN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SOUN: Initiating feature extraction for training.
  [DIAGNOSTIC] SOUN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SOUN: rows after features available: 126
üéØ SOUN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SOUN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SOUN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SOUN: Training LSTM (50 epochs)...
      ‚è≥ AMLX TCN: Epoch 30/50 (60%)
      ‚è≥ AMLX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.104577
         RMSE: 0.323384
         R¬≤ Score: -0.0977
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMLX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMLX Random Forest: Starting GridSearchCV fit...
      ‚è≥ SOUN LSTM: Epoch 10/50 (20%)
      ‚è≥ SOUN LSTM: Epoch 20/50 (40%)
      ‚è≥ SOUN LSTM: Epoch 30/50 (60%)
      ‚è≥ SOUN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.086507
         RMSE: 0.294121
         R¬≤ Score: -0.7925 (Poor - 79.3% variance explained)
      üîπ SOUN: Training TCN (50 epochs)...
      ‚è≥ SOUN TCN: Epoch 10/50 (20%)
       ‚úÖ SOFI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=74.5335 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.1s
    - LSTM: MSE=0.5852
    - TCN: MSE=0.4834
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4834
        ‚Ä¢ LSTM: MSE=0.5852
        ‚Ä¢ Random Forest: MSE=72.7626
        ‚Ä¢ XGBoost: MSE=74.5335
        ‚Ä¢ LightGBM Regressor (CPU): MSE=81.0129
   ‚úÖ SOFI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SOFI (TargetReturn): TCN with MSE=0.4834
üêõ DEBUG: SOFI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SOFI.
üêõ DEBUG: SOFI - Moving model to CPU before return...
üêõ DEBUG [22:26:39.516]: SOFI - Returning result metadata...
üêõ DEBUG [22:26:39.517]: Main received result for SOFI
üêõ DEBUG: train_worker started for TTMI
      ‚è≥ SOUN TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for TTMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - TTMI: Initiating feature extraction for training.
  [DIAGNOSTIC] TTMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TTMI: rows after features available: 126
üéØ TTMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TTMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TTMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TTMI: Training LSTM (50 epochs)...
      ‚è≥ SOUN TCN: Epoch 30/50 (60%)
      ‚è≥ SOUN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.056017
         RMSE: 0.236680
         R¬≤ Score: -0.1607
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SOUN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SOUN Random Forest: Starting GridSearchCV fit...
      ‚è≥ TTMI LSTM: Epoch 10/50 (20%)
       ‚úÖ AMLX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=258.2212 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMLX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TTMI LSTM: Epoch 20/50 (40%)
      ‚è≥ TTMI LSTM: Epoch 30/50 (60%)
       ‚úÖ AMLX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=223.6297 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMLX XGBoost: Starting GridSearchCV fit...
      ‚è≥ TTMI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.558867
         RMSE: 0.747574
         R¬≤ Score: -0.5419 (Poor - 54.2% variance explained)
      üîπ TTMI: Training TCN (50 epochs)...
      ‚è≥ TTMI TCN: Epoch 10/50 (20%)
      ‚è≥ TTMI TCN: Epoch 20/50 (40%)
      ‚è≥ TTMI TCN: Epoch 30/50 (60%)
      ‚è≥ TTMI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.561191
         RMSE: 0.749127
         R¬≤ Score: -0.5483
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TTMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TTMI Random Forest: Starting GridSearchCV fit...
       ‚úÖ SOUN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=171.7725 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SOUN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SOUN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=361.3210 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SOUN XGBoost: Starting GridSearchCV fit...
       ‚úÖ TTMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.4548 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TTMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TTMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=59.0920 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TTMI XGBoost: Starting GridSearchCV fit...
       ‚úÖ AKBA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=215.4598 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.2s
    - LSTM: MSE=0.3232
    - TCN: MSE=0.3139
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3139
        ‚Ä¢ LSTM: MSE=0.3232
        ‚Ä¢ Random Forest: MSE=185.8752
        ‚Ä¢ XGBoost: MSE=215.4598
        ‚Ä¢ LightGBM Regressor (CPU): MSE=234.6810
   ‚úÖ AKBA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AKBA (TargetReturn): TCN with MSE=0.3139
üêõ DEBUG: AKBA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AKBA.
üêõ DEBUG: AKBA - Moving model to CPU before return...
üêõ DEBUG [22:26:49.942]: AKBA - Returning result metadata...
üêõ DEBUG [22:26:49.943]: Main received result for AKBAüêõ DEBUG: train_worker started for SBET

üêõ DEBUG: Training progress: 52/959 done
  ‚öôÔ∏è Training models for SBET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - SBET: Initiating feature extraction for training.
  [DIAGNOSTIC] SBET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SBET: rows after features available: 126
üéØ SBET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SBET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SBET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SBET: Training LSTM (50 epochs)...
      ‚è≥ SBET LSTM: Epoch 10/50 (20%)
      ‚è≥ SBET LSTM: Epoch 20/50 (40%)
       ‚úÖ LX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=192.2712 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 126.7s
    - LSTM: MSE=0.0634
    - TCN: MSE=0.0173
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0173
        ‚Ä¢ LSTM: MSE=0.0634
        ‚Ä¢ Random Forest: MSE=105.7017
        ‚Ä¢ LightGBM Regressor (CPU): MSE=132.7167
        ‚Ä¢ XGBoost: MSE=192.2712
   ‚úÖ LX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LX (TargetReturn): TCN with MSE=0.0173
üêõ DEBUG: LX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LX.
üêõ DEBUG: LX - Moving model to CPU before return...
üêõ DEBUG [22:26:51.150]: LX - Returning result metadata...
üêõ DEBUG: train_worker started for AISP
  ‚öôÔ∏è Training models for AISP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AISP: Initiating feature extraction for training.
  [DIAGNOSTIC] AISP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AISP: rows after features available: 126
üéØ AISP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AISP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AISP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AISP: Training LSTM (50 epochs)...
      ‚è≥ SBET LSTM: Epoch 30/50 (60%)
       ‚úÖ MPU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=220.9919 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 128.8s
    - LSTM: MSE=0.4078
    - TCN: MSE=0.3051
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3051
        ‚Ä¢ LSTM: MSE=0.4078
        ‚Ä¢ Random Forest: MSE=194.7452
        ‚Ä¢ XGBoost: MSE=220.9919
        ‚Ä¢ LightGBM Regressor (CPU): MSE=483.3389
   ‚úÖ MPU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MPU (TargetReturn): TCN with MSE=0.3051
üêõ DEBUG: MPU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MPU.
üêõ DEBUG: MPU - Moving model to CPU before return...
üêõ DEBUG [22:26:51.535]: MPU - Returning result metadata...
üêõ DEBUG: train_worker started for OPFI
üêõ DEBUG [22:26:51.536]: Main received result for MPU
  ‚öôÔ∏è Training models for OPFI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - OPFI: Initiating feature extraction for training.
  [DIAGNOSTIC] OPFI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPFI: rows after features available: 126
üéØ OPFI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPFI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPFI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPFI: Training LSTM (50 epochs)...
      ‚è≥ AISP LSTM: Epoch 10/50 (20%)
      ‚è≥ SBET LSTM: Epoch 40/50 (80%)
      ‚è≥ OPFI LSTM: Epoch 10/50 (20%)
      ‚è≥ AISP LSTM: Epoch 20/50 (40%)
      ‚è≥ OPFI LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.572573
         RMSE: 0.756686
         R¬≤ Score: -0.7492 (Poor - 74.9% variance explained)
      üîπ SBET: Training TCN (50 epochs)...
      ‚è≥ AISP LSTM: Epoch 30/50 (60%)
      ‚è≥ SBET TCN: Epoch 10/50 (20%)
       ‚úÖ SEI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=188.7202 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 127.7s
    - LSTM: MSE=0.2593
    - TCN: MSE=0.1910
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1910
        ‚Ä¢ LSTM: MSE=0.2593
        ‚Ä¢ Random Forest: MSE=137.5024
        ‚Ä¢ LightGBM Regressor (CPU): MSE=137.9801
        ‚Ä¢ XGBoost: MSE=188.7202
   ‚úÖ SEI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SEI (TargetReturn): TCN with MSE=0.1910
üêõ DEBUG: SEI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SEI.
üêõ DEBUG: SEI - Moving model to CPU before return...
üêõ DEBUG [22:26:52.845]: SEI - Returning result metadata...
üêõ DEBUG: train_worker started for NET
       ‚úÖ ORLA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=58.1087 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.1s
    - LSTM: MSE=0.4059
    - TCN: MSE=0.3868
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3868
        ‚Ä¢ LSTM: MSE=0.4059
        ‚Ä¢ XGBoost: MSE=58.1087
        ‚Ä¢ LightGBM Regressor (CPU): MSE=74.0505
        ‚Ä¢ Random Forest: MSE=78.6007
   ‚úÖ ORLA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ORLA (TargetReturn): TCN with MSE=0.3868
üêõ DEBUG: ORLA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ORLA.
üêõ DEBUG: ORLA - Moving model to CPU before return...
üêõ DEBUG [22:26:52.861]: ORLA - Returning result metadata...
üêõ DEBUG: train_worker started for LGCY
  ‚öôÔ∏è Training models for NET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NET: Initiating feature extraction for training.
  [DIAGNOSTIC] NET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NET: rows after features available: 126
üéØ NET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NET: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for LGCY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - LGCY: Initiating feature extraction for training.
  [DIAGNOSTIC] LGCY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LGCY: rows after features available: 126
üéØ LGCY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LGCY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LGCY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LGCY: Training LSTM (50 epochs)...
      ‚è≥ SBET TCN: Epoch 20/50 (40%)
      ‚è≥ SBET TCN: Epoch 30/50 (60%)
      ‚è≥ SBET TCN: Epoch 40/50 (80%)
      ‚è≥ OPFI LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.321306
         RMSE: 0.566839
         R¬≤ Score: 0.0184
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SBET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SBET Random Forest: Starting GridSearchCV fit...
       ‚úÖ RBRK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=110.2153 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.5s
    - LSTM: MSE=0.3735
    - TCN: MSE=0.2557
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2557
        ‚Ä¢ LSTM: MSE=0.3735
        ‚Ä¢ Random Forest: MSE=51.6070
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.6822
        ‚Ä¢ XGBoost: MSE=110.2153
   ‚úÖ RBRK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RBRK (TargetReturn): TCN with MSE=0.2557
üêõ DEBUG: RBRK - train_and_evaluate_models completed
      ‚è≥ AISP LSTM: Epoch 40/50 (80%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RBRK.
üêõ DEBUG: RBRK - Moving model to CPU before return...
üêõ DEBUG [22:26:53.452]: RBRK - Returning result metadata...
üêõ DEBUG: train_worker started for BITU
üêõ DEBUG [22:26:53.453]: Main received result for RBRK
  ‚öôÔ∏è Training models for BITU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - BITU: Initiating feature extraction for training.
  [DIAGNOSTIC] BITU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BITU: rows after features available: 126
üéØ BITU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BITU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BITU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BITU: Training LSTM (50 epochs)...
       ‚úÖ ALLT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=127.4174 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.4s
    - LSTM: MSE=0.2693
    - TCN: MSE=0.2580
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2580
        ‚Ä¢ LSTM: MSE=0.2693
        ‚Ä¢ Random Forest: MSE=84.8721
        ‚Ä¢ LightGBM Regressor (CPU): MSE=107.7470
        ‚Ä¢ XGBoost: MSE=127.4174
   ‚úÖ ALLT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ALLT (TargetReturn): TCN with MSE=0.2580
üêõ DEBUG: ALLT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ALLT.
üêõ DEBUG: ALLT - Moving model to CPU before return...
üêõ DEBUG [22:26:53.530]: ALLT - Returning result metadata...
üêõ DEBUG: train_worker started for LMND
üêõ DEBUG [22:26:53.532]: Main received result for ALLT
üêõ DEBUG [22:26:53.532]: Main received result for ORLA
üêõ DEBUG: Training progress: 56/959 done
üêõ DEBUG [22:26:53.532]: Main received result for LX
üêõ DEBUG [22:26:53.532]: Main received result for SEI
      ‚è≥ LGCY LSTM: Epoch 10/50 (20%)
      ‚è≥ NET LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for LMND (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - LMND: Initiating feature extraction for training.
  [DIAGNOSTIC] LMND: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LMND: rows after features available: 126
üéØ LMND: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LMND: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LMND: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LMND: Training LSTM (50 epochs)...
      ‚è≥ OPFI LSTM: Epoch 40/50 (80%)
      ‚è≥ BITU LSTM: Epoch 10/50 (20%)
      ‚è≥ LMND LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.175083
         RMSE: 0.418429
         R¬≤ Score: -0.5827 (Poor - 58.3% variance explained)
      üîπ AISP: Training TCN (50 epochs)...
      ‚è≥ NET LSTM: Epoch 20/50 (40%)
      ‚è≥ LGCY LSTM: Epoch 20/50 (40%)
      ‚è≥ AISP TCN: Epoch 10/50 (20%)
      ‚è≥ AISP TCN: Epoch 20/50 (40%)
      ‚è≥ AISP TCN: Epoch 30/50 (60%)
      ‚è≥ AISP TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.161105
         RMSE: 0.401379
         R¬≤ Score: -1.1717 (Poor - 117.2% variance explained)
      üîπ OPFI: Training TCN (50 epochs)...
      ‚è≥ LMND LSTM: Epoch 20/50 (40%)
      ‚è≥ NET LSTM: Epoch 30/50 (60%)
      ‚è≥ LGCY LSTM: Epoch 30/50 (60%)
      ‚è≥ OPFI TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.139363
         RMSE: 0.373314
         R¬≤ Score: -0.2598
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AISP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AISP Random Forest: Starting GridSearchCV fit...
      ‚è≥ BITU LSTM: Epoch 20/50 (40%)
      ‚è≥ OPFI TCN: Epoch 20/50 (40%)
      ‚è≥ OPFI TCN: Epoch 30/50 (60%)
       ‚úÖ ETON XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=50.0459 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.6s
    - LSTM: MSE=0.1189
    - TCN: MSE=0.0547
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0547
        ‚Ä¢ LSTM: MSE=0.1189
        ‚Ä¢ Random Forest: MSE=42.0282
        ‚Ä¢ XGBoost: MSE=50.0459
        ‚Ä¢ LightGBM Regressor (CPU): MSE=69.9160
   ‚úÖ ETON: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETON (TargetReturn): TCN with MSE=0.0547
üêõ DEBUG: ETON - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETON.
üêõ DEBUG: ETON - Moving model to CPU before return...
üêõ DEBUG [22:26:55.333]: ETON - Returning result metadata...
üêõ DEBUG [22:26:55.334]: Main received result for ETON
üêõ DEBUG: train_worker started for MSTR
      ‚è≥ OPFI TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for MSTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - MSTR: Initiating feature extraction for training.
  [DIAGNOSTIC] MSTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MSTR: rows after features available: 126
üéØ MSTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MSTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MSTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MSTR: Training LSTM (50 epochs)...
      ‚è≥ LMND LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.099676
         RMSE: 0.315715
         R¬≤ Score: -0.3436
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPFI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPFI Random Forest: Starting GridSearchCV fit...
      ‚è≥ LGCY LSTM: Epoch 40/50 (80%)
      ‚è≥ BITU LSTM: Epoch 30/50 (60%)
      ‚è≥ NET LSTM: Epoch 40/50 (80%)
      ‚è≥ MSTR LSTM: Epoch 10/50 (20%)
      ‚è≥ LMND LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.616571
         RMSE: 0.785221
         R¬≤ Score: -0.5494 (Poor - 54.9% variance explained)
      üîπ LGCY: Training TCN (50 epochs)...
      ‚è≥ BITU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.521052
         RMSE: 0.721839
         R¬≤ Score: -0.5141 (Poor - 51.4% variance explained)
      üîπ NET: Training TCN (50 epochs)...
      ‚è≥ LGCY TCN: Epoch 10/50 (20%)
      ‚è≥ LGCY TCN: Epoch 20/50 (40%)
      ‚è≥ NET TCN: Epoch 10/50 (20%)
      ‚è≥ LGCY TCN: Epoch 30/50 (60%)
      ‚è≥ MSTR LSTM: Epoch 20/50 (40%)
      ‚è≥ NET TCN: Epoch 20/50 (40%)
      ‚è≥ LGCY TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.283975
         RMSE: 0.532893
         R¬≤ Score: -0.5734 (Poor - 57.3% variance explained)
      üîπ LMND: Training TCN (50 epochs)...
      ‚è≥ NET TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.575066
         RMSE: 0.758331
         R¬≤ Score: -0.4451
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LGCY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LGCY Random Forest: Starting GridSearchCV fit...
      ‚è≥ LMND TCN: Epoch 10/50 (20%)
      ‚è≥ NET TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.308095
         RMSE: 0.555063
         R¬≤ Score: -0.8642 (Poor - 86.4% variance explained)
      üîπ BITU: Training TCN (50 epochs)...
      ‚è≥ LMND TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.612287
         RMSE: 0.782488
         R¬≤ Score: -0.7793
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NET Random Forest: Starting GridSearchCV fit...
      ‚è≥ BITU TCN: Epoch 10/50 (20%)
      ‚è≥ LMND TCN: Epoch 30/50 (60%)
       ‚úÖ KTOS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=84.7298 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.8s
    - LSTM: MSE=0.4992
    - TCN: MSE=0.3959
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3959
        ‚Ä¢ LSTM: MSE=0.4992
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.1987
        ‚Ä¢ Random Forest: MSE=52.4171
        ‚Ä¢ XGBoost: MSE=84.7298
   ‚úÖ KTOS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KTOS (TargetReturn): TCN with MSE=0.3959
üêõ DEBUG: KTOS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KTOS.
üêõ DEBUG: KTOS - Moving model to CPU before return...
üêõ DEBUG [22:26:57.269]: KTOS - Returning result metadata...
üêõ DEBUG [22:26:57.270]: Main received result for KTOS
üêõ DEBUG: Training progress: 60/959 done
üêõ DEBUG: train_worker started for NEXT
      ‚è≥ MSTR LSTM: Epoch 30/50 (60%)
      ‚è≥ BITU TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for NEXT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - NEXT: Initiating feature extraction for training.
  [DIAGNOSTIC] NEXT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NEXT: rows after features available: 126
üéØ NEXT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NEXT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NEXT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NEXT: Training LSTM (50 epochs)...
       ‚úÖ SBET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24697.6494 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SBET LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LMND TCN: Epoch 40/50 (80%)
      ‚è≥ BITU TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.292108
         RMSE: 0.540471
         R¬≤ Score: -0.6185
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LMND: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LMND Random Forest: Starting GridSearchCV fit...
      ‚è≥ BITU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.196923
         RMSE: 0.443760
         R¬≤ Score: -0.1915
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BITU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BITU Random Forest: Starting GridSearchCV fit...
      ‚è≥ NEXT LSTM: Epoch 10/50 (20%)
      ‚è≥ MSTR LSTM: Epoch 40/50 (80%)
       ‚úÖ SBET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26069.3750 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SBET XGBoost: Starting GridSearchCV fit...
      ‚è≥ NEXT LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.132255
         RMSE: 0.363669
         R¬≤ Score: -0.4862 (Poor - 48.6% variance explained)
      üîπ MSTR: Training TCN (50 epochs)...
      ‚è≥ MSTR TCN: Epoch 10/50 (20%)
      ‚è≥ MSTR TCN: Epoch 20/50 (40%)
       ‚úÖ AISP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=179.0719 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AISP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MSTR TCN: Epoch 30/50 (60%)
      ‚è≥ MSTR TCN: Epoch 40/50 (80%)
      ‚è≥ NEXT LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.092210
         RMSE: 0.303662
         R¬≤ Score: -0.0362
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MSTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MSTR Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPFI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=136.0601 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPFI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NEXT LSTM: Epoch 40/50 (80%)
       ‚úÖ AISP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=237.1278 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AISP XGBoost: Starting GridSearchCV fit...
       ‚úÖ OPFI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=231.8172 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPFI XGBoost: Starting GridSearchCV fit...
       ‚úÖ LGCY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.9144 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LGCY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=75.6564 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NET LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.359805
         RMSE: 0.599838
         R¬≤ Score: -0.4591 (Poor - 45.9% variance explained)
      üîπ NEXT: Training TCN (50 epochs)...
      ‚è≥ NEXT TCN: Epoch 10/50 (20%)
      ‚è≥ NEXT TCN: Epoch 20/50 (40%)
      ‚è≥ NEXT TCN: Epoch 30/50 (60%)
       ‚úÖ BITU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=74.4473 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BITU LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NEXT TCN: Epoch 40/50 (80%)
       ‚úÖ LMND Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=62.6268 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LMND LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LGCY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.8035 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LGCY XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.358645
         RMSE: 0.598869
         R¬≤ Score: -0.4543
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NEXT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NEXT Random Forest: Starting GridSearchCV fit...
       ‚úÖ NET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=70.8419 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NET XGBoost: Starting GridSearchCV fit...
       ‚úÖ BITU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=57.5348 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BITU XGBoost: Starting GridSearchCV fit...
       ‚úÖ LMND LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=88.1574 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LMND XGBoost: Starting GridSearchCV fit...
       ‚úÖ MSTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=55.1613 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MSTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MSTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=58.3629 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MSTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ NEXT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=72.5657 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NEXT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NEXT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=63.4641 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NEXT XGBoost: Starting GridSearchCV fit...
       ‚úÖ NAGE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=148.5576 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.5s
    - LSTM: MSE=0.2519
    - TCN: MSE=0.1270
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1270
        ‚Ä¢ LSTM: MSE=0.2519
        ‚Ä¢ LightGBM Regressor (CPU): MSE=132.1669
        ‚Ä¢ XGBoost: MSE=148.5576
        ‚Ä¢ Random Forest: MSE=185.9476
   ‚úÖ NAGE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NAGE (TargetReturn): TCN with MSE=0.1270
üêõ DEBUG: NAGE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NAGE.
üêõ DEBUG: NAGE - Moving model to CPU before return...
üêõ DEBUG [22:28:11.939]: NAGE - Returning result metadata...
üêõ DEBUG: train_worker started for BITX
üêõ DEBUG [22:28:11.940]: Main received result for NAGE
  ‚öôÔ∏è Training models for BITX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - BITX: Initiating feature extraction for training.
  [DIAGNOSTIC] BITX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BITX: rows after features available: 126
üéØ BITX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BITX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BITX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BITX: Training LSTM (50 epochs)...
      ‚è≥ BITX LSTM: Epoch 10/50 (20%)
      ‚è≥ BITX LSTM: Epoch 20/50 (40%)
      ‚è≥ BITX LSTM: Epoch 30/50 (60%)
      ‚è≥ BITX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.386929
         RMSE: 0.622036
         R¬≤ Score: -1.3146 (Poor - 131.5% variance explained)
      üîπ BITX: Training TCN (50 epochs)...
      ‚è≥ BITX TCN: Epoch 10/50 (20%)
      ‚è≥ BITX TCN: Epoch 20/50 (40%)
      ‚è≥ BITX TCN: Epoch 30/50 (60%)
      ‚è≥ BITX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.234741
         RMSE: 0.484501
         R¬≤ Score: -0.4042
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BITX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BITX Random Forest: Starting GridSearchCV fit...
       ‚úÖ QMCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1513.0495 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 115.4s
    - LSTM: MSE=0.0129
    - TCN: MSE=0.0045
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0045
        ‚Ä¢ LSTM: MSE=0.0129
        ‚Ä¢ Random Forest: MSE=707.4518
        ‚Ä¢ XGBoost: MSE=1513.0495
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2205.7179
   ‚úÖ QMCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QMCO (TargetReturn): TCN with MSE=0.0045
üêõ DEBUG: QMCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QMCO.
üêõ DEBUG: QMCO - Moving model to CPU before return...
üêõ DEBUG [22:28:16.290]: QMCO - Returning result metadata...
üêõ DEBUG: train_worker started for AS
üêõ DEBUG [22:28:16.290]: Main received result for QMCO
  ‚öôÔ∏è Training models for AS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - AS: Initiating feature extraction for training.
  [DIAGNOSTIC] AS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AS: rows after features available: 126
üéØ AS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AS: Training LSTM (50 epochs)...
      ‚è≥ AS LSTM: Epoch 10/50 (20%)
       ‚úÖ BITX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=88.4541 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BITX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AS LSTM: Epoch 20/50 (40%)
      ‚è≥ AS LSTM: Epoch 30/50 (60%)
       ‚úÖ BITX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=74.2392 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BITX XGBoost: Starting GridSearchCV fit...
      ‚è≥ AS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.651954
         RMSE: 0.807436
         R¬≤ Score: -1.3195 (Poor - 132.0% variance explained)
      üîπ AS: Training TCN (50 epochs)...
      ‚è≥ AS TCN: Epoch 10/50 (20%)
      ‚è≥ AS TCN: Epoch 20/50 (40%)
      ‚è≥ AS TCN: Epoch 30/50 (60%)
      ‚è≥ AS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.429554
         RMSE: 0.655404
         R¬≤ Score: -0.5283
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AS Random Forest: Starting GridSearchCV fit...
       ‚úÖ GEV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=119.0849 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.3522
    - TCN: MSE=0.3264
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3264
        ‚Ä¢ LSTM: MSE=0.3522
        ‚Ä¢ LightGBM Regressor (CPU): MSE=75.2107
        ‚Ä¢ Random Forest: MSE=91.1056
        ‚Ä¢ XGBoost: MSE=119.0849
   ‚úÖ GEV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GEV (TargetReturn): TCN with MSE=0.3264
üêõ DEBUG: GEV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GEV.
üêõ DEBUG: GEV - Moving model to CPU before return...
üêõ DEBUG [22:28:21.415]: GEV - Returning result metadata...
üêõ DEBUG: train_worker started for CAR
üêõ DEBUG [22:28:21.416]: Main received result for GEV
  ‚öôÔ∏è Training models for CAR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - CAR: Initiating feature extraction for training.
  [DIAGNOSTIC] CAR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CAR: rows after features available: 126
üéØ CAR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CAR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CAR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CAR: Training LSTM (50 epochs)...
       ‚úÖ AS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.6669 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CAR LSTM: Epoch 10/50 (20%)
      ‚è≥ CAR LSTM: Epoch 20/50 (40%)
       ‚úÖ AS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.4977 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AS XGBoost: Starting GridSearchCV fit...
      ‚è≥ CAR LSTM: Epoch 30/50 (60%)
      ‚è≥ CAR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.555359
         RMSE: 0.745224
         R¬≤ Score: -1.1562 (Poor - 115.6% variance explained)
      üîπ CAR: Training TCN (50 epochs)...
      ‚è≥ CAR TCN: Epoch 10/50 (20%)
      ‚è≥ CAR TCN: Epoch 20/50 (40%)
      ‚è≥ CAR TCN: Epoch 30/50 (60%)
      ‚è≥ CAR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.440824
         RMSE: 0.663946
         R¬≤ Score: -0.7115
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CAR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CAR Random Forest: Starting GridSearchCV fit...
       ‚úÖ CAR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=107.9902 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CAR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CAR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=119.2936 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CAR XGBoost: Starting GridSearchCV fit...
       ‚úÖ AMLX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=370.7906 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.2298
    - TCN: MSE=0.1046
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1046
        ‚Ä¢ LSTM: MSE=0.2298
        ‚Ä¢ LightGBM Regressor (CPU): MSE=223.6297
        ‚Ä¢ Random Forest: MSE=258.2212
        ‚Ä¢ XGBoost: MSE=370.7906
   ‚úÖ AMLX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMLX (TargetReturn): TCN with MSE=0.1046
üêõ DEBUG: AMLX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMLX.
üêõ DEBUG: AMLX - Moving model to CPU before return...
üêõ DEBUG [22:28:42.323]: AMLX - Returning result metadata...
üêõ DEBUG: train_worker started for SRAD
üêõ DEBUG [22:28:42.324]: Main received result for AMLX
üêõ DEBUG: Training progress: 64/959 done
  ‚öôÔ∏è Training models for SRAD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - SRAD: Initiating feature extraction for training.
  [DIAGNOSTIC] SRAD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SRAD: rows after features available: 126
üéØ SRAD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SRAD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SRAD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SRAD: Training LSTM (50 epochs)...
       ‚úÖ SOUN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=165.6024 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.0865
    - TCN: MSE=0.0560
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0560
        ‚Ä¢ LSTM: MSE=0.0865
        ‚Ä¢ XGBoost: MSE=165.6024
        ‚Ä¢ Random Forest: MSE=171.7725
        ‚Ä¢ LightGBM Regressor (CPU): MSE=361.3210
   ‚úÖ SOUN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SOUN (TargetReturn): TCN with MSE=0.0560
üêõ DEBUG: SOUN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SOUN.
üêõ DEBUG: SOUN - Moving model to CPU before return...
üêõ DEBUG [22:28:42.376]: SOUN - Returning result metadata...
üêõ DEBUG: train_worker started for AGX
üêõ DEBUG [22:28:42.377]: Main received result for SOUN
  ‚öôÔ∏è Training models for AGX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - AGX: Initiating feature extraction for training.
  [DIAGNOSTIC] AGX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AGX: rows after features available: 126
üéØ AGX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AGX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AGX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AGX: Training LSTM (50 epochs)...
      ‚è≥ SRAD LSTM: Epoch 10/50 (20%)
      ‚è≥ AGX LSTM: Epoch 10/50 (20%)
      ‚è≥ SRAD LSTM: Epoch 20/50 (40%)
      ‚è≥ AGX LSTM: Epoch 20/50 (40%)
      ‚è≥ SRAD LSTM: Epoch 30/50 (60%)
      ‚è≥ AGX LSTM: Epoch 30/50 (60%)
      ‚è≥ SRAD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.153175
         RMSE: 0.391376
         R¬≤ Score: -1.0153 (Poor - 101.5% variance explained)
      üîπ SRAD: Training TCN (50 epochs)...
      ‚è≥ AGX LSTM: Epoch 40/50 (80%)
      ‚è≥ SRAD TCN: Epoch 10/50 (20%)
      ‚è≥ SRAD TCN: Epoch 20/50 (40%)
      ‚è≥ SRAD TCN: Epoch 30/50 (60%)
      ‚è≥ SRAD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.095233
         RMSE: 0.308599
         R¬≤ Score: -0.2530
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SRAD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SRAD Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.344041
         RMSE: 0.586550
         R¬≤ Score: -1.0045 (Poor - 100.5% variance explained)
      üîπ AGX: Training TCN (50 epochs)...
      ‚è≥ AGX TCN: Epoch 10/50 (20%)
      ‚è≥ AGX TCN: Epoch 20/50 (40%)
      ‚è≥ AGX TCN: Epoch 30/50 (60%)
      ‚è≥ AGX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.181009
         RMSE: 0.425452
         R¬≤ Score: -0.0546
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AGX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AGX Random Forest: Starting GridSearchCV fit...
       ‚úÖ TTMI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=51.3642 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.3s
    - LSTM: MSE=0.5589
    - TCN: MSE=0.5612
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5589
        ‚Ä¢ TCN: MSE=0.5612
        ‚Ä¢ Random Forest: MSE=43.4548
        ‚Ä¢ XGBoost: MSE=51.3642
        ‚Ä¢ LightGBM Regressor (CPU): MSE=59.0920
   ‚úÖ TTMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TTMI (TargetReturn): LSTM with MSE=0.5589
üêõ DEBUG: TTMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TTMI.
üêõ DEBUG: TTMI - Moving model to CPU before return...
üêõ DEBUG [22:28:47.097]: TTMI - Returning result metadata...
üêõ DEBUG: train_worker started for TPR
üêõ DEBUG [22:28:47.100]: Main received result for TTMI
  ‚öôÔ∏è Training models for TPR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - TPR: Initiating feature extraction for training.
  [DIAGNOSTIC] TPR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TPR: rows after features available: 126
üéØ TPR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TPR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TPR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TPR: Training LSTM (50 epochs)...
      ‚è≥ TPR LSTM: Epoch 10/50 (20%)
      ‚è≥ TPR LSTM: Epoch 20/50 (40%)
       ‚úÖ SRAD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.9920 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SRAD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TPR LSTM: Epoch 30/50 (60%)
       ‚úÖ AGX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=49.1719 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AGX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SRAD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.4964 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SRAD XGBoost: Starting GridSearchCV fit...
      ‚è≥ TPR LSTM: Epoch 40/50 (80%)
       ‚úÖ AGX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=58.7289 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AGX XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.872095
         RMSE: 0.933860
         R¬≤ Score: -1.3951 (Poor - 139.5% variance explained)
      üîπ TPR: Training TCN (50 epochs)...
      ‚è≥ TPR TCN: Epoch 10/50 (20%)
      ‚è≥ TPR TCN: Epoch 20/50 (40%)
      ‚è≥ TPR TCN: Epoch 30/50 (60%)
      ‚è≥ TPR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.705304
         RMSE: 0.839824
         R¬≤ Score: -0.9370
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TPR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TPR Random Forest: Starting GridSearchCV fit...
       ‚úÖ TPR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.4907 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TPR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TPR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.7254 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TPR XGBoost: Starting GridSearchCV fit...
       ‚úÖ SBET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28917.6052 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.5726
    - TCN: MSE=0.3213
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3213
        ‚Ä¢ LSTM: MSE=0.5726
        ‚Ä¢ Random Forest: MSE=24697.6494
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26069.3750
        ‚Ä¢ XGBoost: MSE=28917.6052
   ‚úÖ SBET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SBET (TargetReturn): TCN with MSE=0.3213
üêõ DEBUG: SBET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SBET.
üêõ DEBUG: SBET - Moving model to CPU before return...
üêõ DEBUG [22:28:59.482]: SBET - Returning result metadata...
üêõ DEBUG [22:28:59.483]: Main received result for SBET
üêõ DEBUG: train_worker started for INVZ
  ‚öôÔ∏è Training models for INVZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - INVZ: Initiating feature extraction for training.
  [DIAGNOSTIC] INVZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ INVZ: rows after features available: 126
üéØ INVZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] INVZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö INVZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ INVZ: Training LSTM (50 epochs)...
      ‚è≥ INVZ LSTM: Epoch 10/50 (20%)
      ‚è≥ INVZ LSTM: Epoch 20/50 (40%)
      ‚è≥ INVZ LSTM: Epoch 30/50 (60%)
      ‚è≥ INVZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.811299
         RMSE: 0.900722
         R¬≤ Score: -1.3327 (Poor - 133.3% variance explained)
      üîπ INVZ: Training TCN (50 epochs)...
      ‚è≥ INVZ TCN: Epoch 10/50 (20%)
      ‚è≥ INVZ TCN: Epoch 20/50 (40%)
      ‚è≥ INVZ TCN: Epoch 30/50 (60%)
      ‚è≥ INVZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.660622
         RMSE: 0.812787
         R¬≤ Score: -0.8994
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä INVZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ INVZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ LGCY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.2047 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.6166
    - TCN: MSE=0.5751
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5751
        ‚Ä¢ LSTM: MSE=0.6166
        ‚Ä¢ Random Forest: MSE=35.9144
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.8035
        ‚Ä¢ XGBoost: MSE=38.2047
   ‚úÖ LGCY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LGCY (TargetReturn): TCN with MSE=0.5751
üêõ DEBUG: LGCY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LGCY.
üêõ DEBUG: LGCY - Moving model to CPU before return...
üêõ DEBUG [22:29:03.155]: LGCY - Returning result metadata...
üêõ DEBUG: train_worker started for DPRO
  ‚öôÔ∏è Training models for DPRO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - DPRO: Initiating feature extraction for training.
  [DIAGNOSTIC] DPRO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DPRO: rows after features available: 126
üéØ DPRO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DPRO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DPRO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DPRO: Training LSTM (50 epochs)...
      ‚è≥ DPRO LSTM: Epoch 10/50 (20%)
      ‚è≥ DPRO LSTM: Epoch 20/50 (40%)
      ‚è≥ DPRO LSTM: Epoch 30/50 (60%)
      ‚è≥ DPRO LSTM: Epoch 40/50 (80%)
       ‚úÖ INVZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=230.1456 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ INVZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=144.6861 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 124.2s
    - LSTM: MSE=0.5211
    - TCN: MSE=0.6123
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5211
        ‚Ä¢ TCN: MSE=0.6123
        ‚Ä¢ LightGBM Regressor (CPU): MSE=70.8419
        ‚Ä¢ Random Forest: MSE=75.6564
        ‚Ä¢ XGBoost: MSE=144.6861
   ‚úÖ NET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NET (TargetReturn): LSTM with MSE=0.5211
üêõ DEBUG: NET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NET.
üêõ DEBUG: NET - Moving model to CPU before return...
üêõ DEBUG [22:29:05.584]: NET - Returning result metadata...
üêõ DEBUG: train_worker started for EYE
       ‚úÖ AISP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=199.2362 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.8s
    - LSTM: MSE=0.1751
    - TCN: MSE=0.1394
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1394
        ‚Ä¢ LSTM: MSE=0.1751
        ‚Ä¢ Random Forest: MSE=179.0719
        ‚Ä¢ XGBoost: MSE=199.2362
        ‚Ä¢ LightGBM Regressor (CPU): MSE=237.1278
   ‚úÖ AISP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AISP (TargetReturn): TCN with MSE=0.1394
üêõ DEBUG: AISP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AISP.
üêõ DEBUG: AISP - Moving model to CPU before return...
üêõ DEBUG [22:29:05.590]: AISP - Returning result metadata...
üêõ DEBUG: train_worker started for ESLT
üêõ DEBUG [22:29:05.591]: Main received result for AISP
üêõ DEBUG: Training progress: 68/959 done
  ‚öôÔ∏è Training models for EYE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - EYE: Initiating feature extraction for training.
  [DIAGNOSTIC] EYE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EYE: rows after features available: 126
üéØ EYE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for ESLT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ESLT: Initiating feature extraction for training.
  [DIAGNOSTIC] ESLT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ESLT: rows after features available: 126
üéØ ESLT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EYE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EYE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EYE: Training LSTM (50 epochs)...
  [DIAGNOSTIC] ESLT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ESLT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ESLT: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.550692
         RMSE: 0.742087
         R¬≤ Score: -1.4377 (Poor - 143.8% variance explained)
      üîπ DPRO: Training TCN (50 epochs)...
      ‚è≥ DPRO TCN: Epoch 10/50 (20%)
      ‚è≥ DPRO TCN: Epoch 20/50 (40%)
       ‚úÖ INVZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=271.0936 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ INVZ XGBoost: Starting GridSearchCV fit...
      ‚è≥ DPRO TCN: Epoch 30/50 (60%)
      ‚è≥ DPRO TCN: Epoch 40/50 (80%)
      ‚è≥ EYE LSTM: Epoch 10/50 (20%)
      ‚è≥ ESLT LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.269230
         RMSE: 0.518874
         R¬≤ Score: -0.1918
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DPRO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DPRO Random Forest: Starting GridSearchCV fit...
       ‚úÖ LMND XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=83.0228 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 124.3s
    - LSTM: MSE=0.2840
    - TCN: MSE=0.2921
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2840
        ‚Ä¢ TCN: MSE=0.2921
        ‚Ä¢ Random Forest: MSE=62.6268
        ‚Ä¢ XGBoost: MSE=83.0228
        ‚Ä¢ LightGBM Regressor (CPU): MSE=88.1574
   ‚úÖ LMND: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LMND (TargetReturn): LSTM with MSE=0.2840
üêõ DEBUG: LMND - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LMND.
üêõ DEBUG: LMND - Moving model to CPU before return...
üêõ DEBUG [22:29:06.433]: LMND - Returning result metadata...
üêõ DEBUG: train_worker started for RDDT
  ‚öôÔ∏è Training models for RDDT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - RDDT: Initiating feature extraction for training.
  [DIAGNOSTIC] RDDT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RDDT: rows after features available: 126
üéØ RDDT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RDDT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RDDT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RDDT: Training LSTM (50 epochs)...
       ‚úÖ OPFI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=195.2414 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.5s
    - LSTM: MSE=0.1611
    - TCN: MSE=0.0997
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0997
        ‚Ä¢ LSTM: MSE=0.1611
        ‚Ä¢ Random Forest: MSE=136.0601
        ‚Ä¢ XGBoost: MSE=195.2414
        ‚Ä¢ LightGBM Regressor (CPU): MSE=231.8172
   ‚úÖ OPFI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPFI (TargetReturn): TCN with MSE=0.0997
üêõ DEBUG: OPFI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPFI.
üêõ DEBUG: OPFI - Moving model to CPU before return...
üêõ DEBUG [22:29:06.727]: OPFI - Returning result metadata...
üêõ DEBUG: train_worker started for OPRT
üêõ DEBUG [22:29:06.728]: Main received result for OPFI
üêõ DEBUG [22:29:06.728]: Main received result for NET
üêõ DEBUG [22:29:06.728]: Main received result for LGCY
  ‚öôÔ∏è Training models for OPRT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - OPRT: Initiating feature extraction for training.
  [DIAGNOSTIC] OPRT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPRT: rows after features available: 126
üéØ OPRT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPRT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPRT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPRT: Training LSTM (50 epochs)...
      ‚è≥ EYE LSTM: Epoch 20/50 (40%)
      ‚è≥ ESLT LSTM: Epoch 20/50 (40%)
      ‚è≥ RDDT LSTM: Epoch 10/50 (20%)
      ‚è≥ OPRT LSTM: Epoch 10/50 (20%)
      ‚è≥ EYE LSTM: Epoch 30/50 (60%)
      ‚è≥ ESLT LSTM: Epoch 30/50 (60%)
       ‚úÖ BITU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=74.4993 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 125.9s
    - LSTM: MSE=0.3081
    - TCN: MSE=0.1969
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1969
        ‚Ä¢ LSTM: MSE=0.3081
        ‚Ä¢ LightGBM Regressor (CPU): MSE=57.5348
        ‚Ä¢ Random Forest: MSE=74.4473
        ‚Ä¢ XGBoost: MSE=74.4993
   ‚úÖ BITU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BITU (TargetReturn): TCN with MSE=0.1969
üêõ DEBUG: BITU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BITU.
üêõ DEBUG: BITU - Moving model to CPU before return...
üêõ DEBUG [22:29:07.829]: BITU - Returning result metadata...
üêõ DEBUG: train_worker started for JFIN
üêõ DEBUG [22:29:07.830]: Main received result for BITU
üêõ DEBUG: Training progress: 72/959 done
üêõ DEBUG [22:29:07.831]: Main received result for LMND
      ‚è≥ RDDT LSTM: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for JFIN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - JFIN: Initiating feature extraction for training.
  [DIAGNOSTIC] JFIN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JFIN: rows after features available: 126
üéØ JFIN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JFIN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JFIN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JFIN: Training LSTM (50 epochs)...
      ‚è≥ EYE LSTM: Epoch 40/50 (80%)
      ‚è≥ OPRT LSTM: Epoch 20/50 (40%)
      ‚è≥ ESLT LSTM: Epoch 40/50 (80%)
      ‚è≥ RDDT LSTM: Epoch 30/50 (60%)
      ‚è≥ JFIN LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.629171
         RMSE: 0.793203
         R¬≤ Score: -1.2802 (Poor - 128.0% variance explained)
      üîπ EYE: Training TCN (50 epochs)...
      ‚è≥ OPRT LSTM: Epoch 30/50 (60%)
       ‚úÖ MSTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=76.6899 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.7s
    - LSTM: MSE=0.1323
    - TCN: MSE=0.0922
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0922
        ‚Ä¢ LSTM: MSE=0.1323
        ‚Ä¢ Random Forest: MSE=55.1613
        ‚Ä¢ LightGBM Regressor (CPU): MSE=58.3629
        ‚Ä¢ XGBoost: MSE=76.6899
   ‚úÖ MSTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MSTR (TargetReturn): TCN with MSE=0.0922
üêõ DEBUG: MSTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MSTR.
üêõ DEBUG: MSTR - Moving model to CPU before return...
üêõ DEBUG [22:29:08.689]: MSTR - Returning result metadata...
üêõ DEBUG: train_worker started for CONL
üêõ DEBUG [22:29:08.690]: Main received result for MSTR
      ‚è≥ EYE TCN: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for CONL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - CONL: Initiating feature extraction for training.
  [DIAGNOSTIC] CONL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CONL: rows after features available: 126
üéØ CONL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CONL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CONL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CONL: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.268348
         RMSE: 0.518024
         R¬≤ Score: -1.4672 (Poor - 146.7% variance explained)
      üîπ ESLT: Training TCN (50 epochs)...
      ‚è≥ EYE TCN: Epoch 20/50 (40%)
      ‚è≥ ESLT TCN: Epoch 10/50 (20%)
      ‚è≥ EYE TCN: Epoch 30/50 (60%)
      ‚è≥ ESLT TCN: Epoch 20/50 (40%)
      ‚è≥ RDDT LSTM: Epoch 40/50 (80%)
      ‚è≥ JFIN LSTM: Epoch 20/50 (40%)
      ‚è≥ EYE TCN: Epoch 40/50 (80%)
      ‚è≥ ESLT TCN: Epoch 30/50 (60%)
      ‚è≥ OPRT LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.397821
         RMSE: 0.630731
         R¬≤ Score: -0.4418
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EYE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EYE Random Forest: Starting GridSearchCV fit...
      ‚è≥ ESLT TCN: Epoch 40/50 (80%)
      ‚è≥ CONL LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.114204
         RMSE: 0.337940
         R¬≤ Score: -0.0500
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ESLT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ESLT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.368130
         RMSE: 0.606737
         R¬≤ Score: -1.2354 (Poor - 123.5% variance explained)
      üîπ RDDT: Training TCN (50 epochs)...
      ‚è≥ JFIN LSTM: Epoch 30/50 (60%)
      ‚è≥ RDDT TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.101666
         RMSE: 0.318852
         R¬≤ Score: -0.8947 (Poor - 89.5% variance explained)
      üîπ OPRT: Training TCN (50 epochs)...
      ‚è≥ CONL LSTM: Epoch 20/50 (40%)
      ‚è≥ RDDT TCN: Epoch 20/50 (40%)
      ‚è≥ OPRT TCN: Epoch 10/50 (20%)
      ‚è≥ RDDT TCN: Epoch 30/50 (60%)
       ‚úÖ DPRO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=397.2992 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DPRO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ RDDT TCN: Epoch 40/50 (80%)
      ‚è≥ OPRT TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.206705
         RMSE: 0.454649
         R¬≤ Score: -0.2552
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RDDT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RDDT Random Forest: Starting GridSearchCV fit...
      ‚è≥ OPRT TCN: Epoch 30/50 (60%)
      ‚è≥ JFIN LSTM: Epoch 40/50 (80%)
      ‚è≥ CONL LSTM: Epoch 30/50 (60%)
       ‚úÖ NEXT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=81.3554 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.6s
    - LSTM: MSE=0.3598
    - TCN: MSE=0.3586
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3586
        ‚Ä¢ LSTM: MSE=0.3598
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.4641
        ‚Ä¢ Random Forest: MSE=72.5657
        ‚Ä¢ XGBoost: MSE=81.3554
   ‚úÖ NEXT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NEXT (TargetReturn): TCN with MSE=0.3586
üêõ DEBUG: NEXT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NEXT.
üêõ DEBUG: NEXT - Moving model to CPU before return...
üêõ DEBUG [22:29:10.522]: NEXT - Returning result metadata...
üêõ DEBUG: train_worker started for HTZ
üêõ DEBUG [22:29:10.524]: Main received result for NEXT
      ‚è≥ OPRT TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for HTZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - HTZ: Initiating feature extraction for training.
  [DIAGNOSTIC] HTZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HTZ: rows after features available: 126
üéØ HTZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HTZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HTZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HTZ: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.071451
         RMSE: 0.267304
         R¬≤ Score: -0.3316
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPRT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPRT Random Forest: Starting GridSearchCV fit...
      ‚è≥ CONL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.219536
         RMSE: 0.468546
         R¬≤ Score: -1.1575 (Poor - 115.7% variance explained)
      üîπ JFIN: Training TCN (50 epochs)...
      ‚è≥ HTZ LSTM: Epoch 10/50 (20%)
       ‚úÖ DPRO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=266.6797 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DPRO XGBoost: Starting GridSearchCV fit...
      ‚è≥ JFIN TCN: Epoch 10/50 (20%)
      ‚è≥ JFIN TCN: Epoch 20/50 (40%)
      ‚è≥ JFIN TCN: Epoch 30/50 (60%)
      ‚è≥ JFIN TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 1.094419
         RMSE: 1.046145
         R¬≤ Score: -1.6235 (Poor - 162.4% variance explained)
      üîπ CONL: Training TCN (50 epochs)...
      ‚è≥ HTZ LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.101547
         RMSE: 0.318665
         R¬≤ Score: 0.0021
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JFIN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JFIN Random Forest: Starting GridSearchCV fit...
      ‚è≥ CONL TCN: Epoch 10/50 (20%)
      ‚è≥ CONL TCN: Epoch 20/50 (40%)
      ‚è≥ CONL TCN: Epoch 30/50 (60%)
      ‚è≥ CONL TCN: Epoch 40/50 (80%)
      ‚è≥ HTZ LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.634595
         RMSE: 0.796615
         R¬≤ Score: -0.5213
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CONL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CONL Random Forest: Starting GridSearchCV fit...
      ‚è≥ HTZ LSTM: Epoch 40/50 (80%)
       ‚úÖ EYE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=113.6470 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EYE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ESLT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.1751 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ESLT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.462730
         RMSE: 0.680243
         R¬≤ Score: -1.2420 (Poor - 124.2% variance explained)
      üîπ HTZ: Training TCN (50 epochs)...
       ‚úÖ RDDT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=90.4408 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RDDT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ HTZ TCN: Epoch 10/50 (20%)
      ‚è≥ HTZ TCN: Epoch 20/50 (40%)
       ‚úÖ EYE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=79.7815 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EYE XGBoost: Starting GridSearchCV fit...
      ‚è≥ HTZ TCN: Epoch 30/50 (60%)
       ‚úÖ ESLT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.3434 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ESLT XGBoost: Starting GridSearchCV fit...
      ‚è≥ HTZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.208601
         RMSE: 0.456729
         R¬≤ Score: -0.0107
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HTZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HTZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPRT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=199.6203 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPRT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RDDT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=83.0325 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RDDT XGBoost: Starting GridSearchCV fit...
       ‚úÖ OPRT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=172.0111 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPRT XGBoost: Starting GridSearchCV fit...
       ‚úÖ JFIN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=197.7313 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JFIN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CONL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=486.1139 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CONL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JFIN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=245.1178 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JFIN XGBoost: Starting GridSearchCV fit...
       ‚úÖ CONL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=945.7492 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CONL XGBoost: Starting GridSearchCV fit...
       ‚úÖ HTZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=253.0969 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HTZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HTZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=160.7466 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HTZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ BITX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=74.8295 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 123.8s
    - LSTM: MSE=0.3869
    - TCN: MSE=0.2347
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2347
        ‚Ä¢ LSTM: MSE=0.3869
        ‚Ä¢ LightGBM Regressor (CPU): MSE=74.2392
        ‚Ä¢ XGBoost: MSE=74.8295
        ‚Ä¢ Random Forest: MSE=88.4541
   ‚úÖ BITX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BITX (TargetReturn): TCN with MSE=0.2347
üêõ DEBUG: BITX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BITX.
üêõ DEBUG: BITX - Moving model to CPU before return...
üêõ DEBUG [22:30:21.828]: BITX - Returning result metadata...
üêõ DEBUG: train_worker started for SBSW
üêõ DEBUG [22:30:21.834]: Main received result for BITX
üêõ DEBUG: Training progress: 76/959 done
  ‚öôÔ∏è Training models for SBSW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - SBSW: Initiating feature extraction for training.
  [DIAGNOSTIC] SBSW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SBSW: rows after features available: 126
üéØ SBSW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SBSW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SBSW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SBSW: Training LSTM (50 epochs)...
      ‚è≥ SBSW LSTM: Epoch 10/50 (20%)
      ‚è≥ SBSW LSTM: Epoch 20/50 (40%)
      ‚è≥ SBSW LSTM: Epoch 30/50 (60%)
      ‚è≥ SBSW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.322393
         RMSE: 0.567797
         R¬≤ Score: -1.8043 (Poor - 180.4% variance explained)
      üîπ SBSW: Training TCN (50 epochs)...
      ‚è≥ SBSW TCN: Epoch 10/50 (20%)
      ‚è≥ SBSW TCN: Epoch 20/50 (40%)
      ‚è≥ SBSW TCN: Epoch 30/50 (60%)
      ‚è≥ SBSW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.181538
         RMSE: 0.426073
         R¬≤ Score: -0.5791
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SBSW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SBSW Random Forest: Starting GridSearchCV fit...
       ‚úÖ SBSW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=240.7406 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SBSW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SBSW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=210.4469 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SBSW XGBoost: Starting GridSearchCV fit...
       ‚úÖ AS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.6269 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 129.5s
    - LSTM: MSE=0.6520
    - TCN: MSE=0.4296
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4296
        ‚Ä¢ LSTM: MSE=0.6520
        ‚Ä¢ Random Forest: MSE=31.6669
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.4977
        ‚Ä¢ XGBoost: MSE=37.6269
   ‚úÖ AS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AS (TargetReturn): TCN with MSE=0.4296
üêõ DEBUG: AS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AS.
üêõ DEBUG: AS - Moving model to CPU before return...
üêõ DEBUG [22:30:32.027]: AS - Returning result metadata...
üêõ DEBUG [22:30:32.028]: Main received result for AS
üêõ DEBUG: train_worker started for GRNQ
  ‚öôÔ∏è Training models for GRNQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - GRNQ: Initiating feature extraction for training.
  [DIAGNOSTIC] GRNQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRNQ: rows after features available: 126
üéØ GRNQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRNQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRNQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRNQ: Training LSTM (50 epochs)...
      ‚è≥ GRNQ LSTM: Epoch 10/50 (20%)
      ‚è≥ GRNQ LSTM: Epoch 20/50 (40%)
      ‚è≥ GRNQ LSTM: Epoch 30/50 (60%)
      ‚è≥ GRNQ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.520055
         RMSE: 0.721148
         R¬≤ Score: -1.1503 (Poor - 115.0% variance explained)
      üîπ GRNQ: Training TCN (50 epochs)...
      ‚è≥ GRNQ TCN: Epoch 10/50 (20%)
      ‚è≥ GRNQ TCN: Epoch 20/50 (40%)
      ‚è≥ GRNQ TCN: Epoch 30/50 (60%)
      ‚è≥ GRNQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.441939
         RMSE: 0.664785
         R¬≤ Score: -0.8273
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRNQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRNQ Random Forest: Starting GridSearchCV fit...
       ‚úÖ CAR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=188.7472 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.4s
    - LSTM: MSE=0.5554
    - TCN: MSE=0.4408
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4408
        ‚Ä¢ LSTM: MSE=0.5554
        ‚Ä¢ Random Forest: MSE=107.9902
        ‚Ä¢ LightGBM Regressor (CPU): MSE=119.2936
        ‚Ä¢ XGBoost: MSE=188.7472
   ‚úÖ CAR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CAR (TargetReturn): TCN with MSE=0.4408
üêõ DEBUG: CAR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CAR.
üêõ DEBUG: CAR - Moving model to CPU before return...
üêõ DEBUG [22:30:37.804]: CAR - Returning result metadata...
üêõ DEBUG [22:30:37.805]: Main received result for CAR
üêõ DEBUG: train_worker started for PAYS
  ‚öôÔ∏è Training models for PAYS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - PAYS: Initiating feature extraction for training.
  [DIAGNOSTIC] PAYS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAYS: rows after features available: 126
üéØ PAYS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAYS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAYS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAYS: Training LSTM (50 epochs)...
       ‚úÖ GRNQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=164.3126 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRNQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PAYS LSTM: Epoch 10/50 (20%)
      ‚è≥ PAYS LSTM: Epoch 20/50 (40%)
       ‚úÖ GRNQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=209.9105 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRNQ XGBoost: Starting GridSearchCV fit...
      ‚è≥ PAYS LSTM: Epoch 30/50 (60%)
      ‚è≥ PAYS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.780561
         RMSE: 0.883493
         R¬≤ Score: -1.0396 (Poor - 104.0% variance explained)
      üîπ PAYS: Training TCN (50 epochs)...
      ‚è≥ PAYS TCN: Epoch 10/50 (20%)
      ‚è≥ PAYS TCN: Epoch 20/50 (40%)
      ‚è≥ PAYS TCN: Epoch 30/50 (60%)
      ‚è≥ PAYS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.622382
         RMSE: 0.788912
         R¬≤ Score: -0.6263
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAYS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAYS Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAYS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=95.5097 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAYS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAYS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=275.6376 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAYS XGBoost: Starting GridSearchCV fit...
       ‚úÖ AGX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=55.1467 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 130.1s
    - LSTM: MSE=0.3440
    - TCN: MSE=0.1810
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1810
        ‚Ä¢ LSTM: MSE=0.3440
        ‚Ä¢ Random Forest: MSE=49.1719
        ‚Ä¢ XGBoost: MSE=55.1467
        ‚Ä¢ LightGBM Regressor (CPU): MSE=58.7289
   ‚úÖ AGX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AGX (TargetReturn): TCN with MSE=0.1810
üêõ DEBUG: AGX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AGX.
üêõ DEBUG: AGX - Moving model to CPU before return...
üêõ DEBUG [22:30:59.983]: AGX - Returning result metadata...
üêõ DEBUG: train_worker started for XYF
  ‚öôÔ∏è Training models for XYF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - XYF: Initiating feature extraction for training.
  [DIAGNOSTIC] XYF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XYF: rows after features available: 126
üéØ XYF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XYF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XYF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XYF: Training LSTM (50 epochs)...
      ‚è≥ XYF LSTM: Epoch 10/50 (20%)
      ‚è≥ XYF LSTM: Epoch 20/50 (40%)
      ‚è≥ XYF LSTM: Epoch 30/50 (60%)
      ‚è≥ XYF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.450558
         RMSE: 0.671236
         R¬≤ Score: -1.0648 (Poor - 106.5% variance explained)
      üîπ XYF: Training TCN (50 epochs)...
      ‚è≥ XYF TCN: Epoch 10/50 (20%)
      ‚è≥ XYF TCN: Epoch 20/50 (40%)
      ‚è≥ XYF TCN: Epoch 30/50 (60%)
      ‚è≥ XYF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.221945
         RMSE: 0.471111
         R¬≤ Score: -0.0171
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XYF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XYF Random Forest: Starting GridSearchCV fit...
       ‚úÖ SRAD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=18.3939 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 134.9s
    - LSTM: MSE=0.1532
    - TCN: MSE=0.0952
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 139.1 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0952
        ‚Ä¢ LSTM: MSE=0.1532
        ‚Ä¢ XGBoost: MSE=18.3939
        ‚Ä¢ Random Forest: MSE=19.9920
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.4964
   ‚úÖ SRAD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SRAD (TargetReturn): TCN with MSE=0.0952
üêõ DEBUG: SRAD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SRAD.
üêõ DEBUG: SRAD - Moving model to CPU before return...
üêõ DEBUG [22:31:04.222]: SRAD - Returning result metadata...
üêõ DEBUG: train_worker started for TRVG
üêõ DEBUG [22:31:04.224]: Main received result for SRAD
üêõ DEBUG [22:31:04.224]: Main received result for AGX
üêõ DEBUG: Training progress: 80/959 done
  ‚öôÔ∏è Training models for TRVG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - TRVG: Initiating feature extraction for training.
  [DIAGNOSTIC] TRVG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TRVG: rows after features available: 126
üéØ TRVG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TRVG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TRVG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TRVG: Training LSTM (50 epochs)...
      ‚è≥ TRVG LSTM: Epoch 10/50 (20%)
      ‚è≥ TRVG LSTM: Epoch 20/50 (40%)
      ‚è≥ TRVG LSTM: Epoch 30/50 (60%)
       ‚úÖ XYF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=130.5318 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XYF LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TRVG LSTM: Epoch 40/50 (80%)
       ‚úÖ XYF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=179.6526 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XYF XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.037870
         RMSE: 0.194601
         R¬≤ Score: -0.1568 (Poor - 15.7% variance explained)
      üîπ TRVG: Training TCN (50 epochs)...
      ‚è≥ TRVG TCN: Epoch 10/50 (20%)
      ‚è≥ TRVG TCN: Epoch 20/50 (40%)
      ‚è≥ TRVG TCN: Epoch 30/50 (60%)
      ‚è≥ TRVG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.046641
         RMSE: 0.215964
         R¬≤ Score: -0.4247
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TRVG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TRVG Random Forest: Starting GridSearchCV fit...
       ‚úÖ TRVG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=340.5653 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TRVG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TRVG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=330.9303 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TRVG XGBoost: Starting GridSearchCV fit...
       ‚úÖ TPR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.0542 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 139.4s
    - LSTM: MSE=0.8721
    - TCN: MSE=0.7053
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 142.7 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7053
        ‚Ä¢ LSTM: MSE=0.8721
        ‚Ä¢ Random Forest: MSE=22.4907
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.7254
        ‚Ä¢ XGBoost: MSE=28.0542
   ‚úÖ TPR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TPR (TargetReturn): TCN with MSE=0.7053
üêõ DEBUG: TPR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TPR.
üêõ DEBUG: TPR - Moving model to CPU before return...
üêõ DEBUG [22:31:13.203]: TPR - Returning result metadata...
üêõ DEBUG: train_worker started for AMBR
üêõ DEBUG [22:31:13.208]: Main received result for TPR
  ‚öôÔ∏è Training models for AMBR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - AMBR: Initiating feature extraction for training.
  [DIAGNOSTIC] AMBR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMBR: rows after features available: 126
üéØ AMBR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMBR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMBR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMBR: Training LSTM (50 epochs)...
      ‚è≥ AMBR LSTM: Epoch 10/50 (20%)
      ‚è≥ AMBR LSTM: Epoch 20/50 (40%)
      ‚è≥ AMBR LSTM: Epoch 30/50 (60%)
      ‚è≥ AMBR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.063023
         RMSE: 0.251044
         R¬≤ Score: -0.6123 (Poor - 61.2% variance explained)
      üîπ AMBR: Training TCN (50 epochs)...
      ‚è≥ AMBR TCN: Epoch 10/50 (20%)
      ‚è≥ AMBR TCN: Epoch 20/50 (40%)
      ‚è≥ AMBR TCN: Epoch 30/50 (60%)
      ‚è≥ AMBR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.040181
         RMSE: 0.200451
         R¬≤ Score: -0.0279
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMBR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMBR Random Forest: Starting GridSearchCV fit...
       ‚úÖ AMBR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=121.6683 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMBR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AMBR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=255.2077 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMBR XGBoost: Starting GridSearchCV fit...
       ‚úÖ INVZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=326.7763 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 137.0s
    - LSTM: MSE=0.8113
    - TCN: MSE=0.6606
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 140.5 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6606
        ‚Ä¢ LSTM: MSE=0.8113
        ‚Ä¢ Random Forest: MSE=230.1456
        ‚Ä¢ LightGBM Regressor (CPU): MSE=271.0936
        ‚Ä¢ XGBoost: MSE=326.7763
   ‚úÖ INVZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for INVZ (TargetReturn): TCN with MSE=0.6606
üêõ DEBUG: INVZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for INVZ.
üêõ DEBUG: INVZ - Moving model to CPU before return...
üêõ DEBUG [22:31:23.134]: INVZ - Returning result metadata...
üêõ DEBUG: train_worker started for COIN
üêõ DEBUG [22:31:23.138]: Main received result for INVZ
  ‚öôÔ∏è Training models for COIN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - COIN: Initiating feature extraction for training.
  [DIAGNOSTIC] COIN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COIN: rows after features available: 126
üéØ COIN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COIN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COIN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COIN: Training LSTM (50 epochs)...
      ‚è≥ COIN LSTM: Epoch 10/50 (20%)
      ‚è≥ COIN LSTM: Epoch 20/50 (40%)
      ‚è≥ COIN LSTM: Epoch 30/50 (60%)
      ‚è≥ COIN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.775349
         RMSE: 0.880539
         R¬≤ Score: -0.9952 (Poor - 99.5% variance explained)
      üîπ COIN: Training TCN (50 epochs)...
      ‚è≥ COIN TCN: Epoch 10/50 (20%)
      ‚è≥ COIN TCN: Epoch 20/50 (40%)
      ‚è≥ COIN TCN: Epoch 30/50 (60%)
      ‚è≥ COIN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.452043
         RMSE: 0.672341
         R¬≤ Score: -0.1632
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COIN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COIN Random Forest: Starting GridSearchCV fit...
       ‚úÖ DPRO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=540.1233 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 135.6s
    - LSTM: MSE=0.5507
    - TCN: MSE=0.2692
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 140.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2692
        ‚Ä¢ LSTM: MSE=0.5507
        ‚Ä¢ LightGBM Regressor (CPU): MSE=266.6797
        ‚Ä¢ Random Forest: MSE=397.2992
        ‚Ä¢ XGBoost: MSE=540.1233
   ‚úÖ DPRO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DPRO (TargetReturn): TCN with MSE=0.2692
üêõ DEBUG: DPRO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DPRO.
üêõ DEBUG: DPRO - Moving model to CPU before return...
üêõ DEBUG [22:31:26.792]: DPRO - Returning result metadata...
üêõ DEBUG: train_worker started for GOGO
üêõ DEBUG [22:31:26.792]: Main received result for DPRO
  ‚öôÔ∏è Training models for GOGO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - GOGO: Initiating feature extraction for training.
  [DIAGNOSTIC] GOGO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GOGO: rows after features available: 126
üéØ GOGO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GOGO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GOGO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GOGO: Training LSTM (50 epochs)...
      ‚è≥ GOGO LSTM: Epoch 10/50 (20%)
      ‚è≥ GOGO LSTM: Epoch 20/50 (40%)
      ‚è≥ GOGO LSTM: Epoch 30/50 (60%)
      ‚è≥ GOGO LSTM: Epoch 40/50 (80%)
       ‚úÖ COIN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=106.5060 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COIN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.450853
         RMSE: 0.671456
         R¬≤ Score: -0.5333 (Poor - 53.3% variance explained)
      üîπ GOGO: Training TCN (50 epochs)...
      ‚è≥ GOGO TCN: Epoch 10/50 (20%)
      ‚è≥ GOGO TCN: Epoch 20/50 (40%)
      ‚è≥ GOGO TCN: Epoch 30/50 (60%)
      ‚è≥ GOGO TCN: Epoch 40/50 (80%)
       ‚úÖ COIN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=129.0368 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COIN XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.423754
         RMSE: 0.650964
         R¬≤ Score: -0.4411
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GOGO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GOGO Random Forest: Starting GridSearchCV fit...
       ‚úÖ EYE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=129.8593 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 137.1s
    - LSTM: MSE=0.6292
    - TCN: MSE=0.3978
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 141.7 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3978
        ‚Ä¢ LSTM: MSE=0.6292
        ‚Ä¢ LightGBM Regressor (CPU): MSE=79.7815
        ‚Ä¢ Random Forest: MSE=113.6470
        ‚Ä¢ XGBoost: MSE=129.8593
   ‚úÖ EYE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EYE (TargetReturn): TCN with MSE=0.3978
üêõ DEBUG: EYE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EYE.
üêõ DEBUG: EYE - Moving model to CPU before return...
üêõ DEBUG [22:31:31.051]: EYE - Returning result metadata...
üêõ DEBUG [22:31:31.052]: Main received result for EYE
üêõ DEBUG: Training progress: 84/959 done
üêõ DEBUG: train_worker started for SSRM
  ‚öôÔ∏è Training models for SSRM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - SSRM: Initiating feature extraction for training.
  [DIAGNOSTIC] SSRM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SSRM: rows after features available: 126
üéØ SSRM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SSRM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SSRM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SSRM: Training LSTM (50 epochs)...
      ‚è≥ SSRM LSTM: Epoch 10/50 (20%)
      ‚è≥ SSRM LSTM: Epoch 20/50 (40%)
      ‚è≥ SSRM LSTM: Epoch 30/50 (60%)
      ‚è≥ SSRM LSTM: Epoch 40/50 (80%)
       ‚úÖ GOGO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=228.4207 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GOGO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.132565
         RMSE: 0.364095
         R¬≤ Score: -1.7391 (Poor - 173.9% variance explained)
      üîπ SSRM: Training TCN (50 epochs)...
      ‚è≥ SSRM TCN: Epoch 10/50 (20%)
       ‚úÖ GOGO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=197.4971 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GOGO XGBoost: Starting GridSearchCV fit...
      ‚è≥ SSRM TCN: Epoch 20/50 (40%)
      ‚è≥ SSRM TCN: Epoch 30/50 (60%)
      ‚è≥ SSRM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.048175
         RMSE: 0.219488
         R¬≤ Score: 0.0046
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SSRM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SSRM Random Forest: Starting GridSearchCV fit...
       ‚úÖ RDDT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=43.0547 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 139.9s
    - LSTM: MSE=0.3681
    - TCN: MSE=0.2067
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 144.0 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2067
        ‚Ä¢ LSTM: MSE=0.3681
        ‚Ä¢ XGBoost: MSE=43.0547
        ‚Ä¢ LightGBM Regressor (CPU): MSE=83.0325
        ‚Ä¢ Random Forest: MSE=90.4408
   ‚úÖ RDDT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RDDT (TargetReturn): TCN with MSE=0.2067
üêõ DEBUG: RDDT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RDDT.
üêõ DEBUG: RDDT - Moving model to CPU before return...
üêõ DEBUG [22:31:34.411]: RDDT - Returning result metadata...
üêõ DEBUG: train_worker started for QSI
  ‚öôÔ∏è Training models for QSI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - QSI: Initiating feature extraction for training.
  [DIAGNOSTIC] QSI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QSI: rows after features available: 126
üéØ QSI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QSI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QSI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QSI: Training LSTM (50 epochs)...
      ‚è≥ QSI LSTM: Epoch 10/50 (20%)
       ‚úÖ ESLT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8378 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 141.1s
    - LSTM: MSE=0.2683
    - TCN: MSE=0.1142
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 145.6 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1142
        ‚Ä¢ LSTM: MSE=0.2683
        ‚Ä¢ Random Forest: MSE=15.1751
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.3434
        ‚Ä¢ XGBoost: MSE=16.8378
   ‚úÖ ESLT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ESLT (TargetReturn): TCN with MSE=0.1142
üêõ DEBUG: ESLT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ESLT.
üêõ DEBUG: ESLT - Moving model to CPU before return...
üêõ DEBUG [22:31:35.174]: ESLT - Returning result metadata...
üêõ DEBUG: train_worker started for OSS
üêõ DEBUG [22:31:35.176]: Main received result for ESLT
üêõ DEBUG [22:31:35.176]: Main received result for RDDT
  ‚öôÔ∏è Training models for OSS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - OSS: Initiating feature extraction for training.
  [DIAGNOSTIC] OSS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OSS: rows after features available: 126
üéØ OSS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OSS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OSS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OSS: Training LSTM (50 epochs)...
      ‚è≥ QSI LSTM: Epoch 20/50 (40%)
      ‚è≥ OSS LSTM: Epoch 10/50 (20%)
      ‚è≥ QSI LSTM: Epoch 30/50 (60%)
      ‚è≥ OSS LSTM: Epoch 20/50 (40%)
      ‚è≥ QSI LSTM: Epoch 40/50 (80%)
      ‚è≥ OSS LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.172408
         RMSE: 0.415221
         R¬≤ Score: -1.2026 (Poor - 120.3% variance explained)
      üîπ QSI: Training TCN (50 epochs)...
       ‚úÖ SSRM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.2570 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SSRM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ QSI TCN: Epoch 10/50 (20%)
      ‚è≥ QSI TCN: Epoch 20/50 (40%)
      ‚è≥ QSI TCN: Epoch 30/50 (60%)
      ‚è≥ QSI TCN: Epoch 40/50 (80%)
      ‚è≥ OSS LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.121949
         RMSE: 0.349211
         R¬≤ Score: -0.5579
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QSI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QSI Random Forest: Starting GridSearchCV fit...
       ‚úÖ JFIN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=249.9281 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 141.7s
    - LSTM: MSE=0.2195
    - TCN: MSE=0.1015
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 145.9 seconds (2.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1015
        ‚Ä¢ LSTM: MSE=0.2195
        ‚Ä¢ Random Forest: MSE=197.7313
        ‚Ä¢ LightGBM Regressor (CPU): MSE=245.1178
        ‚Ä¢ XGBoost: MSE=249.9281
   ‚úÖ JFIN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JFIN (TargetReturn): TCN with MSE=0.1015
üêõ DEBUG: JFIN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JFIN.
üêõ DEBUG: JFIN - Moving model to CPU before return...
üêõ DEBUG [22:31:37.772]: JFIN - Returning result metadata...
üêõ DEBUG: train_worker started for GHRS
  ‚öôÔ∏è Training models for GHRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - GHRS: Initiating feature extraction for training.
  [DIAGNOSTIC] GHRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GHRS: rows after features available: 126
üéØ GHRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GHRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GHRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GHRS: Training LSTM (50 epochs)...
       ‚úÖ SSRM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.5250 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SSRM XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.628383
         RMSE: 0.792706
         R¬≤ Score: -0.9641 (Poor - 96.4% variance explained)
      üîπ OSS: Training TCN (50 epochs)...
      ‚è≥ OSS TCN: Epoch 10/50 (20%)
       ‚úÖ OPRT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=167.6502 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 143.3s
    - LSTM: MSE=0.1017
    - TCN: MSE=0.0715
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 147.6 seconds (2.5 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0715
        ‚Ä¢ LSTM: MSE=0.1017
        ‚Ä¢ XGBoost: MSE=167.6502
        ‚Ä¢ LightGBM Regressor (CPU): MSE=172.0111
        ‚Ä¢ Random Forest: MSE=199.6203
   ‚úÖ OPRT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPRT (TargetReturn): TCN with MSE=0.0715
üêõ DEBUG: OPRT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPRT.
üêõ DEBUG: OPRT - Moving model to CPU before return...
üêõ DEBUG [22:31:38.318]: OPRT - Returning result metadata...
üêõ DEBUG: train_worker started for LUXE
üêõ DEBUG [22:31:38.330]: Main received result for OPRT
üêõ DEBUG [22:31:38.331]: Main received result for JFIN
üêõ DEBUG: Training progress: 88/959 done
      ‚è≥ GHRS LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for LUXE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - LUXE: Initiating feature extraction for training.
  [DIAGNOSTIC] LUXE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LUXE: rows after features available: 126
üéØ LUXE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LUXE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LUXE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LUXE: Training LSTM (50 epochs)...
      ‚è≥ OSS TCN: Epoch 20/50 (40%)
      ‚è≥ OSS TCN: Epoch 30/50 (60%)
      ‚è≥ OSS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.531166
         RMSE: 0.728812
         R¬≤ Score: -0.6603
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OSS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OSS Random Forest: Starting GridSearchCV fit...
      ‚è≥ GHRS LSTM: Epoch 20/50 (40%)
      ‚è≥ LUXE LSTM: Epoch 10/50 (20%)
       ‚úÖ CONL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=699.2094 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 143.0s
    - LSTM: MSE=1.0944
    - TCN: MSE=0.6346
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 147.2 seconds (2.5 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6346
        ‚Ä¢ LSTM: MSE=1.0944
        ‚Ä¢ Random Forest: MSE=486.1139
        ‚Ä¢ XGBoost: MSE=699.2094
        ‚Ä¢ LightGBM Regressor (CPU): MSE=945.7492
   ‚úÖ CONL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CONL (TargetReturn): TCN with MSE=0.6346
üêõ DEBUG: CONL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CONL.
üêõ DEBUG: CONL - Moving model to CPU before return...
üêõ DEBUG [22:31:39.687]: CONL - Returning result metadata...
üêõ DEBUG [22:31:39.688]: Main received result for CONL
üêõ DEBUG: train_worker started for EVEX
  ‚öôÔ∏è Training models for EVEX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - EVEX: Initiating feature extraction for training.
  [DIAGNOSTIC] EVEX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EVEX: rows after features available: 126
üéØ EVEX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EVEX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EVEX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EVEX: Training LSTM (50 epochs)...
      ‚è≥ GHRS LSTM: Epoch 30/50 (60%)
      ‚è≥ LUXE LSTM: Epoch 20/50 (40%)
      ‚è≥ EVEX LSTM: Epoch 10/50 (20%)
      ‚è≥ GHRS LSTM: Epoch 40/50 (80%)
      ‚è≥ LUXE LSTM: Epoch 30/50 (60%)
      ‚è≥ EVEX LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.360579
         RMSE: 0.600483
         R¬≤ Score: -0.9301 (Poor - 93.0% variance explained)
      üîπ GHRS: Training TCN (50 epochs)...
      ‚è≥ GHRS TCN: Epoch 10/50 (20%)
      ‚è≥ GHRS TCN: Epoch 20/50 (40%)
       ‚úÖ QSI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=147.2060 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QSI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LUXE LSTM: Epoch 40/50 (80%)
      ‚è≥ GHRS TCN: Epoch 30/50 (60%)
      ‚è≥ EVEX LSTM: Epoch 30/50 (60%)
      ‚è≥ GHRS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.343930
         RMSE: 0.586455
         R¬≤ Score: -0.8410
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GHRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GHRS Random Forest: Starting GridSearchCV fit...
       ‚úÖ HTZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=304.1963 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 143.7s
    - LSTM: MSE=0.4627
    - TCN: MSE=0.2086
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 147.4 seconds (2.5 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2086
        ‚Ä¢ LSTM: MSE=0.4627
        ‚Ä¢ LightGBM Regressor (CPU): MSE=160.7466
        ‚Ä¢ Random Forest: MSE=253.0969
        ‚Ä¢ XGBoost: MSE=304.1963
   ‚úÖ HTZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HTZ (TargetReturn): TCN with MSE=0.2086
üêõ DEBUG: HTZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HTZ.
üêõ DEBUG: HTZ - Moving model to CPU before return...
üêõ DEBUG [22:31:41.694]: HTZ - Returning result metadata...
üêõ DEBUG: train_worker started for EOSE
üêõ DEBUG [22:31:41.695]: Main received result for HTZ
  ‚öôÔ∏è Training models for EOSE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - EOSE: Initiating feature extraction for training.
  [DIAGNOSTIC] EOSE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EOSE: rows after features available: 126
üéØ EOSE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EOSE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EOSE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EOSE: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.066008
         RMSE: 0.256920
         R¬≤ Score: -1.2300 (Poor - 123.0% variance explained)
      üîπ LUXE: Training TCN (50 epochs)...
      ‚è≥ EVEX LSTM: Epoch 40/50 (80%)
      ‚è≥ LUXE TCN: Epoch 10/50 (20%)
       ‚úÖ QSI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=223.4884 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QSI XGBoost: Starting GridSearchCV fit...
      ‚è≥ LUXE TCN: Epoch 20/50 (40%)
      ‚è≥ LUXE TCN: Epoch 30/50 (60%)
      ‚è≥ EOSE LSTM: Epoch 10/50 (20%)
      ‚è≥ LUXE TCN: Epoch 40/50 (80%)
       ‚úÖ OSS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=227.7061 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OSS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.034794
         RMSE: 0.186531
         R¬≤ Score: -0.1755
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LUXE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LUXE Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.595815
         RMSE: 0.771890
         R¬≤ Score: -1.0817 (Poor - 108.2% variance explained)
      üîπ EVEX: Training TCN (50 epochs)...
      ‚è≥ EVEX TCN: Epoch 10/50 (20%)
      ‚è≥ EVEX TCN: Epoch 20/50 (40%)
      ‚è≥ EVEX TCN: Epoch 30/50 (60%)
      ‚è≥ EOSE LSTM: Epoch 20/50 (40%)
      ‚è≥ EVEX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.564116
         RMSE: 0.751077
         R¬≤ Score: -0.9710
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EVEX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EVEX Random Forest: Starting GridSearchCV fit...
       ‚úÖ OSS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=193.1494 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OSS XGBoost: Starting GridSearchCV fit...
      ‚è≥ EOSE LSTM: Epoch 30/50 (60%)
      ‚è≥ EOSE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.156847
         RMSE: 0.396039
         R¬≤ Score: -1.7493 (Poor - 174.9% variance explained)
      üîπ EOSE: Training TCN (50 epochs)...
      ‚è≥ EOSE TCN: Epoch 10/50 (20%)
       ‚úÖ GHRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=294.9734 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GHRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EOSE TCN: Epoch 20/50 (40%)
      ‚è≥ EOSE TCN: Epoch 30/50 (60%)
      ‚è≥ EOSE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.058677
         RMSE: 0.242233
         R¬≤ Score: -0.0285
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EOSE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EOSE Random Forest: Starting GridSearchCV fit...
       ‚úÖ GHRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=299.3136 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GHRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ LUXE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=75.9315 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LUXE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EVEX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=129.7007 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EVEX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LUXE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=46.9149 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LUXE XGBoost: Starting GridSearchCV fit...
       ‚úÖ EVEX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=89.7431 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EVEX XGBoost: Starting GridSearchCV fit...
       ‚úÖ EOSE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=122.6434 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EOSE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EOSE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=157.8246 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EOSE XGBoost: Starting GridSearchCV fit...
       ‚úÖ SBSW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=434.4194 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 134.2s
    - LSTM: MSE=0.3224
    - TCN: MSE=0.1815
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1815
        ‚Ä¢ LSTM: MSE=0.3224
        ‚Ä¢ LightGBM Regressor (CPU): MSE=210.4469
        ‚Ä¢ Random Forest: MSE=240.7406
        ‚Ä¢ XGBoost: MSE=434.4194
   ‚úÖ SBSW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SBSW (TargetReturn): TCN with MSE=0.1815
üêõ DEBUG: SBSW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SBSW.
üêõ DEBUG: SBSW - Moving model to CPU before return...
üêõ DEBUG [22:32:42.994]: SBSW - Returning result metadata...
üêõ DEBUG [22:32:42.995]: Main received result for SBSW
üêõ DEBUG: train_worker started for MASS
  ‚öôÔ∏è Training models for MASS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - MASS: Initiating feature extraction for training.
  [DIAGNOSTIC] MASS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MASS: rows after features available: 126
üéØ MASS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MASS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MASS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MASS: Training LSTM (50 epochs)...
      ‚è≥ MASS LSTM: Epoch 10/50 (20%)
      ‚è≥ MASS LSTM: Epoch 20/50 (40%)
      ‚è≥ MASS LSTM: Epoch 30/50 (60%)
      ‚è≥ MASS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.113443
         RMSE: 0.336812
         R¬≤ Score: -0.3015 (Poor - 30.1% variance explained)
      üîπ MASS: Training TCN (50 epochs)...
      ‚è≥ MASS TCN: Epoch 10/50 (20%)
      ‚è≥ MASS TCN: Epoch 20/50 (40%)
      ‚è≥ MASS TCN: Epoch 30/50 (60%)
      ‚è≥ MASS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.136004
         RMSE: 0.368787
         R¬≤ Score: -0.5603
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MASS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MASS Random Forest: Starting GridSearchCV fit...
       ‚úÖ GRNQ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=288.0896 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.9s
    - LSTM: MSE=0.5201
    - TCN: MSE=0.4419
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4419
        ‚Ä¢ LSTM: MSE=0.5201
        ‚Ä¢ Random Forest: MSE=164.3126
        ‚Ä¢ LightGBM Regressor (CPU): MSE=209.9105
        ‚Ä¢ XGBoost: MSE=288.0896
   ‚úÖ GRNQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRNQ (TargetReturn): TCN with MSE=0.4419
üêõ DEBUG: GRNQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRNQ.
üêõ DEBUG: GRNQ - Moving model to CPU before return...
üêõ DEBUG [22:32:48.938]: GRNQ - Returning result metadata...
üêõ DEBUG: train_worker started for AVXL
üêõ DEBUG [22:32:48.938]: Main received result for GRNQ
üêõ DEBUG: Training progress: 92/959 done
  ‚öôÔ∏è Training models for AVXL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - AVXL: Initiating feature extraction for training.
  [DIAGNOSTIC] AVXL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AVXL: rows after features available: 126
üéØ AVXL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
       ‚úÖ MASS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=268.6309 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MASS LightGBM Regressor (CPU): Starting GridSearchCV fit...
  [DIAGNOSTIC] AVXL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AVXL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AVXL: Training LSTM (50 epochs)...
      ‚è≥ AVXL LSTM: Epoch 10/50 (20%)
       ‚úÖ MASS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=333.3329 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MASS XGBoost: Starting GridSearchCV fit...
      ‚è≥ AVXL LSTM: Epoch 20/50 (40%)
       ‚úÖ PAYS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=323.8374 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 126.1s
    - LSTM: MSE=0.7806
    - TCN: MSE=0.6224
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6224
        ‚Ä¢ LSTM: MSE=0.7806
        ‚Ä¢ Random Forest: MSE=95.5097
        ‚Ä¢ LightGBM Regressor (CPU): MSE=275.6376
        ‚Ä¢ XGBoost: MSE=323.8374
   ‚úÖ PAYS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAYS (TargetReturn): TCN with MSE=0.6224
üêõ DEBUG: PAYS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAYS.
üêõ DEBUG: PAYS - Moving model to CPU before return...
üêõ DEBUG [22:32:50.335]: PAYS - Returning result metadata...
üêõ DEBUG: train_worker started for TESL
üêõ DEBUG [22:32:50.335]: Main received result for PAYS
  ‚öôÔ∏è Training models for TESL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TESL: Initiating feature extraction for training.
  [DIAGNOSTIC] TESL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TESL: rows after features available: 126
üéØ TESL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TESL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TESL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TESL: Training LSTM (50 epochs)...
      ‚è≥ AVXL LSTM: Epoch 30/50 (60%)
      ‚è≥ TESL LSTM: Epoch 10/50 (20%)
      ‚è≥ AVXL LSTM: Epoch 40/50 (80%)
      ‚è≥ TESL LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.091519
         RMSE: 0.302521
         R¬≤ Score: -0.9719 (Poor - 97.2% variance explained)
      üîπ AVXL: Training TCN (50 epochs)...
      ‚è≥ AVXL TCN: Epoch 10/50 (20%)
      ‚è≥ AVXL TCN: Epoch 20/50 (40%)
      ‚è≥ AVXL TCN: Epoch 30/50 (60%)
      ‚è≥ AVXL TCN: Epoch 40/50 (80%)
      ‚è≥ TESL LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.051819
         RMSE: 0.227638
         R¬≤ Score: -0.1165
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AVXL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AVXL Random Forest: Starting GridSearchCV fit...
      ‚è≥ TESL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.189693
         RMSE: 0.435537
         R¬≤ Score: -1.0136 (Poor - 101.4% variance explained)
      üîπ TESL: Training TCN (50 epochs)...
      ‚è≥ TESL TCN: Epoch 10/50 (20%)
      ‚è≥ TESL TCN: Epoch 20/50 (40%)
      ‚è≥ TESL TCN: Epoch 30/50 (60%)
      ‚è≥ TESL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.155136
         RMSE: 0.393873
         R¬≤ Score: -0.6468
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TESL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TESL Random Forest: Starting GridSearchCV fit...
       ‚úÖ AVXL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=56.4586 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AVXL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AVXL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=86.3752 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AVXL XGBoost: Starting GridSearchCV fit...
       ‚úÖ TESL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=66.9958 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TESL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TESL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=63.4982 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TESL XGBoost: Starting GridSearchCV fit...
       ‚úÖ XYF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=131.9913 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.2s
    - LSTM: MSE=0.4506
    - TCN: MSE=0.2219
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2219
        ‚Ä¢ LSTM: MSE=0.4506
        ‚Ä¢ Random Forest: MSE=130.5318
        ‚Ä¢ XGBoost: MSE=131.9913
        ‚Ä¢ LightGBM Regressor (CPU): MSE=179.6526
   ‚úÖ XYF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XYF (TargetReturn): TCN with MSE=0.2219
üêõ DEBUG: XYF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XYF.
üêõ DEBUG: XYF - Moving model to CPU before return...
üêõ DEBUG [22:33:16.278]: XYF - Returning result metadata...
üêõ DEBUG: train_worker started for DOMO
üêõ DEBUG [22:33:16.280]: Main received result for XYF
  ‚öôÔ∏è Training models for DOMO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - DOMO: Initiating feature extraction for training.
  [DIAGNOSTIC] DOMO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DOMO: rows after features available: 126
üéØ DOMO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DOMO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DOMO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DOMO: Training LSTM (50 epochs)...
      ‚è≥ DOMO LSTM: Epoch 10/50 (20%)
      ‚è≥ DOMO LSTM: Epoch 20/50 (40%)
      ‚è≥ DOMO LSTM: Epoch 30/50 (60%)
      ‚è≥ DOMO LSTM: Epoch 40/50 (80%)
       ‚úÖ TRVG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=363.0672 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 127.9s
    - LSTM: MSE=0.0379
    - TCN: MSE=0.0466
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.0379
        ‚Ä¢ TCN: MSE=0.0466
        ‚Ä¢ LightGBM Regressor (CPU): MSE=330.9303
        ‚Ä¢ Random Forest: MSE=340.5653
        ‚Ä¢ XGBoost: MSE=363.0672
   ‚úÖ TRVG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TRVG (TargetReturn): LSTM with MSE=0.0379
üêõ DEBUG: TRVG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TRVG.
üêõ DEBUG: TRVG - Moving model to CPU before return...
üêõ DEBUG [22:33:18.914]: TRVG - Returning result metadata...
üêõ DEBUG [22:33:18.915]: Main received result for TRVG
üêõ DEBUG: train_worker started for DAO
  ‚öôÔ∏è Training models for DAO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - DAO: Initiating feature extraction for training.
  [DIAGNOSTIC] DAO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAO: rows after features available: 126
üéØ DAO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAO: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.552172
         RMSE: 0.743083
         R¬≤ Score: -0.9634 (Poor - 96.3% variance explained)
      üîπ DOMO: Training TCN (50 epochs)...
      ‚è≥ DOMO TCN: Epoch 10/50 (20%)
      ‚è≥ DOMO TCN: Epoch 20/50 (40%)
      ‚è≥ DAO LSTM: Epoch 10/50 (20%)
      ‚è≥ DOMO TCN: Epoch 30/50 (60%)
      ‚è≥ DOMO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.345096
         RMSE: 0.587448
         R¬≤ Score: -0.2271
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DOMO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DOMO Random Forest: Starting GridSearchCV fit...
      ‚è≥ DAO LSTM: Epoch 20/50 (40%)
      ‚è≥ DAO LSTM: Epoch 30/50 (60%)
      ‚è≥ DAO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.068833
         RMSE: 0.262360
         R¬≤ Score: -2.0063 (Poor - 200.6% variance explained)
      üîπ DAO: Training TCN (50 epochs)...
      ‚è≥ DAO TCN: Epoch 10/50 (20%)
      ‚è≥ DAO TCN: Epoch 20/50 (40%)
      ‚è≥ DAO TCN: Epoch 30/50 (60%)
      ‚è≥ DAO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.030776
         RMSE: 0.175430
         R¬≤ Score: -0.3442
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAO Random Forest: Starting GridSearchCV fit...
       ‚úÖ DOMO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=181.7099 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DOMO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DOMO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=116.1036 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DOMO XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.4320 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DAO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=68.8801 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAO XGBoost: Starting GridSearchCV fit...
       ‚úÖ AMBR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=111.1179 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 132.5s
    - LSTM: MSE=0.0630
    - TCN: MSE=0.0402
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0402
        ‚Ä¢ LSTM: MSE=0.0630
        ‚Ä¢ XGBoost: MSE=111.1179
        ‚Ä¢ Random Forest: MSE=121.6683
        ‚Ä¢ LightGBM Regressor (CPU): MSE=255.2077
   ‚úÖ AMBR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMBR (TargetReturn): TCN with MSE=0.0402
üêõ DEBUG: AMBR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMBR.
üêõ DEBUG: AMBR - Moving model to CPU before return...
üêõ DEBUG [22:33:32.337]: AMBR - Returning result metadata...
üêõ DEBUG [22:33:32.337]: Main received result for AMBR
üêõ DEBUG: Training progress: 96/959 done
üêõ DEBUG: train_worker started for FOA
  ‚öôÔ∏è Training models for FOA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - FOA: Initiating feature extraction for training.
  [DIAGNOSTIC] FOA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FOA: rows after features available: 126
üéØ FOA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FOA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FOA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FOA: Training LSTM (50 epochs)...
      ‚è≥ FOA LSTM: Epoch 10/50 (20%)
      ‚è≥ FOA LSTM: Epoch 20/50 (40%)
      ‚è≥ FOA LSTM: Epoch 30/50 (60%)
      ‚è≥ FOA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.073395
         RMSE: 0.270914
         R¬≤ Score: -1.1371 (Poor - 113.7% variance explained)
      üîπ FOA: Training TCN (50 epochs)...
      ‚è≥ FOA TCN: Epoch 10/50 (20%)
      ‚è≥ FOA TCN: Epoch 20/50 (40%)
      ‚è≥ FOA TCN: Epoch 30/50 (60%)
      ‚è≥ FOA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.042332
         RMSE: 0.205747
         R¬≤ Score: -0.2326
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FOA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FOA Random Forest: Starting GridSearchCV fit...
       ‚úÖ COIN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=114.4540 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.1s
    - LSTM: MSE=0.7753
    - TCN: MSE=0.4520
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4520
        ‚Ä¢ LSTM: MSE=0.7753
        ‚Ä¢ Random Forest: MSE=106.5060
        ‚Ä¢ XGBoost: MSE=114.4540
        ‚Ä¢ LightGBM Regressor (CPU): MSE=129.0368
   ‚úÖ COIN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COIN (TargetReturn): TCN with MSE=0.4520
üêõ DEBUG: COIN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COIN.
üêõ DEBUG: COIN - Moving model to CPU before return...
üêõ DEBUG [22:33:37.992]: COIN - Returning result metadata...
üêõ DEBUG [22:33:37.992]: Main received result for COIN
üêõ DEBUG: train_worker started for ALNT
  ‚öôÔ∏è Training models for ALNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - ALNT: Initiating feature extraction for training.
  [DIAGNOSTIC] ALNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ALNT: rows after features available: 126
üéØ ALNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ALNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ALNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ALNT: Training LSTM (50 epochs)...
       ‚úÖ FOA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.5715 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FOA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ALNT LSTM: Epoch 10/50 (20%)
       ‚úÖ FOA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=78.6751 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FOA XGBoost: Starting GridSearchCV fit...
      ‚è≥ ALNT LSTM: Epoch 20/50 (40%)
      ‚è≥ ALNT LSTM: Epoch 30/50 (60%)
      ‚è≥ ALNT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.638892
         RMSE: 0.799307
         R¬≤ Score: -0.7827 (Poor - 78.3% variance explained)
      üîπ ALNT: Training TCN (50 epochs)...
      ‚è≥ ALNT TCN: Epoch 10/50 (20%)
      ‚è≥ ALNT TCN: Epoch 20/50 (40%)
      ‚è≥ ALNT TCN: Epoch 30/50 (60%)
      ‚è≥ ALNT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.635374
         RMSE: 0.797103
         R¬≤ Score: -0.7728
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ALNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ALNT Random Forest: Starting GridSearchCV fit...
       ‚úÖ GOGO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=266.0607 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.0s
    - LSTM: MSE=0.4509
    - TCN: MSE=0.4238
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4238
        ‚Ä¢ LSTM: MSE=0.4509
        ‚Ä¢ LightGBM Regressor (CPU): MSE=197.4971
        ‚Ä¢ Random Forest: MSE=228.4207
        ‚Ä¢ XGBoost: MSE=266.0607
   ‚úÖ GOGO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GOGO (TargetReturn): TCN with MSE=0.4238
üêõ DEBUG: GOGO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GOGO.
üêõ DEBUG: GOGO - Moving model to CPU before return...
üêõ DEBUG [22:33:42.786]: GOGO - Returning result metadata...
üêõ DEBUG [22:33:42.786]: Main received result for GOGOüêõ DEBUG: train_worker started for IREN

  ‚öôÔ∏è Training models for IREN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - IREN: Initiating feature extraction for training.
  [DIAGNOSTIC] IREN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IREN: rows after features available: 126
üéØ IREN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IREN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IREN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IREN: Training LSTM (50 epochs)...
      ‚è≥ IREN LSTM: Epoch 10/50 (20%)
      ‚è≥ IREN LSTM: Epoch 20/50 (40%)
       ‚úÖ ALNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=50.5883 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ALNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IREN LSTM: Epoch 30/50 (60%)
      ‚è≥ IREN LSTM: Epoch 40/50 (80%)
       ‚úÖ ALNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=51.7665 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ALNT XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.865612
         RMSE: 0.930383
         R¬≤ Score: -0.9671 (Poor - 96.7% variance explained)
      üîπ IREN: Training TCN (50 epochs)...
      ‚è≥ IREN TCN: Epoch 10/50 (20%)
      ‚è≥ IREN TCN: Epoch 20/50 (40%)
      ‚è≥ IREN TCN: Epoch 30/50 (60%)
       ‚úÖ SSRM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=40.4561 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 127.7s
    - LSTM: MSE=0.1326
    - TCN: MSE=0.0482
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0482
        ‚Ä¢ LSTM: MSE=0.1326
        ‚Ä¢ XGBoost: MSE=40.4561
        ‚Ä¢ Random Forest: MSE=45.2570
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.5250
   ‚úÖ SSRM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SSRM (TargetReturn): TCN with MSE=0.0482
üêõ DEBUG: SSRM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SSRM.
üêõ DEBUG: SSRM - Moving model to CPU before return...
üêõ DEBUG [22:33:45.585]: SSRM - Returning result metadata...
üêõ DEBUG: train_worker started for UPST
üêõ DEBUG [22:33:45.586]: Main received result for SSRM
      ‚è≥ IREN TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for UPST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - UPST: Initiating feature extraction for training.
  [DIAGNOSTIC] UPST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UPST: rows after features available: 126
üéØ UPST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UPST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UPST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UPST: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.700742
         RMSE: 0.837103
         R¬≤ Score: -0.5924
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IREN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IREN Random Forest: Starting GridSearchCV fit...
      ‚è≥ UPST LSTM: Epoch 10/50 (20%)
      ‚è≥ UPST LSTM: Epoch 20/50 (40%)
      ‚è≥ UPST LSTM: Epoch 30/50 (60%)
      ‚è≥ UPST LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.800321
         RMSE: 0.894607
         R¬≤ Score: -1.0830 (Poor - 108.3% variance explained)
      üîπ UPST: Training TCN (50 epochs)...
      ‚è≥ UPST TCN: Epoch 10/50 (20%)
      ‚è≥ UPST TCN: Epoch 20/50 (40%)
      ‚è≥ UPST TCN: Epoch 30/50 (60%)
      ‚è≥ UPST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.449907
         RMSE: 0.670751
         R¬≤ Score: -0.1710
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UPST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UPST Random Forest: Starting GridSearchCV fit...
       ‚úÖ IREN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=177.2274 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IREN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IREN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=329.6014 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IREN XGBoost: Starting GridSearchCV fit...
       ‚úÖ QSI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=149.5309 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 128.1s
    - LSTM: MSE=0.1724
    - TCN: MSE=0.1219
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1219
        ‚Ä¢ LSTM: MSE=0.1724
        ‚Ä¢ Random Forest: MSE=147.2060
        ‚Ä¢ XGBoost: MSE=149.5309
        ‚Ä¢ LightGBM Regressor (CPU): MSE=223.4884
   ‚úÖ QSI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QSI (TargetReturn): TCN with MSE=0.1219
üêõ DEBUG: QSI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QSI.
üêõ DEBUG: QSI - Moving model to CPU before return...
üêõ DEBUG [22:33:50.160]: QSI - Returning result metadata...
üêõ DEBUG: train_worker started for ASTS
üêõ DEBUG [22:33:50.161]: Main received result for QSI
üêõ DEBUG: Training progress: 100/959 done
  ‚öôÔ∏è Training models for ASTS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ASTS: Initiating feature extraction for training.
  [DIAGNOSTIC] ASTS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ASTS: rows after features available: 126
üéØ ASTS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ASTS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ASTS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ASTS: Training LSTM (50 epochs)...
      ‚è≥ ASTS LSTM: Epoch 10/50 (20%)
      ‚è≥ ASTS LSTM: Epoch 20/50 (40%)
      ‚è≥ ASTS LSTM: Epoch 30/50 (60%)
       ‚úÖ UPST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=77.1657 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UPST LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ASTS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.606521
         RMSE: 0.778795
         R¬≤ Score: -0.7897 (Poor - 79.0% variance explained)
      üîπ ASTS: Training TCN (50 epochs)...
       ‚úÖ UPST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=143.0500 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UPST XGBoost: Starting GridSearchCV fit...
      ‚è≥ ASTS TCN: Epoch 10/50 (20%)
      ‚è≥ ASTS TCN: Epoch 20/50 (40%)
      ‚è≥ ASTS TCN: Epoch 30/50 (60%)
      ‚è≥ ASTS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.414365
         RMSE: 0.643712
         R¬≤ Score: -0.2227
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ASTS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ASTS Random Forest: Starting GridSearchCV fit...
       ‚úÖ OSS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=333.3748 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.5s
    - LSTM: MSE=0.6284
    - TCN: MSE=0.5312
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.6 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5312
        ‚Ä¢ LSTM: MSE=0.6284
        ‚Ä¢ LightGBM Regressor (CPU): MSE=193.1494
        ‚Ä¢ Random Forest: MSE=227.7061
        ‚Ä¢ XGBoost: MSE=333.3748
   ‚úÖ OSS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OSS (TargetReturn): TCN with MSE=0.5312
üêõ DEBUG: OSS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OSS.
üêõ DEBUG: OSS - Moving model to CPU before return...
üêõ DEBUG [22:33:55.762]: OSS - Returning result metadata...
üêõ DEBUG: train_worker started for AENT
üêõ DEBUG [22:33:55.763]: Main received result for OSS
  ‚öôÔ∏è Training models for AENT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AENT: Initiating feature extraction for training.
  [DIAGNOSTIC] AENT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AENT: rows after features available: 126
üéØ AENT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AENT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AENT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AENT: Training LSTM (50 epochs)...
      ‚è≥ AENT LSTM: Epoch 10/50 (20%)
       ‚úÖ ASTS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=600.9010 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ASTS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AENT LSTM: Epoch 20/50 (40%)
       ‚úÖ ASTS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=644.3483 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ASTS XGBoost: Starting GridSearchCV fit...
      ‚è≥ AENT LSTM: Epoch 30/50 (60%)
       ‚úÖ GHRS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=332.2626 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.0s
    - LSTM: MSE=0.3606
    - TCN: MSE=0.3439
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.9 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3439
        ‚Ä¢ LSTM: MSE=0.3606
        ‚Ä¢ Random Forest: MSE=294.9734
        ‚Ä¢ LightGBM Regressor (CPU): MSE=299.3136
        ‚Ä¢ XGBoost: MSE=332.2626
   ‚úÖ GHRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GHRS (TargetReturn): TCN with MSE=0.3439
üêõ DEBUG: GHRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GHRS.
üêõ DEBUG: GHRS - Moving model to CPU before return...
üêõ DEBUG [22:33:57.473]: GHRS - Returning result metadata...
üêõ DEBUG: train_worker started for TARK
üêõ DEBUG [22:33:57.475]: Main received result for GHRS
  ‚öôÔ∏è Training models for TARK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - TARK: Initiating feature extraction for training.
  [DIAGNOSTIC] TARK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TARK: rows after features available: 126
üéØ TARK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TARK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TARK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TARK: Training LSTM (50 epochs)...
      ‚è≥ AENT LSTM: Epoch 40/50 (80%)
      ‚è≥ TARK LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.676907
         RMSE: 0.822744
         R¬≤ Score: -1.1169 (Poor - 111.7% variance explained)
      üîπ AENT: Training TCN (50 epochs)...
      ‚è≥ AENT TCN: Epoch 10/50 (20%)
      ‚è≥ AENT TCN: Epoch 20/50 (40%)
      ‚è≥ TARK LSTM: Epoch 20/50 (40%)
      ‚è≥ AENT TCN: Epoch 30/50 (60%)
      ‚è≥ AENT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.621723
         RMSE: 0.788494
         R¬≤ Score: -0.9443
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AENT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AENT Random Forest: Starting GridSearchCV fit...
      ‚è≥ TARK LSTM: Epoch 30/50 (60%)
      ‚è≥ TARK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.763594
         RMSE: 0.873839
         R¬≤ Score: -0.9790 (Poor - 97.9% variance explained)
      üîπ TARK: Training TCN (50 epochs)...
      ‚è≥ TARK TCN: Epoch 10/50 (20%)
      ‚è≥ TARK TCN: Epoch 20/50 (40%)
      ‚è≥ TARK TCN: Epoch 30/50 (60%)
      ‚è≥ TARK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.634230
         RMSE: 0.796385
         R¬≤ Score: -0.6438
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TARK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TARK Random Forest: Starting GridSearchCV fit...
       ‚úÖ EVEX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=94.6701 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 134.1s
    - LSTM: MSE=0.5958
    - TCN: MSE=0.5641
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.8 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5641
        ‚Ä¢ LSTM: MSE=0.5958
        ‚Ä¢ LightGBM Regressor (CPU): MSE=89.7431
        ‚Ä¢ XGBoost: MSE=94.6701
        ‚Ä¢ Random Forest: MSE=129.7007
   ‚úÖ EVEX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EVEX (TargetReturn): TCN with MSE=0.5641
üêõ DEBUG: EVEX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EVEX.
üêõ DEBUG: EVEX - Moving model to CPU before return...
üêõ DEBUG [22:34:00.991]: EVEX - Returning result metadata...
üêõ DEBUG: train_worker started for IHS
  ‚öôÔ∏è Training models for IHS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - IHS: Initiating feature extraction for training.
  [DIAGNOSTIC] IHS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IHS: rows after features available: 126
üéØ IHS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IHS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IHS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IHS: Training LSTM (50 epochs)...
       ‚úÖ LUXE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=89.9574 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 134.4s
    - LSTM: MSE=0.0660
    - TCN: MSE=0.0348
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.5 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0348
        ‚Ä¢ LSTM: MSE=0.0660
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.9149
        ‚Ä¢ Random Forest: MSE=75.9315
        ‚Ä¢ XGBoost: MSE=89.9574
   ‚úÖ LUXE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LUXE (TargetReturn): TCN with MSE=0.0348
üêõ DEBUG: LUXE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LUXE.
üêõ DEBUG: LUXE - Moving model to CPU before return...
üêõ DEBUG [22:34:01.054]: LUXE - Returning result metadata...
üêõ DEBUG [22:34:01.055]: Main received result for LUXEüêõ DEBUG: train_worker started for APEI

üêõ DEBUG [22:34:01.055]: Main received result for EVEX
üêõ DEBUG: Training progress: 104/959 done
  ‚öôÔ∏è Training models for APEI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - APEI: Initiating feature extraction for training.
  [DIAGNOSTIC] APEI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APEI: rows after features available: 126
üéØ APEI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APEI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APEI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APEI: Training LSTM (50 epochs)...
      ‚è≥ APEI LSTM: Epoch 10/50 (20%)
      ‚è≥ IHS LSTM: Epoch 10/50 (20%)
       ‚úÖ AENT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=115.8226 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AENT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ APEI LSTM: Epoch 20/50 (40%)
      ‚è≥ IHS LSTM: Epoch 20/50 (40%)
       ‚úÖ EOSE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=138.8974 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.6s
    - LSTM: MSE=0.1568
    - TCN: MSE=0.0587
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0587
        ‚Ä¢ LSTM: MSE=0.1568
        ‚Ä¢ Random Forest: MSE=122.6434
        ‚Ä¢ XGBoost: MSE=138.8974
        ‚Ä¢ LightGBM Regressor (CPU): MSE=157.8246
   ‚úÖ EOSE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EOSE (TargetReturn): TCN with MSE=0.0587
üêõ DEBUG: EOSE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EOSE.
üêõ DEBUG: EOSE - Moving model to CPU before return...
üêõ DEBUG [22:34:02.304]: EOSE - Returning result metadata...
üêõ DEBUG [22:34:02.305]: Main received result for EOSEüêõ DEBUG: train_worker started for NN

  ‚öôÔ∏è Training models for NN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - NN: Initiating feature extraction for training.
  [DIAGNOSTIC] NN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NN: rows after features available: 126
üéØ NN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NN: Training LSTM (50 epochs)...
      ‚è≥ IHS LSTM: Epoch 30/50 (60%)
      ‚è≥ APEI LSTM: Epoch 30/50 (60%)
       ‚úÖ AENT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=128.4202 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AENT XGBoost: Starting GridSearchCV fit...
      ‚è≥ NN LSTM: Epoch 10/50 (20%)
      ‚è≥ IHS LSTM: Epoch 40/50 (80%)
      ‚è≥ APEI LSTM: Epoch 40/50 (80%)
      ‚è≥ NN LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.326772
         RMSE: 0.571640
         R¬≤ Score: -0.7918 (Poor - 79.2% variance explained)
      üîπ IHS: Training TCN (50 epochs)...
       ‚úÖ TARK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=120.4436 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TARK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.163500
         RMSE: 0.404352
         R¬≤ Score: -0.4443 (Poor - 44.4% variance explained)
      üîπ APEI: Training TCN (50 epochs)...
      ‚è≥ IHS TCN: Epoch 10/50 (20%)
      ‚è≥ IHS TCN: Epoch 20/50 (40%)
      ‚è≥ APEI TCN: Epoch 10/50 (20%)
      ‚è≥ NN LSTM: Epoch 30/50 (60%)
      ‚è≥ IHS TCN: Epoch 30/50 (60%)
      ‚è≥ APEI TCN: Epoch 20/50 (40%)
      ‚è≥ IHS TCN: Epoch 40/50 (80%)
      ‚è≥ APEI TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.215950
         RMSE: 0.464704
         R¬≤ Score: -0.1841
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IHS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IHS Random Forest: Starting GridSearchCV fit...
      ‚è≥ APEI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.123957
         RMSE: 0.352075
         R¬≤ Score: -0.0950
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APEI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APEI Random Forest: Starting GridSearchCV fit...
      ‚è≥ NN LSTM: Epoch 40/50 (80%)
       ‚úÖ TARK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=143.4527 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TARK XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.128613
         RMSE: 0.358627
         R¬≤ Score: -0.8548 (Poor - 85.5% variance explained)
      üîπ NN: Training TCN (50 epochs)...
      ‚è≥ NN TCN: Epoch 10/50 (20%)
      ‚è≥ NN TCN: Epoch 20/50 (40%)
      ‚è≥ NN TCN: Epoch 30/50 (60%)
      ‚è≥ NN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.069940
         RMSE: 0.264462
         R¬≤ Score: -0.0087
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NN Random Forest: Starting GridSearchCV fit...
       ‚úÖ APEI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.4106 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APEI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IHS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=70.1967 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IHS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ APEI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=34.6584 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APEI XGBoost: Starting GridSearchCV fit...
       ‚úÖ IHS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=60.8425 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IHS XGBoost: Starting GridSearchCV fit...
       ‚úÖ NN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=36.8686 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.8019 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NN XGBoost: Starting GridSearchCV fit...
       ‚úÖ MASS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=291.1699 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.4s
    - LSTM: MSE=0.1134
    - TCN: MSE=0.1360
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1134
        ‚Ä¢ TCN: MSE=0.1360
        ‚Ä¢ Random Forest: MSE=268.6309
        ‚Ä¢ XGBoost: MSE=291.1699
        ‚Ä¢ LightGBM Regressor (CPU): MSE=333.3329
   ‚úÖ MASS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MASS (TargetReturn): LSTM with MSE=0.1134
üêõ DEBUG: MASS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MASS.
üêõ DEBUG: MASS - Moving model to CPU before return...
üêõ DEBUG [22:34:57.117]: MASS - Returning result metadata...
üêõ DEBUG [22:34:57.117]: Main received result for MASS
üêõ DEBUG: train_worker started for DOYU
  ‚öôÔ∏è Training models for DOYU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - DOYU: Initiating feature extraction for training.
  [DIAGNOSTIC] DOYU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DOYU: rows after features available: 126
üéØ DOYU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DOYU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DOYU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DOYU: Training LSTM (50 epochs)...
      ‚è≥ DOYU LSTM: Epoch 10/50 (20%)
      ‚è≥ DOYU LSTM: Epoch 20/50 (40%)
       ‚úÖ AVXL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=61.6135 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 122.7s
    - LSTM: MSE=0.0915
    - TCN: MSE=0.0518
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0518
        ‚Ä¢ LSTM: MSE=0.0915
        ‚Ä¢ Random Forest: MSE=56.4586
        ‚Ä¢ XGBoost: MSE=61.6135
        ‚Ä¢ LightGBM Regressor (CPU): MSE=86.3752
   ‚úÖ AVXL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AVXL (TargetReturn): TCN with MSE=0.0518
üêõ DEBUG: AVXL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AVXL.
üêõ DEBUG: AVXL - Moving model to CPU before return...
üêõ DEBUG [22:34:58.466]: AVXL - Returning result metadata...
üêõ DEBUG [22:34:58.467]: Main received result for AVXL
üêõ DEBUG: train_worker started for APYX
  ‚öôÔ∏è Training models for APYX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - APYX: Initiating feature extraction for training.
  [DIAGNOSTIC] APYX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APYX: rows after features available: 126
üéØ APYX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APYX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APYX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APYX: Training LSTM (50 epochs)...
      ‚è≥ DOYU LSTM: Epoch 30/50 (60%)
      ‚è≥ APYX LSTM: Epoch 10/50 (20%)
      ‚è≥ DOYU LSTM: Epoch 40/50 (80%)
      ‚è≥ APYX LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.094498
         RMSE: 0.307406
         R¬≤ Score: -0.4990 (Poor - 49.9% variance explained)
      üîπ DOYU: Training TCN (50 epochs)...
      ‚è≥ DOYU TCN: Epoch 10/50 (20%)
      ‚è≥ DOYU TCN: Epoch 20/50 (40%)
      ‚è≥ DOYU TCN: Epoch 30/50 (60%)
      ‚è≥ DOYU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.069069
         RMSE: 0.262810
         R¬≤ Score: -0.0957
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DOYU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DOYU Random Forest: Starting GridSearchCV fit...
      ‚è≥ APYX LSTM: Epoch 30/50 (60%)
      ‚è≥ APYX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.534443
         RMSE: 0.731056
         R¬≤ Score: -0.9239 (Poor - 92.4% variance explained)
      üîπ APYX: Training TCN (50 epochs)...
      ‚è≥ APYX TCN: Epoch 10/50 (20%)
      ‚è≥ APYX TCN: Epoch 20/50 (40%)
      ‚è≥ APYX TCN: Epoch 30/50 (60%)
      ‚è≥ APYX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.407495
         RMSE: 0.638353
         R¬≤ Score: -0.4669
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APYX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APYX Random Forest: Starting GridSearchCV fit...
       ‚úÖ DOYU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=59.1155 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DOYU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DOYU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=59.6077 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DOYU XGBoost: Starting GridSearchCV fit...
       ‚úÖ TESL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=63.8557 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.7s
    - LSTM: MSE=0.1897
    - TCN: MSE=0.1551
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1551
        ‚Ä¢ LSTM: MSE=0.1897
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.4982
        ‚Ä¢ XGBoost: MSE=63.8557
        ‚Ä¢ Random Forest: MSE=66.9958
   ‚úÖ TESL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TESL (TargetReturn): TCN with MSE=0.1551
üêõ DEBUG: TESL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TESL.
üêõ DEBUG: TESL - Moving model to CPU before return...
üêõ DEBUG [22:35:03.887]: TESL - Returning result metadata...
üêõ DEBUG [22:35:03.887]: Main received result for TESL
üêõ DEBUG: Training progress: 108/959 done
üêõ DEBUG: train_worker started for LFMD
  ‚öôÔ∏è Training models for LFMD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - LFMD: Initiating feature extraction for training.
  [DIAGNOSTIC] LFMD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LFMD: rows after features available: 126
üéØ LFMD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LFMD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LFMD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LFMD: Training LSTM (50 epochs)...
      ‚è≥ LFMD LSTM: Epoch 10/50 (20%)
      ‚è≥ LFMD LSTM: Epoch 20/50 (40%)
       ‚úÖ APYX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=168.1545 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APYX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LFMD LSTM: Epoch 30/50 (60%)
      ‚è≥ LFMD LSTM: Epoch 40/50 (80%)
       ‚úÖ APYX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=175.1273 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APYX XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.475562
         RMSE: 0.689610
         R¬≤ Score: -0.8738 (Poor - 87.4% variance explained)
      üîπ LFMD: Training TCN (50 epochs)...
      ‚è≥ LFMD TCN: Epoch 10/50 (20%)
      ‚è≥ LFMD TCN: Epoch 20/50 (40%)
      ‚è≥ LFMD TCN: Epoch 30/50 (60%)
      ‚è≥ LFMD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.288340
         RMSE: 0.536973
         R¬≤ Score: -0.1361
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LFMD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LFMD Random Forest: Starting GridSearchCV fit...
       ‚úÖ LFMD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=538.3748 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LFMD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LFMD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=494.7103 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LFMD XGBoost: Starting GridSearchCV fit...
       ‚úÖ DOMO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=202.5407 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.7s
    - LSTM: MSE=0.5522
    - TCN: MSE=0.3451
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3451
        ‚Ä¢ LSTM: MSE=0.5522
        ‚Ä¢ LightGBM Regressor (CPU): MSE=116.1036
        ‚Ä¢ Random Forest: MSE=181.7099
        ‚Ä¢ XGBoost: MSE=202.5407
   ‚úÖ DOMO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DOMO (TargetReturn): TCN with MSE=0.3451
üêõ DEBUG: DOMO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DOMO.
üêõ DEBUG: DOMO - Moving model to CPU before return...
üêõ DEBUG [22:35:27.241]: DOMO - Returning result metadata...
üêõ DEBUG [22:35:27.241]: Main received result for DOMO
üêõ DEBUG: train_worker started for CTMX
  ‚öôÔ∏è Training models for CTMX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - CTMX: Initiating feature extraction for training.
  [DIAGNOSTIC] CTMX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTMX: rows after features available: 126
üéØ CTMX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTMX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTMX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTMX: Training LSTM (50 epochs)...
      ‚è≥ CTMX LSTM: Epoch 10/50 (20%)
      ‚è≥ CTMX LSTM: Epoch 20/50 (40%)
      ‚è≥ CTMX LSTM: Epoch 30/50 (60%)
      ‚è≥ CTMX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.374884
         RMSE: 0.612277
         R¬≤ Score: -0.5442 (Poor - 54.4% variance explained)
      üîπ CTMX: Training TCN (50 epochs)...
      ‚è≥ CTMX TCN: Epoch 10/50 (20%)
      ‚è≥ CTMX TCN: Epoch 20/50 (40%)
      ‚è≥ CTMX TCN: Epoch 30/50 (60%)
      ‚è≥ CTMX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.307292
         RMSE: 0.554339
         R¬≤ Score: -0.2658
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTMX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTMX Random Forest: Starting GridSearchCV fit...
       ‚úÖ DAO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=76.6959 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.6s
    - LSTM: MSE=0.0688
    - TCN: MSE=0.0308
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0308
        ‚Ä¢ LSTM: MSE=0.0688
        ‚Ä¢ Random Forest: MSE=60.4320
        ‚Ä¢ LightGBM Regressor (CPU): MSE=68.8801
        ‚Ä¢ XGBoost: MSE=76.6959
   ‚úÖ DAO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAO (TargetReturn): TCN with MSE=0.0308
üêõ DEBUG: DAO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAO.
üêõ DEBUG: DAO - Moving model to CPU before return...
üêõ DEBUG [22:35:32.195]: DAO - Returning result metadata...
üêõ DEBUG [22:35:32.196]: Main received result for DAO
üêõ DEBUG: train_worker started for GEO
  ‚öôÔ∏è Training models for GEO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - GEO: Initiating feature extraction for training.
  [DIAGNOSTIC] GEO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GEO: rows after features available: 126
üéØ GEO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GEO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GEO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GEO: Training LSTM (50 epochs)...
      ‚è≥ GEO LSTM: Epoch 10/50 (20%)
      ‚è≥ GEO LSTM: Epoch 20/50 (40%)
       ‚úÖ CTMX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=608.0634 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTMX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GEO LSTM: Epoch 30/50 (60%)
       ‚úÖ CTMX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=872.3131 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTMX XGBoost: Starting GridSearchCV fit...
      ‚è≥ GEO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.047197
         RMSE: 0.217249
         R¬≤ Score: -1.1340 (Poor - 113.4% variance explained)
      üîπ GEO: Training TCN (50 epochs)...
      ‚è≥ GEO TCN: Epoch 10/50 (20%)
      ‚è≥ GEO TCN: Epoch 20/50 (40%)
      ‚è≥ GEO TCN: Epoch 30/50 (60%)
      ‚è≥ GEO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.025726
         RMSE: 0.160392
         R¬≤ Score: -0.1632
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GEO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GEO Random Forest: Starting GridSearchCV fit...
       ‚úÖ GEO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=52.9240 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GEO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GEO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=68.2655 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GEO XGBoost: Starting GridSearchCV fit...
       ‚úÖ FOA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.3363 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.4s
    - LSTM: MSE=0.0734
    - TCN: MSE=0.0423
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0423
        ‚Ä¢ LSTM: MSE=0.0734
        ‚Ä¢ Random Forest: MSE=39.5715
        ‚Ä¢ XGBoost: MSE=53.3363
        ‚Ä¢ LightGBM Regressor (CPU): MSE=78.6751
   ‚úÖ FOA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FOA (TargetReturn): TCN with MSE=0.0423
üêõ DEBUG: FOA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FOA.
üêõ DEBUG: FOA - Moving model to CPU before return...
üêõ DEBUG [22:35:43.287]: FOA - Returning result metadata...
üêõ DEBUG [22:35:43.287]: Main received result for FOA
üêõ DEBUG: train_worker started for DXPE
  ‚öôÔ∏è Training models for DXPE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - DXPE: Initiating feature extraction for training.
  [DIAGNOSTIC] DXPE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DXPE: rows after features available: 126
üéØ DXPE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DXPE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DXPE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DXPE: Training LSTM (50 epochs)...
      ‚è≥ DXPE LSTM: Epoch 10/50 (20%)
      ‚è≥ DXPE LSTM: Epoch 20/50 (40%)
      ‚è≥ DXPE LSTM: Epoch 30/50 (60%)
      ‚è≥ DXPE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.080564
         RMSE: 0.283838
         R¬≤ Score: -1.2293 (Poor - 122.9% variance explained)
      üîπ DXPE: Training TCN (50 epochs)...
      ‚è≥ DXPE TCN: Epoch 10/50 (20%)
      ‚è≥ DXPE TCN: Epoch 20/50 (40%)
      ‚è≥ DXPE TCN: Epoch 30/50 (60%)
      ‚è≥ DXPE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.043735
         RMSE: 0.209129
         R¬≤ Score: -0.2102
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DXPE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DXPE Random Forest: Starting GridSearchCV fit...
       ‚úÖ DXPE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.4045 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DXPE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DXPE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.8369 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DXPE XGBoost: Starting GridSearchCV fit...
       ‚úÖ ALNT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=69.5207 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.5s
    - LSTM: MSE=0.6389
    - TCN: MSE=0.6354
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6354
        ‚Ä¢ LSTM: MSE=0.6389
        ‚Ä¢ Random Forest: MSE=50.5883
        ‚Ä¢ LightGBM Regressor (CPU): MSE=51.7665
        ‚Ä¢ XGBoost: MSE=69.5207
   ‚úÖ ALNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ALNT (TargetReturn): TCN with MSE=0.6354
üêõ DEBUG: ALNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ALNT.
üêõ DEBUG: ALNT - Moving model to CPU before return...
üêõ DEBUG [22:35:50.242]: ALNT - Returning result metadata...
üêõ DEBUG [22:35:50.242]: Main received result for ALNT
üêõ DEBUG: Training progress: 112/959 done
üêõ DEBUG: train_worker started for KEN
  ‚öôÔ∏è Training models for KEN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - KEN: Initiating feature extraction for training.
  [DIAGNOSTIC] KEN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KEN: rows after features available: 126
üéØ KEN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KEN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KEN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KEN: Training LSTM (50 epochs)...
      ‚è≥ KEN LSTM: Epoch 10/50 (20%)
      ‚è≥ KEN LSTM: Epoch 20/50 (40%)
      ‚è≥ KEN LSTM: Epoch 30/50 (60%)
      ‚è≥ KEN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.454179
         RMSE: 0.673928
         R¬≤ Score: -0.7731 (Poor - 77.3% variance explained)
      üîπ KEN: Training TCN (50 epochs)...
      ‚è≥ KEN TCN: Epoch 10/50 (20%)
      ‚è≥ KEN TCN: Epoch 20/50 (40%)
      ‚è≥ KEN TCN: Epoch 30/50 (60%)
      ‚è≥ KEN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.434668
         RMSE: 0.659293
         R¬≤ Score: -0.6969
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KEN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KEN Random Forest: Starting GridSearchCV fit...
       ‚úÖ IREN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=297.2981 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.6s
    - LSTM: MSE=0.8656
    - TCN: MSE=0.7007
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7007
        ‚Ä¢ LSTM: MSE=0.8656
        ‚Ä¢ Random Forest: MSE=177.2274
        ‚Ä¢ XGBoost: MSE=297.2981
        ‚Ä¢ LightGBM Regressor (CPU): MSE=329.6014
   ‚úÖ IREN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IREN (TargetReturn): TCN with MSE=0.7007
üêõ DEBUG: IREN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IREN.
üêõ DEBUG: IREN - Moving model to CPU before return...
üêõ DEBUG [22:35:55.544]: IREN - Returning result metadata...
üêõ DEBUG [22:35:55.545]: Main received result for IREN
üêõ DEBUG: train_worker started for VNET
  ‚öôÔ∏è Training models for VNET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - VNET: Initiating feature extraction for training.
  [DIAGNOSTIC] VNET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VNET: rows after features available: 126
üéØ VNET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VNET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VNET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VNET: Training LSTM (50 epochs)...
       ‚úÖ KEN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.0442 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KEN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ VNET LSTM: Epoch 10/50 (20%)
      ‚è≥ VNET LSTM: Epoch 20/50 (40%)
       ‚úÖ KEN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=38.8774 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KEN XGBoost: Starting GridSearchCV fit...
      ‚è≥ VNET LSTM: Epoch 30/50 (60%)
      ‚è≥ VNET LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.096907
         RMSE: 0.311300
         R¬≤ Score: -0.6846 (Poor - 68.5% variance explained)
      üîπ VNET: Training TCN (50 epochs)...
      ‚è≥ VNET TCN: Epoch 10/50 (20%)
      ‚è≥ VNET TCN: Epoch 20/50 (40%)
      ‚è≥ VNET TCN: Epoch 30/50 (60%)
      ‚è≥ VNET TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.062975
         RMSE: 0.250948
         R¬≤ Score: -0.0947
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VNET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VNET Random Forest: Starting GridSearchCV fit...
       ‚úÖ UPST XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=72.6990 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.6s
    - LSTM: MSE=0.8003
    - TCN: MSE=0.4499
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4499
        ‚Ä¢ LSTM: MSE=0.8003
        ‚Ä¢ XGBoost: MSE=72.6990
        ‚Ä¢ Random Forest: MSE=77.1657
        ‚Ä¢ LightGBM Regressor (CPU): MSE=143.0500
   ‚úÖ UPST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UPST (TargetReturn): TCN with MSE=0.4499
üêõ DEBUG: UPST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UPST.
üêõ DEBUG: UPST - Moving model to CPU before return...
üêõ DEBUG [22:36:01.401]: UPST - Returning result metadata...
üêõ DEBUG [22:36:01.401]: Main received result for UPST
üêõ DEBUG: train_worker started for UI
  ‚öôÔ∏è Training models for UI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - UI: Initiating feature extraction for training.
  [DIAGNOSTIC] UI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UI: rows after features available: 126
üéØ UI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UI: Training LSTM (50 epochs)...
       ‚úÖ VNET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=626.0588 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VNET LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ UI LSTM: Epoch 10/50 (20%)
       ‚úÖ VNET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=284.6648 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VNET XGBoost: Starting GridSearchCV fit...
      ‚è≥ UI LSTM: Epoch 20/50 (40%)
      ‚è≥ UI LSTM: Epoch 30/50 (60%)
      ‚è≥ UI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.328406
         RMSE: 0.573067
         R¬≤ Score: -0.8443 (Poor - 84.4% variance explained)
      üîπ UI: Training TCN (50 epochs)...
      ‚è≥ UI TCN: Epoch 10/50 (20%)
      ‚è≥ UI TCN: Epoch 20/50 (40%)
      ‚è≥ UI TCN: Epoch 30/50 (60%)
      ‚è≥ UI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.249057
         RMSE: 0.499057
         R¬≤ Score: -0.3987
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UI Random Forest: Starting GridSearchCV fit...
       ‚úÖ ASTS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=505.8598 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.8s
    - LSTM: MSE=0.6065
    - TCN: MSE=0.4144
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4144
        ‚Ä¢ LSTM: MSE=0.6065
        ‚Ä¢ XGBoost: MSE=505.8598
        ‚Ä¢ Random Forest: MSE=600.9010
        ‚Ä¢ LightGBM Regressor (CPU): MSE=644.3483
   ‚úÖ ASTS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ASTS (TargetReturn): TCN with MSE=0.4144
üêõ DEBUG: ASTS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ASTS.
üêõ DEBUG: ASTS - Moving model to CPU before return...
üêõ DEBUG [22:36:05.801]: ASTS - Returning result metadata...
üêõ DEBUG [22:36:05.801]: Main received result for ASTS
üêõ DEBUG: train_worker started for TWLO
  ‚öôÔ∏è Training models for TWLO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - TWLO: Initiating feature extraction for training.
  [DIAGNOSTIC] TWLO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TWLO: rows after features available: 126
üéØ TWLO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TWLO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TWLO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TWLO: Training LSTM (50 epochs)...
      ‚è≥ TWLO LSTM: Epoch 10/50 (20%)
      ‚è≥ TWLO LSTM: Epoch 20/50 (40%)
       ‚úÖ UI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.0052 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TWLO LSTM: Epoch 30/50 (60%)
       ‚úÖ UI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=46.5870 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UI XGBoost: Starting GridSearchCV fit...
      ‚è≥ TWLO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.271562
         RMSE: 0.521116
         R¬≤ Score: -0.9220 (Poor - 92.2% variance explained)
      üîπ TWLO: Training TCN (50 epochs)...
      ‚è≥ TWLO TCN: Epoch 10/50 (20%)
      ‚è≥ TWLO TCN: Epoch 20/50 (40%)
      ‚è≥ TWLO TCN: Epoch 30/50 (60%)
      ‚è≥ TWLO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.245293
         RMSE: 0.495271
         R¬≤ Score: -0.7361
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TWLO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TWLO Random Forest: Starting GridSearchCV fit...
       ‚úÖ TWLO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=33.1603 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TWLO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TARK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=124.6135 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.6s
    - LSTM: MSE=0.7636
    - TCN: MSE=0.6342
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6342
        ‚Ä¢ LSTM: MSE=0.7636
        ‚Ä¢ Random Forest: MSE=120.4436
        ‚Ä¢ XGBoost: MSE=124.6135
        ‚Ä¢ LightGBM Regressor (CPU): MSE=143.4527
   ‚úÖ TARK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TARK (TargetReturn): TCN with MSE=0.6342
üêõ DEBUG: TARK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TARK.
üêõ DEBUG: TARK - Moving model to CPU before return...
üêõ DEBUG [22:36:12.172]: TARK - Returning result metadata...
üêõ DEBUG: train_worker started for VVOS
  ‚öôÔ∏è Training models for VVOS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - VVOS: Initiating feature extraction for training.
  [DIAGNOSTIC] VVOS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VVOS: rows after features available: 126
üéØ VVOS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VVOS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VVOS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VVOS: Training LSTM (50 epochs)...
       ‚úÖ TWLO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=45.7796 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TWLO XGBoost: Starting GridSearchCV fit...
       ‚úÖ AENT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=139.7586 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.0s
    - LSTM: MSE=0.6769
    - TCN: MSE=0.6217
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6217
        ‚Ä¢ LSTM: MSE=0.6769
        ‚Ä¢ Random Forest: MSE=115.8226
        ‚Ä¢ LightGBM Regressor (CPU): MSE=128.4202
        ‚Ä¢ XGBoost: MSE=139.7586
   ‚úÖ AENT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AENT (TargetReturn): TCN with MSE=0.6217
üêõ DEBUG: AENT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AENT.
üêõ DEBUG: AENT - Moving model to CPU before return...
üêõ DEBUG [22:36:12.749]: AENT - Returning result metadata...
üêõ DEBUG [22:36:12.749]: Main received result for AENT
üêõ DEBUG: Training progress: 116/959 done
üêõ DEBUG [22:36:12.749]: Main received result for TARKüêõ DEBUG: train_worker started for ATOM

      ‚è≥ VVOS LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for ATOM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ATOM: Initiating feature extraction for training.
  [DIAGNOSTIC] ATOM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATOM: rows after features available: 126
üéØ ATOM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATOM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATOM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATOM: Training LSTM (50 epochs)...
      ‚è≥ VVOS LSTM: Epoch 20/50 (40%)
      ‚è≥ ATOM LSTM: Epoch 10/50 (20%)
      ‚è≥ ATOM LSTM: Epoch 20/50 (40%)
      ‚è≥ VVOS LSTM: Epoch 30/50 (60%)
      ‚è≥ ATOM LSTM: Epoch 30/50 (60%)
      ‚è≥ VVOS LSTM: Epoch 40/50 (80%)
      ‚è≥ ATOM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.594050
         RMSE: 0.770746
         R¬≤ Score: -0.9791 (Poor - 97.9% variance explained)
      üîπ VVOS: Training TCN (50 epochs)...
      ‚è≥ VVOS TCN: Epoch 10/50 (20%)
      ‚è≥ VVOS TCN: Epoch 20/50 (40%)
      ‚è≥ VVOS TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.072650
         RMSE: 0.269536
         R¬≤ Score: -1.1205 (Poor - 112.1% variance explained)
      üîπ ATOM: Training TCN (50 epochs)...
      ‚è≥ VVOS TCN: Epoch 40/50 (80%)
      ‚è≥ ATOM TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.373547
         RMSE: 0.611185
         R¬≤ Score: -0.2445
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VVOS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VVOS Random Forest: Starting GridSearchCV fit...
      ‚è≥ ATOM TCN: Epoch 20/50 (40%)
      ‚è≥ ATOM TCN: Epoch 30/50 (60%)
      ‚è≥ ATOM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.037091
         RMSE: 0.192590
         R¬≤ Score: -0.0826
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATOM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATOM Random Forest: Starting GridSearchCV fit...
       ‚úÖ APEI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=34.7087 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.7s
    - LSTM: MSE=0.1635
    - TCN: MSE=0.1240
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1240
        ‚Ä¢ LSTM: MSE=0.1635
        ‚Ä¢ Random Forest: MSE=29.4106
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.6584
        ‚Ä¢ XGBoost: MSE=34.7087
   ‚úÖ APEI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APEI (TargetReturn): TCN with MSE=0.1240
üêõ DEBUG: APEI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APEI.
üêõ DEBUG: APEI - Moving model to CPU before return...
üêõ DEBUG [22:36:17.737]: APEI - Returning result metadata...
üêõ DEBUG: train_worker started for MGTX
  ‚öôÔ∏è Training models for MGTX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - MGTX: Initiating feature extraction for training.
  [DIAGNOSTIC] MGTX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MGTX: rows after features available: 126
üéØ MGTX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MGTX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MGTX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MGTX: Training LSTM (50 epochs)...
      ‚è≥ MGTX LSTM: Epoch 10/50 (20%)
       ‚úÖ IHS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=69.6690 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.4s
    - LSTM: MSE=0.3268
    - TCN: MSE=0.2159
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2159
        ‚Ä¢ LSTM: MSE=0.3268
        ‚Ä¢ LightGBM Regressor (CPU): MSE=60.8425
        ‚Ä¢ XGBoost: MSE=69.6690
        ‚Ä¢ Random Forest: MSE=70.1967
   ‚úÖ IHS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IHS (TargetReturn): TCN with MSE=0.2159
üêõ DEBUG: IHS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IHS.
üêõ DEBUG: IHS - Moving model to CPU before return...
üêõ DEBUG [22:36:18.566]: IHS - Returning result metadata...
üêõ DEBUG [22:36:18.569]: Main received result for IHS
üêõ DEBUG [22:36:18.569]: Main received result for APEI
üêõ DEBUG: train_worker started for RCL
  ‚öôÔ∏è Training models for RCL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - RCL: Initiating feature extraction for training.
  [DIAGNOSTIC] RCL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RCL: rows after features available: 126
üéØ RCL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RCL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RCL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RCL: Training LSTM (50 epochs)...
       ‚úÖ VVOS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=146.4340 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VVOS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MGTX LSTM: Epoch 20/50 (40%)
       ‚úÖ NN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=45.4366 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.1s
    - LSTM: MSE=0.1286
    - TCN: MSE=0.0699
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0699
        ‚Ä¢ LSTM: MSE=0.1286
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.8019
        ‚Ä¢ Random Forest: MSE=36.8686
        ‚Ä¢ XGBoost: MSE=45.4366
   ‚úÖ NN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NN (TargetReturn): TCN with MSE=0.0699
üêõ DEBUG: NN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NN.
üêõ DEBUG: NN - Moving model to CPU before return...
üêõ DEBUG [22:36:19.064]: NN - Returning result metadata...
üêõ DEBUG [22:36:19.065]: Main received result for NN
üêõ DEBUG: train_worker started for EAT
üêõ DEBUG: Training progress: 120/959 done
       ‚úÖ ATOM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=160.3353 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATOM LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for EAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - EAT: Initiating feature extraction for training.
  [DIAGNOSTIC] EAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EAT: rows after features available: 126
üéØ EAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EAT: Training LSTM (50 epochs)...
      ‚è≥ RCL LSTM: Epoch 10/50 (20%)
      ‚è≥ MGTX LSTM: Epoch 30/50 (60%)
       ‚úÖ VVOS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=229.0687 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VVOS XGBoost: Starting GridSearchCV fit...
      ‚è≥ EAT LSTM: Epoch 10/50 (20%)
      ‚è≥ RCL LSTM: Epoch 20/50 (40%)
      ‚è≥ MGTX LSTM: Epoch 40/50 (80%)
       ‚úÖ ATOM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=223.0708 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATOM XGBoost: Starting GridSearchCV fit...
      ‚è≥ EAT LSTM: Epoch 20/50 (40%)
      ‚è≥ RCL LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.480145
         RMSE: 0.692925
         R¬≤ Score: -0.4727 (Poor - 47.3% variance explained)
      üîπ MGTX: Training TCN (50 epochs)...
      ‚è≥ MGTX TCN: Epoch 10/50 (20%)
      ‚è≥ MGTX TCN: Epoch 20/50 (40%)
      ‚è≥ MGTX TCN: Epoch 30/50 (60%)
      ‚è≥ EAT LSTM: Epoch 30/50 (60%)
      ‚è≥ MGTX TCN: Epoch 40/50 (80%)
      ‚è≥ RCL LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.503293
         RMSE: 0.709431
         R¬≤ Score: -0.5437
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MGTX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MGTX Random Forest: Starting GridSearchCV fit...
      ‚è≥ EAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.597099
         RMSE: 0.772722
         R¬≤ Score: -0.5003 (Poor - 50.0% variance explained)
      üîπ RCL: Training TCN (50 epochs)...
      ‚è≥ RCL TCN: Epoch 10/50 (20%)
      ‚è≥ RCL TCN: Epoch 20/50 (40%)
      ‚è≥ RCL TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.165209
         RMSE: 0.406460
         R¬≤ Score: -0.6816 (Poor - 68.2% variance explained)
      üîπ EAT: Training TCN (50 epochs)...
      ‚è≥ RCL TCN: Epoch 40/50 (80%)
      ‚è≥ EAT TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.702968
         RMSE: 0.838432
         R¬≤ Score: -0.7663
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RCL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RCL Random Forest: Starting GridSearchCV fit...
      ‚è≥ EAT TCN: Epoch 20/50 (40%)
      ‚è≥ EAT TCN: Epoch 30/50 (60%)
      ‚è≥ EAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.145073
         RMSE: 0.380885
         R¬≤ Score: -0.4766
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ MGTX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=61.8735 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MGTX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MGTX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=44.2523 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MGTX XGBoost: Starting GridSearchCV fit...
       ‚úÖ RCL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=46.9392 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RCL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.8681 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RCL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.4484 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RCL XGBoost: Starting GridSearchCV fit...
       ‚úÖ EAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.1166 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EAT XGBoost: Starting GridSearchCV fit...
       ‚úÖ DOYU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=66.7203 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 123.0s
    - LSTM: MSE=0.0945
    - TCN: MSE=0.0691
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0691
        ‚Ä¢ LSTM: MSE=0.0945
        ‚Ä¢ Random Forest: MSE=59.1155
        ‚Ä¢ LightGBM Regressor (CPU): MSE=59.6077
        ‚Ä¢ XGBoost: MSE=66.7203
   ‚úÖ DOYU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DOYU (TargetReturn): TCN with MSE=0.0691
üêõ DEBUG: DOYU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DOYU.
üêõ DEBUG: DOYU - Moving model to CPU before return...
üêõ DEBUG [22:37:06.741]: DOYU - Returning result metadata...
üêõ DEBUG: train_worker started for WLDN
üêõ DEBUG [22:37:06.742]: Main received result for DOYU
  ‚öôÔ∏è Training models for WLDN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - WLDN: Initiating feature extraction for training.
  [DIAGNOSTIC] WLDN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WLDN: rows after features available: 126
üéØ WLDN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WLDN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WLDN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WLDN: Training LSTM (50 epochs)...
      ‚è≥ WLDN LSTM: Epoch 10/50 (20%)
      ‚è≥ WLDN LSTM: Epoch 20/50 (40%)
      ‚è≥ WLDN LSTM: Epoch 30/50 (60%)
       ‚úÖ APYX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=129.8138 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 122.1s
    - LSTM: MSE=0.5344
    - TCN: MSE=0.4075
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4075
        ‚Ä¢ LSTM: MSE=0.5344
        ‚Ä¢ XGBoost: MSE=129.8138
        ‚Ä¢ Random Forest: MSE=168.1545
        ‚Ä¢ LightGBM Regressor (CPU): MSE=175.1273
   ‚úÖ APYX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APYX (TargetReturn): TCN with MSE=0.4075
üêõ DEBUG: APYX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APYX.
üêõ DEBUG: APYX - Moving model to CPU before return...
üêõ DEBUG [22:37:08.137]: APYX - Returning result metadata...
üêõ DEBUG [22:37:08.138]: Main received result for APYX
üêõ DEBUG: train_worker started for MESO
  ‚öôÔ∏è Training models for MESO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - MESO: Initiating feature extraction for training.
  [DIAGNOSTIC] MESO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MESO: rows after features available: 126
üéØ MESO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MESO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MESO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MESO: Training LSTM (50 epochs)...
      ‚è≥ WLDN LSTM: Epoch 40/50 (80%)
      ‚è≥ MESO LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.436584
         RMSE: 0.660745
         R¬≤ Score: -1.1202 (Poor - 112.0% variance explained)
      üîπ WLDN: Training TCN (50 epochs)...
      ‚è≥ WLDN TCN: Epoch 10/50 (20%)
      ‚è≥ MESO LSTM: Epoch 20/50 (40%)
      ‚è≥ WLDN TCN: Epoch 20/50 (40%)
      ‚è≥ WLDN TCN: Epoch 30/50 (60%)
      ‚è≥ WLDN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.347265
         RMSE: 0.589292
         R¬≤ Score: -0.6864
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WLDN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WLDN Random Forest: Starting GridSearchCV fit...
      ‚è≥ MESO LSTM: Epoch 30/50 (60%)
      ‚è≥ MESO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.099755
         RMSE: 0.315840
         R¬≤ Score: -0.5824 (Poor - 58.2% variance explained)
      üîπ MESO: Training TCN (50 epochs)...
      ‚è≥ MESO TCN: Epoch 10/50 (20%)
      ‚è≥ MESO TCN: Epoch 20/50 (40%)
      ‚è≥ MESO TCN: Epoch 30/50 (60%)
      ‚è≥ MESO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.066522
         RMSE: 0.257919
         R¬≤ Score: -0.0552
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MESO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MESO Random Forest: Starting GridSearchCV fit...
       ‚úÖ WLDN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.5619 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WLDN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WLDN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=48.7638 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WLDN XGBoost: Starting GridSearchCV fit...
       ‚úÖ MESO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=68.8939 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MESO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MESO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=144.0365 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MESO XGBoost: Starting GridSearchCV fit...
       ‚úÖ LFMD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=582.2234 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 127.8s
    - LSTM: MSE=0.4756
    - TCN: MSE=0.2883
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2883
        ‚Ä¢ LSTM: MSE=0.4756
        ‚Ä¢ LightGBM Regressor (CPU): MSE=494.7103
        ‚Ä¢ Random Forest: MSE=538.3748
        ‚Ä¢ XGBoost: MSE=582.2234
   ‚úÖ LFMD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LFMD (TargetReturn): TCN with MSE=0.2883
üêõ DEBUG: LFMD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LFMD.
üêõ DEBUG: LFMD - Moving model to CPU before return...
üêõ DEBUG [22:37:18.380]: LFMD - Returning result metadata...
üêõ DEBUG: train_worker started for VS
üêõ DEBUG [22:37:18.381]: Main received result for LFMD
  ‚öôÔ∏è Training models for VS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - VS: Initiating feature extraction for training.
  [DIAGNOSTIC] VS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VS: rows after features available: 126
üéØ VS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VS: Training LSTM (50 epochs)...
      ‚è≥ VS LSTM: Epoch 10/50 (20%)
      ‚è≥ VS LSTM: Epoch 20/50 (40%)
      ‚è≥ VS LSTM: Epoch 30/50 (60%)
      ‚è≥ VS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.409822
         RMSE: 0.640174
         R¬≤ Score: -1.3300 (Poor - 133.0% variance explained)
      üîπ VS: Training TCN (50 epochs)...
      ‚è≥ VS TCN: Epoch 10/50 (20%)
      ‚è≥ VS TCN: Epoch 20/50 (40%)
      ‚è≥ VS TCN: Epoch 30/50 (60%)
      ‚è≥ VS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.327454
         RMSE: 0.572236
         R¬≤ Score: -0.8617
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VS Random Forest: Starting GridSearchCV fit...
       ‚úÖ VS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=48.3563 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=33.2222 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CTMX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=656.1770 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.3749
    - TCN: MSE=0.3073
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3073
        ‚Ä¢ LSTM: MSE=0.3749
        ‚Ä¢ Random Forest: MSE=608.0634
        ‚Ä¢ XGBoost: MSE=656.1770
        ‚Ä¢ LightGBM Regressor (CPU): MSE=872.3131
   ‚úÖ CTMX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTMX (TargetReturn): TCN with MSE=0.3073
üêõ DEBUG: CTMX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTMX.
üêõ DEBUG: CTMX - Moving model to CPU before return...
üêõ DEBUG [22:37:35.214]: CTMX - Returning result metadata...
üêõ DEBUG: train_worker started for DJTWW
üêõ DEBUG [22:37:35.220]: Main received result for CTMX
üêõ DEBUG: Training progress: 124/959 done
  ‚öôÔ∏è Training models for DJTWW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - DJTWW: Initiating feature extraction for training.
  [DIAGNOSTIC] DJTWW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DJTWW: rows after features available: 126
üéØ DJTWW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DJTWW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DJTWW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DJTWW: Training LSTM (50 epochs)...
      ‚è≥ DJTWW LSTM: Epoch 10/50 (20%)
      ‚è≥ DJTWW LSTM: Epoch 20/50 (40%)
      ‚è≥ DJTWW LSTM: Epoch 30/50 (60%)
      ‚è≥ DJTWW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.017134
         RMSE: 0.130897
         R¬≤ Score: -2.7723 (Poor - 277.2% variance explained)
      üîπ DJTWW: Training TCN (50 epochs)...
      ‚è≥ DJTWW TCN: Epoch 10/50 (20%)
      ‚è≥ DJTWW TCN: Epoch 20/50 (40%)
      ‚è≥ DJTWW TCN: Epoch 30/50 (60%)
      ‚è≥ DJTWW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.004648
         RMSE: 0.068174
         R¬≤ Score: -0.0233
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DJTWW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DJTWW Random Forest: Starting GridSearchCV fit...
       ‚úÖ DJTWW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.1196 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DJTWW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DJTWW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.3311 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DJTWW XGBoost: Starting GridSearchCV fit...
       ‚úÖ GEO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.3904 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.4s
    - LSTM: MSE=0.0472
    - TCN: MSE=0.0257
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0257
        ‚Ä¢ LSTM: MSE=0.0472
        ‚Ä¢ Random Forest: MSE=52.9240
        ‚Ä¢ XGBoost: MSE=54.3904
        ‚Ä¢ LightGBM Regressor (CPU): MSE=68.2655
   ‚úÖ GEO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GEO (TargetReturn): TCN with MSE=0.0257
üêõ DEBUG: GEO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GEO.
üêõ DEBUG: GEO - Moving model to CPU before return...
üêõ DEBUG [22:37:43.073]: GEO - Returning result metadata...
üêõ DEBUG: train_worker started for DB
üêõ DEBUG [22:37:43.074]: Main received result for GEO
  ‚öôÔ∏è Training models for DB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - DB: Initiating feature extraction for training.
  [DIAGNOSTIC] DB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DB: rows after features available: 126
üéØ DB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DB: Training LSTM (50 epochs)...
      ‚è≥ DB LSTM: Epoch 10/50 (20%)
      ‚è≥ DB LSTM: Epoch 20/50 (40%)
      ‚è≥ DB LSTM: Epoch 30/50 (60%)
      ‚è≥ DB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.332423
         RMSE: 0.576561
         R¬≤ Score: -1.1295 (Poor - 112.9% variance explained)
      üîπ DB: Training TCN (50 epochs)...
      ‚è≥ DB TCN: Epoch 10/50 (20%)
      ‚è≥ DB TCN: Epoch 20/50 (40%)
      ‚è≥ DB TCN: Epoch 30/50 (60%)
      ‚è≥ DB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.158327
         RMSE: 0.397903
         R¬≤ Score: -0.0142
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DB Random Forest: Starting GridSearchCV fit...
       ‚úÖ DB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.0466 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.2183 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DB XGBoost: Starting GridSearchCV fit...
       ‚úÖ DXPE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=59.8165 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 125.6s
    - LSTM: MSE=0.0806
    - TCN: MSE=0.0437
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0437
        ‚Ä¢ LSTM: MSE=0.0806
        ‚Ä¢ Random Forest: MSE=38.4045
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.8369
        ‚Ä¢ XGBoost: MSE=59.8165
   ‚úÖ DXPE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DXPE (TargetReturn): TCN with MSE=0.0437
üêõ DEBUG: DXPE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DXPE.
üêõ DEBUG: DXPE - Moving model to CPU before return...
üêõ DEBUG [22:37:55.430]: DXPE - Returning result metadata...
üêõ DEBUG: train_worker started for MIR
üêõ DEBUG [22:37:55.432]: Main received result for DXPE
  ‚öôÔ∏è Training models for MIR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - MIR: Initiating feature extraction for training.
  [DIAGNOSTIC] MIR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MIR: rows after features available: 126
üéØ MIR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MIR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MIR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MIR: Training LSTM (50 epochs)...
      ‚è≥ MIR LSTM: Epoch 10/50 (20%)
      ‚è≥ MIR LSTM: Epoch 20/50 (40%)
      ‚è≥ MIR LSTM: Epoch 30/50 (60%)
      ‚è≥ MIR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.435724
         RMSE: 0.660094
         R¬≤ Score: -0.8350 (Poor - 83.5% variance explained)
      üîπ MIR: Training TCN (50 epochs)...
      ‚è≥ MIR TCN: Epoch 10/50 (20%)
      ‚è≥ MIR TCN: Epoch 20/50 (40%)
      ‚è≥ MIR TCN: Epoch 30/50 (60%)
      ‚è≥ MIR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.459081
         RMSE: 0.677555
         R¬≤ Score: -0.9334
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MIR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MIR Random Forest: Starting GridSearchCV fit...
       ‚úÖ MIR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.9125 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MIR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MIR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=31.8366 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MIR XGBoost: Starting GridSearchCV fit...
       ‚úÖ KEN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.7896 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.2s
    - LSTM: MSE=0.4542
    - TCN: MSE=0.4347
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4347
        ‚Ä¢ LSTM: MSE=0.4542
        ‚Ä¢ Random Forest: MSE=30.0442
        ‚Ä¢ XGBoost: MSE=37.7896
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.8774
   ‚úÖ KEN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KEN (TargetReturn): TCN with MSE=0.4347
üêõ DEBUG: KEN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KEN.
üêõ DEBUG: KEN - Moving model to CPU before return...
üêõ DEBUG [22:38:01.830]: KEN - Returning result metadata...
üêõ DEBUG [22:38:01.830]: Main received result for KEN
üêõ DEBUG: train_worker started for TSAT
  ‚öôÔ∏è Training models for TSAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - TSAT: Initiating feature extraction for training.
  [DIAGNOSTIC] TSAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TSAT: rows after features available: 126
üéØ TSAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TSAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TSAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TSAT: Training LSTM (50 epochs)...
      ‚è≥ TSAT LSTM: Epoch 10/50 (20%)
      ‚è≥ TSAT LSTM: Epoch 20/50 (40%)
      ‚è≥ TSAT LSTM: Epoch 30/50 (60%)
      ‚è≥ TSAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.486129
         RMSE: 0.697229
         R¬≤ Score: -0.6432 (Poor - 64.3% variance explained)
      üîπ TSAT: Training TCN (50 epochs)...
      ‚è≥ TSAT TCN: Epoch 10/50 (20%)
      ‚è≥ TSAT TCN: Epoch 20/50 (40%)
      ‚è≥ TSAT TCN: Epoch 30/50 (60%)
      ‚è≥ TSAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.494116
         RMSE: 0.702934
         R¬≤ Score: -0.6702
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TSAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TSAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ VNET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=473.7807 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 123.2s
    - LSTM: MSE=0.0969
    - TCN: MSE=0.0630
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0630
        ‚Ä¢ LSTM: MSE=0.0969
        ‚Ä¢ LightGBM Regressor (CPU): MSE=284.6648
        ‚Ä¢ XGBoost: MSE=473.7807
        ‚Ä¢ Random Forest: MSE=626.0588
   ‚úÖ VNET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VNET (TargetReturn): TCN with MSE=0.0630
üêõ DEBUG: VNET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VNET.
üêõ DEBUG: VNET - Moving model to CPU before return...
üêõ DEBUG [22:38:05.556]: VNET - Returning result metadata...
üêõ DEBUG [22:38:05.558]: Main received result for VNET
üêõ DEBUG: Training progress: 128/959 done
üêõ DEBUG: train_worker started for TIGR
  ‚öôÔ∏è Training models for TIGR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TIGR: Initiating feature extraction for training.
  [DIAGNOSTIC] TIGR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TIGR: rows after features available: 126
üéØ TIGR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TIGR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TIGR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TIGR: Training LSTM (50 epochs)...
      ‚è≥ TIGR LSTM: Epoch 10/50 (20%)
      ‚è≥ TIGR LSTM: Epoch 20/50 (40%)
      ‚è≥ TIGR LSTM: Epoch 30/50 (60%)
       ‚úÖ TSAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=79.0790 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TSAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TIGR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.432712
         RMSE: 0.657809
         R¬≤ Score: -0.8807 (Poor - 88.1% variance explained)
      üîπ TIGR: Training TCN (50 epochs)...
       ‚úÖ TSAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=88.6500 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TSAT XGBoost: Starting GridSearchCV fit...
      ‚è≥ TIGR TCN: Epoch 10/50 (20%)
      ‚è≥ TIGR TCN: Epoch 20/50 (40%)
      ‚è≥ TIGR TCN: Epoch 30/50 (60%)
      ‚è≥ TIGR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.238863
         RMSE: 0.488736
         R¬≤ Score: -0.0382
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TIGR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TIGR Random Forest: Starting GridSearchCV fit...
       ‚úÖ TIGR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=70.7050 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TIGR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TIGR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=64.1086 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TIGR XGBoost: Starting GridSearchCV fit...
       ‚úÖ UI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=39.2566 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.0s
    - LSTM: MSE=0.3284
    - TCN: MSE=0.2491
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2491
        ‚Ä¢ LSTM: MSE=0.3284
        ‚Ä¢ Random Forest: MSE=35.0052
        ‚Ä¢ XGBoost: MSE=39.2566
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.5870
   ‚úÖ UI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UI (TargetReturn): TCN with MSE=0.2491
üêõ DEBUG: UI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UI.
üêõ DEBUG: UI - Moving model to CPU before return...
üêõ DEBUG [22:38:15.971]: UI - Returning result metadata...
üêõ DEBUG [22:38:15.971]: Main received result for UI
üêõ DEBUG: train_worker started for RBOT
  ‚öôÔ∏è Training models for RBOT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - RBOT: Initiating feature extraction for training.
  [DIAGNOSTIC] RBOT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RBOT: rows after features available: 126
üéØ RBOT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RBOT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RBOT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RBOT: Training LSTM (50 epochs)...
      ‚è≥ RBOT LSTM: Epoch 10/50 (20%)
       ‚úÖ TWLO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.1236 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.1s
    - LSTM: MSE=0.2716
    - TCN: MSE=0.2453
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2453
        ‚Ä¢ LSTM: MSE=0.2716
        ‚Ä¢ Random Forest: MSE=33.1603
        ‚Ä¢ XGBoost: MSE=37.1236
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.7796
   ‚úÖ TWLO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TWLO (TargetReturn): TCN with MSE=0.2453
üêõ DEBUG: TWLO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TWLO.
üêõ DEBUG: TWLO - Moving model to CPU before return...
üêõ DEBUG [22:38:16.537]: TWLO - Returning result metadata...
üêõ DEBUG: train_worker started for METC
üêõ DEBUG [22:38:16.538]: Main received result for TWLO
  ‚öôÔ∏è Training models for METC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - METC: Initiating feature extraction for training.
  [DIAGNOSTIC] METC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ METC: rows after features available: 126
üéØ METC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] METC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö METC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ METC: Training LSTM (50 epochs)...
      ‚è≥ RBOT LSTM: Epoch 20/50 (40%)
      ‚è≥ METC LSTM: Epoch 10/50 (20%)
      ‚è≥ RBOT LSTM: Epoch 30/50 (60%)
      ‚è≥ METC LSTM: Epoch 20/50 (40%)
      ‚è≥ RBOT LSTM: Epoch 40/50 (80%)
      ‚è≥ METC LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.406800
         RMSE: 0.637808
         R¬≤ Score: -0.6779 (Poor - 67.8% variance explained)
      üîπ RBOT: Training TCN (50 epochs)...
      ‚è≥ METC LSTM: Epoch 40/50 (80%)
      ‚è≥ RBOT TCN: Epoch 10/50 (20%)
      ‚è≥ RBOT TCN: Epoch 20/50 (40%)
      ‚è≥ RBOT TCN: Epoch 30/50 (60%)
      ‚è≥ RBOT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.340372
         RMSE: 0.583414
         R¬≤ Score: -0.4039
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RBOT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RBOT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.505878
         RMSE: 0.711251
         R¬≤ Score: -0.8519 (Poor - 85.2% variance explained)
      üîπ METC: Training TCN (50 epochs)...
      ‚è≥ METC TCN: Epoch 10/50 (20%)
      ‚è≥ METC TCN: Epoch 20/50 (40%)
      ‚è≥ METC TCN: Epoch 30/50 (60%)
      ‚è≥ METC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.531141
         RMSE: 0.728794
         R¬≤ Score: -0.9444
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä METC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ METC Random Forest: Starting GridSearchCV fit...
       ‚úÖ RBOT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=111.8072 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RBOT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ METC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.5090 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ METC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RBOT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=148.7563 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RBOT XGBoost: Starting GridSearchCV fit...
       ‚úÖ METC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=162.0163 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ METC XGBoost: Starting GridSearchCV fit...
       ‚úÖ VVOS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=218.4821 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.5s
    - LSTM: MSE=0.5940
    - TCN: MSE=0.3735
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3735
        ‚Ä¢ LSTM: MSE=0.5940
        ‚Ä¢ Random Forest: MSE=146.4340
        ‚Ä¢ XGBoost: MSE=218.4821
        ‚Ä¢ LightGBM Regressor (CPU): MSE=229.0687
   ‚úÖ VVOS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VVOS (TargetReturn): TCN with MSE=0.3735
üêõ DEBUG: VVOS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VVOS.
üêõ DEBUG: VVOS - Moving model to CPU before return...
üêõ DEBUG [22:38:24.102]: VVOS - Returning result metadata...
üêõ DEBUG: train_worker started for MSTY
üêõ DEBUG [22:38:24.103]: Main received result for VVOS
  ‚öôÔ∏è Training models for MSTY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - MSTY: Initiating feature extraction for training.
  [DIAGNOSTIC] MSTY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MSTY: rows after features available: 126
üéØ MSTY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MSTY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MSTY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MSTY: Training LSTM (50 epochs)...
      ‚è≥ MSTY LSTM: Epoch 10/50 (20%)
      ‚è≥ MSTY LSTM: Epoch 20/50 (40%)
      ‚è≥ MSTY LSTM: Epoch 30/50 (60%)
       ‚úÖ ATOM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=243.7325 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 125.8s
    - LSTM: MSE=0.0726
    - TCN: MSE=0.0371
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0371
        ‚Ä¢ LSTM: MSE=0.0726
        ‚Ä¢ Random Forest: MSE=160.3353
        ‚Ä¢ LightGBM Regressor (CPU): MSE=223.0708
        ‚Ä¢ XGBoost: MSE=243.7325
   ‚úÖ ATOM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATOM (TargetReturn): TCN with MSE=0.0371
üêõ DEBUG: ATOM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATOM.
üêõ DEBUG: ATOM - Moving model to CPU before return...
üêõ DEBUG [22:38:25.869]: ATOM - Returning result metadata...
üêõ DEBUG: train_worker started for ABCL
üêõ DEBUG [22:38:25.870]: Main received result for ATOM
üêõ DEBUG: Training progress: 132/959 done
  ‚öôÔ∏è Training models for ABCL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ABCL: Initiating feature extraction for training.
  [DIAGNOSTIC] ABCL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ABCL: rows after features available: 126
üéØ ABCL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ABCL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ABCL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ABCL: Training LSTM (50 epochs)...
      ‚è≥ MSTY LSTM: Epoch 40/50 (80%)
      ‚è≥ ABCL LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.133921
         RMSE: 0.365953
         R¬≤ Score: -0.6600 (Poor - 66.0% variance explained)
      üîπ MSTY: Training TCN (50 epochs)...
      ‚è≥ MSTY TCN: Epoch 10/50 (20%)
      ‚è≥ MSTY TCN: Epoch 20/50 (40%)
      ‚è≥ MSTY TCN: Epoch 30/50 (60%)
      ‚è≥ ABCL LSTM: Epoch 20/50 (40%)
      ‚è≥ MSTY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.088464
         RMSE: 0.297430
         R¬≤ Score: -0.0966
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MSTY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MSTY Random Forest: Starting GridSearchCV fit...
      ‚è≥ ABCL LSTM: Epoch 30/50 (60%)
      ‚è≥ ABCL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.755109
         RMSE: 0.868970
         R¬≤ Score: -0.8395 (Poor - 83.9% variance explained)
      üîπ ABCL: Training TCN (50 epochs)...
      ‚è≥ ABCL TCN: Epoch 10/50 (20%)
      ‚è≥ ABCL TCN: Epoch 20/50 (40%)
      ‚è≥ ABCL TCN: Epoch 30/50 (60%)
      ‚è≥ ABCL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.605384
         RMSE: 0.778065
         R¬≤ Score: -0.4747
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ABCL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ABCL Random Forest: Starting GridSearchCV fit...
       ‚úÖ MSTY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.8603 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MSTY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MSTY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=37.6827 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MSTY XGBoost: Starting GridSearchCV fit...
       ‚úÖ ABCL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=63.5135 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ABCL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ABCL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=82.7496 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ABCL XGBoost: Starting GridSearchCV fit...
       ‚úÖ MGTX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=63.9114 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.8s
    - LSTM: MSE=0.4801
    - TCN: MSE=0.5033
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4801
        ‚Ä¢ TCN: MSE=0.5033
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.2523
        ‚Ä¢ Random Forest: MSE=61.8735
        ‚Ä¢ XGBoost: MSE=63.9114
   ‚úÖ MGTX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MGTX (TargetReturn): LSTM with MSE=0.4801
üêõ DEBUG: MGTX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MGTX.
üêõ DEBUG: MGTX - Moving model to CPU before return...
üêõ DEBUG [22:38:32.497]: MGTX - Returning result metadata...
üêõ DEBUG: train_worker started for TBRG
üêõ DEBUG [22:38:32.498]: Main received result for MGTX
  ‚öôÔ∏è Training models for TBRG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TBRG: Initiating feature extraction for training.
  [DIAGNOSTIC] TBRG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TBRG: rows after features available: 126
üéØ TBRG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TBRG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TBRG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TBRG: Training LSTM (50 epochs)...
      ‚è≥ TBRG LSTM: Epoch 10/50 (20%)
      ‚è≥ TBRG LSTM: Epoch 20/50 (40%)
       ‚úÖ EAT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=35.1390 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.0s
    - LSTM: MSE=0.1652
    - TCN: MSE=0.1451
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1451
        ‚Ä¢ LSTM: MSE=0.1652
        ‚Ä¢ XGBoost: MSE=35.1390
        ‚Ä¢ Random Forest: MSE=37.8681
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.1166
   ‚úÖ EAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EAT (TargetReturn): TCN with MSE=0.1451
üêõ DEBUG: EAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EAT.
üêõ DEBUG: EAT - Moving model to CPU before return...
üêõ DEBUG [22:38:34.011]: EAT - Returning result metadata...
üêõ DEBUG: train_worker started for LQDA
  ‚öôÔ∏è Training models for LQDA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - LQDA: Initiating feature extraction for training.
  [DIAGNOSTIC] LQDA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LQDA: rows after features available: 126
üéØ LQDA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ TBRG LSTM: Epoch 30/50 (60%)
  [DIAGNOSTIC] LQDA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LQDA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LQDA: Training LSTM (50 epochs)...
       ‚úÖ RCL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=45.0392 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 128.6s
    - LSTM: MSE=0.5971
    - TCN: MSE=0.7030
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5971
        ‚Ä¢ TCN: MSE=0.7030
        ‚Ä¢ XGBoost: MSE=45.0392
        ‚Ä¢ Random Forest: MSE=46.9392
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.4484
   ‚úÖ RCL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RCL (TargetReturn): LSTM with MSE=0.5971
üêõ DEBUG: RCL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RCL.
üêõ DEBUG: RCL - Moving model to CPU before return...
üêõ DEBUG [22:38:34.130]: RCL - Returning result metadata...
üêõ DEBUG [22:38:34.130]: Main received result for RCL
üêõ DEBUG [22:38:34.130]: Main received result for EAT
üêõ DEBUG: train_worker started for RIOT
  ‚öôÔ∏è Training models for RIOT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - RIOT: Initiating feature extraction for training.
  [DIAGNOSTIC] RIOT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RIOT: rows after features available: 126
üéØ RIOT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RIOT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RIOT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RIOT: Training LSTM (50 epochs)...
      ‚è≥ LQDA LSTM: Epoch 10/50 (20%)
      ‚è≥ TBRG LSTM: Epoch 40/50 (80%)
      ‚è≥ RIOT LSTM: Epoch 10/50 (20%)
      ‚è≥ LQDA LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.024458
         RMSE: 0.156390
         R¬≤ Score: -0.6038 (Poor - 60.4% variance explained)
      üîπ TBRG: Training TCN (50 epochs)...
      ‚è≥ TBRG TCN: Epoch 10/50 (20%)
      ‚è≥ RIOT LSTM: Epoch 20/50 (40%)
      ‚è≥ TBRG TCN: Epoch 20/50 (40%)
      ‚è≥ TBRG TCN: Epoch 30/50 (60%)
      ‚è≥ LQDA LSTM: Epoch 30/50 (60%)
      ‚è≥ TBRG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.024433
         RMSE: 0.156311
         R¬≤ Score: -0.6022
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TBRG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TBRG Random Forest: Starting GridSearchCV fit...
      ‚è≥ RIOT LSTM: Epoch 30/50 (60%)
      ‚è≥ LQDA LSTM: Epoch 40/50 (80%)
      ‚è≥ RIOT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.146926
         RMSE: 0.383310
         R¬≤ Score: -0.6400 (Poor - 64.0% variance explained)
      üîπ LQDA: Training TCN (50 epochs)...
      ‚è≥ LQDA TCN: Epoch 10/50 (20%)
      ‚è≥ LQDA TCN: Epoch 20/50 (40%)
      ‚è≥ LQDA TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.488527
         RMSE: 0.698947
         R¬≤ Score: -0.6033 (Poor - 60.3% variance explained)
      üîπ RIOT: Training TCN (50 epochs)...
      ‚è≥ RIOT TCN: Epoch 10/50 (20%)
      ‚è≥ LQDA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.091995
         RMSE: 0.303306
         R¬≤ Score: -0.0268
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LQDA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LQDA Random Forest: Starting GridSearchCV fit...
      ‚è≥ RIOT TCN: Epoch 20/50 (40%)
      ‚è≥ RIOT TCN: Epoch 30/50 (60%)
      ‚è≥ RIOT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.363065
         RMSE: 0.602549
         R¬≤ Score: -0.1915
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RIOT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RIOT Random Forest: Starting GridSearchCV fit...
       ‚úÖ TBRG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.6458 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TBRG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TBRG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.8841 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TBRG XGBoost: Starting GridSearchCV fit...
       ‚úÖ LQDA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.4295 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LQDA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RIOT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=68.1582 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RIOT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LQDA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=58.0844 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LQDA XGBoost: Starting GridSearchCV fit...
       ‚úÖ RIOT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=110.3799 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RIOT XGBoost: Starting GridSearchCV fit...
       ‚úÖ WLDN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=36.5015 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.4366
    - TCN: MSE=0.3473
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3473
        ‚Ä¢ LSTM: MSE=0.4366
        ‚Ä¢ XGBoost: MSE=36.5015
        ‚Ä¢ Random Forest: MSE=38.5619
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.7638
   ‚úÖ WLDN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WLDN (TargetReturn): TCN with MSE=0.3473
üêõ DEBUG: WLDN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WLDN.
üêõ DEBUG: WLDN - Moving model to CPU before return...
üêõ DEBUG [22:39:11.816]: WLDN - Returning result metadata...
üêõ DEBUG: train_worker started for MYRG
üêõ DEBUG [22:39:11.818]: Main received result for WLDN
üêõ DEBUG: Training progress: 136/959 done
  ‚öôÔ∏è Training models for MYRG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - MYRG: Initiating feature extraction for training.
  [DIAGNOSTIC] MYRG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MYRG: rows after features available: 126
üéØ MYRG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MYRG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MYRG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MYRG: Training LSTM (50 epochs)...
      ‚è≥ MYRG LSTM: Epoch 10/50 (20%)
      ‚è≥ MYRG LSTM: Epoch 20/50 (40%)
      ‚è≥ MYRG LSTM: Epoch 30/50 (60%)
      ‚è≥ MYRG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.375290
         RMSE: 0.612609
         R¬≤ Score: -0.9034 (Poor - 90.3% variance explained)
      üîπ MYRG: Training TCN (50 epochs)...
      ‚è≥ MYRG TCN: Epoch 10/50 (20%)
      ‚è≥ MYRG TCN: Epoch 20/50 (40%)
      ‚è≥ MYRG TCN: Epoch 30/50 (60%)
      ‚è≥ MYRG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.272254
         RMSE: 0.521780
         R¬≤ Score: -0.3808
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MYRG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MYRG Random Forest: Starting GridSearchCV fit...
       ‚úÖ MESO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=74.3430 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.0998
    - TCN: MSE=0.0665
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0665
        ‚Ä¢ LSTM: MSE=0.0998
        ‚Ä¢ Random Forest: MSE=68.8939
        ‚Ä¢ XGBoost: MSE=74.3430
        ‚Ä¢ LightGBM Regressor (CPU): MSE=144.0365
   ‚úÖ MESO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MESO (TargetReturn): TCN with MSE=0.0665
üêõ DEBUG: MESO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MESO.
üêõ DEBUG: MESO - Moving model to CPU before return...
üêõ DEBUG [22:39:16.014]: MESO - Returning result metadata...
üêõ DEBUG [22:39:16.016]: Main received result for MESO
üêõ DEBUG: train_worker started for LIF
  ‚öôÔ∏è Training models for LIF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - LIF: Initiating feature extraction for training.
  [DIAGNOSTIC] LIF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LIF: rows after features available: 126
üéØ LIF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LIF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LIF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LIF: Training LSTM (50 epochs)...
      ‚è≥ LIF LSTM: Epoch 10/50 (20%)
      ‚è≥ LIF LSTM: Epoch 20/50 (40%)
       ‚úÖ MYRG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=62.9351 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MYRG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LIF LSTM: Epoch 30/50 (60%)
       ‚úÖ MYRG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=46.9862 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MYRG XGBoost: Starting GridSearchCV fit...
      ‚è≥ LIF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.708057
         RMSE: 0.841461
         R¬≤ Score: -1.4357 (Poor - 143.6% variance explained)
      üîπ LIF: Training TCN (50 epochs)...
      ‚è≥ LIF TCN: Epoch 10/50 (20%)
      ‚è≥ LIF TCN: Epoch 20/50 (40%)
      ‚è≥ LIF TCN: Epoch 30/50 (60%)
      ‚è≥ LIF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.305023
         RMSE: 0.552289
         R¬≤ Score: -0.0493
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LIF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LIF Random Forest: Starting GridSearchCV fit...
       ‚úÖ LIF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=83.1354 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LIF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LIF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=116.5380 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LIF XGBoost: Starting GridSearchCV fit...
       ‚úÖ VS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.3430 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.8s
    - LSTM: MSE=0.4098
    - TCN: MSE=0.3275
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3275
        ‚Ä¢ LSTM: MSE=0.4098
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.2222
        ‚Ä¢ Random Forest: MSE=48.3563
        ‚Ä¢ XGBoost: MSE=52.3430
   ‚úÖ VS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VS (TargetReturn): TCN with MSE=0.3275
üêõ DEBUG: VS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VS.
üêõ DEBUG: VS - Moving model to CPU before return...
üêõ DEBUG [22:39:25.484]: VS - Returning result metadata...
üêõ DEBUG: train_worker started for TLN
üêõ DEBUG [22:39:25.484]: Main received result for VS
  ‚öôÔ∏è Training models for TLN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TLN: Initiating feature extraction for training.
  [DIAGNOSTIC] TLN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TLN: rows after features available: 126
üéØ TLN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TLN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TLN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TLN: Training LSTM (50 epochs)...
      ‚è≥ TLN LSTM: Epoch 10/50 (20%)
      ‚è≥ TLN LSTM: Epoch 20/50 (40%)
      ‚è≥ TLN LSTM: Epoch 30/50 (60%)
      ‚è≥ TLN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.514142
         RMSE: 0.717037
         R¬≤ Score: -1.0756 (Poor - 107.6% variance explained)
      üîπ TLN: Training TCN (50 epochs)...
      ‚è≥ TLN TCN: Epoch 10/50 (20%)
      ‚è≥ TLN TCN: Epoch 20/50 (40%)
      ‚è≥ TLN TCN: Epoch 30/50 (60%)
      ‚è≥ TLN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.408278
         RMSE: 0.638967
         R¬≤ Score: -0.6482
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TLN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TLN Random Forest: Starting GridSearchCV fit...
       ‚úÖ TLN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=72.2762 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TLN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TLN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=63.8530 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TLN XGBoost: Starting GridSearchCV fit...
       ‚úÖ DJTWW XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.9882 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 114.2s
    - LSTM: MSE=0.0171
    - TCN: MSE=0.0046
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0046
        ‚Ä¢ LSTM: MSE=0.0171
        ‚Ä¢ XGBoost: MSE=11.9882
        ‚Ä¢ Random Forest: MSE=16.1196
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3311
   ‚úÖ DJTWW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DJTWW (TargetReturn): TCN with MSE=0.0046
üêõ DEBUG: DJTWW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DJTWW.
üêõ DEBUG: DJTWW - Moving model to CPU before return...
üêõ DEBUG [22:39:35.735]: DJTWW - Returning result metadata...
üêõ DEBUG: train_worker started for VUZI
üêõ DEBUG [22:39:35.745]: Main received result for DJTWW
  ‚öôÔ∏è Training models for VUZI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - VUZI: Initiating feature extraction for training.
  [DIAGNOSTIC] VUZI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VUZI: rows after features available: 126
üéØ VUZI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VUZI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VUZI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VUZI: Training LSTM (50 epochs)...
      ‚è≥ VUZI LSTM: Epoch 10/50 (20%)
      ‚è≥ VUZI LSTM: Epoch 20/50 (40%)
      ‚è≥ VUZI LSTM: Epoch 30/50 (60%)
      ‚è≥ VUZI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.053344
         RMSE: 0.230963
         R¬≤ Score: -0.6418 (Poor - 64.2% variance explained)
      üîπ VUZI: Training TCN (50 epochs)...
      ‚è≥ VUZI TCN: Epoch 10/50 (20%)
      ‚è≥ VUZI TCN: Epoch 20/50 (40%)
      ‚è≥ VUZI TCN: Epoch 30/50 (60%)
      ‚è≥ VUZI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.034892
         RMSE: 0.186794
         R¬≤ Score: -0.0739
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VUZI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VUZI Random Forest: Starting GridSearchCV fit...
       ‚úÖ VUZI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=443.8555 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VUZI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VUZI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=815.7187 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VUZI XGBoost: Starting GridSearchCV fit...
       ‚úÖ DB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.3199 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.3324
    - TCN: MSE=0.1583
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1583
        ‚Ä¢ LSTM: MSE=0.3324
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.2183
        ‚Ä¢ Random Forest: MSE=26.0466
        ‚Ä¢ XGBoost: MSE=26.3199
   ‚úÖ DB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DB (TargetReturn): TCN with MSE=0.1583
üêõ DEBUG: DB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DB.
üêõ DEBUG: DB - Moving model to CPU before return...
üêõ DEBUG [22:39:49.334]: DB - Returning result metadata...
üêõ DEBUG: train_worker started for UTI
üêõ DEBUG [22:39:49.335]: Main received result for DB
üêõ DEBUG: Training progress: 140/959 done
  ‚öôÔ∏è Training models for UTI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - UTI: Initiating feature extraction for training.
  [DIAGNOSTIC] UTI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UTI: rows after features available: 126
üéØ UTI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UTI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UTI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UTI: Training LSTM (50 epochs)...
      ‚è≥ UTI LSTM: Epoch 10/50 (20%)
      ‚è≥ UTI LSTM: Epoch 20/50 (40%)
      ‚è≥ UTI LSTM: Epoch 30/50 (60%)
      ‚è≥ UTI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.140143
         RMSE: 0.374357
         R¬≤ Score: -0.7751 (Poor - 77.5% variance explained)
      üîπ UTI: Training TCN (50 epochs)...
      ‚è≥ UTI TCN: Epoch 10/50 (20%)
      ‚è≥ UTI TCN: Epoch 20/50 (40%)
      ‚è≥ UTI TCN: Epoch 30/50 (60%)
      ‚è≥ UTI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.081927
         RMSE: 0.286229
         R¬≤ Score: -0.0377
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UTI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UTI Random Forest: Starting GridSearchCV fit...
       ‚úÖ UTI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.0703 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UTI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UTI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.7722 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UTI XGBoost: Starting GridSearchCV fit...
       ‚úÖ MIR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.4464 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.4357
    - TCN: MSE=0.4591
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4357
        ‚Ä¢ TCN: MSE=0.4591
        ‚Ä¢ Random Forest: MSE=30.9125
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.8366
        ‚Ä¢ XGBoost: MSE=46.4464
   ‚úÖ MIR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MIR (TargetReturn): LSTM with MSE=0.4357
üêõ DEBUG: MIR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MIR.
üêõ DEBUG: MIR - Moving model to CPU before return...
üêõ DEBUG [22:40:03.117]: MIR - Returning result metadata...
üêõ DEBUG [22:40:03.117]: Main received result for MIR
üêõ DEBUG: train_worker started for AU
  ‚öôÔ∏è Training models for AU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - AU: Initiating feature extraction for training.
  [DIAGNOSTIC] AU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AU: rows after features available: 126
üéØ AU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AU: Training LSTM (50 epochs)...
      ‚è≥ AU LSTM: Epoch 10/50 (20%)
      ‚è≥ AU LSTM: Epoch 20/50 (40%)
      ‚è≥ AU LSTM: Epoch 30/50 (60%)
       ‚úÖ TSAT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=124.2903 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.5s
    - LSTM: MSE=0.4861
    - TCN: MSE=0.4941
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4861
        ‚Ä¢ TCN: MSE=0.4941
        ‚Ä¢ Random Forest: MSE=79.0790
        ‚Ä¢ LightGBM Regressor (CPU): MSE=88.6500
        ‚Ä¢ XGBoost: MSE=124.2903
   ‚úÖ TSAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TSAT (TargetReturn): LSTM with MSE=0.4861
üêõ DEBUG: TSAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TSAT.
üêõ DEBUG: TSAT - Moving model to CPU before return...
üêõ DEBUG [22:40:04.779]: TSAT - Returning result metadata...
üêõ DEBUG: train_worker started for LINC
üêõ DEBUG [22:40:04.783]: Main received result for TSAT
  ‚öôÔ∏è Training models for LINC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - LINC: Initiating feature extraction for training.
  [DIAGNOSTIC] LINC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LINC: rows after features available: 126
üéØ LINC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LINC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LINC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LINC: Training LSTM (50 epochs)...
      ‚è≥ AU LSTM: Epoch 40/50 (80%)
      ‚è≥ LINC LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.258232
         RMSE: 0.508165
         R¬≤ Score: -0.9532 (Poor - 95.3% variance explained)
      üîπ AU: Training TCN (50 epochs)...
      ‚è≥ AU TCN: Epoch 10/50 (20%)
      ‚è≥ AU TCN: Epoch 20/50 (40%)
      ‚è≥ LINC LSTM: Epoch 20/50 (40%)
      ‚è≥ AU TCN: Epoch 30/50 (60%)
      ‚è≥ AU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.182626
         RMSE: 0.427348
         R¬≤ Score: -0.3813
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AU Random Forest: Starting GridSearchCV fit...
      ‚è≥ LINC LSTM: Epoch 30/50 (60%)
      ‚è≥ LINC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.394448
         RMSE: 0.628051
         R¬≤ Score: -0.7612 (Poor - 76.1% variance explained)
      üîπ LINC: Training TCN (50 epochs)...
      ‚è≥ LINC TCN: Epoch 10/50 (20%)
      ‚è≥ LINC TCN: Epoch 20/50 (40%)
      ‚è≥ LINC TCN: Epoch 30/50 (60%)
      ‚è≥ LINC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.278056
         RMSE: 0.527310
         R¬≤ Score: -0.2415
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LINC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LINC Random Forest: Starting GridSearchCV fit...
       ‚úÖ AU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=67.1487 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=93.4486 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AU XGBoost: Starting GridSearchCV fit...
       ‚úÖ TIGR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=68.0965 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.4327
    - TCN: MSE=0.2389
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2389
        ‚Ä¢ LSTM: MSE=0.4327
        ‚Ä¢ LightGBM Regressor (CPU): MSE=64.1086
        ‚Ä¢ XGBoost: MSE=68.0965
        ‚Ä¢ Random Forest: MSE=70.7050
   ‚úÖ TIGR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TIGR (TargetReturn): TCN with MSE=0.2389
üêõ DEBUG: TIGR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TIGR.
üêõ DEBUG: TIGR - Moving model to CPU before return...
üêõ DEBUG [22:40:10.138]: TIGR - Returning result metadata...
üêõ DEBUG [22:40:10.140]: Main received result for TIGR
üêõ DEBUG: train_worker started for CVNA
  ‚öôÔ∏è Training models for CVNA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - CVNA: Initiating feature extraction for training.
  [DIAGNOSTIC] CVNA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CVNA: rows after features available: 126
üéØ CVNA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CVNA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CVNA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CVNA: Training LSTM (50 epochs)...
       ‚úÖ LINC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.1089 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LINC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CVNA LSTM: Epoch 10/50 (20%)
      ‚è≥ CVNA LSTM: Epoch 20/50 (40%)
       ‚úÖ LINC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.1215 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LINC XGBoost: Starting GridSearchCV fit...
      ‚è≥ CVNA LSTM: Epoch 30/50 (60%)
      ‚è≥ CVNA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.390314
         RMSE: 0.624751
         R¬≤ Score: -0.7408 (Poor - 74.1% variance explained)
      üîπ CVNA: Training TCN (50 epochs)...
      ‚è≥ CVNA TCN: Epoch 10/50 (20%)
      ‚è≥ CVNA TCN: Epoch 20/50 (40%)
      ‚è≥ CVNA TCN: Epoch 30/50 (60%)
      ‚è≥ CVNA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.236519
         RMSE: 0.486332
         R¬≤ Score: -0.0549
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CVNA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CVNA Random Forest: Starting GridSearchCV fit...
       ‚úÖ CVNA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=84.8861 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CVNA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CVNA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=115.7326 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CVNA XGBoost: Starting GridSearchCV fit...
       ‚úÖ RBOT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=116.8324 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.2s
    - LSTM: MSE=0.4068
    - TCN: MSE=0.3404
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3404
        ‚Ä¢ LSTM: MSE=0.4068
        ‚Ä¢ Random Forest: MSE=111.8072
        ‚Ä¢ XGBoost: MSE=116.8324
        ‚Ä¢ LightGBM Regressor (CPU): MSE=148.7563
   ‚úÖ RBOT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RBOT (TargetReturn): TCN with MSE=0.3404
üêõ DEBUG: RBOT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RBOT.
üêõ DEBUG: RBOT - Moving model to CPU before return...
üêõ DEBUG [22:40:19.908]: RBOT - Returning result metadata...
üêõ DEBUG: train_worker started for AAOI
üêõ DEBUG [22:40:19.914]: Main received result for RBOT
üêõ DEBUG: Training progress: 144/959 done
  ‚öôÔ∏è Training models for AAOI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - AAOI: Initiating feature extraction for training.
  [DIAGNOSTIC] AAOI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AAOI: rows after features available: 126
üéØ AAOI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AAOI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AAOI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AAOI: Training LSTM (50 epochs)...
      ‚è≥ AAOI LSTM: Epoch 10/50 (20%)
      ‚è≥ AAOI LSTM: Epoch 20/50 (40%)
      ‚è≥ AAOI LSTM: Epoch 30/50 (60%)
      ‚è≥ AAOI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.742423
         RMSE: 0.861640
         R¬≤ Score: -1.1370 (Poor - 113.7% variance explained)
      üîπ AAOI: Training TCN (50 epochs)...
      ‚è≥ AAOI TCN: Epoch 10/50 (20%)
      ‚è≥ AAOI TCN: Epoch 20/50 (40%)
      ‚è≥ AAOI TCN: Epoch 30/50 (60%)
      ‚è≥ AAOI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.400718
         RMSE: 0.633023
         R¬≤ Score: -0.1534
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AAOI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AAOI Random Forest: Starting GridSearchCV fit...
       ‚úÖ METC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=68.7582 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.5059
    - TCN: MSE=0.5311
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5059
        ‚Ä¢ TCN: MSE=0.5311
        ‚Ä¢ Random Forest: MSE=60.5090
        ‚Ä¢ XGBoost: MSE=68.7582
        ‚Ä¢ LightGBM Regressor (CPU): MSE=162.0163
   ‚úÖ METC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for METC (TargetReturn): LSTM with MSE=0.5059
üêõ DEBUG: METC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for METC.
üêõ DEBUG: METC - Moving model to CPU before return...
üêõ DEBUG [22:40:24.235]: METC - Returning result metadata...
üêõ DEBUG [22:40:24.236]: Main received result for METC
üêõ DEBUG: train_worker started for CCB
  ‚öôÔ∏è Training models for CCB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CCB: Initiating feature extraction for training.
  [DIAGNOSTIC] CCB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CCB: rows after features available: 126
üéØ CCB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CCB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CCB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CCB: Training LSTM (50 epochs)...
      ‚è≥ CCB LSTM: Epoch 10/50 (20%)
      ‚è≥ CCB LSTM: Epoch 20/50 (40%)
      ‚è≥ CCB LSTM: Epoch 30/50 (60%)
      ‚è≥ CCB LSTM: Epoch 40/50 (80%)
       ‚úÖ AAOI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=109.9049 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AAOI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.286817
         RMSE: 0.535553
         R¬≤ Score: -0.7180 (Poor - 71.8% variance explained)
      üîπ CCB: Training TCN (50 epochs)...
      ‚è≥ CCB TCN: Epoch 10/50 (20%)
      ‚è≥ CCB TCN: Epoch 20/50 (40%)
      ‚è≥ CCB TCN: Epoch 30/50 (60%)
      ‚è≥ CCB TCN: Epoch 40/50 (80%)
       ‚úÖ AAOI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=250.2210 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AAOI XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.198197
         RMSE: 0.445193
         R¬≤ Score: -0.1872
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CCB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CCB Random Forest: Starting GridSearchCV fit...
       ‚úÖ MSTY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=67.9725 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 118.8s
    - LSTM: MSE=0.1339
    - TCN: MSE=0.0885
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0885
        ‚Ä¢ LSTM: MSE=0.1339
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.6827
        ‚Ä¢ Random Forest: MSE=45.8603
        ‚Ä¢ XGBoost: MSE=67.9725
   ‚úÖ MSTY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MSTY (TargetReturn): TCN with MSE=0.0885
üêõ DEBUG: MSTY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MSTY.
üêõ DEBUG: MSTY - Moving model to CPU before return...
üêõ DEBUG [22:40:29.263]: MSTY - Returning result metadata...
üêõ DEBUG [22:40:29.264]: Main received result for MSTY
üêõ DEBUG: train_worker started for MGIC
  ‚öôÔ∏è Training models for MGIC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - MGIC: Initiating feature extraction for training.
  [DIAGNOSTIC] MGIC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MGIC: rows after features available: 126
üéØ MGIC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MGIC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MGIC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MGIC: Training LSTM (50 epochs)...
      ‚è≥ MGIC LSTM: Epoch 10/50 (20%)
       ‚úÖ CCB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.0011 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CCB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MGIC LSTM: Epoch 20/50 (40%)
       ‚úÖ CCB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.0826 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CCB XGBoost: Starting GridSearchCV fit...
      ‚è≥ MGIC LSTM: Epoch 30/50 (60%)
      ‚è≥ MGIC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.575685
         RMSE: 0.758739
         R¬≤ Score: -0.9845 (Poor - 98.5% variance explained)
      üîπ MGIC: Training TCN (50 epochs)...
      ‚è≥ MGIC TCN: Epoch 10/50 (20%)
      ‚è≥ MGIC TCN: Epoch 20/50 (40%)
      ‚è≥ MGIC TCN: Epoch 30/50 (60%)
      ‚è≥ MGIC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.426922
         RMSE: 0.653393
         R¬≤ Score: -0.4717
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MGIC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MGIC Random Forest: Starting GridSearchCV fit...
       ‚úÖ MGIC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.3883 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MGIC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ABCL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=62.9914 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.9s
    - LSTM: MSE=0.7551
    - TCN: MSE=0.6054
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6054
        ‚Ä¢ LSTM: MSE=0.7551
        ‚Ä¢ XGBoost: MSE=62.9914
        ‚Ä¢ Random Forest: MSE=63.5135
        ‚Ä¢ LightGBM Regressor (CPU): MSE=82.7496
   ‚úÖ ABCL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ABCL (TargetReturn): TCN with MSE=0.6054
üêõ DEBUG: ABCL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ABCL.
üêõ DEBUG: ABCL - Moving model to CPU before return...
üêõ DEBUG [22:40:35.202]: ABCL - Returning result metadata...
üêõ DEBUG [22:40:35.203]: Main received result for ABCL
üêõ DEBUG: train_worker started for HWM
  ‚öôÔ∏è Training models for HWM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - HWM: Initiating feature extraction for training.
  [DIAGNOSTIC] HWM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HWM: rows after features available: 126
üéØ HWM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
       ‚úÖ MGIC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.4919 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MGIC XGBoost: Starting GridSearchCV fit...
  [DIAGNOSTIC] HWM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HWM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HWM: Training LSTM (50 epochs)...
      ‚è≥ HWM LSTM: Epoch 10/50 (20%)
      ‚è≥ HWM LSTM: Epoch 20/50 (40%)
      ‚è≥ HWM LSTM: Epoch 30/50 (60%)
      ‚è≥ HWM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.290133
         RMSE: 0.538640
         R¬≤ Score: -0.6450 (Poor - 64.5% variance explained)
      üîπ HWM: Training TCN (50 epochs)...
      ‚è≥ HWM TCN: Epoch 10/50 (20%)
      ‚è≥ HWM TCN: Epoch 20/50 (40%)
      ‚è≥ HWM TCN: Epoch 30/50 (60%)
      ‚è≥ HWM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.249051
         RMSE: 0.499050
         R¬≤ Score: -0.4121
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HWM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HWM Random Forest: Starting GridSearchCV fit...
       ‚úÖ TBRG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.9821 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.0245
    - TCN: MSE=0.0244
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0244
        ‚Ä¢ LSTM: MSE=0.0245
        ‚Ä¢ Random Forest: MSE=15.6458
        ‚Ä¢ XGBoost: MSE=18.9821
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.8841
   ‚úÖ TBRG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TBRG (TargetReturn): TCN with MSE=0.0244
üêõ DEBUG: TBRG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TBRG.
üêõ DEBUG: TBRG - Moving model to CPU before return...
üêõ DEBUG [22:40:41.059]: TBRG - Returning result metadata...
üêõ DEBUG [22:40:41.059]: Main received result for TBRG
üêõ DEBUG: Training progress: 148/959 done
üêõ DEBUG: train_worker started for CYD
  ‚öôÔ∏è Training models for CYD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - CYD: Initiating feature extraction for training.
  [DIAGNOSTIC] CYD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CYD: rows after features available: 126
üéØ CYD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CYD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CYD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CYD: Training LSTM (50 epochs)...
       ‚úÖ HWM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.6696 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HWM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CYD LSTM: Epoch 10/50 (20%)
       ‚úÖ HWM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.9422 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HWM XGBoost: Starting GridSearchCV fit...
       ‚úÖ LQDA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=71.5636 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.5s
    - LSTM: MSE=0.1469
    - TCN: MSE=0.0920
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0920
        ‚Ä¢ LSTM: MSE=0.1469
        ‚Ä¢ LightGBM Regressor (CPU): MSE=58.0844
        ‚Ä¢ Random Forest: MSE=60.4295
        ‚Ä¢ XGBoost: MSE=71.5636
   ‚úÖ LQDA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LQDA (TargetReturn): TCN with MSE=0.0920
üêõ DEBUG: LQDA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LQDA.
üêõ DEBUG: LQDA - Moving model to CPU before return...
üêõ DEBUG [22:40:42.052]: LQDA - Returning result metadata...
üêõ DEBUG [22:40:42.052]: Main received result for LQDA
üêõ DEBUG: train_worker started for IBKR
       ‚úÖ RIOT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=64.5343 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.2s
    - LSTM: MSE=0.4885
    - TCN: MSE=0.3631
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3631
        ‚Ä¢ LSTM: MSE=0.4885
        ‚Ä¢ XGBoost: MSE=64.5343
        ‚Ä¢ Random Forest: MSE=68.1582
        ‚Ä¢ LightGBM Regressor (CPU): MSE=110.3799
   ‚úÖ RIOT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RIOT (TargetReturn): TCN with MSE=0.3631
üêõ DEBUG: RIOT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RIOT.
üêõ DEBUG: RIOT - Moving model to CPU before return...
üêõ DEBUG [22:40:42.070]: RIOT - Returning result metadata...
üêõ DEBUG [22:40:42.070]: Main received result for RIOT
üêõ DEBUG: train_worker started for WGS
  ‚öôÔ∏è Training models for IBKR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - IBKR: Initiating feature extraction for training.
  [DIAGNOSTIC] IBKR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IBKR: rows after features available: 126
üéØ IBKR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IBKR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IBKR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IBKR: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for WGS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - WGS: Initiating feature extraction for training.
  [DIAGNOSTIC] WGS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WGS: rows after features available: 126
üéØ WGS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WGS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WGS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WGS: Training LSTM (50 epochs)...
      ‚è≥ CYD LSTM: Epoch 20/50 (40%)
      ‚è≥ IBKR LSTM: Epoch 10/50 (20%)
      ‚è≥ CYD LSTM: Epoch 30/50 (60%)
      ‚è≥ WGS LSTM: Epoch 10/50 (20%)
      ‚è≥ IBKR LSTM: Epoch 20/50 (40%)
      ‚è≥ WGS LSTM: Epoch 20/50 (40%)
      ‚è≥ CYD LSTM: Epoch 40/50 (80%)
      ‚è≥ IBKR LSTM: Epoch 30/50 (60%)
      ‚è≥ WGS LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.278709
         RMSE: 0.527929
         R¬≤ Score: -1.2632 (Poor - 126.3% variance explained)
      üîπ CYD: Training TCN (50 epochs)...
      ‚è≥ CYD TCN: Epoch 10/50 (20%)
      ‚è≥ IBKR LSTM: Epoch 40/50 (80%)
      ‚è≥ CYD TCN: Epoch 20/50 (40%)
      ‚è≥ WGS LSTM: Epoch 40/50 (80%)
      ‚è≥ CYD TCN: Epoch 30/50 (60%)
      ‚è≥ CYD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.141646
         RMSE: 0.376359
         R¬≤ Score: -0.1502
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CYD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CYD Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.712091
         RMSE: 0.843855
         R¬≤ Score: -1.0644 (Poor - 106.4% variance explained)
      üîπ IBKR: Training TCN (50 epochs)...
      ‚è≥ IBKR TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.297363
         RMSE: 0.545310
         R¬≤ Score: -0.6902 (Poor - 69.0% variance explained)
      üîπ WGS: Training TCN (50 epochs)...
      ‚è≥ IBKR TCN: Epoch 20/50 (40%)
      ‚è≥ WGS TCN: Epoch 10/50 (20%)
      ‚è≥ WGS TCN: Epoch 20/50 (40%)
      ‚è≥ IBKR TCN: Epoch 30/50 (60%)
      ‚è≥ WGS TCN: Epoch 30/50 (60%)
      ‚è≥ IBKR TCN: Epoch 40/50 (80%)
      ‚è≥ WGS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.593973
         RMSE: 0.770696
         R¬≤ Score: -0.7220
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IBKR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IBKR Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.176741
         RMSE: 0.420406
         R¬≤ Score: -0.0046
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WGS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WGS Random Forest: Starting GridSearchCV fit...
       ‚úÖ CYD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=209.9957 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CYD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IBKR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.0659 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IBKR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WGS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=116.6874 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WGS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CYD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=188.6263 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CYD XGBoost: Starting GridSearchCV fit...
       ‚úÖ IBKR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.7258 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IBKR XGBoost: Starting GridSearchCV fit...
       ‚úÖ WGS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=116.1304 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WGS XGBoost: Starting GridSearchCV fit...
       ‚úÖ MYRG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=61.5526 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.3753
    - TCN: MSE=0.2723
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2723
        ‚Ä¢ LSTM: MSE=0.3753
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.9862
        ‚Ä¢ XGBoost: MSE=61.5526
        ‚Ä¢ Random Forest: MSE=62.9351
   ‚úÖ MYRG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MYRG (TargetReturn): TCN with MSE=0.2723
üêõ DEBUG: MYRG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MYRG.
üêõ DEBUG: MYRG - Moving model to CPU before return...
üêõ DEBUG [22:41:19.948]: MYRG - Returning result metadata...
üêõ DEBUG: train_worker started for MNY
üêõ DEBUG [22:41:19.949]: Main received result for MYRG
  ‚öôÔ∏è Training models for MNY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - MNY: Initiating feature extraction for training.
  [DIAGNOSTIC] MNY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MNY: rows after features available: 126
üéØ MNY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MNY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MNY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MNY: Training LSTM (50 epochs)...
      ‚è≥ MNY LSTM: Epoch 10/50 (20%)
      ‚è≥ MNY LSTM: Epoch 20/50 (40%)
      ‚è≥ MNY LSTM: Epoch 30/50 (60%)
      ‚è≥ MNY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.612675
         RMSE: 0.782735
         R¬≤ Score: -1.3924 (Poor - 139.2% variance explained)
      üîπ MNY: Training TCN (50 epochs)...
      ‚è≥ MNY TCN: Epoch 10/50 (20%)
      ‚è≥ MNY TCN: Epoch 20/50 (40%)
      ‚è≥ MNY TCN: Epoch 30/50 (60%)
      ‚è≥ MNY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.415113
         RMSE: 0.644293
         R¬≤ Score: -0.6210
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MNY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MNY Random Forest: Starting GridSearchCV fit...
       ‚úÖ LIF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=156.8863 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.7081
    - TCN: MSE=0.3050
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3050
        ‚Ä¢ LSTM: MSE=0.7081
        ‚Ä¢ Random Forest: MSE=83.1354
        ‚Ä¢ LightGBM Regressor (CPU): MSE=116.5380
        ‚Ä¢ XGBoost: MSE=156.8863
   ‚úÖ LIF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LIF (TargetReturn): TCN with MSE=0.3050
üêõ DEBUG: LIF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LIF.
üêõ DEBUG: LIF - Moving model to CPU before return...
üêõ DEBUG [22:41:22.934]: LIF - Returning result metadata...
üêõ DEBUG [22:41:22.935]: Main received result for LIF
üêõ DEBUG: Training progress: 152/959 done
üêõ DEBUG: train_worker started for SPOT
  ‚öôÔ∏è Training models for SPOT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - SPOT: Initiating feature extraction for training.
  [DIAGNOSTIC] SPOT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPOT: rows after features available: 126
üéØ SPOT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPOT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPOT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPOT: Training LSTM (50 epochs)...
      ‚è≥ SPOT LSTM: Epoch 10/50 (20%)
      ‚è≥ SPOT LSTM: Epoch 20/50 (40%)
      ‚è≥ SPOT LSTM: Epoch 30/50 (60%)
      ‚è≥ SPOT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.191488
         RMSE: 0.437593
         R¬≤ Score: -0.3138 (Poor - 31.4% variance explained)
      üîπ SPOT: Training TCN (50 epochs)...
      ‚è≥ SPOT TCN: Epoch 10/50 (20%)
      ‚è≥ SPOT TCN: Epoch 20/50 (40%)
       ‚úÖ MNY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=124.6652 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MNY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SPOT TCN: Epoch 30/50 (60%)
      ‚è≥ SPOT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.186938
         RMSE: 0.432363
         R¬≤ Score: -0.2826
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPOT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPOT Random Forest: Starting GridSearchCV fit...
       ‚úÖ MNY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=297.5803 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MNY XGBoost: Starting GridSearchCV fit...
       ‚úÖ SPOT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.6378 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPOT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SPOT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=45.7869 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPOT XGBoost: Starting GridSearchCV fit...
       ‚úÖ TLN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=67.5152 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 124.7s
    - LSTM: MSE=0.5141
    - TCN: MSE=0.4083
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4083
        ‚Ä¢ LSTM: MSE=0.5141
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.8530
        ‚Ä¢ XGBoost: MSE=67.5152
        ‚Ä¢ Random Forest: MSE=72.2762
   ‚úÖ TLN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TLN (TargetReturn): TCN with MSE=0.4083
üêõ DEBUG: TLN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TLN.
üêõ DEBUG: TLN - Moving model to CPU before return...
üêõ DEBUG [22:41:36.248]: TLN - Returning result metadata...
üêõ DEBUG [22:41:36.248]: Main received result for TLN
üêõ DEBUG: train_worker started for CRK
  ‚öôÔ∏è Training models for CRK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - CRK: Initiating feature extraction for training.
  [DIAGNOSTIC] CRK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRK: rows after features available: 126
üéØ CRK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRK: Training LSTM (50 epochs)...
      ‚è≥ CRK LSTM: Epoch 10/50 (20%)
      ‚è≥ CRK LSTM: Epoch 20/50 (40%)
      ‚è≥ CRK LSTM: Epoch 30/50 (60%)
      ‚è≥ CRK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.206494
         RMSE: 0.454416
         R¬≤ Score: -0.8533 (Poor - 85.3% variance explained)
      üîπ CRK: Training TCN (50 epochs)...
      ‚è≥ CRK TCN: Epoch 10/50 (20%)
      ‚è≥ CRK TCN: Epoch 20/50 (40%)
      ‚è≥ CRK TCN: Epoch 30/50 (60%)
      ‚è≥ CRK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.142718
         RMSE: 0.377781
         R¬≤ Score: -0.2809
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRK Random Forest: Starting GridSearchCV fit...
       ‚úÖ CRK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.0267 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=48.2116 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRK XGBoost: Starting GridSearchCV fit...
       ‚úÖ VUZI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=341.1791 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.3s
    - LSTM: MSE=0.0533
    - TCN: MSE=0.0349
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0349
        ‚Ä¢ LSTM: MSE=0.0533
        ‚Ä¢ XGBoost: MSE=341.1791
        ‚Ä¢ Random Forest: MSE=443.8555
        ‚Ä¢ LightGBM Regressor (CPU): MSE=815.7187
   ‚úÖ VUZI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VUZI (TargetReturn): TCN with MSE=0.0349
üêõ DEBUG: VUZI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VUZI.
üêõ DEBUG: VUZI - Moving model to CPU before return...
üêõ DEBUG [22:41:45.104]: VUZI - Returning result metadata...
üêõ DEBUG [22:41:45.105]: Main received result for VUZI
üêõ DEBUG: train_worker started for JBL
  ‚öôÔ∏è Training models for JBL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - JBL: Initiating feature extraction for training.
  [DIAGNOSTIC] JBL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JBL: rows after features available: 126
üéØ JBL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JBL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JBL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JBL: Training LSTM (50 epochs)...
      ‚è≥ JBL LSTM: Epoch 10/50 (20%)
      ‚è≥ JBL LSTM: Epoch 20/50 (40%)
      ‚è≥ JBL LSTM: Epoch 30/50 (60%)
      ‚è≥ JBL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.631181
         RMSE: 0.794469
         R¬≤ Score: -1.1687 (Poor - 116.9% variance explained)
      üîπ JBL: Training TCN (50 epochs)...
      ‚è≥ JBL TCN: Epoch 10/50 (20%)
      ‚è≥ JBL TCN: Epoch 20/50 (40%)
      ‚è≥ JBL TCN: Epoch 30/50 (60%)
      ‚è≥ JBL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.640851
         RMSE: 0.800532
         R¬≤ Score: -1.2020
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JBL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JBL Random Forest: Starting GridSearchCV fit...
       ‚úÖ JBL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=51.9582 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JBL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JBL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.2748 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JBL XGBoost: Starting GridSearchCV fit...
       ‚úÖ UTI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.0838 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.9s
    - LSTM: MSE=0.1401
    - TCN: MSE=0.0819
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0819
        ‚Ä¢ LSTM: MSE=0.1401
        ‚Ä¢ Random Forest: MSE=17.0703
        ‚Ä¢ XGBoost: MSE=18.0838
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.7722
   ‚úÖ UTI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UTI (TargetReturn): TCN with MSE=0.0819
üêõ DEBUG: UTI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UTI.
üêõ DEBUG: UTI - Moving model to CPU before return...
üêõ DEBUG [22:41:59.757]: UTI - Returning result metadata...
üêõ DEBUG [22:41:59.757]: Main received result for UTI
üêõ DEBUG: train_worker started for BROS
  ‚öôÔ∏è Training models for BROS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - BROS: Initiating feature extraction for training.
  [DIAGNOSTIC] BROS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BROS: rows after features available: 126
üéØ BROS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BROS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BROS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BROS: Training LSTM (50 epochs)...
      ‚è≥ BROS LSTM: Epoch 10/50 (20%)
      ‚è≥ BROS LSTM: Epoch 20/50 (40%)
      ‚è≥ BROS LSTM: Epoch 30/50 (60%)
      ‚è≥ BROS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.071040
         RMSE: 0.266533
         R¬≤ Score: -0.6179 (Poor - 61.8% variance explained)
      üîπ BROS: Training TCN (50 epochs)...
      ‚è≥ BROS TCN: Epoch 10/50 (20%)
      ‚è≥ BROS TCN: Epoch 20/50 (40%)
      ‚è≥ BROS TCN: Epoch 30/50 (60%)
      ‚è≥ BROS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.051216
         RMSE: 0.226309
         R¬≤ Score: -0.1664
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BROS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BROS Random Forest: Starting GridSearchCV fit...
       ‚úÖ BROS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=61.2641 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BROS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BROS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=96.0447 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BROS XGBoost: Starting GridSearchCV fit...
       ‚úÖ LINC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.5972 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.7s
    - LSTM: MSE=0.3944
    - TCN: MSE=0.2781
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2781
        ‚Ä¢ LSTM: MSE=0.3944
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.1215
        ‚Ä¢ Random Forest: MSE=32.1089
        ‚Ä¢ XGBoost: MSE=47.5972
   ‚úÖ LINC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LINC (TargetReturn): TCN with MSE=0.2781
üêõ DEBUG: LINC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LINC.
üêõ DEBUG: LINC - Moving model to CPU before return...
üêõ DEBUG [22:42:16.966]: LINC - Returning result metadata...
üêõ DEBUG: train_worker started for CVAC
  ‚öôÔ∏è Training models for CVAC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CVAC: Initiating feature extraction for training.
  [DIAGNOSTIC] CVAC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CVAC: rows after features available: 126
üéØ CVAC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CVAC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CVAC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CVAC: Training LSTM (50 epochs)...
       ‚úÖ AU XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=48.4035 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.5s
    - LSTM: MSE=0.2582
    - TCN: MSE=0.1826
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1826
        ‚Ä¢ LSTM: MSE=0.2582
        ‚Ä¢ XGBoost: MSE=48.4035
        ‚Ä¢ Random Forest: MSE=67.1487
        ‚Ä¢ LightGBM Regressor (CPU): MSE=93.4486
   ‚úÖ AU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AU (TargetReturn): TCN with MSE=0.1826
üêõ DEBUG: AU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AU.
üêõ DEBUG: AU - Moving model to CPU before return...
üêõ DEBUG [22:42:17.187]: AU - Returning result metadata...
üêõ DEBUG [22:42:17.187]: Main received result for AU
üêõ DEBUG: Training progress: 156/959 done
üêõ DEBUG [22:42:17.187]: Main received result for LINC
üêõ DEBUG: train_worker started for GHM
  ‚öôÔ∏è Training models for GHM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - GHM: Initiating feature extraction for training.
  [DIAGNOSTIC] GHM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GHM: rows after features available: 126
üéØ GHM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GHM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GHM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GHM: Training LSTM (50 epochs)...
      ‚è≥ CVAC LSTM: Epoch 10/50 (20%)
      ‚è≥ GHM LSTM: Epoch 10/50 (20%)
      ‚è≥ CVAC LSTM: Epoch 20/50 (40%)
      ‚è≥ GHM LSTM: Epoch 20/50 (40%)
      ‚è≥ CVAC LSTM: Epoch 30/50 (60%)
      ‚è≥ GHM LSTM: Epoch 30/50 (60%)
      ‚è≥ CVAC LSTM: Epoch 40/50 (80%)
      ‚è≥ GHM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.480204
         RMSE: 0.692968
         R¬≤ Score: -0.8930 (Poor - 89.3% variance explained)
      üîπ CVAC: Training TCN (50 epochs)...
      ‚è≥ CVAC TCN: Epoch 10/50 (20%)
      ‚è≥ CVAC TCN: Epoch 20/50 (40%)
      ‚è≥ CVAC TCN: Epoch 30/50 (60%)
      ‚è≥ CVAC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.455454
         RMSE: 0.674873
         R¬≤ Score: -0.7954
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CVAC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CVAC Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.654726
         RMSE: 0.809151
         R¬≤ Score: -0.9758 (Poor - 97.6% variance explained)
      üîπ GHM: Training TCN (50 epochs)...
      ‚è≥ GHM TCN: Epoch 10/50 (20%)
      ‚è≥ GHM TCN: Epoch 20/50 (40%)
      ‚è≥ GHM TCN: Epoch 30/50 (60%)
      ‚è≥ GHM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.677006
         RMSE: 0.822804
         R¬≤ Score: -1.0430
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GHM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GHM Random Forest: Starting GridSearchCV fit...
       ‚úÖ CVNA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=59.6736 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.6s
    - LSTM: MSE=0.3903
    - TCN: MSE=0.2365
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2365
        ‚Ä¢ LSTM: MSE=0.3903
        ‚Ä¢ XGBoost: MSE=59.6736
        ‚Ä¢ Random Forest: MSE=84.8861
        ‚Ä¢ LightGBM Regressor (CPU): MSE=115.7326
   ‚úÖ CVNA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CVNA (TargetReturn): TCN with MSE=0.2365
üêõ DEBUG: CVNA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CVNA.
üêõ DEBUG: CVNA - Moving model to CPU before return...
üêõ DEBUG [22:42:22.838]: CVNA - Returning result metadata...
üêõ DEBUG: train_worker started for ARKW
üêõ DEBUG [22:42:22.839]: Main received result for CVNA
  ‚öôÔ∏è Training models for ARKW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ARKW: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKW: rows after features available: 126
üéØ ARKW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKW: Training LSTM (50 epochs)...
       ‚úÖ CVAC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.8813 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CVAC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ARKW LSTM: Epoch 10/50 (20%)
       ‚úÖ GHM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.9968 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GHM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ARKW LSTM: Epoch 20/50 (40%)
       ‚úÖ CVAC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=55.1470 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CVAC XGBoost: Starting GridSearchCV fit...
       ‚úÖ GHM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.6375 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GHM XGBoost: Starting GridSearchCV fit...
      ‚è≥ ARKW LSTM: Epoch 30/50 (60%)
      ‚è≥ ARKW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.688354
         RMSE: 0.829671
         R¬≤ Score: -0.9697 (Poor - 97.0% variance explained)
      üîπ ARKW: Training TCN (50 epochs)...
      ‚è≥ ARKW TCN: Epoch 10/50 (20%)
      ‚è≥ ARKW TCN: Epoch 20/50 (40%)
      ‚è≥ ARKW TCN: Epoch 30/50 (60%)
      ‚è≥ ARKW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.657492
         RMSE: 0.810859
         R¬≤ Score: -0.8814
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKW Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.0547 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARKW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.1021 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKW XGBoost: Starting GridSearchCV fit...
       ‚úÖ AAOI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=185.4107 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 126.0s
    - LSTM: MSE=0.7424
    - TCN: MSE=0.4007
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4007
        ‚Ä¢ LSTM: MSE=0.7424
        ‚Ä¢ Random Forest: MSE=109.9049
        ‚Ä¢ XGBoost: MSE=185.4107
        ‚Ä¢ LightGBM Regressor (CPU): MSE=250.2210
   ‚úÖ AAOI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AAOI (TargetReturn): TCN with MSE=0.4007
üêõ DEBUG: AAOI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AAOI.
üêõ DEBUG: AAOI - Moving model to CPU before return...
üêõ DEBUG [22:42:33.216]: AAOI - Returning result metadata...
üêõ DEBUG [22:42:33.217]: Main received result for AAOI
üêõ DEBUG: train_worker started for STRL
  ‚öôÔ∏è Training models for STRL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - STRL: Initiating feature extraction for training.
  [DIAGNOSTIC] STRL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STRL: rows after features available: 126
üéØ STRL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STRL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STRL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STRL: Training LSTM (50 epochs)...
      ‚è≥ STRL LSTM: Epoch 10/50 (20%)
      ‚è≥ STRL LSTM: Epoch 20/50 (40%)
      ‚è≥ STRL LSTM: Epoch 30/50 (60%)
      ‚è≥ STRL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.307142
         RMSE: 0.554204
         R¬≤ Score: -1.0989 (Poor - 109.9% variance explained)
      üîπ STRL: Training TCN (50 epochs)...
      ‚è≥ STRL TCN: Epoch 10/50 (20%)
      ‚è≥ STRL TCN: Epoch 20/50 (40%)
      ‚è≥ STRL TCN: Epoch 30/50 (60%)
      ‚è≥ STRL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.221390
         RMSE: 0.470521
         R¬≤ Score: -0.5129
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STRL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STRL Random Forest: Starting GridSearchCV fit...
       ‚úÖ CCB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.9322 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.9s
    - LSTM: MSE=0.2868
    - TCN: MSE=0.1982
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1982
        ‚Ä¢ LSTM: MSE=0.2868
        ‚Ä¢ Random Forest: MSE=25.0011
        ‚Ä¢ XGBoost: MSE=25.9322
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.0826
   ‚úÖ CCB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CCB (TargetReturn): TCN with MSE=0.1982
üêõ DEBUG: CCB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CCB.
üêõ DEBUG: CCB - Moving model to CPU before return...
üêõ DEBUG [22:42:39.540]: CCB - Returning result metadata...
üêõ DEBUG: train_worker started for BTCW
üêõ DEBUG [22:42:39.542]: Main received result for CCB
üêõ DEBUG: Training progress: 160/959 done
  ‚öôÔ∏è Training models for BTCW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - BTCW: Initiating feature extraction for training.
  [DIAGNOSTIC] BTCW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BTCW: rows after features available: 126
üéØ BTCW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BTCW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BTCW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BTCW: Training LSTM (50 epochs)...
       ‚úÖ STRL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=55.7689 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STRL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BTCW LSTM: Epoch 10/50 (20%)
       ‚úÖ STRL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=56.6891 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STRL XGBoost: Starting GridSearchCV fit...
      ‚è≥ BTCW LSTM: Epoch 20/50 (40%)
      ‚è≥ BTCW LSTM: Epoch 30/50 (60%)
      ‚è≥ BTCW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.306642
         RMSE: 0.553753
         R¬≤ Score: -1.1726 (Poor - 117.3% variance explained)
      üîπ BTCW: Training TCN (50 epochs)...
      ‚è≥ BTCW TCN: Epoch 10/50 (20%)
      ‚è≥ BTCW TCN: Epoch 20/50 (40%)
      ‚è≥ BTCW TCN: Epoch 30/50 (60%)
      ‚è≥ BTCW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.227611
         RMSE: 0.477086
         R¬≤ Score: -0.6127
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BTCW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BTCW Random Forest: Starting GridSearchCV fit...
       ‚úÖ MGIC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.1209 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.1s
    - LSTM: MSE=0.5757
    - TCN: MSE=0.4269
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4269
        ‚Ä¢ LSTM: MSE=0.5757
        ‚Ä¢ XGBoost: MSE=21.1209
        ‚Ä¢ Random Forest: MSE=21.3883
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.4919
   ‚úÖ MGIC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MGIC (TargetReturn): TCN with MSE=0.4269
üêõ DEBUG: MGIC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MGIC.
üêõ DEBUG: MGIC - Moving model to CPU before return...
üêõ DEBUG [22:42:44.353]: MGIC - Returning result metadata...
üêõ DEBUG: train_worker started for HODL
üêõ DEBUG [22:42:44.355]: Main received result for MGIC
  ‚öôÔ∏è Training models for HODL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - HODL: Initiating feature extraction for training.
  [DIAGNOSTIC] HODL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HODL: rows after features available: 126
üéØ HODL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HODL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HODL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HODL: Training LSTM (50 epochs)...
      ‚è≥ HODL LSTM: Epoch 10/50 (20%)
       ‚úÖ BTCW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.1412 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BTCW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ HODL LSTM: Epoch 20/50 (40%)
       ‚úÖ BTCW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.7706 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BTCW XGBoost: Starting GridSearchCV fit...
      ‚è≥ HODL LSTM: Epoch 30/50 (60%)
      ‚è≥ HODL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.268935
         RMSE: 0.518590
         R¬≤ Score: -0.9246 (Poor - 92.5% variance explained)
      üîπ HODL: Training TCN (50 epochs)...
      ‚è≥ HODL TCN: Epoch 10/50 (20%)
      ‚è≥ HODL TCN: Epoch 20/50 (40%)
      ‚è≥ HODL TCN: Epoch 30/50 (60%)
      ‚è≥ HODL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.221971
         RMSE: 0.471138
         R¬≤ Score: -0.5885
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HODL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HODL Random Forest: Starting GridSearchCV fit...
       ‚úÖ HODL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.2066 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HODL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HWM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.7799 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.0s
    - LSTM: MSE=0.2901
    - TCN: MSE=0.2491
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2491
        ‚Ä¢ LSTM: MSE=0.2901
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.9422
        ‚Ä¢ XGBoost: MSE=25.7799
        ‚Ä¢ Random Forest: MSE=27.6696
   ‚úÖ HWM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HWM (TargetReturn): TCN with MSE=0.2491
üêõ DEBUG: HWM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HWM.
üêõ DEBUG: HWM - Moving model to CPU before return...
üêõ DEBUG [22:42:50.967]: HWM - Returning result metadata...
üêõ DEBUG: train_worker started for AXON
üêõ DEBUG [22:42:50.968]: Main received result for HWM
       ‚úÖ HODL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.8718 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HODL XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for AXON (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AXON: Initiating feature extraction for training.
  [DIAGNOSTIC] AXON: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AXON: rows after features available: 126
üéØ AXON: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AXON: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AXON: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AXON: Training LSTM (50 epochs)...
      ‚è≥ AXON LSTM: Epoch 10/50 (20%)
      ‚è≥ AXON LSTM: Epoch 20/50 (40%)
      ‚è≥ AXON LSTM: Epoch 30/50 (60%)
      ‚è≥ AXON LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.367558
         RMSE: 0.606266
         R¬≤ Score: -0.5306 (Poor - 53.1% variance explained)
      üîπ AXON: Training TCN (50 epochs)...
      ‚è≥ AXON TCN: Epoch 10/50 (20%)
      ‚è≥ AXON TCN: Epoch 20/50 (40%)
      ‚è≥ AXON TCN: Epoch 30/50 (60%)
      ‚è≥ AXON TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.323171
         RMSE: 0.568481
         R¬≤ Score: -0.3458
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AXON: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AXON Random Forest: Starting GridSearchCV fit...
       ‚úÖ AXON Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=54.0723 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AXON LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AXON LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=38.0195 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AXON XGBoost: Starting GridSearchCV fit...
       ‚úÖ IBKR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.7142 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.5s
    - LSTM: MSE=0.7121
    - TCN: MSE=0.5940
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5940
        ‚Ä¢ LSTM: MSE=0.7121
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.7258
        ‚Ä¢ Random Forest: MSE=31.0659
        ‚Ä¢ XGBoost: MSE=33.7142
   ‚úÖ IBKR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IBKR (TargetReturn): TCN with MSE=0.5940
üêõ DEBUG: IBKR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IBKR.
üêõ DEBUG: IBKR - Moving model to CPU before return...
üêõ DEBUG [22:42:59.008]: IBKR - Returning result metadata...
üêõ DEBUG: train_worker started for EZBC
  ‚öôÔ∏è Training models for EZBC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - EZBC: Initiating feature extraction for training.
  [DIAGNOSTIC] EZBC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EZBC: rows after features available: 126
üéØ EZBC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EZBC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EZBC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EZBC: Training LSTM (50 epochs)...
       ‚úÖ CYD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=226.2025 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.9s
    - LSTM: MSE=0.2787
    - TCN: MSE=0.1416
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1416
        ‚Ä¢ LSTM: MSE=0.2787
        ‚Ä¢ LightGBM Regressor (CPU): MSE=188.6263
        ‚Ä¢ Random Forest: MSE=209.9957
        ‚Ä¢ XGBoost: MSE=226.2025
   ‚úÖ CYD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CYD (TargetReturn): TCN with MSE=0.1416
üêõ DEBUG: CYD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CYD.
üêõ DEBUG: CYD - Moving model to CPU before return...
üêõ DEBUG [22:42:59.247]: CYD - Returning result metadata...
üêõ DEBUG: train_worker started for BITB
üêõ DEBUG [22:42:59.255]: Main received result for CYD
üêõ DEBUG [22:42:59.255]: Main received result for IBKR
üêõ DEBUG: Training progress: 164/959 done
  ‚öôÔ∏è Training models for BITB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - BITB: Initiating feature extraction for training.
  [DIAGNOSTIC] BITB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BITB: rows after features available: 126
üéØ BITB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BITB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BITB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BITB: Training LSTM (50 epochs)...
      ‚è≥ EZBC LSTM: Epoch 10/50 (20%)
      ‚è≥ BITB LSTM: Epoch 10/50 (20%)
      ‚è≥ EZBC LSTM: Epoch 20/50 (40%)
      ‚è≥ BITB LSTM: Epoch 20/50 (40%)
      ‚è≥ EZBC LSTM: Epoch 30/50 (60%)
      ‚è≥ BITB LSTM: Epoch 30/50 (60%)
      ‚è≥ EZBC LSTM: Epoch 40/50 (80%)
      ‚è≥ BITB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.346244
         RMSE: 0.588425
         R¬≤ Score: -1.4773 (Poor - 147.7% variance explained)
      üîπ EZBC: Training TCN (50 epochs)...
      ‚è≥ EZBC TCN: Epoch 10/50 (20%)
      ‚è≥ EZBC TCN: Epoch 20/50 (40%)
      ‚è≥ EZBC TCN: Epoch 30/50 (60%)
      ‚è≥ EZBC TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.376243
         RMSE: 0.613387
         R¬≤ Score: -1.6907 (Poor - 169.1% variance explained)
      üîπ BITB: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.223012
         RMSE: 0.472241
         R¬≤ Score: -0.5956
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EZBC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EZBC Random Forest: Starting GridSearchCV fit...
      ‚è≥ BITB TCN: Epoch 10/50 (20%)
       ‚úÖ WGS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=136.1174 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 133.6s
    - LSTM: MSE=0.2974
    - TCN: MSE=0.1767
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1767
        ‚Ä¢ LSTM: MSE=0.2974
        ‚Ä¢ LightGBM Regressor (CPU): MSE=116.1304
        ‚Ä¢ Random Forest: MSE=116.6874
        ‚Ä¢ XGBoost: MSE=136.1174
   ‚úÖ WGS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WGS (TargetReturn): TCN with MSE=0.1767
üêõ DEBUG: WGS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WGS.
üêõ DEBUG: WGS - Moving model to CPU before return...
üêõ DEBUG [22:43:02.221]: WGS - Returning result metadata...
üêõ DEBUG [22:43:02.221]: Main received result for WGS
üêõ DEBUG: train_worker started for BRRR
      ‚è≥ BITB TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for BRRR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BRRR: Initiating feature extraction for training.
  [DIAGNOSTIC] BRRR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BRRR: rows after features available: 126
üéØ BRRR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BRRR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BRRR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BRRR: Training LSTM (50 epochs)...
      ‚è≥ BITB TCN: Epoch 30/50 (60%)
      ‚è≥ BITB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.225091
         RMSE: 0.474438
         R¬≤ Score: -0.6097
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BITB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BITB Random Forest: Starting GridSearchCV fit...
      ‚è≥ BRRR LSTM: Epoch 10/50 (20%)
      ‚è≥ BRRR LSTM: Epoch 20/50 (40%)
      ‚è≥ BRRR LSTM: Epoch 30/50 (60%)
      ‚è≥ BRRR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.304574
         RMSE: 0.551883
         R¬≤ Score: -1.1821 (Poor - 118.2% variance explained)
      üîπ BRRR: Training TCN (50 epochs)...
      ‚è≥ BRRR TCN: Epoch 10/50 (20%)
       ‚úÖ EZBC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.5210 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EZBC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BRRR TCN: Epoch 20/50 (40%)
      ‚è≥ BRRR TCN: Epoch 30/50 (60%)
       ‚úÖ BITB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.2921 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BITB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BRRR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.226806
         RMSE: 0.476242
         R¬≤ Score: -0.6249
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BRRR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BRRR Random Forest: Starting GridSearchCV fit...
       ‚úÖ EZBC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.1140 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EZBC XGBoost: Starting GridSearchCV fit...
       ‚úÖ BITB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.3307 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BITB XGBoost: Starting GridSearchCV fit...
       ‚úÖ BRRR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.5109 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BRRR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BRRR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.0407 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BRRR XGBoost: Starting GridSearchCV fit...
       ‚úÖ MNY XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=109.8472 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.1s
    - LSTM: MSE=0.6127
    - TCN: MSE=0.4151
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4151
        ‚Ä¢ LSTM: MSE=0.6127
        ‚Ä¢ XGBoost: MSE=109.8472
        ‚Ä¢ Random Forest: MSE=124.6652
        ‚Ä¢ LightGBM Regressor (CPU): MSE=297.5803
   ‚úÖ MNY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MNY (TargetReturn): TCN with MSE=0.4151
üêõ DEBUG: MNY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MNY.
üêõ DEBUG: MNY - Moving model to CPU before return...
üêõ DEBUG [22:43:32.662]: MNY - Returning result metadata...
üêõ DEBUG [22:43:32.663]: Main received result for MNY
üêõ DEBUG: train_worker started for IBIT
  ‚öôÔ∏è Training models for IBIT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - IBIT: Initiating feature extraction for training.
  [DIAGNOSTIC] IBIT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IBIT: rows after features available: 126
üéØ IBIT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IBIT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IBIT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IBIT: Training LSTM (50 epochs)...
      ‚è≥ IBIT LSTM: Epoch 10/50 (20%)
      ‚è≥ IBIT LSTM: Epoch 20/50 (40%)
      ‚è≥ IBIT LSTM: Epoch 30/50 (60%)
      ‚è≥ IBIT LSTM: Epoch 40/50 (80%)
       ‚úÖ SPOT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.5453 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 124.9s
    - LSTM: MSE=0.1915
    - TCN: MSE=0.1869
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1869
        ‚Ä¢ LSTM: MSE=0.1915
        ‚Ä¢ Random Forest: MSE=37.6378
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.7869
        ‚Ä¢ XGBoost: MSE=47.5453
   ‚úÖ SPOT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPOT (TargetReturn): TCN with MSE=0.1869
üêõ DEBUG: SPOT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPOT.
üêõ DEBUG: SPOT - Moving model to CPU before return...
üêõ DEBUG [22:43:34.991]: SPOT - Returning result metadata...
üêõ DEBUG [22:43:34.991]: Main received result for SPOT
üêõ DEBUG: train_worker started for FBTC
  ‚öôÔ∏è Training models for FBTC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FBTC: Initiating feature extraction for training.
  [DIAGNOSTIC] FBTC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FBTC: rows after features available: 126
üéØ FBTC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FBTC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FBTC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FBTC: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.343182
         RMSE: 0.585817
         R¬≤ Score: -1.4564 (Poor - 145.6% variance explained)
      üîπ IBIT: Training TCN (50 epochs)...
      ‚è≥ IBIT TCN: Epoch 10/50 (20%)
      ‚è≥ IBIT TCN: Epoch 20/50 (40%)
      ‚è≥ FBTC LSTM: Epoch 10/50 (20%)
      ‚è≥ IBIT TCN: Epoch 30/50 (60%)
      ‚è≥ IBIT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.182765
         RMSE: 0.427510
         R¬≤ Score: -0.3082
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IBIT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IBIT Random Forest: Starting GridSearchCV fit...
      ‚è≥ FBTC LSTM: Epoch 20/50 (40%)
      ‚è≥ FBTC LSTM: Epoch 30/50 (60%)
      ‚è≥ FBTC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.310836
         RMSE: 0.557527
         R¬≤ Score: -1.2264 (Poor - 122.6% variance explained)
      üîπ FBTC: Training TCN (50 epochs)...
      ‚è≥ FBTC TCN: Epoch 10/50 (20%)
      ‚è≥ FBTC TCN: Epoch 20/50 (40%)
      ‚è≥ FBTC TCN: Epoch 30/50 (60%)
      ‚è≥ FBTC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180658
         RMSE: 0.425039
         R¬≤ Score: -0.2940
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FBTC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FBTC Random Forest: Starting GridSearchCV fit...
       ‚úÖ IBIT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.3919 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IBIT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IBIT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.1698 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IBIT XGBoost: Starting GridSearchCV fit...
       ‚úÖ FBTC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.6724 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FBTC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FBTC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.6221 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FBTC XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.0691 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.8s
    - LSTM: MSE=0.2065
    - TCN: MSE=0.1427
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1427
        ‚Ä¢ LSTM: MSE=0.2065
        ‚Ä¢ Random Forest: MSE=42.0267
        ‚Ä¢ XGBoost: MSE=44.0691
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.2116
   ‚úÖ CRK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRK (TargetReturn): TCN with MSE=0.1427
üêõ DEBUG: CRK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRK.
üêõ DEBUG: CRK - Moving model to CPU before return...
üêõ DEBUG [22:43:49.385]: CRK - Returning result metadata...
üêõ DEBUG: train_worker started for ARKB
üêõ DEBUG [22:43:49.391]: Main received result for CRK
üêõ DEBUG: Training progress: 168/959 done
  ‚öôÔ∏è Training models for ARKB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - ARKB: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKB: rows after features available: 126
üéØ ARKB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKB: Training LSTM (50 epochs)...
      ‚è≥ ARKB LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKB LSTM: Epoch 20/50 (40%)
      ‚è≥ ARKB LSTM: Epoch 30/50 (60%)
      ‚è≥ ARKB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.383601
         RMSE: 0.619355
         R¬≤ Score: -1.7510 (Poor - 175.1% variance explained)
      üîπ ARKB: Training TCN (50 epochs)...
      ‚è≥ ARKB TCN: Epoch 10/50 (20%)
      ‚è≥ ARKB TCN: Epoch 20/50 (40%)
      ‚è≥ ARKB TCN: Epoch 30/50 (60%)
      ‚è≥ ARKB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.187501
         RMSE: 0.433014
         R¬≤ Score: -0.3447
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKB Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.0385 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARKB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.0296 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKB XGBoost: Starting GridSearchCV fit...
       ‚úÖ JBL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.1814 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.3s
    - LSTM: MSE=0.6312
    - TCN: MSE=0.6409
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6312
        ‚Ä¢ TCN: MSE=0.6409
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.2748
        ‚Ä¢ XGBoost: MSE=41.1814
        ‚Ä¢ Random Forest: MSE=51.9582
   ‚úÖ JBL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JBL (TargetReturn): LSTM with MSE=0.6312
üêõ DEBUG: JBL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JBL.
üêõ DEBUG: JBL - Moving model to CPU before return...
üêõ DEBUG [22:43:56.764]: JBL - Returning result metadata...
üêõ DEBUG [22:43:56.764]: Main received result for JBL
üêõ DEBUG: train_worker started for BTCO
  ‚öôÔ∏è Training models for BTCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - BTCO: Initiating feature extraction for training.
  [DIAGNOSTIC] BTCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BTCO: rows after features available: 126
üéØ BTCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BTCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BTCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BTCO: Training LSTM (50 epochs)...
      ‚è≥ BTCO LSTM: Epoch 10/50 (20%)
      ‚è≥ BTCO LSTM: Epoch 20/50 (40%)
      ‚è≥ BTCO LSTM: Epoch 30/50 (60%)
      ‚è≥ BTCO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.303659
         RMSE: 0.551053
         R¬≤ Score: -1.1646 (Poor - 116.5% variance explained)
      üîπ BTCO: Training TCN (50 epochs)...
      ‚è≥ BTCO TCN: Epoch 10/50 (20%)
      ‚è≥ BTCO TCN: Epoch 20/50 (40%)
      ‚è≥ BTCO TCN: Epoch 30/50 (60%)
      ‚è≥ BTCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.169470
         RMSE: 0.411668
         R¬≤ Score: -0.2081
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BTCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BTCO Random Forest: Starting GridSearchCV fit...
       ‚úÖ BTCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.1398 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BTCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BTCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.0097 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BTCO XGBoost: Starting GridSearchCV fit...
       ‚úÖ BROS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=70.0764 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 129.0s
    - LSTM: MSE=0.0710
    - TCN: MSE=0.0512
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0512
        ‚Ä¢ LSTM: MSE=0.0710
        ‚Ä¢ Random Forest: MSE=61.2641
        ‚Ä¢ XGBoost: MSE=70.0764
        ‚Ä¢ LightGBM Regressor (CPU): MSE=96.0447
   ‚úÖ BROS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BROS (TargetReturn): TCN with MSE=0.0512
üêõ DEBUG: BROS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BROS.
üêõ DEBUG: BROS - Moving model to CPU before return...
üêõ DEBUG [22:44:15.800]: BROS - Returning result metadata...
üêõ DEBUG [22:44:15.800]: Main received result for BROS
üêõ DEBUG: train_worker started for REAL
  ‚öôÔ∏è Training models for REAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - REAL: Initiating feature extraction for training.
  [DIAGNOSTIC] REAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ REAL: rows after features available: 126
üéØ REAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] REAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö REAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ REAL: Training LSTM (50 epochs)...
      ‚è≥ REAL LSTM: Epoch 10/50 (20%)
      ‚è≥ REAL LSTM: Epoch 20/50 (40%)
      ‚è≥ REAL LSTM: Epoch 30/50 (60%)
      ‚è≥ REAL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.034738
         RMSE: 0.186382
         R¬≤ Score: -1.9557 (Poor - 195.6% variance explained)
      üîπ REAL: Training TCN (50 epochs)...
      ‚è≥ REAL TCN: Epoch 10/50 (20%)
      ‚è≥ REAL TCN: Epoch 20/50 (40%)
      ‚è≥ REAL TCN: Epoch 30/50 (60%)
      ‚è≥ REAL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.012858
         RMSE: 0.113391
         R¬≤ Score: -0.0940
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä REAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ REAL Random Forest: Starting GridSearchCV fit...
       ‚úÖ REAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=51.4637 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ REAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ REAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=222.3129 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ REAL XGBoost: Starting GridSearchCV fit...
       ‚úÖ CVAC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=56.0289 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.9s
    - LSTM: MSE=0.4802
    - TCN: MSE=0.4555
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4555
        ‚Ä¢ LSTM: MSE=0.4802
        ‚Ä¢ Random Forest: MSE=43.8813
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.1470
        ‚Ä¢ XGBoost: MSE=56.0289
   ‚úÖ CVAC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CVAC (TargetReturn): TCN with MSE=0.4555
üêõ DEBUG: CVAC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CVAC.
üêõ DEBUG: CVAC - Moving model to CPU before return...
üêõ DEBUG [22:44:30.807]: CVAC - Returning result metadata...
üêõ DEBUG: train_worker started for SAN
üêõ DEBUG [22:44:30.807]: Main received result for CVAC
  ‚öôÔ∏è Training models for SAN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - SAN: Initiating feature extraction for training.
  [DIAGNOSTIC] SAN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SAN: rows after features available: 126
üéØ SAN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SAN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SAN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SAN: Training LSTM (50 epochs)...
      ‚è≥ SAN LSTM: Epoch 10/50 (20%)
       ‚úÖ GHM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.3969 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 127.7s
    - LSTM: MSE=0.6547
    - TCN: MSE=0.6770
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6547
        ‚Ä¢ TCN: MSE=0.6770
        ‚Ä¢ Random Forest: MSE=31.9968
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.6375
        ‚Ä¢ XGBoost: MSE=40.3969
   ‚úÖ GHM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GHM (TargetReturn): LSTM with MSE=0.6547
üêõ DEBUG: GHM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GHM.
üêõ DEBUG: GHM - Moving model to CPU before return...
üêõ DEBUG [22:44:31.864]: GHM - Returning result metadata...
üêõ DEBUG [22:44:31.865]: Main received result for GHM
üêõ DEBUG: Training progress: 172/959 done
üêõ DEBUG: train_worker started for AMSC
  ‚öôÔ∏è Training models for AMSC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - AMSC: Initiating feature extraction for training.
  [DIAGNOSTIC] AMSC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMSC: rows after features available: 126
üéØ AMSC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMSC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMSC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMSC: Training LSTM (50 epochs)...
      ‚è≥ SAN LSTM: Epoch 20/50 (40%)
      ‚è≥ AMSC LSTM: Epoch 10/50 (20%)
      ‚è≥ SAN LSTM: Epoch 30/50 (60%)
      ‚è≥ AMSC LSTM: Epoch 20/50 (40%)
      ‚è≥ SAN LSTM: Epoch 40/50 (80%)
      ‚è≥ AMSC LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.124968
         RMSE: 0.353508
         R¬≤ Score: -0.4305 (Poor - 43.1% variance explained)
      üîπ SAN: Training TCN (50 epochs)...
      ‚è≥ SAN TCN: Epoch 10/50 (20%)
      ‚è≥ SAN TCN: Epoch 20/50 (40%)
      ‚è≥ SAN TCN: Epoch 30/50 (60%)
      ‚è≥ AMSC LSTM: Epoch 40/50 (80%)
      ‚è≥ SAN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.090152
         RMSE: 0.300254
         R¬≤ Score: -0.0320
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SAN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SAN Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.904976
         RMSE: 0.951302
         R¬≤ Score: -1.4177 (Poor - 141.8% variance explained)
      üîπ AMSC: Training TCN (50 epochs)...
      ‚è≥ AMSC TCN: Epoch 10/50 (20%)
      ‚è≥ AMSC TCN: Epoch 20/50 (40%)
      ‚è≥ AMSC TCN: Epoch 30/50 (60%)
      ‚è≥ AMSC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.484181
         RMSE: 0.695831
         R¬≤ Score: -0.2935
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMSC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMSC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.6647 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.0s
    - LSTM: MSE=0.6884
    - TCN: MSE=0.6575
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6575
        ‚Ä¢ LSTM: MSE=0.6884
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.1021
        ‚Ä¢ Random Forest: MSE=34.0547
        ‚Ä¢ XGBoost: MSE=36.6647
   ‚úÖ ARKW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKW (TargetReturn): TCN with MSE=0.6575
üêõ DEBUG: ARKW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKW.
üêõ DEBUG: ARKW - Moving model to CPU before return...
üêõ DEBUG [22:44:36.286]: ARKW - Returning result metadata...
üêõ DEBUG [22:44:36.287]: Main received result for ARKWüêõ DEBUG: train_worker started for SKE

  ‚öôÔ∏è Training models for SKE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - SKE: Initiating feature extraction for training.
  [DIAGNOSTIC] SKE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SKE: rows after features available: 126
üéØ SKE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SKE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SKE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SKE: Training LSTM (50 epochs)...
      ‚è≥ SKE LSTM: Epoch 10/50 (20%)
       ‚úÖ SAN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.1765 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SAN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SKE LSTM: Epoch 20/50 (40%)
       ‚úÖ SAN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.3217 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SAN XGBoost: Starting GridSearchCV fit...
      ‚è≥ SKE LSTM: Epoch 30/50 (60%)
       ‚úÖ AMSC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=97.6323 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMSC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SKE LSTM: Epoch 40/50 (80%)
       ‚úÖ AMSC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=136.5922 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMSC XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.304095
         RMSE: 0.551448
         R¬≤ Score: -1.0601 (Poor - 106.0% variance explained)
      üîπ SKE: Training TCN (50 epochs)...
      ‚è≥ SKE TCN: Epoch 10/50 (20%)
      ‚è≥ SKE TCN: Epoch 20/50 (40%)
      ‚è≥ SKE TCN: Epoch 30/50 (60%)
      ‚è≥ SKE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.161838
         RMSE: 0.402290
         R¬≤ Score: -0.0964
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SKE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SKE Random Forest: Starting GridSearchCV fit...
       ‚úÖ SKE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.0199 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SKE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SKE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=46.8354 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SKE XGBoost: Starting GridSearchCV fit...
       ‚úÖ STRL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=49.5687 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.3s
    - LSTM: MSE=0.3071
    - TCN: MSE=0.2214
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2214
        ‚Ä¢ LSTM: MSE=0.3071
        ‚Ä¢ XGBoost: MSE=49.5687
        ‚Ä¢ Random Forest: MSE=55.7689
        ‚Ä¢ LightGBM Regressor (CPU): MSE=56.6891
   ‚úÖ STRL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STRL (TargetReturn): TCN with MSE=0.2214
üêõ DEBUG: STRL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STRL.
üêõ DEBUG: STRL - Moving model to CPU before return...
üêõ DEBUG [22:44:47.658]: STRL - Returning result metadata...
üêõ DEBUG [22:44:47.658]: Main received result for STRL
üêõ DEBUG: train_worker started for DEFI
  ‚öôÔ∏è Training models for DEFI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - DEFI: Initiating feature extraction for training.
  [DIAGNOSTIC] DEFI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DEFI: rows after features available: 126
üéØ DEFI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DEFI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DEFI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DEFI: Training LSTM (50 epochs)...
      ‚è≥ DEFI LSTM: Epoch 10/50 (20%)
      ‚è≥ DEFI LSTM: Epoch 20/50 (40%)
      ‚è≥ DEFI LSTM: Epoch 30/50 (60%)
      ‚è≥ DEFI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.342321
         RMSE: 0.585082
         R¬≤ Score: -1.5659 (Poor - 156.6% variance explained)
      üîπ DEFI: Training TCN (50 epochs)...
      ‚è≥ DEFI TCN: Epoch 10/50 (20%)
      ‚è≥ DEFI TCN: Epoch 20/50 (40%)
       ‚úÖ BTCW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.1374 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.5s
    - LSTM: MSE=0.3066
    - TCN: MSE=0.2276
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2276
        ‚Ä¢ LSTM: MSE=0.3066
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.7706
        ‚Ä¢ Random Forest: MSE=24.1412
        ‚Ä¢ XGBoost: MSE=28.1374
   ‚úÖ BTCW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BTCW (TargetReturn): TCN with MSE=0.2276
üêõ DEBUG: BTCW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BTCW.
üêõ DEBUG: BTCW - Moving model to CPU before return...
üêõ DEBUG [22:44:50.393]: BTCW - Returning result metadata...
üêõ DEBUG [22:44:50.394]: Main received result for BTCW
üêõ DEBUG: train_worker started for OM
      ‚è≥ DEFI TCN: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for OM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - OM: Initiating feature extraction for training.
  [DIAGNOSTIC] OM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OM: rows after features available: 126
üéØ OM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OM: Training LSTM (50 epochs)...
      ‚è≥ DEFI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.172746
         RMSE: 0.415628
         R¬≤ Score: -0.2948
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DEFI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DEFI Random Forest: Starting GridSearchCV fit...
      ‚è≥ OM LSTM: Epoch 10/50 (20%)
      ‚è≥ OM LSTM: Epoch 20/50 (40%)
      ‚è≥ OM LSTM: Epoch 30/50 (60%)
      ‚è≥ OM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.487739
         RMSE: 0.698383
         R¬≤ Score: -1.6812 (Poor - 168.1% variance explained)
      üîπ OM: Training TCN (50 epochs)...
      ‚è≥ OM TCN: Epoch 10/50 (20%)
      ‚è≥ OM TCN: Epoch 20/50 (40%)
      ‚è≥ OM TCN: Epoch 30/50 (60%)
      ‚è≥ OM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.176280
         RMSE: 0.419857
         R¬≤ Score: 0.0309
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OM Random Forest: Starting GridSearchCV fit...
       ‚úÖ DEFI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.6090 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DEFI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DEFI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.4342 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DEFI XGBoost: Starting GridSearchCV fit...
       ‚úÖ HODL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8978 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.5s
    - LSTM: MSE=0.2689
    - TCN: MSE=0.2220
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2220
        ‚Ä¢ LSTM: MSE=0.2689
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.8718
        ‚Ä¢ XGBoost: MSE=16.8978
        ‚Ä¢ Random Forest: MSE=19.2066
   ‚úÖ HODL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HODL (TargetReturn): TCN with MSE=0.2220
üêõ DEBUG: HODL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HODL.
üêõ DEBUG: HODL - Moving model to CPU before return...
üêõ DEBUG [22:44:55.487]: HODL - Returning result metadata...
üêõ DEBUG: train_worker started for UUUU
üêõ DEBUG [22:44:55.488]: Main received result for HODL
üêõ DEBUG: Training progress: 176/959 done
  ‚öôÔ∏è Training models for UUUU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - UUUU: Initiating feature extraction for training.
  [DIAGNOSTIC] UUUU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UUUU: rows after features available: 126
üéØ UUUU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UUUU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UUUU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UUUU: Training LSTM (50 epochs)...
      ‚è≥ UUUU LSTM: Epoch 10/50 (20%)
       ‚úÖ OM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=182.6624 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ UUUU LSTM: Epoch 20/50 (40%)
       ‚úÖ OM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=241.8845 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OM XGBoost: Starting GridSearchCV fit...
      ‚è≥ UUUU LSTM: Epoch 30/50 (60%)
      ‚è≥ UUUU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.443781
         RMSE: 0.666169
         R¬≤ Score: -1.1288 (Poor - 112.9% variance explained)
      üîπ UUUU: Training TCN (50 epochs)...
      ‚è≥ UUUU TCN: Epoch 10/50 (20%)
      ‚è≥ UUUU TCN: Epoch 20/50 (40%)
      ‚è≥ UUUU TCN: Epoch 30/50 (60%)
      ‚è≥ UUUU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.366469
         RMSE: 0.605367
         R¬≤ Score: -0.7579
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UUUU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UUUU Random Forest: Starting GridSearchCV fit...
       ‚úÖ UUUU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=59.8435 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UUUU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UUUU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=96.7218 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UUUU XGBoost: Starting GridSearchCV fit...
       ‚úÖ AXON XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.5047 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.9s
    - LSTM: MSE=0.3676
    - TCN: MSE=0.3232
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3232
        ‚Ä¢ LSTM: MSE=0.3676
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.0195
        ‚Ä¢ XGBoost: MSE=53.5047
        ‚Ä¢ Random Forest: MSE=54.0723
   ‚úÖ AXON: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AXON (TargetReturn): TCN with MSE=0.3232
üêõ DEBUG: AXON - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AXON.
üêõ DEBUG: AXON - Moving model to CPU before return...
üêõ DEBUG [22:45:06.200]: AXON - Returning result metadata...
üêõ DEBUG: train_worker started for FRHC
üêõ DEBUG [22:45:06.201]: Main received result for AXON
  ‚öôÔ∏è Training models for FRHC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - FRHC: Initiating feature extraction for training.
  [DIAGNOSTIC] FRHC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FRHC: rows after features available: 126
üéØ FRHC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FRHC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FRHC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FRHC: Training LSTM (50 epochs)...
      ‚è≥ FRHC LSTM: Epoch 10/50 (20%)
      ‚è≥ FRHC LSTM: Epoch 20/50 (40%)
      ‚è≥ FRHC LSTM: Epoch 30/50 (60%)
      ‚è≥ FRHC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.263668
         RMSE: 0.513486
         R¬≤ Score: -1.0253 (Poor - 102.5% variance explained)
      üîπ FRHC: Training TCN (50 epochs)...
      ‚è≥ FRHC TCN: Epoch 10/50 (20%)
      ‚è≥ FRHC TCN: Epoch 20/50 (40%)
      ‚è≥ FRHC TCN: Epoch 30/50 (60%)
      ‚è≥ FRHC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.156148
         RMSE: 0.395156
         R¬≤ Score: -0.1994
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FRHC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FRHC Random Forest: Starting GridSearchCV fit...
       ‚úÖ FRHC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.9258 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FRHC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FRHC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.8194 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FRHC XGBoost: Starting GridSearchCV fit...
       ‚úÖ EZBC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.5149 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 127.1s
    - LSTM: MSE=0.3462
    - TCN: MSE=0.2230
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2230
        ‚Ä¢ LSTM: MSE=0.3462
        ‚Ä¢ Random Forest: MSE=12.5210
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.1140
        ‚Ä¢ XGBoost: MSE=18.5149
   ‚úÖ EZBC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EZBC (TargetReturn): TCN with MSE=0.2230
üêõ DEBUG: EZBC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EZBC.
üêõ DEBUG: EZBC - Moving model to CPU before return...
üêõ DEBUG [22:45:13.077]: EZBC - Returning result metadata...
üêõ DEBUG [22:45:13.078]: Main received result for EZBC
üêõ DEBUG: train_worker started for TPB
  ‚öôÔ∏è Training models for TPB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - TPB: Initiating feature extraction for training.
  [DIAGNOSTIC] TPB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TPB: rows after features available: 126
üéØ TPB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TPB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TPB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TPB: Training LSTM (50 epochs)...
      ‚è≥ TPB LSTM: Epoch 10/50 (20%)
      ‚è≥ TPB LSTM: Epoch 20/50 (40%)
      ‚è≥ TPB LSTM: Epoch 30/50 (60%)
      ‚è≥ TPB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.733653
         RMSE: 0.856536
         R¬≤ Score: -1.2235 (Poor - 122.3% variance explained)
      üîπ TPB: Training TCN (50 epochs)...
      ‚è≥ TPB TCN: Epoch 10/50 (20%)
      ‚è≥ TPB TCN: Epoch 20/50 (40%)
      ‚è≥ TPB TCN: Epoch 30/50 (60%)
      ‚è≥ TPB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.408183
         RMSE: 0.638892
         R¬≤ Score: -0.2371
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TPB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TPB Random Forest: Starting GridSearchCV fit...
       ‚úÖ BRRR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.6892 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 127.0s
    - LSTM: MSE=0.3046
    - TCN: MSE=0.2268
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2268
        ‚Ä¢ LSTM: MSE=0.3046
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.0407
        ‚Ä¢ Random Forest: MSE=18.5109
        ‚Ä¢ XGBoost: MSE=29.6892
   ‚úÖ BRRR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BRRR (TargetReturn): TCN with MSE=0.2268
üêõ DEBUG: BRRR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BRRR.
üêõ DEBUG: BRRR - Moving model to CPU before return...
üêõ DEBUG [22:45:16.369]: BRRR - Returning result metadata...
üêõ DEBUG: train_worker started for GBTC
  ‚öôÔ∏è Training models for GBTC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - GBTC: Initiating feature extraction for training.
  [DIAGNOSTIC] GBTC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GBTC: rows after features available: 126
üéØ GBTC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GBTC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GBTC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GBTC: Training LSTM (50 epochs)...
      ‚è≥ GBTC LSTM: Epoch 10/50 (20%)
      ‚è≥ GBTC LSTM: Epoch 20/50 (40%)
       ‚úÖ BITB XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.8570 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 131.3s
    - LSTM: MSE=0.3762
    - TCN: MSE=0.2251
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2251
        ‚Ä¢ LSTM: MSE=0.3762
        ‚Ä¢ XGBoost: MSE=21.8570
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.3307
        ‚Ä¢ Random Forest: MSE=24.2921
   ‚úÖ BITB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BITB (TargetReturn): TCN with MSE=0.2251
üêõ DEBUG: BITB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BITB.
üêõ DEBUG: BITB - Moving model to CPU before return...
üêõ DEBUG [22:45:17.458]: BITB - Returning result metadata...
üêõ DEBUG: train_worker started for BBIO
üêõ DEBUG [22:45:17.459]: Main received result for BITB
üêõ DEBUG [22:45:17.459]: Main received result for BRRR
üêõ DEBUG: Training progress: 180/959 done
  ‚öôÔ∏è Training models for BBIO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - BBIO: Initiating feature extraction for training.
  [DIAGNOSTIC] BBIO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBIO: rows after features available: 126
üéØ BBIO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BBIO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBIO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBIO: Training LSTM (50 epochs)...
      ‚è≥ GBTC LSTM: Epoch 30/50 (60%)
      ‚è≥ BBIO LSTM: Epoch 10/50 (20%)
      ‚è≥ GBTC LSTM: Epoch 40/50 (80%)
      ‚è≥ BBIO LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.298690
         RMSE: 0.546525
         R¬≤ Score: -1.1471 (Poor - 114.7% variance explained)
      üîπ GBTC: Training TCN (50 epochs)...
      ‚è≥ GBTC TCN: Epoch 10/50 (20%)
      ‚è≥ BBIO LSTM: Epoch 30/50 (60%)
       ‚úÖ TPB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.8420 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TPB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GBTC TCN: Epoch 20/50 (40%)
      ‚è≥ GBTC TCN: Epoch 30/50 (60%)
      ‚è≥ GBTC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180028
         RMSE: 0.424298
         R¬≤ Score: -0.2941
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GBTC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GBTC Random Forest: Starting GridSearchCV fit...
      ‚è≥ BBIO LSTM: Epoch 40/50 (80%)
       ‚úÖ TPB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=31.3092 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TPB XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.432207
         RMSE: 0.657424
         R¬≤ Score: -1.2591 (Poor - 125.9% variance explained)
      üîπ BBIO: Training TCN (50 epochs)...
      ‚è≥ BBIO TCN: Epoch 10/50 (20%)
      ‚è≥ BBIO TCN: Epoch 20/50 (40%)
      ‚è≥ BBIO TCN: Epoch 30/50 (60%)
      ‚è≥ BBIO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.303829
         RMSE: 0.551207
         R¬≤ Score: -0.5881
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBIO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBIO Random Forest: Starting GridSearchCV fit...
       ‚úÖ GBTC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.0767 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GBTC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GBTC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.1575 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GBTC XGBoost: Starting GridSearchCV fit...
       ‚úÖ BBIO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=50.8761 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBIO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BBIO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=47.5997 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBIO XGBoost: Starting GridSearchCV fit...
       ‚úÖ IBIT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.1974 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 122.6s
    - LSTM: MSE=0.3432
    - TCN: MSE=0.1828
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1828
        ‚Ä¢ LSTM: MSE=0.3432
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.1698
        ‚Ä¢ XGBoost: MSE=15.1974
        ‚Ä¢ Random Forest: MSE=19.3919
   ‚úÖ IBIT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IBIT (TargetReturn): TCN with MSE=0.1828
üêõ DEBUG: IBIT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IBIT.
üêõ DEBUG: IBIT - Moving model to CPU before return...
üêõ DEBUG [22:45:42.508]: IBIT - Returning result metadata...
üêõ DEBUG [22:45:42.508]: Main received result for IBIT
üêõ DEBUG: train_worker started for ARKF
  ‚öôÔ∏è Training models for ARKF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ARKF: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKF: rows after features available: 126
üéØ ARKF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKF: Training LSTM (50 epochs)...
      ‚è≥ ARKF LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKF LSTM: Epoch 20/50 (40%)
       ‚úÖ FBTC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.6506 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.7s
    - LSTM: MSE=0.3108
    - TCN: MSE=0.1807
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1807
        ‚Ä¢ LSTM: MSE=0.3108
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6221
        ‚Ä¢ Random Forest: MSE=19.6724
        ‚Ä¢ XGBoost: MSE=21.6506
   ‚úÖ FBTC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FBTC (TargetReturn): TCN with MSE=0.1807
üêõ DEBUG: FBTC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FBTC.
üêõ DEBUG: FBTC - Moving model to CPU before return...
üêõ DEBUG [22:45:43.728]: FBTC - Returning result metadata...
üêõ DEBUG [22:45:43.728]: Main received result for FBTC
üêõ DEBUG: train_worker started for LASR
  ‚öôÔ∏è Training models for LASR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - LASR: Initiating feature extraction for training.
  [DIAGNOSTIC] LASR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LASR: rows after features available: 126
üéØ LASR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LASR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LASR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LASR: Training LSTM (50 epochs)...
      ‚è≥ ARKF LSTM: Epoch 30/50 (60%)
      ‚è≥ LASR LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKF LSTM: Epoch 40/50 (80%)
      ‚è≥ LASR LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.590751
         RMSE: 0.768603
         R¬≤ Score: -0.6631 (Poor - 66.3% variance explained)
      üîπ ARKF: Training TCN (50 epochs)...
      ‚è≥ ARKF TCN: Epoch 10/50 (20%)
      ‚è≥ ARKF TCN: Epoch 20/50 (40%)
      ‚è≥ ARKF TCN: Epoch 30/50 (60%)
      ‚è≥ ARKF TCN: Epoch 40/50 (80%)
      ‚è≥ LASR LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.781295
         RMSE: 0.883909
         R¬≤ Score: -1.1995
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKF Random Forest: Starting GridSearchCV fit...
      ‚è≥ LASR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.646826
         RMSE: 0.804255
         R¬≤ Score: -0.7619 (Poor - 76.2% variance explained)
      üîπ LASR: Training TCN (50 epochs)...
      ‚è≥ LASR TCN: Epoch 10/50 (20%)
      ‚è≥ LASR TCN: Epoch 20/50 (40%)
      ‚è≥ LASR TCN: Epoch 30/50 (60%)
      ‚è≥ LASR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.804156
         RMSE: 0.896747
         R¬≤ Score: -1.1905
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LASR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LASR Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.2557 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARKF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.5990 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKF XGBoost: Starting GridSearchCV fit...
       ‚úÖ LASR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=78.2956 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LASR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LASR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=125.1440 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LASR XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARKB XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=29.9444 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 122.3s
    - LSTM: MSE=0.3836
    - TCN: MSE=0.1875
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1875
        ‚Ä¢ LSTM: MSE=0.3836
        ‚Ä¢ XGBoost: MSE=29.9444
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.0296
        ‚Ä¢ Random Forest: MSE=30.0385
   ‚úÖ ARKB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKB (TargetReturn): TCN with MSE=0.1875
üêõ DEBUG: ARKB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKB.
üêõ DEBUG: ARKB - Moving model to CPU before return...
üêõ DEBUG [22:45:58.054]: ARKB - Returning result metadata...
üêõ DEBUG [22:45:58.054]: Main received result for ARKB
üêõ DEBUG: train_worker started for KEP
  ‚öôÔ∏è Training models for KEP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - KEP: Initiating feature extraction for training.
  [DIAGNOSTIC] KEP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KEP: rows after features available: 126
üéØ KEP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KEP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KEP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KEP: Training LSTM (50 epochs)...
      ‚è≥ KEP LSTM: Epoch 10/50 (20%)
      ‚è≥ KEP LSTM: Epoch 20/50 (40%)
      ‚è≥ KEP LSTM: Epoch 30/50 (60%)
      ‚è≥ KEP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.205530
         RMSE: 0.453354
         R¬≤ Score: -0.7520 (Poor - 75.2% variance explained)
      üîπ KEP: Training TCN (50 epochs)...
      ‚è≥ KEP TCN: Epoch 10/50 (20%)
      ‚è≥ KEP TCN: Epoch 20/50 (40%)
      ‚è≥ KEP TCN: Epoch 30/50 (60%)
      ‚è≥ KEP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.196351
         RMSE: 0.443115
         R¬≤ Score: -0.6737
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KEP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KEP Random Forest: Starting GridSearchCV fit...
       ‚úÖ KEP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=59.9533 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KEP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KEP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=77.4013 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KEP XGBoost: Starting GridSearchCV fit...
       ‚úÖ BTCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.3757 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 121.8s
    - LSTM: MSE=0.3037
    - TCN: MSE=0.1695
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1695
        ‚Ä¢ LSTM: MSE=0.3037
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.0097
        ‚Ä¢ Random Forest: MSE=15.1398
        ‚Ä¢ XGBoost: MSE=16.3757
   ‚úÖ BTCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BTCO (TargetReturn): TCN with MSE=0.1695
üêõ DEBUG: BTCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BTCO.
üêõ DEBUG: BTCO - Moving model to CPU before return...
üêõ DEBUG [22:46:04.841]: BTCO - Returning result metadata...
üêõ DEBUG: train_worker started for IRTC
üêõ DEBUG [22:46:04.842]: Main received result for BTCO
üêõ DEBUG: Training progress: 184/959 done
  ‚öôÔ∏è Training models for IRTC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - IRTC: Initiating feature extraction for training.
  [DIAGNOSTIC] IRTC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IRTC: rows after features available: 126
üéØ IRTC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IRTC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IRTC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IRTC: Training LSTM (50 epochs)...
      ‚è≥ IRTC LSTM: Epoch 10/50 (20%)
      ‚è≥ IRTC LSTM: Epoch 20/50 (40%)
      ‚è≥ IRTC LSTM: Epoch 30/50 (60%)
      ‚è≥ IRTC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.321965
         RMSE: 0.567420
         R¬≤ Score: -0.9237 (Poor - 92.4% variance explained)
      üîπ IRTC: Training TCN (50 epochs)...
      ‚è≥ IRTC TCN: Epoch 10/50 (20%)
      ‚è≥ IRTC TCN: Epoch 20/50 (40%)
      ‚è≥ IRTC TCN: Epoch 30/50 (60%)
      ‚è≥ IRTC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.253037
         RMSE: 0.503028
         R¬≤ Score: -0.5119
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IRTC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IRTC Random Forest: Starting GridSearchCV fit...
       ‚úÖ IRTC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=48.0372 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IRTC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IRTC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=40.9365 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IRTC XGBoost: Starting GridSearchCV fit...
       ‚úÖ REAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=124.2642 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 124.1s
    - LSTM: MSE=0.0347
    - TCN: MSE=0.0129
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0129
        ‚Ä¢ LSTM: MSE=0.0347
        ‚Ä¢ Random Forest: MSE=51.4637
        ‚Ä¢ XGBoost: MSE=124.2642
        ‚Ä¢ LightGBM Regressor (CPU): MSE=222.3129
   ‚úÖ REAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for REAL (TargetReturn): TCN with MSE=0.0129
üêõ DEBUG: REAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for REAL.
üêõ DEBUG: REAL - Moving model to CPU before return...
üêõ DEBUG [22:46:26.098]: REAL - Returning result metadata...
üêõ DEBUG: train_worker started for HIMX
üêõ DEBUG [22:46:26.099]: Main received result for REAL
  ‚öôÔ∏è Training models for HIMX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - HIMX: Initiating feature extraction for training.
  [DIAGNOSTIC] HIMX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HIMX: rows after features available: 126
üéØ HIMX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HIMX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HIMX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HIMX: Training LSTM (50 epochs)...
      ‚è≥ HIMX LSTM: Epoch 10/50 (20%)
      ‚è≥ HIMX LSTM: Epoch 20/50 (40%)
      ‚è≥ HIMX LSTM: Epoch 30/50 (60%)
      ‚è≥ HIMX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.244319
         RMSE: 0.494286
         R¬≤ Score: -0.8197 (Poor - 82.0% variance explained)
      üîπ HIMX: Training TCN (50 epochs)...
      ‚è≥ HIMX TCN: Epoch 10/50 (20%)
      ‚è≥ HIMX TCN: Epoch 20/50 (40%)
      ‚è≥ HIMX TCN: Epoch 30/50 (60%)
      ‚è≥ HIMX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.291771
         RMSE: 0.540159
         R¬≤ Score: -1.1732
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HIMX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HIMX Random Forest: Starting GridSearchCV fit...
       ‚úÖ HIMX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=111.9541 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HIMX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HIMX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=84.6255 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HIMX XGBoost: Starting GridSearchCV fit...
       ‚úÖ SAN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.6843 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 123.5s
    - LSTM: MSE=0.1250
    - TCN: MSE=0.0902
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0902
        ‚Ä¢ LSTM: MSE=0.1250
        ‚Ä¢ Random Forest: MSE=27.1765
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.3217
        ‚Ä¢ XGBoost: MSE=29.6843
   ‚úÖ SAN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SAN (TargetReturn): TCN with MSE=0.0902
üêõ DEBUG: SAN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SAN.
üêõ DEBUG: SAN - Moving model to CPU before return...
üêõ DEBUG [22:46:41.309]: SAN - Returning result metadata...
üêõ DEBUG [22:46:41.309]: Main received result for SAN
üêõ DEBUG: train_worker started for GDS
  ‚öôÔ∏è Training models for GDS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - GDS: Initiating feature extraction for training.
  [DIAGNOSTIC] GDS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GDS: rows after features available: 126
üéØ GDS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GDS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GDS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GDS: Training LSTM (50 epochs)...
      ‚è≥ GDS LSTM: Epoch 10/50 (20%)
      ‚è≥ GDS LSTM: Epoch 20/50 (40%)
      ‚è≥ GDS LSTM: Epoch 30/50 (60%)
      ‚è≥ GDS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.208646
         RMSE: 0.456778
         R¬≤ Score: -0.5153 (Poor - 51.5% variance explained)
      üîπ GDS: Training TCN (50 epochs)...
      ‚è≥ GDS TCN: Epoch 10/50 (20%)
      ‚è≥ GDS TCN: Epoch 20/50 (40%)
      ‚è≥ GDS TCN: Epoch 30/50 (60%)
      ‚è≥ GDS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.240153
         RMSE: 0.490054
         R¬≤ Score: -0.7441
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GDS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GDS Random Forest: Starting GridSearchCV fit...
       ‚úÖ AMSC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=96.0971 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.2s
    - LSTM: MSE=0.9050
    - TCN: MSE=0.4842
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4842
        ‚Ä¢ LSTM: MSE=0.9050
        ‚Ä¢ XGBoost: MSE=96.0971
        ‚Ä¢ Random Forest: MSE=97.6323
        ‚Ä¢ LightGBM Regressor (CPU): MSE=136.5922
   ‚úÖ AMSC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMSC (TargetReturn): TCN with MSE=0.4842
üêõ DEBUG: AMSC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMSC.
üêõ DEBUG: AMSC - Moving model to CPU before return...
üêõ DEBUG [22:46:45.875]: AMSC - Returning result metadata...
üêõ DEBUG: train_worker started for ETHZ
üêõ DEBUG [22:46:45.876]: Main received result for AMSC
  ‚öôÔ∏è Training models for ETHZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ETHZ: Initiating feature extraction for training.
  [DIAGNOSTIC] ETHZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETHZ: rows after features available: 126
üéØ ETHZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETHZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETHZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETHZ: Training LSTM (50 epochs)...
      ‚è≥ ETHZ LSTM: Epoch 10/50 (20%)
      ‚è≥ ETHZ LSTM: Epoch 20/50 (40%)
       ‚úÖ GDS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=298.6665 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GDS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ETHZ LSTM: Epoch 30/50 (60%)
       ‚úÖ GDS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=177.2721 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GDS XGBoost: Starting GridSearchCV fit...
      ‚è≥ ETHZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.324876
         RMSE: 0.569979
         R¬≤ Score: -1.2599 (Poor - 126.0% variance explained)
      üîπ ETHZ: Training TCN (50 epochs)...
      ‚è≥ ETHZ TCN: Epoch 10/50 (20%)
      ‚è≥ ETHZ TCN: Epoch 20/50 (40%)
      ‚è≥ ETHZ TCN: Epoch 30/50 (60%)
      ‚è≥ ETHZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.205700
         RMSE: 0.453542
         R¬≤ Score: -0.4309
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETHZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETHZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ SKE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=64.0828 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.2s
    - LSTM: MSE=0.3041
    - TCN: MSE=0.1618
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1618
        ‚Ä¢ LSTM: MSE=0.3041
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.8354
        ‚Ä¢ Random Forest: MSE=60.0199
        ‚Ä¢ XGBoost: MSE=64.0828
   ‚úÖ SKE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SKE (TargetReturn): TCN with MSE=0.1618
üêõ DEBUG: SKE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SKE.
üêõ DEBUG: SKE - Moving model to CPU before return...
üêõ DEBUG [22:46:50.987]: SKE - Returning result metadata...
üêõ DEBUG [22:46:50.988]: Main received result for SKE
üêõ DEBUG: Training progress: 188/959 done
üêõ DEBUG: train_worker started for IESC
  ‚öôÔ∏è Training models for IESC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - IESC: Initiating feature extraction for training.
  [DIAGNOSTIC] IESC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IESC: rows after features available: 126
üéØ IESC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IESC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IESC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IESC: Training LSTM (50 epochs)...
      ‚è≥ IESC LSTM: Epoch 10/50 (20%)
      ‚è≥ IESC LSTM: Epoch 20/50 (40%)
       ‚úÖ ETHZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=335.5970 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETHZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IESC LSTM: Epoch 30/50 (60%)
       ‚úÖ ETHZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=555.5867 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETHZ XGBoost: Starting GridSearchCV fit...
      ‚è≥ IESC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.319507
         RMSE: 0.565250
         R¬≤ Score: -0.7058 (Poor - 70.6% variance explained)
      üîπ IESC: Training TCN (50 epochs)...
      ‚è≥ IESC TCN: Epoch 10/50 (20%)
      ‚è≥ IESC TCN: Epoch 20/50 (40%)
      ‚è≥ IESC TCN: Epoch 30/50 (60%)
      ‚è≥ IESC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.389758
         RMSE: 0.624306
         R¬≤ Score: -1.0808
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IESC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IESC Random Forest: Starting GridSearchCV fit...
       ‚úÖ IESC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=72.9353 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IESC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IESC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=70.0610 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IESC XGBoost: Starting GridSearchCV fit...
       ‚úÖ DEFI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.2725 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 125.3s
    - LSTM: MSE=0.3423
    - TCN: MSE=0.1727
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1727
        ‚Ä¢ LSTM: MSE=0.3423
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.4342
        ‚Ä¢ Random Forest: MSE=28.6090
        ‚Ä¢ XGBoost: MSE=36.2725
   ‚úÖ DEFI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DEFI (TargetReturn): TCN with MSE=0.1727
üêõ DEBUG: DEFI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DEFI.
üêõ DEBUG: DEFI - Moving model to CPU before return...
üêõ DEBUG [22:46:59.668]: DEFI - Returning result metadata...
üêõ DEBUG: train_worker started for TME
üêõ DEBUG [22:46:59.669]: Main received result for DEFI
  ‚öôÔ∏è Training models for TME (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - TME: Initiating feature extraction for training.
  [DIAGNOSTIC] TME: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TME: rows after features available: 126
üéØ TME: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TME: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TME: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TME: Training LSTM (50 epochs)...
      ‚è≥ TME LSTM: Epoch 10/50 (20%)
      ‚è≥ TME LSTM: Epoch 20/50 (40%)
      ‚è≥ TME LSTM: Epoch 30/50 (60%)
      ‚è≥ TME LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.282748
         RMSE: 0.531741
         R¬≤ Score: -0.5655 (Poor - 56.5% variance explained)
      üîπ TME: Training TCN (50 epochs)...
      ‚è≥ TME TCN: Epoch 10/50 (20%)
      ‚è≥ TME TCN: Epoch 20/50 (40%)
      ‚è≥ TME TCN: Epoch 30/50 (60%)
      ‚è≥ TME TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.373877
         RMSE: 0.611455
         R¬≤ Score: -1.0700
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TME: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TME Random Forest: Starting GridSearchCV fit...
       ‚úÖ TME Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=50.0744 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TME LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TME LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=55.6874 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TME XGBoost: Starting GridSearchCV fit...
       ‚úÖ OM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=183.2929 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.1s
    - LSTM: MSE=0.4877
    - TCN: MSE=0.1763
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1763
        ‚Ä¢ LSTM: MSE=0.4877
        ‚Ä¢ Random Forest: MSE=182.6624
        ‚Ä¢ XGBoost: MSE=183.2929
        ‚Ä¢ LightGBM Regressor (CPU): MSE=241.8845
   ‚úÖ OM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OM (TargetReturn): TCN with MSE=0.1763
üêõ DEBUG: OM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OM.
üêõ DEBUG: OM - Moving model to CPU before return...
üêõ DEBUG [22:47:06.067]: OM - Returning result metadata...
üêõ DEBUG: train_worker started for PARR
üêõ DEBUG [22:47:06.070]: Main received result for OM
  ‚öôÔ∏è Training models for PARR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - PARR: Initiating feature extraction for training.
  [DIAGNOSTIC] PARR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PARR: rows after features available: 126
üéØ PARR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PARR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PARR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PARR: Training LSTM (50 epochs)...
      ‚è≥ PARR LSTM: Epoch 10/50 (20%)
      ‚è≥ PARR LSTM: Epoch 20/50 (40%)
      ‚è≥ PARR LSTM: Epoch 30/50 (60%)
      ‚è≥ PARR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.512582
         RMSE: 0.715948
         R¬≤ Score: -0.8500 (Poor - 85.0% variance explained)
      üîπ PARR: Training TCN (50 epochs)...
      ‚è≥ PARR TCN: Epoch 10/50 (20%)
      ‚è≥ PARR TCN: Epoch 20/50 (40%)
      ‚è≥ PARR TCN: Epoch 30/50 (60%)
      ‚è≥ PARR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.606194
         RMSE: 0.778584
         R¬≤ Score: -1.1879
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PARR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PARR Random Forest: Starting GridSearchCV fit...
       ‚úÖ UUUU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=80.5699 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.5s
    - LSTM: MSE=0.4438
    - TCN: MSE=0.3665
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3665
        ‚Ä¢ LSTM: MSE=0.4438
        ‚Ä¢ Random Forest: MSE=59.8435
        ‚Ä¢ XGBoost: MSE=80.5699
        ‚Ä¢ LightGBM Regressor (CPU): MSE=96.7218
   ‚úÖ UUUU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UUUU (TargetReturn): TCN with MSE=0.3665
üêõ DEBUG: UUUU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UUUU.
üêõ DEBUG: UUUU - Moving model to CPU before return...
üêõ DEBUG [22:47:10.149]: UUUU - Returning result metadata...
üêõ DEBUG: train_worker started for APPS
üêõ DEBUG [22:47:10.152]: Main received result for UUUU
  ‚öôÔ∏è Training models for APPS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - APPS: Initiating feature extraction for training.
  [DIAGNOSTIC] APPS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APPS: rows after features available: 126
üéØ APPS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APPS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APPS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APPS: Training LSTM (50 epochs)...
      ‚è≥ APPS LSTM: Epoch 10/50 (20%)
      ‚è≥ APPS LSTM: Epoch 20/50 (40%)
      ‚è≥ APPS LSTM: Epoch 30/50 (60%)
       ‚úÖ PARR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=108.0651 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PARR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ APPS LSTM: Epoch 40/50 (80%)
       ‚úÖ PARR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=121.5529 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PARR XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.109539
         RMSE: 0.330967
         R¬≤ Score: -1.1058 (Poor - 110.6% variance explained)
      üîπ APPS: Training TCN (50 epochs)...
      ‚è≥ APPS TCN: Epoch 10/50 (20%)
      ‚è≥ APPS TCN: Epoch 20/50 (40%)
      ‚è≥ APPS TCN: Epoch 30/50 (60%)
      ‚è≥ APPS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.093777
         RMSE: 0.306231
         R¬≤ Score: -0.8028
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APPS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APPS Random Forest: Starting GridSearchCV fit...
       ‚úÖ APPS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=426.6960 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APPS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ APPS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=508.3726 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APPS XGBoost: Starting GridSearchCV fit...
       ‚úÖ FRHC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.6755 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.7s
    - LSTM: MSE=0.2637
    - TCN: MSE=0.1561
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1561
        ‚Ä¢ LSTM: MSE=0.2637
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.8194
        ‚Ä¢ Random Forest: MSE=24.9258
        ‚Ä¢ XGBoost: MSE=29.6755
   ‚úÖ FRHC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FRHC (TargetReturn): TCN with MSE=0.1561
üêõ DEBUG: FRHC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FRHC.
üêõ DEBUG: FRHC - Moving model to CPU before return...
üêõ DEBUG [22:47:22.023]: FRHC - Returning result metadata...
üêõ DEBUG: train_worker started for VEON
üêõ DEBUG [22:47:22.027]: Main received result for FRHC
üêõ DEBUG: Training progress: 192/959 done
  ‚öôÔ∏è Training models for VEON (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - VEON: Initiating feature extraction for training.
  [DIAGNOSTIC] VEON: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VEON: rows after features available: 126
üéØ VEON: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VEON: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VEON: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VEON: Training LSTM (50 epochs)...
      ‚è≥ VEON LSTM: Epoch 10/50 (20%)
      ‚è≥ VEON LSTM: Epoch 20/50 (40%)
      ‚è≥ VEON LSTM: Epoch 30/50 (60%)
      ‚è≥ VEON LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.299045
         RMSE: 0.546850
         R¬≤ Score: -1.2472 (Poor - 124.7% variance explained)
      üîπ VEON: Training TCN (50 epochs)...
      ‚è≥ VEON TCN: Epoch 10/50 (20%)
      ‚è≥ VEON TCN: Epoch 20/50 (40%)
      ‚è≥ VEON TCN: Epoch 30/50 (60%)
      ‚è≥ VEON TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.133868
         RMSE: 0.365880
         R¬≤ Score: -0.0060
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VEON: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VEON Random Forest: Starting GridSearchCV fit...
       ‚úÖ TPB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=57.2392 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 126.8s
    - LSTM: MSE=0.7337
    - TCN: MSE=0.4082
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4082
        ‚Ä¢ LSTM: MSE=0.7337
        ‚Ä¢ Random Forest: MSE=24.8420
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.3092
        ‚Ä¢ XGBoost: MSE=57.2392
   ‚úÖ TPB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TPB (TargetReturn): TCN with MSE=0.4082
üêõ DEBUG: TPB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TPB.
üêõ DEBUG: TPB - Moving model to CPU before return...
üêõ DEBUG [22:47:26.766]: TPB - Returning result metadata...
üêõ DEBUG: train_worker started for XMTR
üêõ DEBUG [22:47:26.766]: Main received result for TPB
  ‚öôÔ∏è Training models for XMTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - XMTR: Initiating feature extraction for training.
  [DIAGNOSTIC] XMTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XMTR: rows after features available: 126
üéØ XMTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XMTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XMTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XMTR: Training LSTM (50 epochs)...
      ‚è≥ XMTR LSTM: Epoch 10/50 (20%)
       ‚úÖ VEON Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.5935 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VEON LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ XMTR LSTM: Epoch 20/50 (40%)
       ‚úÖ VEON LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.0597 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VEON XGBoost: Starting GridSearchCV fit...
      ‚è≥ XMTR LSTM: Epoch 30/50 (60%)
      ‚è≥ XMTR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.440717
         RMSE: 0.663865
         R¬≤ Score: -0.9384 (Poor - 93.8% variance explained)
      üîπ XMTR: Training TCN (50 epochs)...
      ‚è≥ XMTR TCN: Epoch 10/50 (20%)
      ‚è≥ XMTR TCN: Epoch 20/50 (40%)
      ‚è≥ XMTR TCN: Epoch 30/50 (60%)
      ‚è≥ XMTR TCN: Epoch 40/50 (80%)
       ‚úÖ GBTC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=18.6863 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 126.8s
    - LSTM: MSE=0.2987
    - TCN: MSE=0.1800
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1800
        ‚Ä¢ LSTM: MSE=0.2987
        ‚Ä¢ XGBoost: MSE=18.6863
        ‚Ä¢ Random Forest: MSE=20.0767
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.1575
   ‚úÖ GBTC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GBTC (TargetReturn): TCN with MSE=0.1800
üêõ DEBUG: GBTC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GBTC.
üêõ DEBUG: GBTC - Moving model to CPU before return...
üêõ DEBUG [22:47:29.876]: GBTC - Returning result metadata...
üêõ DEBUG: train_worker started for ATRO
üêõ DEBUG [22:47:29.877]: Main received result for GBTC
      üìä TCN Regression Metrics:
         MSE: 0.443760
         RMSE: 0.666153
         R¬≤ Score: -0.9518
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XMTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XMTR Random Forest: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for ATRO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - ATRO: Initiating feature extraction for training.
  [DIAGNOSTIC] ATRO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATRO: rows after features available: 126
üéØ ATRO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATRO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATRO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATRO: Training LSTM (50 epochs)...
      ‚è≥ ATRO LSTM: Epoch 10/50 (20%)
      ‚è≥ ATRO LSTM: Epoch 20/50 (40%)
      ‚è≥ ATRO LSTM: Epoch 30/50 (60%)
      ‚è≥ ATRO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.212820
         RMSE: 0.461324
         R¬≤ Score: -0.8642 (Poor - 86.4% variance explained)
      üîπ ATRO: Training TCN (50 epochs)...
      ‚è≥ ATRO TCN: Epoch 10/50 (20%)
      ‚è≥ ATRO TCN: Epoch 20/50 (40%)
      ‚è≥ ATRO TCN: Epoch 30/50 (60%)
      ‚è≥ ATRO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.121940
         RMSE: 0.349199
         R¬≤ Score: -0.0681
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATRO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATRO Random Forest: Starting GridSearchCV fit...
       ‚úÖ XMTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.8515 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XMTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ XMTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=55.1162 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XMTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ BBIO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=43.7362 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.9s
    - LSTM: MSE=0.4322
    - TCN: MSE=0.3038
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3038
        ‚Ä¢ LSTM: MSE=0.4322
        ‚Ä¢ XGBoost: MSE=43.7362
        ‚Ä¢ LightGBM Regressor (CPU): MSE=47.5997
        ‚Ä¢ Random Forest: MSE=50.8761
   ‚úÖ BBIO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBIO (TargetReturn): TCN with MSE=0.3038
üêõ DEBUG: BBIO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBIO.
üêõ DEBUG: BBIO - Moving model to CPU before return...
üêõ DEBUG [22:47:34.242]: BBIO - Returning result metadata...
üêõ DEBUG [22:47:34.243]: Main received result for BBIO
üêõ DEBUG: train_worker started for FIX
  ‚öôÔ∏è Training models for FIX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FIX: Initiating feature extraction for training.
  [DIAGNOSTIC] FIX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FIX: rows after features available: 126
üéØ FIX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FIX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FIX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FIX: Training LSTM (50 epochs)...
      ‚è≥ FIX LSTM: Epoch 10/50 (20%)
      ‚è≥ FIX LSTM: Epoch 20/50 (40%)
       ‚úÖ ATRO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=68.1712 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATRO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FIX LSTM: Epoch 30/50 (60%)
       ‚úÖ ATRO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=76.0332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATRO XGBoost: Starting GridSearchCV fit...
      ‚è≥ FIX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.326396
         RMSE: 0.571310
         R¬≤ Score: -0.6572 (Poor - 65.7% variance explained)
      üîπ FIX: Training TCN (50 epochs)...
      ‚è≥ FIX TCN: Epoch 10/50 (20%)
      ‚è≥ FIX TCN: Epoch 20/50 (40%)
      ‚è≥ FIX TCN: Epoch 30/50 (60%)
      ‚è≥ FIX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.328915
         RMSE: 0.573511
         R¬≤ Score: -0.6700
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FIX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FIX Random Forest: Starting GridSearchCV fit...
       ‚úÖ FIX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=88.4453 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FIX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FIX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=65.5433 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FIX XGBoost: Starting GridSearchCV fit...
       ‚úÖ LASR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=125.9848 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 124.6s
    - LSTM: MSE=0.6468
    - TCN: MSE=0.8042
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6468
        ‚Ä¢ TCN: MSE=0.8042
        ‚Ä¢ Random Forest: MSE=78.2956
        ‚Ä¢ LightGBM Regressor (CPU): MSE=125.1440
        ‚Ä¢ XGBoost: MSE=125.9848
   ‚úÖ LASR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LASR (TargetReturn): LSTM with MSE=0.6468
üêõ DEBUG: LASR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LASR.
üêõ DEBUG: LASR - Moving model to CPU before return...
üêõ DEBUG [22:47:55.279]: LASR - Returning result metadata...
üêõ DEBUG: train_worker started for LMB
  ‚öôÔ∏è Training models for LMB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - LMB: Initiating feature extraction for training.
  [DIAGNOSTIC] LMB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LMB: rows after features available: 126
üéØ LMB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LMB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LMB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LMB: Training LSTM (50 epochs)...
      ‚è≥ LMB LSTM: Epoch 10/50 (20%)
       ‚úÖ ARKF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.1919 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 127.1s
    - LSTM: MSE=0.5908
    - TCN: MSE=0.7813
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5908
        ‚Ä¢ TCN: MSE=0.7813
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.5990
        ‚Ä¢ Random Forest: MSE=42.2557
        ‚Ä¢ XGBoost: MSE=53.1919
   ‚úÖ ARKF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKF (TargetReturn): LSTM with MSE=0.5908
üêõ DEBUG: ARKF - train_and_evaluate_models completed
      ‚è≥ LMB LSTM: Epoch 20/50 (40%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKF.
üêõ DEBUG: ARKF - Moving model to CPU before return...
üêõ DEBUG [22:47:56.248]: ARKF - Returning result metadata...
üêõ DEBUG: train_worker started for CIB
üêõ DEBUG [22:47:56.249]: Main received result for ARKF
üêõ DEBUG: Training progress: 196/959 done
üêõ DEBUG [22:47:56.250]: Main received result for LASR
  ‚öôÔ∏è Training models for CIB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CIB: Initiating feature extraction for training.
  [DIAGNOSTIC] CIB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CIB: rows after features available: 126
üéØ CIB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CIB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CIB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CIB: Training LSTM (50 epochs)...
      ‚è≥ LMB LSTM: Epoch 30/50 (60%)
      ‚è≥ CIB LSTM: Epoch 10/50 (20%)
      ‚è≥ LMB LSTM: Epoch 40/50 (80%)
      ‚è≥ CIB LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.394639
         RMSE: 0.628203
         R¬≤ Score: -0.8605 (Poor - 86.1% variance explained)
      üîπ LMB: Training TCN (50 epochs)...
      ‚è≥ LMB TCN: Epoch 10/50 (20%)
      ‚è≥ LMB TCN: Epoch 20/50 (40%)
      ‚è≥ CIB LSTM: Epoch 30/50 (60%)
      ‚è≥ LMB TCN: Epoch 30/50 (60%)
      ‚è≥ LMB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.232192
         RMSE: 0.481863
         R¬≤ Score: -0.0947
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LMB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LMB Random Forest: Starting GridSearchCV fit...
      ‚è≥ CIB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.107827
         RMSE: 0.328370
         R¬≤ Score: -0.5708 (Poor - 57.1% variance explained)
      üîπ CIB: Training TCN (50 epochs)...
      ‚è≥ CIB TCN: Epoch 10/50 (20%)
      ‚è≥ CIB TCN: Epoch 20/50 (40%)
      ‚è≥ CIB TCN: Epoch 30/50 (60%)
      ‚è≥ CIB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.095586
         RMSE: 0.309169
         R¬≤ Score: -0.3925
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CIB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CIB Random Forest: Starting GridSearchCV fit...
       ‚úÖ LMB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=90.9693 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LMB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LMB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=86.7249 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LMB XGBoost: Starting GridSearchCV fit...
       ‚úÖ CIB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.4722 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CIB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CIB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.3356 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CIB XGBoost: Starting GridSearchCV fit...
       ‚úÖ KEP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=136.3338 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.6s
    - LSTM: MSE=0.2055
    - TCN: MSE=0.1964
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1964
        ‚Ä¢ LSTM: MSE=0.2055
        ‚Ä¢ Random Forest: MSE=59.9533
        ‚Ä¢ LightGBM Regressor (CPU): MSE=77.4013
        ‚Ä¢ XGBoost: MSE=136.3338
   ‚úÖ KEP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KEP (TargetReturn): TCN with MSE=0.1964
üêõ DEBUG: KEP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KEP.
üêõ DEBUG: KEP - Moving model to CPU before return...
üêõ DEBUG [22:48:10.622]: KEP - Returning result metadata...
üêõ DEBUG [22:48:10.622]: Main received result for KEP
üêõ DEBUG: train_worker started for TPC
  ‚öôÔ∏è Training models for TPC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TPC: Initiating feature extraction for training.
  [DIAGNOSTIC] TPC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TPC: rows after features available: 126
üéØ TPC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TPC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TPC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TPC: Training LSTM (50 epochs)...
      ‚è≥ TPC LSTM: Epoch 10/50 (20%)
      ‚è≥ TPC LSTM: Epoch 20/50 (40%)
      ‚è≥ TPC LSTM: Epoch 30/50 (60%)
      ‚è≥ TPC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.584282
         RMSE: 0.764384
         R¬≤ Score: -1.0145 (Poor - 101.4% variance explained)
      üîπ TPC: Training TCN (50 epochs)...
      ‚è≥ TPC TCN: Epoch 10/50 (20%)
      ‚è≥ TPC TCN: Epoch 20/50 (40%)
      ‚è≥ TPC TCN: Epoch 30/50 (60%)
      ‚è≥ TPC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.460334
         RMSE: 0.678479
         R¬≤ Score: -0.5871
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TPC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TPC Random Forest: Starting GridSearchCV fit...
       ‚úÖ TPC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=59.5156 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TPC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IRTC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=56.5227 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 124.9s
    - LSTM: MSE=0.3220
    - TCN: MSE=0.2530
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2530
        ‚Ä¢ LSTM: MSE=0.3220
        ‚Ä¢ LightGBM Regressor (CPU): MSE=40.9365
        ‚Ä¢ Random Forest: MSE=48.0372
        ‚Ä¢ XGBoost: MSE=56.5227
   ‚úÖ IRTC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IRTC (TargetReturn): TCN with MSE=0.2530
üêõ DEBUG: IRTC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IRTC.
üêõ DEBUG: IRTC - Moving model to CPU before return...
üêõ DEBUG [22:48:16.737]: IRTC - Returning result metadata...
üêõ DEBUG: train_worker started for ORGO
üêõ DEBUG [22:48:16.737]: Main received result for IRTC
  ‚öôÔ∏è Training models for ORGO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - ORGO: Initiating feature extraction for training.
  [DIAGNOSTIC] ORGO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ORGO: rows after features available: 126
üéØ ORGO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ORGO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ORGO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ORGO: Training LSTM (50 epochs)...
       ‚úÖ TPC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=88.0982 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TPC XGBoost: Starting GridSearchCV fit...
      ‚è≥ ORGO LSTM: Epoch 10/50 (20%)
      ‚è≥ ORGO LSTM: Epoch 20/50 (40%)
      ‚è≥ ORGO LSTM: Epoch 30/50 (60%)
      ‚è≥ ORGO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.435912
         RMSE: 0.660237
         R¬≤ Score: -1.0863 (Poor - 108.6% variance explained)
      üîπ ORGO: Training TCN (50 epochs)...
      ‚è≥ ORGO TCN: Epoch 10/50 (20%)
      ‚è≥ ORGO TCN: Epoch 20/50 (40%)
      ‚è≥ ORGO TCN: Epoch 30/50 (60%)
      ‚è≥ ORGO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.211849
         RMSE: 0.460271
         R¬≤ Score: -0.0139
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ORGO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ORGO Random Forest: Starting GridSearchCV fit...
       ‚úÖ ORGO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=159.8324 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ORGO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ORGO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=119.9174 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ORGO XGBoost: Starting GridSearchCV fit...
       ‚úÖ HIMX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=112.8912 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.9s
    - LSTM: MSE=0.2443
    - TCN: MSE=0.2918
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2443
        ‚Ä¢ TCN: MSE=0.2918
        ‚Ä¢ LightGBM Regressor (CPU): MSE=84.6255
        ‚Ä¢ Random Forest: MSE=111.9541
        ‚Ä¢ XGBoost: MSE=112.8912
   ‚úÖ HIMX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HIMX (TargetReturn): LSTM with MSE=0.2443
üêõ DEBUG: HIMX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HIMX.
üêõ DEBUG: HIMX - Moving model to CPU before return...
üêõ DEBUG [22:48:39.098]: HIMX - Returning result metadata...
üêõ DEBUG: train_worker started for INDV
üêõ DEBUG [22:48:39.102]: Main received result for HIMX
üêõ DEBUG: Training progress: 200/959 done
  ‚öôÔ∏è Training models for INDV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - INDV: Initiating feature extraction for training.
  [DIAGNOSTIC] INDV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ INDV: rows after features available: 126
üéØ INDV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] INDV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö INDV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ INDV: Training LSTM (50 epochs)...
      ‚è≥ INDV LSTM: Epoch 10/50 (20%)
      ‚è≥ INDV LSTM: Epoch 20/50 (40%)
      ‚è≥ INDV LSTM: Epoch 30/50 (60%)
      ‚è≥ INDV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.292921
         RMSE: 0.541222
         R¬≤ Score: -0.4060 (Poor - 40.6% variance explained)
      üîπ INDV: Training TCN (50 epochs)...
      ‚è≥ INDV TCN: Epoch 10/50 (20%)
      ‚è≥ INDV TCN: Epoch 20/50 (40%)
      ‚è≥ INDV TCN: Epoch 30/50 (60%)
      ‚è≥ INDV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.353996
         RMSE: 0.594976
         R¬≤ Score: -0.6991
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä INDV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ INDV Random Forest: Starting GridSearchCV fit...
       ‚úÖ INDV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=82.4903 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ INDV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ INDV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=44.1416 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ INDV XGBoost: Starting GridSearchCV fit...
       ‚úÖ GDS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=324.6253 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 123.4s
    - LSTM: MSE=0.2086
    - TCN: MSE=0.2402
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2086
        ‚Ä¢ TCN: MSE=0.2402
        ‚Ä¢ LightGBM Regressor (CPU): MSE=177.2721
        ‚Ä¢ Random Forest: MSE=298.6665
        ‚Ä¢ XGBoost: MSE=324.6253
   ‚úÖ GDS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GDS (TargetReturn): LSTM with MSE=0.2086
üêõ DEBUG: GDS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GDS.
üêõ DEBUG: GDS - Moving model to CPU before return...
üêõ DEBUG [22:48:51.077]: GDS - Returning result metadata...
üêõ DEBUG [22:48:51.078]: Main received result for GDS
üêõ DEBUG: train_worker started for EXEL
  ‚öôÔ∏è Training models for EXEL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - EXEL: Initiating feature extraction for training.
  [DIAGNOSTIC] EXEL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EXEL: rows after features available: 126
üéØ EXEL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EXEL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EXEL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EXEL: Training LSTM (50 epochs)...
      ‚è≥ EXEL LSTM: Epoch 10/50 (20%)
      ‚è≥ EXEL LSTM: Epoch 20/50 (40%)
      ‚è≥ EXEL LSTM: Epoch 30/50 (60%)
      ‚è≥ EXEL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.133743
         RMSE: 0.365710
         R¬≤ Score: -0.2918 (Poor - 29.2% variance explained)
      üîπ EXEL: Training TCN (50 epochs)...
      ‚è≥ EXEL TCN: Epoch 10/50 (20%)
      ‚è≥ EXEL TCN: Epoch 20/50 (40%)
      ‚è≥ EXEL TCN: Epoch 30/50 (60%)
      ‚è≥ EXEL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.104335
         RMSE: 0.323009
         R¬≤ Score: -0.0078
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EXEL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EXEL Random Forest: Starting GridSearchCV fit...
       ‚úÖ EXEL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.8969 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EXEL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EXEL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.7243 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EXEL XGBoost: Starting GridSearchCV fit...
       ‚úÖ ETHZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=376.3652 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 124.9s
    - LSTM: MSE=0.3249
    - TCN: MSE=0.2057
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2057
        ‚Ä¢ LSTM: MSE=0.3249
        ‚Ä¢ Random Forest: MSE=335.5970
        ‚Ä¢ XGBoost: MSE=376.3652
        ‚Ä¢ LightGBM Regressor (CPU): MSE=555.5867
   ‚úÖ ETHZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETHZ (TargetReturn): TCN with MSE=0.2057
üêõ DEBUG: ETHZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETHZ.
üêõ DEBUG: ETHZ - Moving model to CPU before return...
üêõ DEBUG [22:48:57.698]: ETHZ - Returning result metadata...
üêõ DEBUG: train_worker started for PBI
üêõ DEBUG [22:48:57.699]: Main received result for ETHZ
  ‚öôÔ∏è Training models for PBI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - PBI: Initiating feature extraction for training.
  [DIAGNOSTIC] PBI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PBI: rows after features available: 126
üéØ PBI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PBI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PBI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PBI: Training LSTM (50 epochs)...
      ‚è≥ PBI LSTM: Epoch 10/50 (20%)
      ‚è≥ PBI LSTM: Epoch 20/50 (40%)
      ‚è≥ PBI LSTM: Epoch 30/50 (60%)
      ‚è≥ PBI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.621798
         RMSE: 0.788542
         R¬≤ Score: -0.6717 (Poor - 67.2% variance explained)
      üîπ PBI: Training TCN (50 epochs)...
      ‚è≥ PBI TCN: Epoch 10/50 (20%)
      ‚è≥ PBI TCN: Epoch 20/50 (40%)
      ‚è≥ PBI TCN: Epoch 30/50 (60%)
      ‚è≥ PBI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.765201
         RMSE: 0.874758
         R¬≤ Score: -1.0572
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PBI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PBI Random Forest: Starting GridSearchCV fit...
       ‚úÖ IESC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=77.9958 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 124.7s
    - LSTM: MSE=0.3195
    - TCN: MSE=0.3898
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3195
        ‚Ä¢ TCN: MSE=0.3898
        ‚Ä¢ LightGBM Regressor (CPU): MSE=70.0610
        ‚Ä¢ Random Forest: MSE=72.9353
        ‚Ä¢ XGBoost: MSE=77.9958
   ‚úÖ IESC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IESC (TargetReturn): LSTM with MSE=0.3195
üêõ DEBUG: IESC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IESC.
üêõ DEBUG: IESC - Moving model to CPU before return...
üêõ DEBUG [22:49:02.173]: IESC - Returning result metadata...
üêõ DEBUG [22:49:02.173]: Main received result for IESC
üêõ DEBUG: train_worker started for ICCM
  ‚öôÔ∏è Training models for ICCM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ICCM: Initiating feature extraction for training.
  [DIAGNOSTIC] ICCM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ICCM: rows after features available: 126
üéØ ICCM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ICCM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ICCM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ICCM: Training LSTM (50 epochs)...
      ‚è≥ ICCM LSTM: Epoch 10/50 (20%)
      ‚è≥ ICCM LSTM: Epoch 20/50 (40%)
      ‚è≥ ICCM LSTM: Epoch 30/50 (60%)
       ‚úÖ PBI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=47.7867 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PBI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ICCM LSTM: Epoch 40/50 (80%)
       ‚úÖ PBI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.3843 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PBI XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.017563
         RMSE: 0.132526
         R¬≤ Score: -0.8167 (Poor - 81.7% variance explained)
      üîπ ICCM: Training TCN (50 epochs)...
      ‚è≥ ICCM TCN: Epoch 10/50 (20%)
      ‚è≥ ICCM TCN: Epoch 20/50 (40%)
      ‚è≥ ICCM TCN: Epoch 30/50 (60%)
      ‚è≥ ICCM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.010435
         RMSE: 0.102153
         R¬≤ Score: -0.0794
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ICCM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ICCM Random Forest: Starting GridSearchCV fit...
       ‚úÖ ICCM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.9975 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ICCM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ICCM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=106.3051 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ICCM XGBoost: Starting GridSearchCV fit...
       ‚úÖ TME XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=47.6069 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.0s
    - LSTM: MSE=0.2827
    - TCN: MSE=0.3739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2827
        ‚Ä¢ TCN: MSE=0.3739
        ‚Ä¢ XGBoost: MSE=47.6069
        ‚Ä¢ Random Forest: MSE=50.0744
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.6874
   ‚úÖ TME: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TME (TargetReturn): LSTM with MSE=0.2827
üêõ DEBUG: TME - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TME.
üêõ DEBUG: TME - Moving model to CPU before return...
üêõ DEBUG [22:49:11.727]: TME - Returning result metadata...
üêõ DEBUG [22:49:11.728]: Main received result for TME
üêõ DEBUG: Training progress: 204/959 done
üêõ DEBUG: train_worker started for DASH
  ‚öôÔ∏è Training models for DASH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - DASH: Initiating feature extraction for training.
  [DIAGNOSTIC] DASH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DASH: rows after features available: 126
üéØ DASH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DASH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DASH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DASH: Training LSTM (50 epochs)...
      ‚è≥ DASH LSTM: Epoch 10/50 (20%)
      ‚è≥ DASH LSTM: Epoch 20/50 (40%)
      ‚è≥ DASH LSTM: Epoch 30/50 (60%)
      ‚è≥ DASH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.510834
         RMSE: 0.714726
         R¬≤ Score: -0.7620 (Poor - 76.2% variance explained)
      üîπ DASH: Training TCN (50 epochs)...
      ‚è≥ DASH TCN: Epoch 10/50 (20%)
      ‚è≥ DASH TCN: Epoch 20/50 (40%)
      ‚è≥ DASH TCN: Epoch 30/50 (60%)
      ‚è≥ DASH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.598730
         RMSE: 0.773776
         R¬≤ Score: -1.0652
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DASH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DASH Random Forest: Starting GridSearchCV fit...
       ‚úÖ PARR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=460.8898 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.3s
    - LSTM: MSE=0.5126
    - TCN: MSE=0.6062
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5126
        ‚Ä¢ TCN: MSE=0.6062
        ‚Ä¢ Random Forest: MSE=108.0651
        ‚Ä¢ LightGBM Regressor (CPU): MSE=121.5529
        ‚Ä¢ XGBoost: MSE=460.8898
   ‚úÖ PARR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PARR (TargetReturn): LSTM with MSE=0.5126
üêõ DEBUG: PARR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PARR.
üêõ DEBUG: PARR - Moving model to CPU before return...
üêõ DEBUG [22:49:16.348]: PARR - Returning result metadata...
üêõ DEBUG [22:49:16.349]: Main received result for PARR
üêõ DEBUG: train_worker started for FUTU
  ‚öôÔ∏è Training models for FUTU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - FUTU: Initiating feature extraction for training.
  [DIAGNOSTIC] FUTU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FUTU: rows after features available: 126
üéØ FUTU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FUTU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FUTU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FUTU: Training LSTM (50 epochs)...
      ‚è≥ FUTU LSTM: Epoch 10/50 (20%)
      ‚è≥ FUTU LSTM: Epoch 20/50 (40%)
       ‚úÖ DASH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.5650 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DASH LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FUTU LSTM: Epoch 30/50 (60%)
       ‚úÖ DASH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.6271 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DASH XGBoost: Starting GridSearchCV fit...
      ‚è≥ FUTU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.717049
         RMSE: 0.846787
         R¬≤ Score: -0.7077 (Poor - 70.8% variance explained)
      üîπ FUTU: Training TCN (50 epochs)...
      ‚è≥ FUTU TCN: Epoch 10/50 (20%)
      ‚è≥ FUTU TCN: Epoch 20/50 (40%)
      ‚è≥ FUTU TCN: Epoch 30/50 (60%)
      ‚è≥ FUTU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.795168
         RMSE: 0.891722
         R¬≤ Score: -0.8937
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FUTU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FUTU Random Forest: Starting GridSearchCV fit...
       ‚úÖ FUTU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=84.2996 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FUTU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ APPS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1277.3850 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.1s
    - LSTM: MSE=0.1095
    - TCN: MSE=0.0938
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0938
        ‚Ä¢ LSTM: MSE=0.1095
        ‚Ä¢ Random Forest: MSE=426.6960
        ‚Ä¢ LightGBM Regressor (CPU): MSE=508.3726
        ‚Ä¢ XGBoost: MSE=1277.3850
   ‚úÖ APPS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APPS (TargetReturn): TCN with MSE=0.0938
üêõ DEBUG: APPS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APPS.
üêõ DEBUG: APPS - Moving model to CPU before return...
üêõ DEBUG [22:49:22.289]: APPS - Returning result metadata...
üêõ DEBUG [22:49:22.290]: Main received result for APPSüêõ DEBUG: train_worker started for TATT

  ‚öôÔ∏è Training models for TATT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - TATT: Initiating feature extraction for training.
  [DIAGNOSTIC] TATT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TATT: rows after features available: 126
üéØ TATT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TATT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TATT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TATT: Training LSTM (50 epochs)...
       ‚úÖ FUTU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=76.3018 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.1s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FUTU XGBoost: Starting GridSearchCV fit...
      ‚è≥ TATT LSTM: Epoch 10/50 (20%)
      ‚è≥ TATT LSTM: Epoch 20/50 (40%)
      ‚è≥ TATT LSTM: Epoch 30/50 (60%)
      ‚è≥ TATT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.092385
         RMSE: 0.303949
         R¬≤ Score: -0.6738 (Poor - 67.4% variance explained)
      üîπ TATT: Training TCN (50 epochs)...
      ‚è≥ TATT TCN: Epoch 10/50 (20%)
      ‚è≥ TATT TCN: Epoch 20/50 (40%)
      ‚è≥ TATT TCN: Epoch 30/50 (60%)
      ‚è≥ TATT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.056270
         RMSE: 0.237213
         R¬≤ Score: -0.0195
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TATT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TATT Random Forest: Starting GridSearchCV fit...
       ‚úÖ TATT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.6754 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TATT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TATT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.4107 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TATT XGBoost: Starting GridSearchCV fit...
       ‚úÖ VEON XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.3155 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 126.6s
    - LSTM: MSE=0.2990
    - TCN: MSE=0.1339
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1339
        ‚Ä¢ LSTM: MSE=0.2990
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.0597
        ‚Ä¢ Random Forest: MSE=28.5935
        ‚Ä¢ XGBoost: MSE=31.3155
   ‚úÖ VEON: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VEON (TargetReturn): TCN with MSE=0.1339
üêõ DEBUG: VEON - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VEON.
üêõ DEBUG: VEON - Moving model to CPU before return...
üêõ DEBUG [22:49:34.956]: VEON - Returning result metadata...
üêõ DEBUG [22:49:34.956]: Main received result for VEON
üêõ DEBUG: train_worker started for CPS
  ‚öôÔ∏è Training models for CPS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - CPS: Initiating feature extraction for training.
  [DIAGNOSTIC] CPS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CPS: rows after features available: 126
üéØ CPS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CPS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CPS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CPS: Training LSTM (50 epochs)...
      ‚è≥ CPS LSTM: Epoch 10/50 (20%)
      ‚è≥ CPS LSTM: Epoch 20/50 (40%)
      ‚è≥ CPS LSTM: Epoch 30/50 (60%)
       ‚úÖ XMTR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=31.6909 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 122.9s
    - LSTM: MSE=0.4407
    - TCN: MSE=0.4438
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4407
        ‚Ä¢ TCN: MSE=0.4438
        ‚Ä¢ XGBoost: MSE=31.6909
        ‚Ä¢ Random Forest: MSE=44.8515
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.1162
   ‚úÖ XMTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XMTR (TargetReturn): LSTM with MSE=0.4407
üêõ DEBUG: XMTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XMTR.
üêõ DEBUG: XMTR - Moving model to CPU before return...
üêõ DEBUG [22:49:36.547]: XMTR - Returning result metadata...
üêõ DEBUG [22:49:36.547]: Main received result for XMTR
üêõ DEBUG: Training progress: 208/959 done
üêõ DEBUG: train_worker started for TENX
  ‚öôÔ∏è Training models for TENX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - TENX: Initiating feature extraction for training.
  [DIAGNOSTIC] TENX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TENX: rows after features available: 126
üéØ TENX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TENX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TENX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TENX: Training LSTM (50 epochs)...
      ‚è≥ CPS LSTM: Epoch 40/50 (80%)
      ‚è≥ TENX LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.404938
         RMSE: 0.636347
         R¬≤ Score: -0.8851 (Poor - 88.5% variance explained)
      üîπ CPS: Training TCN (50 epochs)...
      ‚è≥ CPS TCN: Epoch 10/50 (20%)
      ‚è≥ CPS TCN: Epoch 20/50 (40%)
      ‚è≥ TENX LSTM: Epoch 20/50 (40%)
      ‚è≥ CPS TCN: Epoch 30/50 (60%)
      ‚è≥ CPS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.212441
         RMSE: 0.460914
         R¬≤ Score: 0.0110
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CPS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CPS Random Forest: Starting GridSearchCV fit...
      ‚è≥ TENX LSTM: Epoch 30/50 (60%)
      ‚è≥ TENX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.073745
         RMSE: 0.271560
         R¬≤ Score: -0.4048 (Poor - 40.5% variance explained)
      üîπ TENX: Training TCN (50 epochs)...
      ‚è≥ TENX TCN: Epoch 10/50 (20%)
      ‚è≥ TENX TCN: Epoch 20/50 (40%)
      ‚è≥ TENX TCN: Epoch 30/50 (60%)
      ‚è≥ TENX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.056431
         RMSE: 0.237553
         R¬≤ Score: -0.0750
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TENX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TENX Random Forest: Starting GridSearchCV fit...
       ‚úÖ CPS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=144.6099 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CPS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CPS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=130.3425 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CPS XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATRO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=84.9080 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.4s
    - LSTM: MSE=0.2128
    - TCN: MSE=0.1219
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1219
        ‚Ä¢ LSTM: MSE=0.2128
        ‚Ä¢ Random Forest: MSE=68.1712
        ‚Ä¢ LightGBM Regressor (CPU): MSE=76.0332
        ‚Ä¢ XGBoost: MSE=84.9080
   ‚úÖ ATRO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATRO (TargetReturn): TCN with MSE=0.1219
üêõ DEBUG: ATRO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATRO.
üêõ DEBUG: ATRO - Moving model to CPU before return...
üêõ DEBUG [22:49:41.550]: ATRO - Returning result metadata...
üêõ DEBUG [22:49:41.551]: Main received result for ATRO
üêõ DEBUG: train_worker started for BITO
  ‚öôÔ∏è Training models for BITO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BITO: Initiating feature extraction for training.
  [DIAGNOSTIC] BITO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BITO: rows after features available: 126
üéØ BITO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BITO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BITO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BITO: Training LSTM (50 epochs)...
       ‚úÖ TENX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.1747 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TENX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BITO LSTM: Epoch 10/50 (20%)
      ‚è≥ BITO LSTM: Epoch 20/50 (40%)
       ‚úÖ TENX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.8204 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TENX XGBoost: Starting GridSearchCV fit...
      ‚è≥ BITO LSTM: Epoch 30/50 (60%)
      ‚è≥ BITO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.256505
         RMSE: 0.506463
         R¬≤ Score: -0.8128 (Poor - 81.3% variance explained)
      üîπ BITO: Training TCN (50 epochs)...
       ‚úÖ FIX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=126.7800 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.2s
    - LSTM: MSE=0.3264
    - TCN: MSE=0.3289
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3264
        ‚Ä¢ TCN: MSE=0.3289
        ‚Ä¢ LightGBM Regressor (CPU): MSE=65.5433
        ‚Ä¢ Random Forest: MSE=88.4453
        ‚Ä¢ XGBoost: MSE=126.7800
   ‚úÖ FIX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FIX (TargetReturn): LSTM with MSE=0.3264
üêõ DEBUG: FIX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FIX.
üêõ DEBUG: FIX - Moving model to CPU before return...
üêõ DEBUG [22:49:44.129]: FIX - Returning result metadata...
üêõ DEBUG [22:49:44.129]: Main received result for FIXüêõ DEBUG: train_worker started for NRG

  ‚öôÔ∏è Training models for NRG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - NRG: Initiating feature extraction for training.
  [DIAGNOSTIC] NRG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NRG: rows after features available: 126
üéØ NRG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NRG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NRG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NRG: Training LSTM (50 epochs)...
      ‚è≥ BITO TCN: Epoch 10/50 (20%)
      ‚è≥ BITO TCN: Epoch 20/50 (40%)
      ‚è≥ BITO TCN: Epoch 30/50 (60%)
      ‚è≥ BITO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.202647
         RMSE: 0.450163
         R¬≤ Score: -0.4322
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BITO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BITO Random Forest: Starting GridSearchCV fit...
      ‚è≥ NRG LSTM: Epoch 10/50 (20%)
      ‚è≥ NRG LSTM: Epoch 20/50 (40%)
      ‚è≥ NRG LSTM: Epoch 30/50 (60%)
      ‚è≥ NRG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.323433
         RMSE: 0.568712
         R¬≤ Score: -0.5946 (Poor - 59.5% variance explained)
      üîπ NRG: Training TCN (50 epochs)...
      ‚è≥ NRG TCN: Epoch 10/50 (20%)
      ‚è≥ NRG TCN: Epoch 20/50 (40%)
      ‚è≥ NRG TCN: Epoch 30/50 (60%)
      ‚è≥ NRG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.265713
         RMSE: 0.515474
         R¬≤ Score: -0.3100
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NRG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NRG Random Forest: Starting GridSearchCV fit...
       ‚úÖ BITO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.6040 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BITO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BITO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.4066 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BITO XGBoost: Starting GridSearchCV fit...
       ‚úÖ NRG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=96.1304 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NRG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NRG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=69.2577 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NRG XGBoost: Starting GridSearchCV fit...
       ‚úÖ LMB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=127.5877 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.3946
    - TCN: MSE=0.2322
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2322
        ‚Ä¢ LSTM: MSE=0.3946
        ‚Ä¢ LightGBM Regressor (CPU): MSE=86.7249
        ‚Ä¢ Random Forest: MSE=90.9693
        ‚Ä¢ XGBoost: MSE=127.5877
   ‚úÖ LMB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LMB (TargetReturn): TCN with MSE=0.2322
üêõ DEBUG: LMB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LMB.
üêõ DEBUG: LMB - Moving model to CPU before return...
üêõ DEBUG [22:49:59.849]: LMB - Returning result metadata...
üêõ DEBUG [22:49:59.850]: Main received result for LMB
üêõ DEBUG: train_worker started for CRS
  ‚öôÔ∏è Training models for CRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - CRS: Initiating feature extraction for training.
  [DIAGNOSTIC] CRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRS: rows after features available: 126
üéØ CRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRS: Training LSTM (50 epochs)...
      ‚è≥ CRS LSTM: Epoch 10/50 (20%)
      ‚è≥ CRS LSTM: Epoch 20/50 (40%)
      ‚è≥ CRS LSTM: Epoch 30/50 (60%)
      ‚è≥ CRS LSTM: Epoch 40/50 (80%)
       ‚úÖ CIB XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.8396 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.1078
    - TCN: MSE=0.0956
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0956
        ‚Ä¢ LSTM: MSE=0.1078
        ‚Ä¢ XGBoost: MSE=14.8396
        ‚Ä¢ Random Forest: MSE=16.4722
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.3356
   ‚úÖ CIB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CIB (TargetReturn): TCN with MSE=0.0956
üêõ DEBUG: CIB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CIB.
üêõ DEBUG: CIB - Moving model to CPU before return...
üêõ DEBUG [22:50:01.774]: CIB - Returning result metadata...
üêõ DEBUG [22:50:01.775]: Main received result for CIB
üêõ DEBUG: Training progress: 212/959 done
üêõ DEBUG: train_worker started for ODC
  ‚öôÔ∏è Training models for ODC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ODC: Initiating feature extraction for training.
  [DIAGNOSTIC] ODC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ODC: rows after features available: 126
üéØ ODC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ODC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ODC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ODC: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.298619
         RMSE: 0.546460
         R¬≤ Score: -0.5784 (Poor - 57.8% variance explained)
      üîπ CRS: Training TCN (50 epochs)...
      ‚è≥ CRS TCN: Epoch 10/50 (20%)
      ‚è≥ CRS TCN: Epoch 20/50 (40%)
      ‚è≥ ODC LSTM: Epoch 10/50 (20%)
      ‚è≥ CRS TCN: Epoch 30/50 (60%)
      ‚è≥ CRS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.280357
         RMSE: 0.529488
         R¬≤ Score: -0.4819
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRS Random Forest: Starting GridSearchCV fit...
      ‚è≥ ODC LSTM: Epoch 20/50 (40%)
      ‚è≥ ODC LSTM: Epoch 30/50 (60%)
      ‚è≥ ODC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.543592
         RMSE: 0.737287
         R¬≤ Score: -0.6588 (Poor - 65.9% variance explained)
      üîπ ODC: Training TCN (50 epochs)...
      ‚è≥ ODC TCN: Epoch 10/50 (20%)
      ‚è≥ ODC TCN: Epoch 20/50 (40%)
      ‚è≥ ODC TCN: Epoch 30/50 (60%)
      ‚è≥ ODC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.627199
         RMSE: 0.791959
         R¬≤ Score: -0.9139
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ODC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ODC Random Forest: Starting GridSearchCV fit...
       ‚úÖ CRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=62.4216 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=63.7289 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ ODC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.0740 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ODC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ODC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.8056 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ODC XGBoost: Starting GridSearchCV fit...
       ‚úÖ TPC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=91.8875 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.5843
    - TCN: MSE=0.4603
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4603
        ‚Ä¢ LSTM: MSE=0.5843
        ‚Ä¢ Random Forest: MSE=59.5156
        ‚Ä¢ LightGBM Regressor (CPU): MSE=88.0982
        ‚Ä¢ XGBoost: MSE=91.8875
   ‚úÖ TPC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TPC (TargetReturn): TCN with MSE=0.4603
üêõ DEBUG: TPC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TPC.
üêõ DEBUG: TPC - Moving model to CPU before return...
üêõ DEBUG [22:50:14.212]: TPC - Returning result metadata...
üêõ DEBUG [22:50:14.213]: Main received result for TPC
üêõ DEBUG: train_worker started for PPTA
  ‚öôÔ∏è Training models for PPTA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - PPTA: Initiating feature extraction for training.
  [DIAGNOSTIC] PPTA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PPTA: rows after features available: 126
üéØ PPTA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PPTA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PPTA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PPTA: Training LSTM (50 epochs)...
      ‚è≥ PPTA LSTM: Epoch 10/50 (20%)
      ‚è≥ PPTA LSTM: Epoch 20/50 (40%)
      ‚è≥ PPTA LSTM: Epoch 30/50 (60%)
      ‚è≥ PPTA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.297541
         RMSE: 0.545473
         R¬≤ Score: -1.4319 (Poor - 143.2% variance explained)
      üîπ PPTA: Training TCN (50 epochs)...
      ‚è≥ PPTA TCN: Epoch 10/50 (20%)
      ‚è≥ PPTA TCN: Epoch 20/50 (40%)
      ‚è≥ PPTA TCN: Epoch 30/50 (60%)
      ‚è≥ PPTA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.125570
         RMSE: 0.354358
         R¬≤ Score: -0.0263
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PPTA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PPTA Random Forest: Starting GridSearchCV fit...
       ‚úÖ PPTA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=129.1247 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PPTA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PPTA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=161.1471 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PPTA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ORGO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=126.5614 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.7s
    - LSTM: MSE=0.4359
    - TCN: MSE=0.2118
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2118
        ‚Ä¢ LSTM: MSE=0.4359
        ‚Ä¢ LightGBM Regressor (CPU): MSE=119.9174
        ‚Ä¢ XGBoost: MSE=126.5614
        ‚Ä¢ Random Forest: MSE=159.8324
   ‚úÖ ORGO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ORGO (TargetReturn): TCN with MSE=0.2118
üêõ DEBUG: ORGO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ORGO.
üêõ DEBUG: ORGO - Moving model to CPU before return...
üêõ DEBUG [22:50:21.236]: ORGO - Returning result metadata...
üêõ DEBUG: train_worker started for KGC
üêõ DEBUG [22:50:21.237]: Main received result for ORGO
  ‚öôÔ∏è Training models for KGC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - KGC: Initiating feature extraction for training.
  [DIAGNOSTIC] KGC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KGC: rows after features available: 126
üéØ KGC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KGC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KGC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KGC: Training LSTM (50 epochs)...
      ‚è≥ KGC LSTM: Epoch 10/50 (20%)
      ‚è≥ KGC LSTM: Epoch 20/50 (40%)
      ‚è≥ KGC LSTM: Epoch 30/50 (60%)
      ‚è≥ KGC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.397285
         RMSE: 0.630305
         R¬≤ Score: -0.8651 (Poor - 86.5% variance explained)
      üîπ KGC: Training TCN (50 epochs)...
      ‚è≥ KGC TCN: Epoch 10/50 (20%)
      ‚è≥ KGC TCN: Epoch 20/50 (40%)
      ‚è≥ KGC TCN: Epoch 30/50 (60%)
      ‚è≥ KGC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.335070
         RMSE: 0.578852
         R¬≤ Score: -0.5730
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KGC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KGC Random Forest: Starting GridSearchCV fit...
       ‚úÖ KGC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.5330 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KGC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KGC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.4335 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KGC XGBoost: Starting GridSearchCV fit...
       ‚úÖ INDV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=81.8497 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.2929
    - TCN: MSE=0.3540
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2929
        ‚Ä¢ TCN: MSE=0.3540
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.1416
        ‚Ä¢ XGBoost: MSE=81.8497
        ‚Ä¢ Random Forest: MSE=82.4903
   ‚úÖ INDV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for INDV (TargetReturn): LSTM with MSE=0.2929
üêõ DEBUG: INDV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for INDV.
üêõ DEBUG: INDV - Moving model to CPU before return...
üêõ DEBUG [22:50:43.492]: INDV - Returning result metadata...
üêõ DEBUG [22:50:43.493]: Main received result for INDV
üêõ DEBUG: train_worker started for SNEX
  ‚öôÔ∏è Training models for SNEX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - SNEX: Initiating feature extraction for training.
  [DIAGNOSTIC] SNEX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SNEX: rows after features available: 126
üéØ SNEX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SNEX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SNEX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SNEX: Training LSTM (50 epochs)...
      ‚è≥ SNEX LSTM: Epoch 10/50 (20%)
      ‚è≥ SNEX LSTM: Epoch 20/50 (40%)
      ‚è≥ SNEX LSTM: Epoch 30/50 (60%)
      ‚è≥ SNEX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.345683
         RMSE: 0.587948
         R¬≤ Score: -0.5437 (Poor - 54.4% variance explained)
      üîπ SNEX: Training TCN (50 epochs)...
      ‚è≥ SNEX TCN: Epoch 10/50 (20%)
      ‚è≥ SNEX TCN: Epoch 20/50 (40%)
      ‚è≥ SNEX TCN: Epoch 30/50 (60%)
      ‚è≥ SNEX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.223414
         RMSE: 0.472666
         R¬≤ Score: 0.0023
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SNEX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SNEX Random Forest: Starting GridSearchCV fit...
       ‚úÖ SNEX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.4973 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SNEX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SNEX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.7223 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SNEX XGBoost: Starting GridSearchCV fit...
       ‚úÖ EXEL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.4750 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.5s
    - LSTM: MSE=0.1337
    - TCN: MSE=0.1043
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1043
        ‚Ä¢ LSTM: MSE=0.1337
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.7243
        ‚Ä¢ Random Forest: MSE=25.8969
        ‚Ä¢ XGBoost: MSE=31.4750
   ‚úÖ EXEL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EXEL (TargetReturn): TCN with MSE=0.1043
üêõ DEBUG: EXEL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EXEL.
üêõ DEBUG: EXEL - Moving model to CPU before return...
üêõ DEBUG [22:50:56.650]: EXEL - Returning result metadata...
üêõ DEBUG [22:50:56.650]: Main received result for EXEL
üêõ DEBUG: Training progress: 216/959 doneüêõ DEBUG: train_worker started for CMPX

  ‚öôÔ∏è Training models for CMPX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CMPX: Initiating feature extraction for training.
  [DIAGNOSTIC] CMPX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CMPX: rows after features available: 126
üéØ CMPX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CMPX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CMPX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CMPX: Training LSTM (50 epochs)...
      ‚è≥ CMPX LSTM: Epoch 10/50 (20%)
      ‚è≥ CMPX LSTM: Epoch 20/50 (40%)
      ‚è≥ CMPX LSTM: Epoch 30/50 (60%)
      ‚è≥ CMPX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.366856
         RMSE: 0.605686
         R¬≤ Score: -0.9147 (Poor - 91.5% variance explained)
      üîπ CMPX: Training TCN (50 epochs)...
      ‚è≥ CMPX TCN: Epoch 10/50 (20%)
      ‚è≥ CMPX TCN: Epoch 20/50 (40%)
      ‚è≥ CMPX TCN: Epoch 30/50 (60%)
      ‚è≥ CMPX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.434071
         RMSE: 0.658841
         R¬≤ Score: -1.2656
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CMPX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CMPX Random Forest: Starting GridSearchCV fit...
       ‚úÖ CMPX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=143.5305 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CMPX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PBI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.0707 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.6218
    - TCN: MSE=0.7652
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6218
        ‚Ä¢ TCN: MSE=0.7652
        ‚Ä¢ Random Forest: MSE=47.7867
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.3843
        ‚Ä¢ XGBoost: MSE=54.0707
   ‚úÖ PBI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PBI (TargetReturn): LSTM with MSE=0.6218
üêõ DEBUG: PBI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PBI.
üêõ DEBUG: PBI - Moving model to CPU before return...
üêõ DEBUG [22:51:03.253]: PBI - Returning result metadata...
üêõ DEBUG: train_worker started for CTOS
üêõ DEBUG [22:51:03.254]: Main received result for PBI
       ‚úÖ CMPX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=116.8377 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CMPX XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for CTOS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CTOS: Initiating feature extraction for training.
  [DIAGNOSTIC] CTOS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTOS: rows after features available: 126
üéØ CTOS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTOS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTOS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTOS: Training LSTM (50 epochs)...
      ‚è≥ CTOS LSTM: Epoch 10/50 (20%)
      ‚è≥ CTOS LSTM: Epoch 20/50 (40%)
      ‚è≥ CTOS LSTM: Epoch 30/50 (60%)
      ‚è≥ CTOS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.412439
         RMSE: 0.642214
         R¬≤ Score: -0.6820 (Poor - 68.2% variance explained)
      üîπ CTOS: Training TCN (50 epochs)...
      ‚è≥ CTOS TCN: Epoch 10/50 (20%)
      ‚è≥ CTOS TCN: Epoch 20/50 (40%)
      ‚è≥ CTOS TCN: Epoch 30/50 (60%)
      ‚è≥ CTOS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.436255
         RMSE: 0.660496
         R¬≤ Score: -0.7791
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTOS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTOS Random Forest: Starting GridSearchCV fit...
       ‚úÖ ICCM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=75.4853 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.0176
    - TCN: MSE=0.0104
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0104
        ‚Ä¢ LSTM: MSE=0.0176
        ‚Ä¢ Random Forest: MSE=42.9975
        ‚Ä¢ XGBoost: MSE=75.4853
        ‚Ä¢ LightGBM Regressor (CPU): MSE=106.3051
   ‚úÖ ICCM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ICCM (TargetReturn): TCN with MSE=0.0104
üêõ DEBUG: ICCM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ICCM.
üêõ DEBUG: ICCM - Moving model to CPU before return...
üêõ DEBUG [22:51:06.204]: ICCM - Returning result metadata...
üêõ DEBUG: train_worker started for DRS
üêõ DEBUG [22:51:06.205]: Main received result for ICCM
  ‚öôÔ∏è Training models for DRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - DRS: Initiating feature extraction for training.
  [DIAGNOSTIC] DRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DRS: rows after features available: 126
üéØ DRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DRS: Training LSTM (50 epochs)...
      ‚è≥ DRS LSTM: Epoch 10/50 (20%)
      ‚è≥ DRS LSTM: Epoch 20/50 (40%)
      ‚è≥ DRS LSTM: Epoch 30/50 (60%)
      ‚è≥ DRS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.234580
         RMSE: 0.484334
         R¬≤ Score: -0.9751 (Poor - 97.5% variance explained)
      üîπ DRS: Training TCN (50 epochs)...
      ‚è≥ DRS TCN: Epoch 10/50 (20%)
      ‚è≥ DRS TCN: Epoch 20/50 (40%)
      ‚è≥ DRS TCN: Epoch 30/50 (60%)
      ‚è≥ DRS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.119606
         RMSE: 0.345840
         R¬≤ Score: -0.0070
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DRS Random Forest: Starting GridSearchCV fit...
       ‚úÖ CTOS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=50.9801 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTOS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CTOS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=48.7842 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTOS XGBoost: Starting GridSearchCV fit...
       ‚úÖ DRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=69.9720 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=53.4427 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ DASH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.3701 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.5108
    - TCN: MSE=0.5987
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5108
        ‚Ä¢ TCN: MSE=0.5987
        ‚Ä¢ Random Forest: MSE=20.5650
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.6271
        ‚Ä¢ XGBoost: MSE=32.3701
   ‚úÖ DASH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DASH (TargetReturn): LSTM with MSE=0.5108
üêõ DEBUG: DASH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DASH.
üêõ DEBUG: DASH - Moving model to CPU before return...
üêõ DEBUG [22:51:20.561]: DASH - Returning result metadata...
üêõ DEBUG [22:51:20.561]: Main received result for DASH
üêõ DEBUG: train_worker started for NXT
  ‚öôÔ∏è Training models for NXT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NXT: Initiating feature extraction for training.
  [DIAGNOSTIC] NXT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NXT: rows after features available: 126
üéØ NXT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NXT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NXT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NXT: Training LSTM (50 epochs)...
      ‚è≥ NXT LSTM: Epoch 10/50 (20%)
      ‚è≥ NXT LSTM: Epoch 20/50 (40%)
      ‚è≥ NXT LSTM: Epoch 30/50 (60%)
      ‚è≥ NXT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.427761
         RMSE: 0.654034
         R¬≤ Score: -0.8484 (Poor - 84.8% variance explained)
      üîπ NXT: Training TCN (50 epochs)...
      ‚è≥ NXT TCN: Epoch 10/50 (20%)
      ‚è≥ NXT TCN: Epoch 20/50 (40%)
      ‚è≥ NXT TCN: Epoch 30/50 (60%)
      ‚è≥ NXT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.407279
         RMSE: 0.638184
         R¬≤ Score: -0.7599
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NXT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NXT Random Forest: Starting GridSearchCV fit...
       ‚úÖ FUTU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=132.1457 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.7170
    - TCN: MSE=0.7952
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7170
        ‚Ä¢ TCN: MSE=0.7952
        ‚Ä¢ LightGBM Regressor (CPU): MSE=76.3018
        ‚Ä¢ Random Forest: MSE=84.2996
        ‚Ä¢ XGBoost: MSE=132.1457
   ‚úÖ FUTU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FUTU (TargetReturn): LSTM with MSE=0.7170
üêõ DEBUG: FUTU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FUTU.
üêõ DEBUG: FUTU - Moving model to CPU before return...
üêõ DEBUG [22:51:23.498]: FUTU - Returning result metadata...
üêõ DEBUG: train_worker started for TOST
üêõ DEBUG [22:51:23.504]: Main received result for FUTU
üêõ DEBUG: Training progress: 220/959 done
  ‚öôÔ∏è Training models for TOST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - TOST: Initiating feature extraction for training.
  [DIAGNOSTIC] TOST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TOST: rows after features available: 126
üéØ TOST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TOST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TOST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TOST: Training LSTM (50 epochs)...
      ‚è≥ TOST LSTM: Epoch 10/50 (20%)
      ‚è≥ TOST LSTM: Epoch 20/50 (40%)
      ‚è≥ TOST LSTM: Epoch 30/50 (60%)
      ‚è≥ TOST LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.420974
         RMSE: 0.648825
         R¬≤ Score: -0.7290 (Poor - 72.9% variance explained)
      üîπ TOST: Training TCN (50 epochs)...
       ‚úÖ NXT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=71.7575 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NXT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TOST TCN: Epoch 10/50 (20%)
      ‚è≥ TOST TCN: Epoch 20/50 (40%)
      ‚è≥ TOST TCN: Epoch 30/50 (60%)
      ‚è≥ TOST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.305763
         RMSE: 0.552959
         R¬≤ Score: -0.2558
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TOST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TOST Random Forest: Starting GridSearchCV fit...
       ‚úÖ NXT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=45.7145 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NXT XGBoost: Starting GridSearchCV fit...
       ‚úÖ TATT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.3296 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.0924
    - TCN: MSE=0.0563
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0563
        ‚Ä¢ LSTM: MSE=0.0924
        ‚Ä¢ Random Forest: MSE=23.6754
        ‚Ä¢ XGBoost: MSE=30.3296
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.4107
   ‚úÖ TATT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TATT (TargetReturn): TCN with MSE=0.0563
üêõ DEBUG: TATT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TATT.
üêõ DEBUG: TATT - Moving model to CPU before return...
üêõ DEBUG [22:51:28.649]: TATT - Returning result metadata...
üêõ DEBUG [22:51:28.650]: Main received result for TATT
üêõ DEBUG: train_worker started for REVG
  ‚öôÔ∏è Training models for REVG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - REVG: Initiating feature extraction for training.
  [DIAGNOSTIC] REVG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ REVG: rows after features available: 126
üéØ REVG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] REVG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö REVG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ REVG: Training LSTM (50 epochs)...
      ‚è≥ REVG LSTM: Epoch 10/50 (20%)
       ‚úÖ TOST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.1889 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TOST LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ REVG LSTM: Epoch 20/50 (40%)
       ‚úÖ TOST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=32.1587 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TOST XGBoost: Starting GridSearchCV fit...
      ‚è≥ REVG LSTM: Epoch 30/50 (60%)
      ‚è≥ REVG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.299446
         RMSE: 0.547217
         R¬≤ Score: -0.3017 (Poor - 30.2% variance explained)
      üîπ REVG: Training TCN (50 epochs)...
      ‚è≥ REVG TCN: Epoch 10/50 (20%)
      ‚è≥ REVG TCN: Epoch 20/50 (40%)
      ‚è≥ REVG TCN: Epoch 30/50 (60%)
      ‚è≥ REVG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.330291
         RMSE: 0.574709
         R¬≤ Score: -0.4358
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä REVG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ REVG Random Forest: Starting GridSearchCV fit...
       ‚úÖ REVG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=53.6518 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ REVG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ REVG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=32.6282 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ REVG XGBoost: Starting GridSearchCV fit...
       ‚úÖ CPS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=138.9920 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.4049
    - TCN: MSE=0.2124
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2124
        ‚Ä¢ LSTM: MSE=0.4049
        ‚Ä¢ LightGBM Regressor (CPU): MSE=130.3425
        ‚Ä¢ XGBoost: MSE=138.9920
        ‚Ä¢ Random Forest: MSE=144.6099
   ‚úÖ CPS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CPS (TargetReturn): TCN with MSE=0.2124
üêõ DEBUG: CPS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CPS.
üêõ DEBUG: CPS - Moving model to CPU before return...
üêõ DEBUG [22:51:42.846]: CPS - Returning result metadata...
üêõ DEBUG [22:51:42.846]: Main received result for CPS
üêõ DEBUG: train_worker started for HNRG
  ‚öôÔ∏è Training models for HNRG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - HNRG: Initiating feature extraction for training.
  [DIAGNOSTIC] HNRG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HNRG: rows after features available: 126
üéØ HNRG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HNRG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HNRG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HNRG: Training LSTM (50 epochs)...
      ‚è≥ HNRG LSTM: Epoch 10/50 (20%)
      ‚è≥ HNRG LSTM: Epoch 20/50 (40%)
      ‚è≥ HNRG LSTM: Epoch 30/50 (60%)
      ‚è≥ HNRG LSTM: Epoch 40/50 (80%)
       ‚úÖ TENX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.0728 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.0737
    - TCN: MSE=0.0564
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0564
        ‚Ä¢ LSTM: MSE=0.0737
        ‚Ä¢ XGBoost: MSE=17.0728
        ‚Ä¢ Random Forest: MSE=21.1747
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.8204
   ‚úÖ TENX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TENX (TargetReturn): TCN with MSE=0.0564
üêõ DEBUG: TENX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TENX.
üêõ DEBUG: TENX - Moving model to CPU before return...
üêõ DEBUG [22:51:44.845]: TENX - Returning result metadata...
üêõ DEBUG: train_worker started for BLOK
üêõ DEBUG [22:51:44.846]: Main received result for TENX
  ‚öôÔ∏è Training models for BLOK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - BLOK: Initiating feature extraction for training.
  [DIAGNOSTIC] BLOK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BLOK: rows after features available: 126
üéØ BLOK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BLOK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BLOK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BLOK: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.345204
         RMSE: 0.587541
         R¬≤ Score: -1.0462 (Poor - 104.6% variance explained)
      üîπ HNRG: Training TCN (50 epochs)...
      ‚è≥ HNRG TCN: Epoch 10/50 (20%)
      ‚è≥ HNRG TCN: Epoch 20/50 (40%)
      ‚è≥ BLOK LSTM: Epoch 10/50 (20%)
      ‚è≥ HNRG TCN: Epoch 30/50 (60%)
      ‚è≥ HNRG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.194514
         RMSE: 0.441037
         R¬≤ Score: -0.1530
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HNRG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HNRG Random Forest: Starting GridSearchCV fit...
      ‚è≥ BLOK LSTM: Epoch 20/50 (40%)
      ‚è≥ BLOK LSTM: Epoch 30/50 (60%)
       ‚úÖ BITO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.3232 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.7s
    - LSTM: MSE=0.2565
    - TCN: MSE=0.2026
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2026
        ‚Ä¢ LSTM: MSE=0.2565
        ‚Ä¢ XGBoost: MSE=13.3232
        ‚Ä¢ Random Forest: MSE=14.6040
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.4066
   ‚úÖ BITO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BITO (TargetReturn): TCN with MSE=0.2026
üêõ DEBUG: BITO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BITO.
üêõ DEBUG: BITO - Moving model to CPU before return...
üêõ DEBUG [22:51:46.692]: BITO - Returning result metadata...
üêõ DEBUG [22:51:46.692]: Main received result for BITO
üêõ DEBUG: Training progress: 224/959 done
üêõ DEBUG: train_worker started for MNMD
      ‚è≥ BLOK LSTM: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for MNMD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - MNMD: Initiating feature extraction for training.
  [DIAGNOSTIC] MNMD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MNMD: rows after features available: 126
üéØ MNMD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MNMD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MNMD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MNMD: Training LSTM (50 epochs)...
      ‚è≥ MNMD LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.558049
         RMSE: 0.747027
         R¬≤ Score: -0.7551 (Poor - 75.5% variance explained)
      üîπ BLOK: Training TCN (50 epochs)...
      ‚è≥ BLOK TCN: Epoch 10/50 (20%)
      ‚è≥ BLOK TCN: Epoch 20/50 (40%)
      ‚è≥ BLOK TCN: Epoch 30/50 (60%)
      ‚è≥ BLOK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.426409
         RMSE: 0.653000
         R¬≤ Score: -0.3411
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BLOK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BLOK Random Forest: Starting GridSearchCV fit...
      ‚è≥ MNMD LSTM: Epoch 20/50 (40%)
      ‚è≥ MNMD LSTM: Epoch 30/50 (60%)
       ‚úÖ HNRG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=104.7036 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HNRG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MNMD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.507202
         RMSE: 0.712181
         R¬≤ Score: -0.7738 (Poor - 77.4% variance explained)
      üîπ MNMD: Training TCN (50 epochs)...
      ‚è≥ MNMD TCN: Epoch 10/50 (20%)
       ‚úÖ HNRG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=105.9664 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HNRG XGBoost: Starting GridSearchCV fit...
      ‚è≥ MNMD TCN: Epoch 20/50 (40%)
      ‚è≥ MNMD TCN: Epoch 30/50 (60%)
      ‚è≥ MNMD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.478013
         RMSE: 0.691385
         R¬≤ Score: -0.6717
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MNMD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MNMD Random Forest: Starting GridSearchCV fit...
       ‚úÖ NRG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=137.2463 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.3234
    - TCN: MSE=0.2657
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2657
        ‚Ä¢ LSTM: MSE=0.3234
        ‚Ä¢ LightGBM Regressor (CPU): MSE=69.2577
        ‚Ä¢ Random Forest: MSE=96.1304
        ‚Ä¢ XGBoost: MSE=137.2463
   ‚úÖ NRG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NRG (TargetReturn): TCN with MSE=0.2657
üêõ DEBUG: NRG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NRG.
üêõ DEBUG: NRG - Moving model to CPU before return...
üêõ DEBUG [22:51:49.770]: NRG - Returning result metadata...
üêõ DEBUG [22:51:49.771]: Main received result for NRGüêõ DEBUG: train_worker started for DNA

  ‚öôÔ∏è Training models for DNA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - DNA: Initiating feature extraction for training.
  [DIAGNOSTIC] DNA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DNA: rows after features available: 126
üéØ DNA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DNA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DNA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DNA: Training LSTM (50 epochs)...
      ‚è≥ DNA LSTM: Epoch 10/50 (20%)
       ‚úÖ BLOK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.9740 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BLOK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ DNA LSTM: Epoch 20/50 (40%)
      ‚è≥ DNA LSTM: Epoch 30/50 (60%)
       ‚úÖ BLOK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=37.2089 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BLOK XGBoost: Starting GridSearchCV fit...
      ‚è≥ DNA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.402903
         RMSE: 0.634746
         R¬≤ Score: -0.6256 (Poor - 62.6% variance explained)
      üîπ DNA: Training TCN (50 epochs)...
      ‚è≥ DNA TCN: Epoch 10/50 (20%)
      ‚è≥ DNA TCN: Epoch 20/50 (40%)
      ‚è≥ DNA TCN: Epoch 30/50 (60%)
      ‚è≥ DNA TCN: Epoch 40/50 (80%)
       ‚úÖ MNMD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.1489 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MNMD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.449608
         RMSE: 0.670528
         R¬≤ Score: -0.8141
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DNA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DNA Random Forest: Starting GridSearchCV fit...
       ‚úÖ MNMD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=65.8754 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MNMD XGBoost: Starting GridSearchCV fit...
       ‚úÖ DNA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=196.5968 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DNA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DNA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=151.0511 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DNA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ODC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.2462 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 114.5s
    - LSTM: MSE=0.5436
    - TCN: MSE=0.6272
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5436
        ‚Ä¢ TCN: MSE=0.6272
        ‚Ä¢ XGBoost: MSE=14.2462
        ‚Ä¢ Random Forest: MSE=15.0740
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.8056
   ‚úÖ ODC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ODC (TargetReturn): LSTM with MSE=0.5436
üêõ DEBUG: ODC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ODC.
üêõ DEBUG: ODC - Moving model to CPU before return...
üêõ DEBUG [22:52:02.361]: ODC - Returning result metadata...
üêõ DEBUG: train_worker started for SLNO
  ‚öôÔ∏è Training models for SLNO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - SLNO: Initiating feature extraction for training.
  [DIAGNOSTIC] SLNO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLNO: rows after features available: 126
üéØ SLNO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLNO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLNO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLNO: Training LSTM (50 epochs)...
      ‚è≥ SLNO LSTM: Epoch 10/50 (20%)
       ‚úÖ CRS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=65.0500 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.2986
    - TCN: MSE=0.2804
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2804
        ‚Ä¢ LSTM: MSE=0.2986
        ‚Ä¢ Random Forest: MSE=62.4216
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.7289
        ‚Ä¢ XGBoost: MSE=65.0500
   ‚úÖ CRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRS (TargetReturn): TCN with MSE=0.2804
üêõ DEBUG: CRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRS.
üêõ DEBUG: CRS - Moving model to CPU before return...
üêõ DEBUG [22:52:03.095]: CRS - Returning result metadata...
üêõ DEBUG [22:52:03.095]: Main received result for CRS
üêõ DEBUG [22:52:03.095]: Main received result for ODC
üêõ DEBUG: train_worker started for YALA
  ‚öôÔ∏è Training models for YALA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - YALA: Initiating feature extraction for training.
  [DIAGNOSTIC] YALA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YALA: rows after features available: 126
üéØ YALA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YALA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YALA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YALA: Training LSTM (50 epochs)...
      ‚è≥ SLNO LSTM: Epoch 20/50 (40%)
      ‚è≥ SLNO LSTM: Epoch 30/50 (60%)
      ‚è≥ YALA LSTM: Epoch 10/50 (20%)
      ‚è≥ SLNO LSTM: Epoch 40/50 (80%)
      ‚è≥ YALA LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.265358
         RMSE: 0.515129
         R¬≤ Score: -0.8532 (Poor - 85.3% variance explained)
      üîπ SLNO: Training TCN (50 epochs)...
      ‚è≥ YALA LSTM: Epoch 30/50 (60%)
      ‚è≥ SLNO TCN: Epoch 10/50 (20%)
      ‚è≥ SLNO TCN: Epoch 20/50 (40%)
      ‚è≥ SLNO TCN: Epoch 30/50 (60%)
      ‚è≥ SLNO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.188742
         RMSE: 0.434445
         R¬≤ Score: -0.3181
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLNO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLNO Random Forest: Starting GridSearchCV fit...
      ‚è≥ YALA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.573070
         RMSE: 0.757014
         R¬≤ Score: -0.8323 (Poor - 83.2% variance explained)
      üîπ YALA: Training TCN (50 epochs)...
      ‚è≥ YALA TCN: Epoch 10/50 (20%)
      ‚è≥ YALA TCN: Epoch 20/50 (40%)
      ‚è≥ YALA TCN: Epoch 30/50 (60%)
      ‚è≥ YALA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.520877
         RMSE: 0.721718
         R¬≤ Score: -0.6655
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YALA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YALA Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLNO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.1905 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLNO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLNO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=42.4248 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLNO XGBoost: Starting GridSearchCV fit...
       ‚úÖ YALA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=70.4427 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YALA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ YALA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=63.2891 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YALA XGBoost: Starting GridSearchCV fit...
       ‚úÖ PPTA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=164.4732 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.6s
    - LSTM: MSE=0.2975
    - TCN: MSE=0.1256
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1256
        ‚Ä¢ LSTM: MSE=0.2975
        ‚Ä¢ Random Forest: MSE=129.1247
        ‚Ä¢ LightGBM Regressor (CPU): MSE=161.1471
        ‚Ä¢ XGBoost: MSE=164.4732
   ‚úÖ PPTA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PPTA (TargetReturn): TCN with MSE=0.1256
üêõ DEBUG: PPTA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PPTA.
üêõ DEBUG: PPTA - Moving model to CPU before return...
üêõ DEBUG [22:52:16.013]: PPTA - Returning result metadata...
üêõ DEBUG: train_worker started for JNUG
üêõ DEBUG [22:52:16.013]: Main received result for PPTA
üêõ DEBUG: Training progress: 228/959 done
  ‚öôÔ∏è Training models for JNUG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - JNUG: Initiating feature extraction for training.
  [DIAGNOSTIC] JNUG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JNUG: rows after features available: 126
üéØ JNUG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JNUG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JNUG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JNUG: Training LSTM (50 epochs)...
      ‚è≥ JNUG LSTM: Epoch 10/50 (20%)
      ‚è≥ JNUG LSTM: Epoch 20/50 (40%)
      ‚è≥ JNUG LSTM: Epoch 30/50 (60%)
      ‚è≥ JNUG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.270620
         RMSE: 0.520211
         R¬≤ Score: -0.9202 (Poor - 92.0% variance explained)
      üîπ JNUG: Training TCN (50 epochs)...
      ‚è≥ JNUG TCN: Epoch 10/50 (20%)
      ‚è≥ JNUG TCN: Epoch 20/50 (40%)
      ‚è≥ JNUG TCN: Epoch 30/50 (60%)
      ‚è≥ JNUG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.155907
         RMSE: 0.394851
         R¬≤ Score: -0.1062
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JNUG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JNUG Random Forest: Starting GridSearchCV fit...
       ‚úÖ JNUG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=128.7511 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JNUG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JNUG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=125.3818 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JNUG XGBoost: Starting GridSearchCV fit...
       ‚úÖ KGC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=30.1924 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.3973
    - TCN: MSE=0.3351
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3351
        ‚Ä¢ LSTM: MSE=0.3973
        ‚Ä¢ XGBoost: MSE=30.1924
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.4335
        ‚Ä¢ Random Forest: MSE=30.5330
   ‚úÖ KGC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KGC (TargetReturn): TCN with MSE=0.3351
üêõ DEBUG: KGC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KGC.
üêõ DEBUG: KGC - Moving model to CPU before return...
üêõ DEBUG [22:52:25.289]: KGC - Returning result metadata...
üêõ DEBUG: train_worker started for KD
üêõ DEBUG [22:52:25.290]: Main received result for KGC
  ‚öôÔ∏è Training models for KD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - KD: Initiating feature extraction for training.
  [DIAGNOSTIC] KD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KD: rows after features available: 126
üéØ KD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KD: Training LSTM (50 epochs)...
      ‚è≥ KD LSTM: Epoch 10/50 (20%)
      ‚è≥ KD LSTM: Epoch 20/50 (40%)
      ‚è≥ KD LSTM: Epoch 30/50 (60%)
      ‚è≥ KD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.262328
         RMSE: 0.512179
         R¬≤ Score: -0.8310 (Poor - 83.1% variance explained)
      üîπ KD: Training TCN (50 epochs)...
      ‚è≥ KD TCN: Epoch 10/50 (20%)
      ‚è≥ KD TCN: Epoch 20/50 (40%)
      ‚è≥ KD TCN: Epoch 30/50 (60%)
      ‚è≥ KD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.239835
         RMSE: 0.489729
         R¬≤ Score: -0.6740
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KD Random Forest: Starting GridSearchCV fit...
       ‚úÖ KD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.1172 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.1885 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KD XGBoost: Starting GridSearchCV fit...
       ‚úÖ SNEX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.4900 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.3457
    - TCN: MSE=0.2234
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2234
        ‚Ä¢ LSTM: MSE=0.3457
        ‚Ä¢ XGBoost: MSE=15.4900
        ‚Ä¢ Random Forest: MSE=17.4973
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.7223
   ‚úÖ SNEX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SNEX (TargetReturn): TCN with MSE=0.2234
üêõ DEBUG: SNEX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SNEX.
üêõ DEBUG: SNEX - Moving model to CPU before return...
üêõ DEBUG [22:52:48.031]: SNEX - Returning result metadata...
üêõ DEBUG: train_worker started for NBP
üêõ DEBUG [22:52:48.031]: Main received result for SNEX
  ‚öôÔ∏è Training models for NBP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - NBP: Initiating feature extraction for training.
  [DIAGNOSTIC] NBP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NBP: rows after features available: 126
üéØ NBP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NBP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NBP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NBP: Training LSTM (50 epochs)...
      ‚è≥ NBP LSTM: Epoch 10/50 (20%)
      ‚è≥ NBP LSTM: Epoch 20/50 (40%)
      ‚è≥ NBP LSTM: Epoch 30/50 (60%)
      ‚è≥ NBP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.829052
         RMSE: 0.910523
         R¬≤ Score: -0.8920 (Poor - 89.2% variance explained)
      üîπ NBP: Training TCN (50 epochs)...
      ‚è≥ NBP TCN: Epoch 10/50 (20%)
      ‚è≥ NBP TCN: Epoch 20/50 (40%)
      ‚è≥ NBP TCN: Epoch 30/50 (60%)
      ‚è≥ NBP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.703298
         RMSE: 0.838629
         R¬≤ Score: -0.6050
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NBP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NBP Random Forest: Starting GridSearchCV fit...
       ‚úÖ NBP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=338.4545 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NBP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NBP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=327.7351 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NBP XGBoost: Starting GridSearchCV fit...
       ‚úÖ CMPX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=168.7960 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.3669
    - TCN: MSE=0.4341
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3669
        ‚Ä¢ TCN: MSE=0.4341
        ‚Ä¢ LightGBM Regressor (CPU): MSE=116.8377
        ‚Ä¢ Random Forest: MSE=143.5305
        ‚Ä¢ XGBoost: MSE=168.7960
   ‚úÖ CMPX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CMPX (TargetReturn): LSTM with MSE=0.3669
üêõ DEBUG: CMPX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CMPX.
üêõ DEBUG: CMPX - Moving model to CPU before return...
üêõ DEBUG [22:53:02.125]: CMPX - Returning result metadata...
üêõ DEBUG: train_worker started for MGNI
üêõ DEBUG [22:53:02.125]: Main received result for CMPX
  ‚öôÔ∏è Training models for MGNI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - MGNI: Initiating feature extraction for training.
  [DIAGNOSTIC] MGNI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MGNI: rows after features available: 126
üéØ MGNI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MGNI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MGNI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MGNI: Training LSTM (50 epochs)...
      ‚è≥ MGNI LSTM: Epoch 10/50 (20%)
      ‚è≥ MGNI LSTM: Epoch 20/50 (40%)
      ‚è≥ MGNI LSTM: Epoch 30/50 (60%)
      ‚è≥ MGNI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.609612
         RMSE: 0.780776
         R¬≤ Score: -0.8030 (Poor - 80.3% variance explained)
      üîπ MGNI: Training TCN (50 epochs)...
      ‚è≥ MGNI TCN: Epoch 10/50 (20%)
      ‚è≥ MGNI TCN: Epoch 20/50 (40%)
      ‚è≥ MGNI TCN: Epoch 30/50 (60%)
      ‚è≥ MGNI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.686669
         RMSE: 0.828655
         R¬≤ Score: -1.0310
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MGNI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MGNI Random Forest: Starting GridSearchCV fit...
       ‚úÖ MGNI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=83.1445 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MGNI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MGNI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=146.1361 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MGNI XGBoost: Starting GridSearchCV fit...
       ‚úÖ CTOS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=48.8574 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.4124
    - TCN: MSE=0.4363
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4124
        ‚Ä¢ TCN: MSE=0.4363
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.7842
        ‚Ä¢ XGBoost: MSE=48.8574
        ‚Ä¢ Random Forest: MSE=50.9801
   ‚úÖ CTOS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTOS (TargetReturn): LSTM with MSE=0.4124
üêõ DEBUG: CTOS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTOS.
üêõ DEBUG: CTOS - Moving model to CPU before return...
üêõ DEBUG [22:53:09.675]: CTOS - Returning result metadata...
üêõ DEBUG [22:53:09.677]: Main received result for CTOS
üêõ DEBUG: Training progress: 232/959 done
üêõ DEBUG: train_worker started for IVA
  ‚öôÔ∏è Training models for IVA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - IVA: Initiating feature extraction for training.
  [DIAGNOSTIC] IVA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IVA: rows after features available: 126
üéØ IVA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IVA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IVA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IVA: Training LSTM (50 epochs)...
      ‚è≥ IVA LSTM: Epoch 10/50 (20%)
      ‚è≥ IVA LSTM: Epoch 20/50 (40%)
      ‚è≥ IVA LSTM: Epoch 30/50 (60%)
       ‚úÖ DRS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=71.7966 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.2346
    - TCN: MSE=0.1196
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1196
        ‚Ä¢ LSTM: MSE=0.2346
        ‚Ä¢ LightGBM Regressor (CPU): MSE=53.4427
        ‚Ä¢ Random Forest: MSE=69.9720
        ‚Ä¢ XGBoost: MSE=71.7966
   ‚úÖ DRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DRS (TargetReturn): TCN with MSE=0.1196
üêõ DEBUG: DRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DRS.
üêõ DEBUG: DRS - Moving model to CPU before return...
üêõ DEBUG [22:53:11.375]: DRS - Returning result metadata...
üêõ DEBUG: train_worker started for AVGO
üêõ DEBUG [22:53:11.378]: Main received result for DRS
  ‚öôÔ∏è Training models for AVGO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - AVGO: Initiating feature extraction for training.
  [DIAGNOSTIC] AVGO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AVGO: rows after features available: 126
üéØ AVGO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AVGO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AVGO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AVGO: Training LSTM (50 epochs)...
      ‚è≥ IVA LSTM: Epoch 40/50 (80%)
      ‚è≥ AVGO LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.467414
         RMSE: 0.683676
         R¬≤ Score: -0.8362 (Poor - 83.6% variance explained)
      üîπ IVA: Training TCN (50 epochs)...
      ‚è≥ IVA TCN: Epoch 10/50 (20%)
      ‚è≥ IVA TCN: Epoch 20/50 (40%)
      ‚è≥ IVA TCN: Epoch 30/50 (60%)
      ‚è≥ IVA TCN: Epoch 40/50 (80%)
      ‚è≥ AVGO LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.292617
         RMSE: 0.540941
         R¬≤ Score: -0.1495
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IVA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IVA Random Forest: Starting GridSearchCV fit...
      ‚è≥ AVGO LSTM: Epoch 30/50 (60%)
      ‚è≥ AVGO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.342807
         RMSE: 0.585497
         R¬≤ Score: -0.4431 (Poor - 44.3% variance explained)
      üîπ AVGO: Training TCN (50 epochs)...
      ‚è≥ AVGO TCN: Epoch 10/50 (20%)
      ‚è≥ AVGO TCN: Epoch 20/50 (40%)
      ‚è≥ AVGO TCN: Epoch 30/50 (60%)
      ‚è≥ AVGO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.525219
         RMSE: 0.724720
         R¬≤ Score: -1.2111
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AVGO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AVGO Random Forest: Starting GridSearchCV fit...
       ‚úÖ IVA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=41.0368 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IVA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IVA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=57.9616 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IVA XGBoost: Starting GridSearchCV fit...
       ‚úÖ AVGO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=64.0304 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AVGO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AVGO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=53.1365 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AVGO XGBoost: Starting GridSearchCV fit...
       ‚úÖ NXT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=79.9961 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.4278
    - TCN: MSE=0.4073
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4073
        ‚Ä¢ LSTM: MSE=0.4278
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.7145
        ‚Ä¢ Random Forest: MSE=71.7575
        ‚Ä¢ XGBoost: MSE=79.9961
   ‚úÖ NXT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NXT (TargetReturn): TCN with MSE=0.4073
üêõ DEBUG: NXT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NXT.
üêõ DEBUG: NXT - Moving model to CPU before return...
üêõ DEBUG [22:53:28.078]: NXT - Returning result metadata...
üêõ DEBUG [22:53:28.079]: Main received result for NXT
üêõ DEBUG: train_worker started for CMP
  ‚öôÔ∏è Training models for CMP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - CMP: Initiating feature extraction for training.
  [DIAGNOSTIC] CMP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CMP: rows after features available: 126
üéØ CMP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CMP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CMP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CMP: Training LSTM (50 epochs)...
       ‚úÖ TOST XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=31.2604 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 118.2s
    - LSTM: MSE=0.4210
    - TCN: MSE=0.3058
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3058
        ‚Ä¢ LSTM: MSE=0.4210
        ‚Ä¢ XGBoost: MSE=31.2604
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.1587
        ‚Ä¢ Random Forest: MSE=37.1889
   ‚úÖ TOST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TOST (TargetReturn): TCN with MSE=0.3058
üêõ DEBUG: TOST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TOST.
üêõ DEBUG: TOST - Moving model to CPU before return...
üêõ DEBUG [22:53:28.212]: TOST - Returning result metadata...
üêõ DEBUG: train_worker started for AEHR
üêõ DEBUG [22:53:28.213]: Main received result for TOST
  ‚öôÔ∏è Training models for AEHR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - AEHR: Initiating feature extraction for training.
  [DIAGNOSTIC] AEHR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AEHR: rows after features available: 126
üéØ AEHR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AEHR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AEHR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AEHR: Training LSTM (50 epochs)...
      ‚è≥ CMP LSTM: Epoch 10/50 (20%)
      ‚è≥ AEHR LSTM: Epoch 10/50 (20%)
      ‚è≥ AEHR LSTM: Epoch 20/50 (40%)
      ‚è≥ CMP LSTM: Epoch 20/50 (40%)
      ‚è≥ AEHR LSTM: Epoch 30/50 (60%)
      ‚è≥ CMP LSTM: Epoch 30/50 (60%)
      ‚è≥ AEHR LSTM: Epoch 40/50 (80%)
      ‚è≥ CMP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.491365
         RMSE: 0.700974
         R¬≤ Score: -1.2183 (Poor - 121.8% variance explained)
      üîπ AEHR: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.248954
         RMSE: 0.498952
         R¬≤ Score: -0.3519 (Poor - 35.2% variance explained)
      üîπ CMP: Training TCN (50 epochs)...
      ‚è≥ AEHR TCN: Epoch 10/50 (20%)
      ‚è≥ CMP TCN: Epoch 10/50 (20%)
      ‚è≥ AEHR TCN: Epoch 20/50 (40%)
      ‚è≥ CMP TCN: Epoch 20/50 (40%)
      ‚è≥ AEHR TCN: Epoch 30/50 (60%)
      ‚è≥ CMP TCN: Epoch 30/50 (60%)
      ‚è≥ AEHR TCN: Epoch 40/50 (80%)
      ‚è≥ CMP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.188350
         RMSE: 0.433993
         R¬≤ Score: -0.0228
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CMP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CMP Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.414214
         RMSE: 0.643595
         R¬≤ Score: -0.8700
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AEHR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AEHR Random Forest: Starting GridSearchCV fit...
       ‚úÖ CMP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=273.6798 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CMP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AEHR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=129.0957 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AEHR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CMP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=167.2469 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CMP XGBoost: Starting GridSearchCV fit...
       ‚úÖ AEHR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=206.7149 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AEHR XGBoost: Starting GridSearchCV fit...
       ‚úÖ REVG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=48.2067 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.2994
    - TCN: MSE=0.3303
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2994
        ‚Ä¢ TCN: MSE=0.3303
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.6282
        ‚Ä¢ XGBoost: MSE=48.2067
        ‚Ä¢ Random Forest: MSE=53.6518
   ‚úÖ REVG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for REVG (TargetReturn): LSTM with MSE=0.2994
üêõ DEBUG: REVG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for REVG.
üêõ DEBUG: REVG - Moving model to CPU before return...
üêõ DEBUG [22:53:36.214]: REVG - Returning result metadata...
üêõ DEBUG: train_worker started for EVLV
üêõ DEBUG [22:53:36.217]: Main received result for REVG
üêõ DEBUG: Training progress: 236/959 done
  ‚öôÔ∏è Training models for EVLV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - EVLV: Initiating feature extraction for training.
  [DIAGNOSTIC] EVLV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EVLV: rows after features available: 126
üéØ EVLV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EVLV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EVLV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EVLV: Training LSTM (50 epochs)...
      ‚è≥ EVLV LSTM: Epoch 10/50 (20%)
      ‚è≥ EVLV LSTM: Epoch 20/50 (40%)
      ‚è≥ EVLV LSTM: Epoch 30/50 (60%)
      ‚è≥ EVLV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.482275
         RMSE: 0.694460
         R¬≤ Score: -0.7609 (Poor - 76.1% variance explained)
      üîπ EVLV: Training TCN (50 epochs)...
      ‚è≥ EVLV TCN: Epoch 10/50 (20%)
      ‚è≥ EVLV TCN: Epoch 20/50 (40%)
      ‚è≥ EVLV TCN: Epoch 30/50 (60%)
      ‚è≥ EVLV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.544274
         RMSE: 0.737750
         R¬≤ Score: -0.9873
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EVLV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EVLV Random Forest: Starting GridSearchCV fit...
       ‚úÖ EVLV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=296.4798 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EVLV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EVLV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=243.4980 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EVLV XGBoost: Starting GridSearchCV fit...
       ‚úÖ HNRG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=98.4636 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.3452
    - TCN: MSE=0.1945
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1945
        ‚Ä¢ LSTM: MSE=0.3452
        ‚Ä¢ XGBoost: MSE=98.4636
        ‚Ä¢ Random Forest: MSE=104.7036
        ‚Ä¢ LightGBM Regressor (CPU): MSE=105.9664
   ‚úÖ HNRG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HNRG (TargetReturn): TCN with MSE=0.1945
üêõ DEBUG: HNRG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HNRG.
üêõ DEBUG: HNRG - Moving model to CPU before return...
üêõ DEBUG [22:53:52.000]: HNRG - Returning result metadata...
üêõ DEBUG: train_worker started for HRTG
üêõ DEBUG [22:53:52.001]: Main received result for HNRG
  ‚öôÔ∏è Training models for HRTG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - HRTG: Initiating feature extraction for training.
  [DIAGNOSTIC] HRTG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HRTG: rows after features available: 126
üéØ HRTG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HRTG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HRTG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HRTG: Training LSTM (50 epochs)...
      ‚è≥ HRTG LSTM: Epoch 10/50 (20%)
      ‚è≥ HRTG LSTM: Epoch 20/50 (40%)
       ‚úÖ BLOK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=29.7362 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.5580
    - TCN: MSE=0.4264
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4264
        ‚Ä¢ LSTM: MSE=0.5580
        ‚Ä¢ XGBoost: MSE=29.7362
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.2089
        ‚Ä¢ Random Forest: MSE=40.9740
   ‚úÖ BLOK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BLOK (TargetReturn): TCN with MSE=0.4264
üêõ DEBUG: BLOK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BLOK.
üêõ DEBUG: BLOK - Moving model to CPU before return...
üêõ DEBUG [22:53:52.940]: BLOK - Returning result metadata...
üêõ DEBUG: train_worker started for ODD
üêõ DEBUG [22:53:52.941]: Main received result for BLOK
  ‚öôÔ∏è Training models for ODD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ODD: Initiating feature extraction for training.
  [DIAGNOSTIC] ODD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ODD: rows after features available: 126
üéØ ODD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ODD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ODD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ODD: Training LSTM (50 epochs)...
      ‚è≥ ODD LSTM: Epoch 10/50 (20%)
      ‚è≥ HRTG LSTM: Epoch 30/50 (60%)
      ‚è≥ ODD LSTM: Epoch 20/50 (40%)
      ‚è≥ HRTG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.382613
         RMSE: 0.618557
         R¬≤ Score: -0.6896 (Poor - 69.0% variance explained)
      üîπ HRTG: Training TCN (50 epochs)...
      ‚è≥ ODD LSTM: Epoch 30/50 (60%)
      ‚è≥ HRTG TCN: Epoch 10/50 (20%)
      ‚è≥ HRTG TCN: Epoch 20/50 (40%)
      ‚è≥ HRTG TCN: Epoch 30/50 (60%)
      ‚è≥ HRTG TCN: Epoch 40/50 (80%)
      ‚è≥ ODD LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.401915
         RMSE: 0.633967
         R¬≤ Score: -0.7749
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HRTG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HRTG Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.230300
         RMSE: 0.479896
         R¬≤ Score: -0.3568 (Poor - 35.7% variance explained)
      üîπ ODD: Training TCN (50 epochs)...
      ‚è≥ ODD TCN: Epoch 10/50 (20%)
      ‚è≥ ODD TCN: Epoch 20/50 (40%)
      ‚è≥ ODD TCN: Epoch 30/50 (60%)
      ‚è≥ ODD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.238438
         RMSE: 0.488301
         R¬≤ Score: -0.4048
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ODD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ODD Random Forest: Starting GridSearchCV fit...
       ‚úÖ MNMD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=74.5037 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.1s
    - LSTM: MSE=0.5072
    - TCN: MSE=0.4780
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4780
        ‚Ä¢ LSTM: MSE=0.5072
        ‚Ä¢ Random Forest: MSE=60.1489
        ‚Ä¢ LightGBM Regressor (CPU): MSE=65.8754
        ‚Ä¢ XGBoost: MSE=74.5037
   ‚úÖ MNMD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MNMD (TargetReturn): TCN with MSE=0.4780
üêõ DEBUG: MNMD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MNMD.
üêõ DEBUG: MNMD - Moving model to CPU before return...
üêõ DEBUG [22:53:56.271]: MNMD - Returning result metadata...
üêõ DEBUG: train_worker started for OMAB
üêõ DEBUG [22:53:56.272]: Main received result for MNMD
  ‚öôÔ∏è Training models for OMAB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - OMAB: Initiating feature extraction for training.
  [DIAGNOSTIC] OMAB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OMAB: rows after features available: 126
üéØ OMAB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OMAB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OMAB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OMAB: Training LSTM (50 epochs)...
      ‚è≥ OMAB LSTM: Epoch 10/50 (20%)
      ‚è≥ OMAB LSTM: Epoch 20/50 (40%)
       ‚úÖ HRTG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=141.3029 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HRTG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ OMAB LSTM: Epoch 30/50 (60%)
       ‚úÖ DNA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=180.0290 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.4029
    - TCN: MSE=0.4496
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4029
        ‚Ä¢ TCN: MSE=0.4496
        ‚Ä¢ LightGBM Regressor (CPU): MSE=151.0511
        ‚Ä¢ XGBoost: MSE=180.0290
        ‚Ä¢ Random Forest: MSE=196.5968
   ‚úÖ DNA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DNA (TargetReturn): LSTM with MSE=0.4029
üêõ DEBUG: DNA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DNA.
üêõ DEBUG: DNA - Moving model to CPU before return...
üêõ DEBUG [22:53:58.085]: DNA - Returning result metadata...
üêõ DEBUG: train_worker started for UNFI
üêõ DEBUG [22:53:58.087]: Main received result for DNA
üêõ DEBUG: Training progress: 240/959 done
  ‚öôÔ∏è Training models for UNFI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - UNFI: Initiating feature extraction for training.
  [DIAGNOSTIC] UNFI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UNFI: rows after features available: 126
üéØ UNFI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UNFI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UNFI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UNFI: Training LSTM (50 epochs)...
      ‚è≥ OMAB LSTM: Epoch 40/50 (80%)
       ‚úÖ HRTG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=128.4479 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HRTG XGBoost: Starting GridSearchCV fit...
      ‚è≥ UNFI LSTM: Epoch 10/50 (20%)
       ‚úÖ ODD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=97.9150 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ODD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.280333
         RMSE: 0.529465
         R¬≤ Score: -0.9932 (Poor - 99.3% variance explained)
      üîπ OMAB: Training TCN (50 epochs)...
      ‚è≥ OMAB TCN: Epoch 10/50 (20%)
      ‚è≥ OMAB TCN: Epoch 20/50 (40%)
      ‚è≥ OMAB TCN: Epoch 30/50 (60%)
      ‚è≥ OMAB TCN: Epoch 40/50 (80%)
      ‚è≥ UNFI LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.273259
         RMSE: 0.522742
         R¬≤ Score: -0.9429
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OMAB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OMAB Random Forest: Starting GridSearchCV fit...
       ‚úÖ ODD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=60.3450 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ODD XGBoost: Starting GridSearchCV fit...
      ‚è≥ UNFI LSTM: Epoch 30/50 (60%)
      ‚è≥ UNFI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.100533
         RMSE: 0.317070
         R¬≤ Score: -0.7784 (Poor - 77.8% variance explained)
      üîπ UNFI: Training TCN (50 epochs)...
      ‚è≥ UNFI TCN: Epoch 10/50 (20%)
      ‚è≥ UNFI TCN: Epoch 20/50 (40%)
      ‚è≥ UNFI TCN: Epoch 30/50 (60%)
      ‚è≥ UNFI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.061510
         RMSE: 0.248013
         R¬≤ Score: -0.0881
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UNFI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UNFI Random Forest: Starting GridSearchCV fit...
       ‚úÖ OMAB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.9399 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OMAB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OMAB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.1642 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OMAB XGBoost: Starting GridSearchCV fit...
       ‚úÖ UNFI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.1945 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UNFI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UNFI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.4449 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UNFI XGBoost: Starting GridSearchCV fit...
       ‚úÖ YALA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=101.9074 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.9s
    - LSTM: MSE=0.5731
    - TCN: MSE=0.5209
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5209
        ‚Ä¢ LSTM: MSE=0.5731
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.2891
        ‚Ä¢ Random Forest: MSE=70.4427
        ‚Ä¢ XGBoost: MSE=101.9074
   ‚úÖ YALA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YALA (TargetReturn): TCN with MSE=0.5209
üêõ DEBUG: YALA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YALA.
üêõ DEBUG: YALA - Moving model to CPU before return...
üêõ DEBUG [22:54:04.957]: YALA - Returning result metadata...
üêõ DEBUG: train_worker started for LLYVA
  ‚öôÔ∏è Training models for LLYVA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - LLYVA: Initiating feature extraction for training.
  [DIAGNOSTIC] LLYVA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LLYVA: rows after features available: 126
üéØ LLYVA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LLYVA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LLYVA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LLYVA: Training LSTM (50 epochs)...
      ‚è≥ LLYVA LSTM: Epoch 10/50 (20%)
       ‚úÖ SLNO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=95.7733 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.2654
    - TCN: MSE=0.1887
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1887
        ‚Ä¢ LSTM: MSE=0.2654
        ‚Ä¢ Random Forest: MSE=30.1905
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.4248
        ‚Ä¢ XGBoost: MSE=95.7733
   ‚úÖ SLNO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLNO (TargetReturn): TCN with MSE=0.1887
üêõ DEBUG: SLNO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLNO.
üêõ DEBUG: SLNO - Moving model to CPU before return...
üêõ DEBUG [22:54:05.565]: SLNO - Returning result metadata...
üêõ DEBUG: train_worker started for ZS
üêõ DEBUG [22:54:05.566]: Main received result for SLNO
üêõ DEBUG [22:54:05.566]: Main received result for YALA
  ‚öôÔ∏è Training models for ZS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ZS: Initiating feature extraction for training.
  [DIAGNOSTIC] ZS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ZS: rows after features available: 126
üéØ ZS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ZS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ZS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ZS: Training LSTM (50 epochs)...
      ‚è≥ LLYVA LSTM: Epoch 20/50 (40%)
      ‚è≥ ZS LSTM: Epoch 10/50 (20%)
      ‚è≥ LLYVA LSTM: Epoch 30/50 (60%)
      ‚è≥ ZS LSTM: Epoch 20/50 (40%)
      ‚è≥ LLYVA LSTM: Epoch 40/50 (80%)
      ‚è≥ ZS LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.429816
         RMSE: 0.655603
         R¬≤ Score: -1.2532 (Poor - 125.3% variance explained)
      üîπ LLYVA: Training TCN (50 epochs)...
      ‚è≥ ZS LSTM: Epoch 40/50 (80%)
      ‚è≥ LLYVA TCN: Epoch 10/50 (20%)
      ‚è≥ LLYVA TCN: Epoch 20/50 (40%)
      ‚è≥ LLYVA TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.474801
         RMSE: 0.689058
         R¬≤ Score: -1.4765 (Poor - 147.7% variance explained)
      üîπ ZS: Training TCN (50 epochs)...
      ‚è≥ LLYVA TCN: Epoch 40/50 (80%)
      ‚è≥ ZS TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.342306
         RMSE: 0.585069
         R¬≤ Score: -0.7944
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LLYVA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LLYVA Random Forest: Starting GridSearchCV fit...
      ‚è≥ ZS TCN: Epoch 20/50 (40%)
      ‚è≥ ZS TCN: Epoch 30/50 (60%)
      ‚è≥ ZS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.211501
         RMSE: 0.459893
         R¬≤ Score: -0.1032
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ZS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ZS Random Forest: Starting GridSearchCV fit...
       ‚úÖ LLYVA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.8635 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LLYVA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ZS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.6421 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ZS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LLYVA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.2109 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LLYVA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ZS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=35.8947 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ZS XGBoost: Starting GridSearchCV fit...
       ‚úÖ JNUG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=122.4357 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.2706
    - TCN: MSE=0.1559
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1559
        ‚Ä¢ LSTM: MSE=0.2706
        ‚Ä¢ XGBoost: MSE=122.4357
        ‚Ä¢ LightGBM Regressor (CPU): MSE=125.3818
        ‚Ä¢ Random Forest: MSE=128.7511
   ‚úÖ JNUG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JNUG (TargetReturn): TCN with MSE=0.1559
üêõ DEBUG: JNUG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JNUG.
üêõ DEBUG: JNUG - Moving model to CPU before return...
üêõ DEBUG [22:54:20.399]: JNUG - Returning result metadata...
üêõ DEBUG: train_worker started for NIU
üêõ DEBUG [22:54:20.402]: Main received result for JNUG
  ‚öôÔ∏è Training models for NIU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - NIU: Initiating feature extraction for training.
  [DIAGNOSTIC] NIU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NIU: rows after features available: 126
üéØ NIU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NIU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NIU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NIU: Training LSTM (50 epochs)...
      ‚è≥ NIU LSTM: Epoch 10/50 (20%)
      ‚è≥ NIU LSTM: Epoch 20/50 (40%)
      ‚è≥ NIU LSTM: Epoch 30/50 (60%)
      ‚è≥ NIU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.257140
         RMSE: 0.507090
         R¬≤ Score: -1.1840 (Poor - 118.4% variance explained)
      üîπ NIU: Training TCN (50 epochs)...
      ‚è≥ NIU TCN: Epoch 10/50 (20%)
      ‚è≥ NIU TCN: Epoch 20/50 (40%)
      ‚è≥ NIU TCN: Epoch 30/50 (60%)
      ‚è≥ NIU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.154734
         RMSE: 0.393362
         R¬≤ Score: -0.3142
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NIU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NIU Random Forest: Starting GridSearchCV fit...
       ‚úÖ NIU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=223.6819 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NIU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NIU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=225.1025 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NIU XGBoost: Starting GridSearchCV fit...
       ‚úÖ KD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=22.4067 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.2623
    - TCN: MSE=0.2398
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2398
        ‚Ä¢ LSTM: MSE=0.2623
        ‚Ä¢ XGBoost: MSE=22.4067
        ‚Ä¢ Random Forest: MSE=23.1172
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.1885
   ‚úÖ KD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KD (TargetReturn): TCN with MSE=0.2398
üêõ DEBUG: KD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KD.
üêõ DEBUG: KD - Moving model to CPU before return...
üêõ DEBUG [22:54:30.126]: KD - Returning result metadata...
üêõ DEBUG: train_worker started for SE
üêõ DEBUG [22:54:30.133]: Main received result for KD
üêõ DEBUG: Training progress: 244/959 done
  ‚öôÔ∏è Training models for SE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SE: Initiating feature extraction for training.
  [DIAGNOSTIC] SE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SE: rows after features available: 126
üéØ SE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SE: Training LSTM (50 epochs)...
      ‚è≥ SE LSTM: Epoch 10/50 (20%)
      ‚è≥ SE LSTM: Epoch 20/50 (40%)
      ‚è≥ SE LSTM: Epoch 30/50 (60%)
      ‚è≥ SE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.303104
         RMSE: 0.550549
         R¬≤ Score: -0.6697 (Poor - 67.0% variance explained)
      üîπ SE: Training TCN (50 epochs)...
      ‚è≥ SE TCN: Epoch 10/50 (20%)
      ‚è≥ SE TCN: Epoch 20/50 (40%)
      ‚è≥ SE TCN: Epoch 30/50 (60%)
      ‚è≥ SE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.262489
         RMSE: 0.512337
         R¬≤ Score: -0.4460
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SE Random Forest: Starting GridSearchCV fit...
       ‚úÖ SE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.1641 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.3690 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SE XGBoost: Starting GridSearchCV fit...
       ‚úÖ NBP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=754.6946 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.5s
    - LSTM: MSE=0.8291
    - TCN: MSE=0.7033
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7033
        ‚Ä¢ LSTM: MSE=0.8291
        ‚Ä¢ LightGBM Regressor (CPU): MSE=327.7351
        ‚Ä¢ Random Forest: MSE=338.4545
        ‚Ä¢ XGBoost: MSE=754.6946
   ‚úÖ NBP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NBP (TargetReturn): TCN with MSE=0.7033
üêõ DEBUG: NBP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NBP.
üêõ DEBUG: NBP - Moving model to CPU before return...
üêõ DEBUG [22:54:49.237]: NBP - Returning result metadata...
üêõ DEBUG: train_worker started for VIK
üêõ DEBUG [22:54:49.242]: Main received result for NBP
  ‚öôÔ∏è Training models for VIK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - VIK: Initiating feature extraction for training.
  [DIAGNOSTIC] VIK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VIK: rows after features available: 126
üéØ VIK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VIK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VIK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VIK: Training LSTM (50 epochs)...
      ‚è≥ VIK LSTM: Epoch 10/50 (20%)
      ‚è≥ VIK LSTM: Epoch 20/50 (40%)
      ‚è≥ VIK LSTM: Epoch 30/50 (60%)
      ‚è≥ VIK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.735086
         RMSE: 0.857371
         R¬≤ Score: -1.0780 (Poor - 107.8% variance explained)
      üîπ VIK: Training TCN (50 epochs)...
      ‚è≥ VIK TCN: Epoch 10/50 (20%)
      ‚è≥ VIK TCN: Epoch 20/50 (40%)
      ‚è≥ VIK TCN: Epoch 30/50 (60%)
      ‚è≥ VIK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.681346
         RMSE: 0.825437
         R¬≤ Score: -0.9261
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VIK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VIK Random Forest: Starting GridSearchCV fit...
       ‚úÖ VIK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.2773 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VIK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VIK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=23.8720 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VIK XGBoost: Starting GridSearchCV fit...
       ‚úÖ MGNI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=131.5785 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.5s
    - LSTM: MSE=0.6096
    - TCN: MSE=0.6867
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6096
        ‚Ä¢ TCN: MSE=0.6867
        ‚Ä¢ Random Forest: MSE=83.1445
        ‚Ä¢ XGBoost: MSE=131.5785
        ‚Ä¢ LightGBM Regressor (CPU): MSE=146.1361
   ‚úÖ MGNI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MGNI (TargetReturn): LSTM with MSE=0.6096
üêõ DEBUG: MGNI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MGNI.
üêõ DEBUG: MGNI - Moving model to CPU before return...
üêõ DEBUG [22:55:03.788]: MGNI - Returning result metadata...
üêõ DEBUG [22:55:03.788]: Main received result for MGNI
üêõ DEBUG: train_worker started for CIFR
  ‚öôÔ∏è Training models for CIFR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CIFR: Initiating feature extraction for training.
  [DIAGNOSTIC] CIFR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CIFR: rows after features available: 126
üéØ CIFR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CIFR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CIFR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CIFR: Training LSTM (50 epochs)...
      ‚è≥ CIFR LSTM: Epoch 10/50 (20%)
      ‚è≥ CIFR LSTM: Epoch 20/50 (40%)
      ‚è≥ CIFR LSTM: Epoch 30/50 (60%)
      ‚è≥ CIFR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.939415
         RMSE: 0.969234
         R¬≤ Score: -1.3353 (Poor - 133.5% variance explained)
      üîπ CIFR: Training TCN (50 epochs)...
      ‚è≥ CIFR TCN: Epoch 10/50 (20%)
      ‚è≥ CIFR TCN: Epoch 20/50 (40%)
      ‚è≥ CIFR TCN: Epoch 30/50 (60%)
      ‚è≥ CIFR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.580825
         RMSE: 0.762119
         R¬≤ Score: -0.4439
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CIFR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CIFR Random Forest: Starting GridSearchCV fit...
       ‚úÖ CIFR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=133.7924 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CIFR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CIFR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=375.3860 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CIFR XGBoost: Starting GridSearchCV fit...
       ‚úÖ IVA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.1407 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.3s
    - LSTM: MSE=0.4674
    - TCN: MSE=0.2926
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2926
        ‚Ä¢ LSTM: MSE=0.4674
        ‚Ä¢ Random Forest: MSE=41.0368
        ‚Ä¢ XGBoost: MSE=44.1407
        ‚Ä¢ LightGBM Regressor (CPU): MSE=57.9616
   ‚úÖ IVA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IVA (TargetReturn): TCN with MSE=0.2926
üêõ DEBUG: IVA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IVA.
üêõ DEBUG: IVA - Moving model to CPU before return...
üêõ DEBUG [22:55:12.909]: IVA - Returning result metadata...
üêõ DEBUG [22:55:12.910]: Main received result for IVA
üêõ DEBUG: train_worker started for SHLD
  ‚öôÔ∏è Training models for SHLD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - SHLD: Initiating feature extraction for training.
  [DIAGNOSTIC] SHLD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SHLD: rows after features available: 126
üéØ SHLD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SHLD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SHLD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SHLD: Training LSTM (50 epochs)...
      ‚è≥ SHLD LSTM: Epoch 10/50 (20%)
      ‚è≥ SHLD LSTM: Epoch 20/50 (40%)
      ‚è≥ SHLD LSTM: Epoch 30/50 (60%)
      ‚è≥ SHLD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.105865
         RMSE: 0.325368
         R¬≤ Score: -0.8904 (Poor - 89.0% variance explained)
      üîπ SHLD: Training TCN (50 epochs)...
      ‚è≥ SHLD TCN: Epoch 10/50 (20%)
      ‚è≥ SHLD TCN: Epoch 20/50 (40%)
      ‚è≥ SHLD TCN: Epoch 30/50 (60%)
      ‚è≥ SHLD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.054461
         RMSE: 0.233368
         R¬≤ Score: 0.0275
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SHLD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SHLD Random Forest: Starting GridSearchCV fit...
       ‚úÖ AVGO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.7056 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.3428
    - TCN: MSE=0.5252
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3428
        ‚Ä¢ TCN: MSE=0.5252
        ‚Ä¢ LightGBM Regressor (CPU): MSE=53.1365
        ‚Ä¢ XGBoost: MSE=53.7056
        ‚Ä¢ Random Forest: MSE=64.0304
   ‚úÖ AVGO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AVGO (TargetReturn): LSTM with MSE=0.3428
üêõ DEBUG: AVGO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AVGO.
üêõ DEBUG: AVGO - Moving model to CPU before return...
üêõ DEBUG [22:55:17.276]: AVGO - Returning result metadata...
üêõ DEBUG [22:55:17.276]: Main received result for AVGO
üêõ DEBUG: Training progress: 248/959 done
üêõ DEBUG: train_worker started for CCJ
  ‚öôÔ∏è Training models for CCJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - CCJ: Initiating feature extraction for training.
  [DIAGNOSTIC] CCJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CCJ: rows after features available: 126
üéØ CCJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CCJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CCJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CCJ: Training LSTM (50 epochs)...
      ‚è≥ CCJ LSTM: Epoch 10/50 (20%)
      ‚è≥ CCJ LSTM: Epoch 20/50 (40%)
       ‚úÖ SHLD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.6087 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SHLD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CCJ LSTM: Epoch 30/50 (60%)
      ‚è≥ CCJ LSTM: Epoch 40/50 (80%)
       ‚úÖ SHLD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.5248 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SHLD XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.679450
         RMSE: 0.824287
         R¬≤ Score: -1.4925 (Poor - 149.2% variance explained)
      üîπ CCJ: Training TCN (50 epochs)...
      ‚è≥ CCJ TCN: Epoch 10/50 (20%)
      ‚è≥ CCJ TCN: Epoch 20/50 (40%)
      ‚è≥ CCJ TCN: Epoch 30/50 (60%)
      ‚è≥ CCJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.603527
         RMSE: 0.776870
         R¬≤ Score: -1.2140
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CCJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CCJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ CCJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=41.7508 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CCJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CCJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.0095 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CCJ XGBoost: Starting GridSearchCV fit...
       ‚úÖ CMP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=284.9874 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.2490
    - TCN: MSE=0.1884
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1884
        ‚Ä¢ LSTM: MSE=0.2490
        ‚Ä¢ LightGBM Regressor (CPU): MSE=167.2469
        ‚Ä¢ Random Forest: MSE=273.6798
        ‚Ä¢ XGBoost: MSE=284.9874
   ‚úÖ CMP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CMP (TargetReturn): TCN with MSE=0.1884
üêõ DEBUG: CMP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CMP.
üêõ DEBUG: CMP - Moving model to CPU before return...
üêõ DEBUG [22:55:35.195]: CMP - Returning result metadata...
üêõ DEBUG: train_worker started for CRWD
üêõ DEBUG [22:55:35.196]: Main received result for CMP
  ‚öôÔ∏è Training models for CRWD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - CRWD: Initiating feature extraction for training.
  [DIAGNOSTIC] CRWD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRWD: rows after features available: 126
üéØ CRWD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRWD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRWD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRWD: Training LSTM (50 epochs)...
       ‚úÖ AEHR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=181.7478 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.4914
    - TCN: MSE=0.4142
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4142
        ‚Ä¢ LSTM: MSE=0.4914
        ‚Ä¢ Random Forest: MSE=129.0957
        ‚Ä¢ XGBoost: MSE=181.7478
        ‚Ä¢ LightGBM Regressor (CPU): MSE=206.7149
   ‚úÖ AEHR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AEHR (TargetReturn): TCN with MSE=0.4142
üêõ DEBUG: AEHR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AEHR.
üêõ DEBUG: AEHR - Moving model to CPU before return...
üêõ DEBUG [22:55:35.347]: AEHR - Returning result metadata...
üêõ DEBUG [22:55:35.349]: Main received result for AEHR
üêõ DEBUG: train_worker started for SII
  ‚öôÔ∏è Training models for SII (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - SII: Initiating feature extraction for training.
  [DIAGNOSTIC] SII: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SII: rows after features available: 126
üéØ SII: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SII: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SII: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SII: Training LSTM (50 epochs)...
      ‚è≥ CRWD LSTM: Epoch 10/50 (20%)
      ‚è≥ SII LSTM: Epoch 10/50 (20%)
      ‚è≥ CRWD LSTM: Epoch 20/50 (40%)
      ‚è≥ SII LSTM: Epoch 20/50 (40%)
      ‚è≥ CRWD LSTM: Epoch 30/50 (60%)
      ‚è≥ SII LSTM: Epoch 30/50 (60%)
      ‚è≥ CRWD LSTM: Epoch 40/50 (80%)
      ‚è≥ SII LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.498297
         RMSE: 0.705902
         R¬≤ Score: -1.2693 (Poor - 126.9% variance explained)
      üîπ CRWD: Training TCN (50 epochs)...
      ‚è≥ CRWD TCN: Epoch 10/50 (20%)
      ‚è≥ CRWD TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.201542
         RMSE: 0.448934
         R¬≤ Score: -1.1944 (Poor - 119.4% variance explained)
      üîπ SII: Training TCN (50 epochs)...
      ‚è≥ CRWD TCN: Epoch 30/50 (60%)
      ‚è≥ SII TCN: Epoch 10/50 (20%)
      ‚è≥ CRWD TCN: Epoch 40/50 (80%)
      ‚è≥ SII TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.317939
         RMSE: 0.563861
         R¬≤ Score: -0.4479
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRWD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRWD Random Forest: Starting GridSearchCV fit...
      ‚è≥ SII TCN: Epoch 30/50 (60%)
      ‚è≥ SII TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.098629
         RMSE: 0.314052
         R¬≤ Score: -0.0739
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SII: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SII Random Forest: Starting GridSearchCV fit...
       ‚úÖ CRWD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.4786 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRWD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SII Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=97.4676 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SII LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRWD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.5697 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRWD XGBoost: Starting GridSearchCV fit...
       ‚úÖ SII LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=69.7303 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SII XGBoost: Starting GridSearchCV fit...
       ‚úÖ EVLV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=266.8187 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.4823
    - TCN: MSE=0.5443
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4823
        ‚Ä¢ TCN: MSE=0.5443
        ‚Ä¢ LightGBM Regressor (CPU): MSE=243.4980
        ‚Ä¢ XGBoost: MSE=266.8187
        ‚Ä¢ Random Forest: MSE=296.4798
   ‚úÖ EVLV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EVLV (TargetReturn): LSTM with MSE=0.4823
üêõ DEBUG: EVLV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EVLV.
üêõ DEBUG: EVLV - Moving model to CPU before return...
üêõ DEBUG [22:55:44.050]: EVLV - Returning result metadata...
üêõ DEBUG [22:55:44.050]: Main received result for EVLV
üêõ DEBUG: train_worker started for NFLX
  ‚öôÔ∏è Training models for NFLX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - NFLX: Initiating feature extraction for training.
  [DIAGNOSTIC] NFLX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NFLX: rows after features available: 126
üéØ NFLX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NFLX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NFLX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NFLX: Training LSTM (50 epochs)...
      ‚è≥ NFLX LSTM: Epoch 10/50 (20%)
      ‚è≥ NFLX LSTM: Epoch 20/50 (40%)
      ‚è≥ NFLX LSTM: Epoch 30/50 (60%)
      ‚è≥ NFLX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.444377
         RMSE: 0.666616
         R¬≤ Score: -1.2965 (Poor - 129.7% variance explained)
      üîπ NFLX: Training TCN (50 epochs)...
      ‚è≥ NFLX TCN: Epoch 10/50 (20%)
      ‚è≥ NFLX TCN: Epoch 20/50 (40%)
      ‚è≥ NFLX TCN: Epoch 30/50 (60%)
      ‚è≥ NFLX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.252247
         RMSE: 0.502242
         R¬≤ Score: -0.3036
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NFLX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NFLX Random Forest: Starting GridSearchCV fit...
       ‚úÖ NFLX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.5018 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NFLX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NFLX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.9025 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NFLX XGBoost: Starting GridSearchCV fit...
       ‚úÖ HRTG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=256.9834 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 119.0s
    - LSTM: MSE=0.3826
    - TCN: MSE=0.4019
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3826
        ‚Ä¢ TCN: MSE=0.4019
        ‚Ä¢ LightGBM Regressor (CPU): MSE=128.4479
        ‚Ä¢ Random Forest: MSE=141.3029
        ‚Ä¢ XGBoost: MSE=256.9834
   ‚úÖ HRTG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HRTG (TargetReturn): LSTM with MSE=0.3826
üêõ DEBUG: HRTG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HRTG.
üêõ DEBUG: HRTG - Moving model to CPU before return...
üêõ DEBUG [22:55:57.420]: HRTG - Returning result metadata...
üêõ DEBUG: train_worker started for HUT
üêõ DEBUG [22:55:57.421]: Main received result for HRTG
üêõ DEBUG: Training progress: 252/959 done
  ‚öôÔ∏è Training models for HUT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - HUT: Initiating feature extraction for training.
  [DIAGNOSTIC] HUT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HUT: rows after features available: 126
üéØ HUT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HUT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HUT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HUT: Training LSTM (50 epochs)...
      ‚è≥ HUT LSTM: Epoch 10/50 (20%)
      ‚è≥ HUT LSTM: Epoch 20/50 (40%)
      ‚è≥ HUT LSTM: Epoch 30/50 (60%)
      ‚è≥ HUT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.678887
         RMSE: 0.823946
         R¬≤ Score: -1.3489 (Poor - 134.9% variance explained)
      üîπ HUT: Training TCN (50 epochs)...
      ‚è≥ HUT TCN: Epoch 10/50 (20%)
      ‚è≥ HUT TCN: Epoch 20/50 (40%)
      ‚è≥ HUT TCN: Epoch 30/50 (60%)
      ‚è≥ HUT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.679917
         RMSE: 0.824571
         R¬≤ Score: -1.3525
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HUT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HUT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ODD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=138.4412 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.2303
    - TCN: MSE=0.2384
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2303
        ‚Ä¢ TCN: MSE=0.2384
        ‚Ä¢ LightGBM Regressor (CPU): MSE=60.3450
        ‚Ä¢ Random Forest: MSE=97.9150
        ‚Ä¢ XGBoost: MSE=138.4412
   ‚úÖ ODD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ODD (TargetReturn): LSTM with MSE=0.2303
üêõ DEBUG: ODD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ODD.
üêõ DEBUG: ODD - Moving model to CPU before return...
üêõ DEBUG [22:56:01.164]: ODD - Returning result metadata...
üêõ DEBUG [22:56:01.164]: Main received result for ODD
üêõ DEBUG: train_worker started for LLYVK
  ‚öôÔ∏è Training models for LLYVK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - LLYVK: Initiating feature extraction for training.
  [DIAGNOSTIC] LLYVK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LLYVK: rows after features available: 126
üéØ LLYVK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LLYVK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LLYVK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LLYVK: Training LSTM (50 epochs)...
      ‚è≥ LLYVK LSTM: Epoch 10/50 (20%)
      ‚è≥ LLYVK LSTM: Epoch 20/50 (40%)
      ‚è≥ LLYVK LSTM: Epoch 30/50 (60%)
       ‚úÖ HUT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=86.1979 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HUT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LLYVK LSTM: Epoch 40/50 (80%)
       ‚úÖ UNFI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.7023 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.8s
    - LSTM: MSE=0.1005
    - TCN: MSE=0.0615
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0615
        ‚Ä¢ LSTM: MSE=0.1005
        ‚Ä¢ XGBoost: MSE=16.7023
        ‚Ä¢ Random Forest: MSE=21.1945
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.4449
   ‚úÖ UNFI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UNFI (TargetReturn): TCN with MSE=0.0615
üêõ DEBUG: UNFI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UNFI.
üêõ DEBUG: UNFI - Moving model to CPU before return...
üêõ DEBUG [22:56:03.338]: UNFI - Returning result metadata...
üêõ DEBUG: train_worker started for API
  ‚öôÔ∏è Training models for API (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - API: Initiating feature extraction for training.
  [DIAGNOSTIC] API: fetch_training_data - Initial data rows: 205
   ‚Ü≥ API: rows after features available: 126
üéØ API: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] API: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö API: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ API: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.495276
         RMSE: 0.703758
         R¬≤ Score: -1.2500 (Poor - 125.0% variance explained)
      üîπ LLYVK: Training TCN (50 epochs)...
      ‚è≥ LLYVK TCN: Epoch 10/50 (20%)
      ‚è≥ API LSTM: Epoch 10/50 (20%)
       ‚úÖ HUT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=109.8974 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HUT XGBoost: Starting GridSearchCV fit...
      ‚è≥ LLYVK TCN: Epoch 20/50 (40%)
      ‚è≥ LLYVK TCN: Epoch 30/50 (60%)
      ‚è≥ LLYVK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.403355
         RMSE: 0.635102
         R¬≤ Score: -0.8324
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LLYVK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LLYVK Random Forest: Starting GridSearchCV fit...
      ‚è≥ API LSTM: Epoch 20/50 (40%)
       ‚úÖ OMAB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.8793 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.2803
    - TCN: MSE=0.2733
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2733
        ‚Ä¢ LSTM: MSE=0.2803
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.1642
        ‚Ä¢ Random Forest: MSE=16.9399
        ‚Ä¢ XGBoost: MSE=21.8793
   ‚úÖ OMAB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OMAB (TargetReturn): TCN with MSE=0.2733
üêõ DEBUG: OMAB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OMAB.
üêõ DEBUG: OMAB - Moving model to CPU before return...
üêõ DEBUG [22:56:04.687]: OMAB - Returning result metadata...
üêõ DEBUG: train_worker started for VSEC
üêõ DEBUG [22:56:04.692]: Main received result for OMAB
üêõ DEBUG [22:56:04.692]: Main received result for UNFI
      ‚è≥ API LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for VSEC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - VSEC: Initiating feature extraction for training.
  [DIAGNOSTIC] VSEC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VSEC: rows after features available: 126
üéØ VSEC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VSEC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VSEC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VSEC: Training LSTM (50 epochs)...
      ‚è≥ API LSTM: Epoch 40/50 (80%)
      ‚è≥ VSEC LSTM: Epoch 10/50 (20%)
      ‚è≥ VSEC LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.269237
         RMSE: 0.518881
         R¬≤ Score: -0.9242 (Poor - 92.4% variance explained)
      üîπ API: Training TCN (50 epochs)...
      ‚è≥ API TCN: Epoch 10/50 (20%)
      ‚è≥ API TCN: Epoch 20/50 (40%)
      ‚è≥ API TCN: Epoch 30/50 (60%)
      ‚è≥ VSEC LSTM: Epoch 30/50 (60%)
      ‚è≥ API TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.160058
         RMSE: 0.400073
         R¬≤ Score: -0.1439
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä API: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ API Random Forest: Starting GridSearchCV fit...
      ‚è≥ VSEC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.157820
         RMSE: 0.397266
         R¬≤ Score: -1.2334 (Poor - 123.3% variance explained)
      üîπ VSEC: Training TCN (50 epochs)...
       ‚úÖ ZS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=26.6365 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.1s
    - LSTM: MSE=0.4748
    - TCN: MSE=0.2115
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2115
        ‚Ä¢ LSTM: MSE=0.4748
        ‚Ä¢ XGBoost: MSE=26.6365
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.8947
        ‚Ä¢ Random Forest: MSE=37.6421
   ‚úÖ ZS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ZS (TargetReturn): TCN with MSE=0.2115
üêõ DEBUG: ZS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ZS.
üêõ DEBUG: ZS - Moving model to CPU before return...
üêõ DEBUG [22:56:07.083]: ZS - Returning result metadata...
üêõ DEBUG: train_worker started for WEBL
  ‚öôÔ∏è Training models for WEBL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - WEBL: Initiating feature extraction for training.
  [DIAGNOSTIC] WEBL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WEBL: rows after features available: 126
üéØ WEBL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WEBL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WEBL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WEBL: Training LSTM (50 epochs)...
      ‚è≥ VSEC TCN: Epoch 10/50 (20%)
       ‚úÖ LLYVK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.3048 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LLYVK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ VSEC TCN: Epoch 20/50 (40%)
      ‚è≥ VSEC TCN: Epoch 30/50 (60%)
      ‚è≥ VSEC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.070500
         RMSE: 0.265518
         R¬≤ Score: 0.0023
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VSEC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VSEC Random Forest: Starting GridSearchCV fit...
      ‚è≥ WEBL LSTM: Epoch 10/50 (20%)
       ‚úÖ LLYVK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.7817 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LLYVK XGBoost: Starting GridSearchCV fit...
      ‚è≥ WEBL LSTM: Epoch 20/50 (40%)
      ‚è≥ WEBL LSTM: Epoch 30/50 (60%)
       ‚úÖ LLYVA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.7126 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.4298
    - TCN: MSE=0.3423
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3423
        ‚Ä¢ LSTM: MSE=0.4298
        ‚Ä¢ Random Forest: MSE=11.8635
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.2109
        ‚Ä¢ XGBoost: MSE=15.7126
   ‚úÖ LLYVA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LLYVA (TargetReturn): TCN with MSE=0.3423
üêõ DEBUG: LLYVA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LLYVA.
üêõ DEBUG: LLYVA - Moving model to CPU before return...
üêõ DEBUG [22:56:08.633]: LLYVA - Returning result metadata...
üêõ DEBUG [22:56:08.634]: Main received result for LLYVA
üêõ DEBUG: Training progress: 256/959 done
üêõ DEBUG [22:56:08.634]: Main received result for ZS
üêõ DEBUG: train_worker started for DOUG
  ‚öôÔ∏è Training models for DOUG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DOUG: Initiating feature extraction for training.
  [DIAGNOSTIC] DOUG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DOUG: rows after features available: 126
üéØ DOUG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DOUG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DOUG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DOUG: Training LSTM (50 epochs)...
      ‚è≥ WEBL LSTM: Epoch 40/50 (80%)
      ‚è≥ DOUG LSTM: Epoch 10/50 (20%)
       ‚úÖ API Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=96.5377 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ API LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.513037
         RMSE: 0.716266
         R¬≤ Score: -0.7421 (Poor - 74.2% variance explained)
      üîπ WEBL: Training TCN (50 epochs)...
      ‚è≥ WEBL TCN: Epoch 10/50 (20%)
      ‚è≥ DOUG LSTM: Epoch 20/50 (40%)
      ‚è≥ WEBL TCN: Epoch 20/50 (40%)
      ‚è≥ WEBL TCN: Epoch 30/50 (60%)
      ‚è≥ WEBL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.606886
         RMSE: 0.779029
         R¬≤ Score: -1.0608
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WEBL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WEBL Random Forest: Starting GridSearchCV fit...
      ‚è≥ DOUG LSTM: Epoch 30/50 (60%)
       ‚úÖ API LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=133.6218 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ API XGBoost: Starting GridSearchCV fit...
       ‚úÖ VSEC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.8166 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VSEC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ DOUG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.563877
         RMSE: 0.750917
         R¬≤ Score: -1.4391 (Poor - 143.9% variance explained)
      üîπ DOUG: Training TCN (50 epochs)...
      ‚è≥ DOUG TCN: Epoch 10/50 (20%)
       ‚úÖ VSEC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.8128 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VSEC XGBoost: Starting GridSearchCV fit...
      ‚è≥ DOUG TCN: Epoch 20/50 (40%)
      ‚è≥ DOUG TCN: Epoch 30/50 (60%)
      ‚è≥ DOUG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.355211
         RMSE: 0.595996
         R¬≤ Score: -0.5365
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DOUG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DOUG Random Forest: Starting GridSearchCV fit...
       ‚úÖ WEBL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=76.7082 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WEBL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WEBL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=81.8273 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WEBL XGBoost: Starting GridSearchCV fit...
       ‚úÖ DOUG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=91.1341 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DOUG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DOUG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=83.4999 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DOUG XGBoost: Starting GridSearchCV fit...
       ‚úÖ NIU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=334.7978 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.2571
    - TCN: MSE=0.1547
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1547
        ‚Ä¢ LSTM: MSE=0.2571
        ‚Ä¢ Random Forest: MSE=223.6819
        ‚Ä¢ LightGBM Regressor (CPU): MSE=225.1025
        ‚Ä¢ XGBoost: MSE=334.7978
   ‚úÖ NIU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NIU (TargetReturn): TCN with MSE=0.1547
üêõ DEBUG: NIU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NIU.
üêõ DEBUG: NIU - Moving model to CPU before return...
üêõ DEBUG [22:56:25.079]: NIU - Returning result metadata...
üêõ DEBUG [22:56:25.080]: Main received result for NIU
üêõ DEBUG: train_worker started for RYTM
  ‚öôÔ∏è Training models for RYTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - RYTM: Initiating feature extraction for training.
  [DIAGNOSTIC] RYTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RYTM: rows after features available: 126
üéØ RYTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RYTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RYTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RYTM: Training LSTM (50 epochs)...
      ‚è≥ RYTM LSTM: Epoch 10/50 (20%)
      ‚è≥ RYTM LSTM: Epoch 20/50 (40%)
      ‚è≥ RYTM LSTM: Epoch 30/50 (60%)
      ‚è≥ RYTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.292609
         RMSE: 0.540934
         R¬≤ Score: -0.8837 (Poor - 88.4% variance explained)
      üîπ RYTM: Training TCN (50 epochs)...
      ‚è≥ RYTM TCN: Epoch 10/50 (20%)
      ‚è≥ RYTM TCN: Epoch 20/50 (40%)
      ‚è≥ RYTM TCN: Epoch 30/50 (60%)
      ‚è≥ RYTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.160343
         RMSE: 0.400428
         R¬≤ Score: -0.0322
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RYTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RYTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ RYTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.7778 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RYTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RYTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=54.0332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RYTM XGBoost: Starting GridSearchCV fit...
       ‚úÖ SE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.0859 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.3031
    - TCN: MSE=0.2625
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2625
        ‚Ä¢ LSTM: MSE=0.3031
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.3690
        ‚Ä¢ XGBoost: MSE=27.0859
        ‚Ä¢ Random Forest: MSE=32.1641
   ‚úÖ SE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SE (TargetReturn): TCN with MSE=0.2625
üêõ DEBUG: SE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SE.
üêõ DEBUG: SE - Moving model to CPU before return...
üêõ DEBUG [22:56:33.375]: SE - Returning result metadata...
üêõ DEBUG [22:56:33.376]: Main received result for SE
üêõ DEBUG: train_worker started for LITE
  ‚öôÔ∏è Training models for LITE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - LITE: Initiating feature extraction for training.
  [DIAGNOSTIC] LITE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LITE: rows after features available: 126
üéØ LITE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LITE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LITE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LITE: Training LSTM (50 epochs)...
      ‚è≥ LITE LSTM: Epoch 10/50 (20%)
      ‚è≥ LITE LSTM: Epoch 20/50 (40%)
      ‚è≥ LITE LSTM: Epoch 30/50 (60%)
      ‚è≥ LITE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.515236
         RMSE: 0.717799
         R¬≤ Score: -0.9829 (Poor - 98.3% variance explained)
      üîπ LITE: Training TCN (50 epochs)...
      ‚è≥ LITE TCN: Epoch 10/50 (20%)
      ‚è≥ LITE TCN: Epoch 20/50 (40%)
      ‚è≥ LITE TCN: Epoch 30/50 (60%)
      ‚è≥ LITE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.592623
         RMSE: 0.769820
         R¬≤ Score: -1.2807
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LITE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LITE Random Forest: Starting GridSearchCV fit...
       ‚úÖ LITE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=64.5332 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LITE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LITE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=68.1737 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LITE XGBoost: Starting GridSearchCV fit...
       ‚úÖ VIK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.4636 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.7351
    - TCN: MSE=0.6813
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6813
        ‚Ä¢ LSTM: MSE=0.7351
        ‚Ä¢ XGBoost: MSE=17.4636
        ‚Ä¢ Random Forest: MSE=22.2773
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.8720
   ‚úÖ VIK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VIK (TargetReturn): TCN with MSE=0.6813
üêõ DEBUG: VIK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VIK.
üêõ DEBUG: VIK - Moving model to CPU before return...
üêõ DEBUG [22:56:54.712]: VIK - Returning result metadata...
üêõ DEBUG [22:56:54.712]: Main received result for VIK
üêõ DEBUG: Training progress: 260/959 done
üêõ DEBUG: train_worker started for CAKE
  ‚öôÔ∏è Training models for CAKE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CAKE: Initiating feature extraction for training.
  [DIAGNOSTIC] CAKE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CAKE: rows after features available: 126
üéØ CAKE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CAKE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CAKE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CAKE: Training LSTM (50 epochs)...
      ‚è≥ CAKE LSTM: Epoch 10/50 (20%)
      ‚è≥ CAKE LSTM: Epoch 20/50 (40%)
      ‚è≥ CAKE LSTM: Epoch 30/50 (60%)
      ‚è≥ CAKE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.604654
         RMSE: 0.777595
         R¬≤ Score: -1.1179 (Poor - 111.8% variance explained)
      üîπ CAKE: Training TCN (50 epochs)...
      ‚è≥ CAKE TCN: Epoch 10/50 (20%)
      ‚è≥ CAKE TCN: Epoch 20/50 (40%)
      ‚è≥ CAKE TCN: Epoch 30/50 (60%)
      ‚è≥ CAKE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.593754
         RMSE: 0.770555
         R¬≤ Score: -1.0797
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CAKE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CAKE Random Forest: Starting GridSearchCV fit...
       ‚úÖ CAKE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.6339 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CAKE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CAKE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.7738 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CAKE XGBoost: Starting GridSearchCV fit...
       ‚úÖ CIFR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=268.3860 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 115.9s
    - LSTM: MSE=0.9394
    - TCN: MSE=0.5808
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5808
        ‚Ä¢ LSTM: MSE=0.9394
        ‚Ä¢ Random Forest: MSE=133.7924
        ‚Ä¢ XGBoost: MSE=268.3860
        ‚Ä¢ LightGBM Regressor (CPU): MSE=375.3860
   ‚úÖ CIFR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CIFR (TargetReturn): TCN with MSE=0.5808
üêõ DEBUG: CIFR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CIFR.
üêõ DEBUG: CIFR - Moving model to CPU before return...
üêõ DEBUG [22:57:06.106]: CIFR - Returning result metadata...
üêõ DEBUG [22:57:06.106]: Main received result for CIFR
üêõ DEBUG: train_worker started for ARKX
  ‚öôÔ∏è Training models for ARKX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - ARKX: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKX: rows after features available: 126
üéØ ARKX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKX: Training LSTM (50 epochs)...
      ‚è≥ ARKX LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKX LSTM: Epoch 20/50 (40%)
      ‚è≥ ARKX LSTM: Epoch 30/50 (60%)
      ‚è≥ ARKX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.535661
         RMSE: 0.731889
         R¬≤ Score: -1.0079 (Poor - 100.8% variance explained)
      üîπ ARKX: Training TCN (50 epochs)...
      ‚è≥ ARKX TCN: Epoch 10/50 (20%)
      ‚è≥ ARKX TCN: Epoch 20/50 (40%)
      ‚è≥ ARKX TCN: Epoch 30/50 (60%)
      ‚è≥ ARKX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.591345
         RMSE: 0.768989
         R¬≤ Score: -1.2166
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKX Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.5240 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARKX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.0077 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKX XGBoost: Starting GridSearchCV fit...
       ‚úÖ SHLD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.2340 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.1059
    - TCN: MSE=0.0545
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0545
        ‚Ä¢ LSTM: MSE=0.1059
        ‚Ä¢ XGBoost: MSE=6.2340
        ‚Ä¢ Random Forest: MSE=6.6087
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.5248
   ‚úÖ SHLD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SHLD (TargetReturn): TCN with MSE=0.0545
üêõ DEBUG: SHLD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SHLD.
üêõ DEBUG: SHLD - Moving model to CPU before return...
üêõ DEBUG [22:57:17.446]: SHLD - Returning result metadata...
üêõ DEBUG [22:57:17.446]: Main received result for SHLD
üêõ DEBUG: train_worker started for LTM
  ‚öôÔ∏è Training models for LTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - LTM: Initiating feature extraction for training.
  [DIAGNOSTIC] LTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LTM: rows after features available: 126
üéØ LTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LTM: Training LSTM (50 epochs)...
      ‚è≥ LTM LSTM: Epoch 10/50 (20%)
      ‚è≥ LTM LSTM: Epoch 20/50 (40%)
      ‚è≥ LTM LSTM: Epoch 30/50 (60%)
      ‚è≥ LTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.469390
         RMSE: 0.685120
         R¬≤ Score: -1.1205 (Poor - 112.1% variance explained)
      üîπ LTM: Training TCN (50 epochs)...
      ‚è≥ LTM TCN: Epoch 10/50 (20%)
      ‚è≥ LTM TCN: Epoch 20/50 (40%)
      ‚è≥ LTM TCN: Epoch 30/50 (60%)
      ‚è≥ LTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.475118
         RMSE: 0.689288
         R¬≤ Score: -1.1464
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ CCJ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=43.7546 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.6794
    - TCN: MSE=0.6035
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6035
        ‚Ä¢ LSTM: MSE=0.6794
        ‚Ä¢ Random Forest: MSE=41.7508
        ‚Ä¢ XGBoost: MSE=43.7546
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.0095
   ‚úÖ CCJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CCJ (TargetReturn): TCN with MSE=0.6035
üêõ DEBUG: CCJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CCJ.
üêõ DEBUG: CCJ - Moving model to CPU before return...
üêõ DEBUG [22:57:20.807]: CCJ - Returning result metadata...
üêõ DEBUG: train_worker started for AFRM
üêõ DEBUG [22:57:20.808]: Main received result for CCJ
  ‚öôÔ∏è Training models for AFRM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - AFRM: Initiating feature extraction for training.
  [DIAGNOSTIC] AFRM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AFRM: rows after features available: 126
üéØ AFRM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AFRM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AFRM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AFRM: Training LSTM (50 epochs)...
      ‚è≥ AFRM LSTM: Epoch 10/50 (20%)
      ‚è≥ AFRM LSTM: Epoch 20/50 (40%)
      ‚è≥ AFRM LSTM: Epoch 30/50 (60%)
      ‚è≥ AFRM LSTM: Epoch 40/50 (80%)
       ‚úÖ LTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.7943 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.550734
         RMSE: 0.742114
         R¬≤ Score: -0.8729 (Poor - 87.3% variance explained)
      üîπ AFRM: Training TCN (50 epochs)...
      ‚è≥ AFRM TCN: Epoch 10/50 (20%)
      ‚è≥ AFRM TCN: Epoch 20/50 (40%)
      ‚è≥ AFRM TCN: Epoch 30/50 (60%)
      ‚è≥ AFRM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.551412
         RMSE: 0.742571
         R¬≤ Score: -0.8752
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AFRM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AFRM Random Forest: Starting GridSearchCV fit...
       ‚úÖ LTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.5431 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LTM XGBoost: Starting GridSearchCV fit...
       ‚úÖ AFRM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=56.3530 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AFRM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AFRM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=65.3896 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AFRM XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRWD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.6207 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 118.4s
    - LSTM: MSE=0.4983
    - TCN: MSE=0.3179
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3179
        ‚Ä¢ LSTM: MSE=0.4983
        ‚Ä¢ XGBoost: MSE=21.6207
        ‚Ä¢ Random Forest: MSE=24.4786
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.5697
   ‚úÖ CRWD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRWD (TargetReturn): TCN with MSE=0.3179
üêõ DEBUG: CRWD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRWD.
üêõ DEBUG: CRWD - Moving model to CPU before return...
üêõ DEBUG [22:57:40.088]: CRWD - Returning result metadata...
üêõ DEBUG: train_worker started for BCS
üêõ DEBUG [22:57:40.089]: Main received result for CRWD
üêõ DEBUG: Training progress: 264/959 done
  ‚öôÔ∏è Training models for BCS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - BCS: Initiating feature extraction for training.
  [DIAGNOSTIC] BCS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BCS: rows after features available: 126
üéØ BCS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BCS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BCS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BCS: Training LSTM (50 epochs)...
      ‚è≥ BCS LSTM: Epoch 10/50 (20%)
      ‚è≥ BCS LSTM: Epoch 20/50 (40%)
       ‚úÖ SII XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=71.3832 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.2015
    - TCN: MSE=0.0986
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0986
        ‚Ä¢ LSTM: MSE=0.2015
        ‚Ä¢ LightGBM Regressor (CPU): MSE=69.7303
        ‚Ä¢ XGBoost: MSE=71.3832
        ‚Ä¢ Random Forest: MSE=97.4676
   ‚úÖ SII: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SII (TargetReturn): TCN with MSE=0.0986
üêõ DEBUG: SII - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SII.
üêõ DEBUG: SII - Moving model to CPU before return...
üêõ DEBUG [22:57:41.467]: SII - Returning result metadata...
üêõ DEBUG: train_worker started for MPAA
üêõ DEBUG [22:57:41.468]: Main received result for SII
  ‚öôÔ∏è Training models for MPAA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - MPAA: Initiating feature extraction for training.
  [DIAGNOSTIC] MPAA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MPAA: rows after features available: 126
üéØ MPAA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MPAA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MPAA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MPAA: Training LSTM (50 epochs)...
      ‚è≥ BCS LSTM: Epoch 30/50 (60%)
      ‚è≥ MPAA LSTM: Epoch 10/50 (20%)
      ‚è≥ BCS LSTM: Epoch 40/50 (80%)
      ‚è≥ MPAA LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.282177
         RMSE: 0.531203
         R¬≤ Score: -0.8511 (Poor - 85.1% variance explained)
      üîπ BCS: Training TCN (50 epochs)...
      ‚è≥ BCS TCN: Epoch 10/50 (20%)
      ‚è≥ BCS TCN: Epoch 20/50 (40%)
      ‚è≥ BCS TCN: Epoch 30/50 (60%)
      ‚è≥ BCS TCN: Epoch 40/50 (80%)
      ‚è≥ MPAA LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.246404
         RMSE: 0.496391
         R¬≤ Score: -0.6165
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BCS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BCS Random Forest: Starting GridSearchCV fit...
      ‚è≥ MPAA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.398376
         RMSE: 0.631170
         R¬≤ Score: -0.8328 (Poor - 83.3% variance explained)
      üîπ MPAA: Training TCN (50 epochs)...
      ‚è≥ MPAA TCN: Epoch 10/50 (20%)
      ‚è≥ MPAA TCN: Epoch 20/50 (40%)
      ‚è≥ MPAA TCN: Epoch 30/50 (60%)
      ‚è≥ MPAA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.229265
         RMSE: 0.478816
         R¬≤ Score: -0.0548
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MPAA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MPAA Random Forest: Starting GridSearchCV fit...
       ‚úÖ BCS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.9322 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BCS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BCS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.6485 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BCS XGBoost: Starting GridSearchCV fit...
       ‚úÖ MPAA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=56.5295 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MPAA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MPAA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=63.1901 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MPAA XGBoost: Starting GridSearchCV fit...
       ‚úÖ NFLX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.2810 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.4444
    - TCN: MSE=0.2522
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2522
        ‚Ä¢ LSTM: MSE=0.4444
        ‚Ä¢ Random Forest: MSE=24.5018
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.9025
        ‚Ä¢ XGBoost: MSE=29.2810
   ‚úÖ NFLX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NFLX (TargetReturn): TCN with MSE=0.2522
üêõ DEBUG: NFLX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NFLX.
üêõ DEBUG: NFLX - Moving model to CPU before return...
üêõ DEBUG [22:57:50.992]: NFLX - Returning result metadata...
üêõ DEBUG: train_worker started for SLI
üêõ DEBUG [22:57:50.993]: Main received result for NFLX
  ‚öôÔ∏è Training models for SLI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - SLI: Initiating feature extraction for training.
  [DIAGNOSTIC] SLI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLI: rows after features available: 126
üéØ SLI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLI: Training LSTM (50 epochs)...
      ‚è≥ SLI LSTM: Epoch 10/50 (20%)
      ‚è≥ SLI LSTM: Epoch 20/50 (40%)
      ‚è≥ SLI LSTM: Epoch 30/50 (60%)
      ‚è≥ SLI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.502308
         RMSE: 0.708737
         R¬≤ Score: -1.3428 (Poor - 134.3% variance explained)
      üîπ SLI: Training TCN (50 epochs)...
      ‚è≥ SLI TCN: Epoch 10/50 (20%)
      ‚è≥ SLI TCN: Epoch 20/50 (40%)
      ‚è≥ SLI TCN: Epoch 30/50 (60%)
      ‚è≥ SLI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.435823
         RMSE: 0.660169
         R¬≤ Score: -1.0327
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLI Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=57.6457 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=62.5310 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLI XGBoost: Starting GridSearchCV fit...
       ‚úÖ HUT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=93.0578 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.6789
    - TCN: MSE=0.6799
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6789
        ‚Ä¢ TCN: MSE=0.6799
        ‚Ä¢ Random Forest: MSE=86.1979
        ‚Ä¢ XGBoost: MSE=93.0578
        ‚Ä¢ LightGBM Regressor (CPU): MSE=109.8974
   ‚úÖ HUT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HUT (TargetReturn): LSTM with MSE=0.6789
üêõ DEBUG: HUT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HUT.
üêõ DEBUG: HUT - Moving model to CPU before return...
üêõ DEBUG [22:58:03.914]: HUT - Returning result metadata...
üêõ DEBUG [22:58:03.915]: Main received result for HUT
üêõ DEBUG: train_worker started for CXW
  ‚öôÔ∏è Training models for CXW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - CXW: Initiating feature extraction for training.
  [DIAGNOSTIC] CXW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CXW: rows after features available: 126
üéØ CXW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CXW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CXW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CXW: Training LSTM (50 epochs)...
      ‚è≥ CXW LSTM: Epoch 10/50 (20%)
      ‚è≥ CXW LSTM: Epoch 20/50 (40%)
      ‚è≥ CXW LSTM: Epoch 30/50 (60%)
      ‚è≥ CXW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.101139
         RMSE: 0.318023
         R¬≤ Score: -0.5648 (Poor - 56.5% variance explained)
      üîπ CXW: Training TCN (50 epochs)...
      ‚è≥ CXW TCN: Epoch 10/50 (20%)
      ‚è≥ CXW TCN: Epoch 20/50 (40%)
      ‚è≥ CXW TCN: Epoch 30/50 (60%)
      ‚è≥ CXW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.162176
         RMSE: 0.402710
         R¬≤ Score: -1.5091
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CXW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CXW Random Forest: Starting GridSearchCV fit...
       ‚úÖ LLYVK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.2993 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.4953
    - TCN: MSE=0.4034
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4034
        ‚Ä¢ LSTM: MSE=0.4953
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.7817
        ‚Ä¢ Random Forest: MSE=10.3048
        ‚Ä¢ XGBoost: MSE=12.2993
   ‚úÖ LLYVK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LLYVK (TargetReturn): TCN with MSE=0.4034
üêõ DEBUG: LLYVK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LLYVK.
üêõ DEBUG: LLYVK - Moving model to CPU before return...
üêõ DEBUG [22:58:09.175]: LLYVK - Returning result metadata...
üêõ DEBUG [22:58:09.176]: Main received result for LLYVK
üêõ DEBUG: Training progress: 268/959 done
üêõ DEBUG: train_worker started for ICL
  ‚öôÔ∏è Training models for ICL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ICL: Initiating feature extraction for training.
  [DIAGNOSTIC] ICL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ICL: rows after features available: 126
üéØ ICL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ICL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ICL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ICL: Training LSTM (50 epochs)...
      ‚è≥ ICL LSTM: Epoch 10/50 (20%)
       ‚úÖ CXW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.9807 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CXW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ICL LSTM: Epoch 20/50 (40%)
       ‚úÖ CXW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=28.7628 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CXW XGBoost: Starting GridSearchCV fit...
      ‚è≥ ICL LSTM: Epoch 30/50 (60%)
      ‚è≥ ICL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.144932
         RMSE: 0.380699
         R¬≤ Score: -0.8099 (Poor - 81.0% variance explained)
      üîπ ICL: Training TCN (50 epochs)...
      ‚è≥ ICL TCN: Epoch 10/50 (20%)
       ‚úÖ WEBL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=69.8823 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.5130
    - TCN: MSE=0.6069
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5130
        ‚Ä¢ TCN: MSE=0.6069
        ‚Ä¢ XGBoost: MSE=69.8823
        ‚Ä¢ Random Forest: MSE=76.7082
        ‚Ä¢ LightGBM Regressor (CPU): MSE=81.8273
   ‚úÖ WEBL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WEBL (TargetReturn): LSTM with MSE=0.5130
üêõ DEBUG: WEBL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WEBL.
üêõ DEBUG: WEBL - Moving model to CPU before return...
üêõ DEBUG [22:58:11.652]: WEBL - Returning result metadata...
üêõ DEBUG: train_worker started for LPLA
      ‚è≥ ICL TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for LPLA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - LPLA: Initiating feature extraction for training.
  [DIAGNOSTIC] LPLA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LPLA: rows after features available: 126
üéØ LPLA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LPLA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LPLA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LPLA: Training LSTM (50 epochs)...
      ‚è≥ ICL TCN: Epoch 30/50 (60%)
      ‚è≥ ICL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.083962
         RMSE: 0.289762
         R¬≤ Score: -0.0485
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ICL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ICL Random Forest: Starting GridSearchCV fit...
       ‚úÖ API XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=112.9200 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.2692
    - TCN: MSE=0.1601
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1601
        ‚Ä¢ LSTM: MSE=0.2692
        ‚Ä¢ Random Forest: MSE=96.5377
        ‚Ä¢ XGBoost: MSE=112.9200
        ‚Ä¢ LightGBM Regressor (CPU): MSE=133.6218
   ‚úÖ API: Phase 3/3 - Model selection complete!
  üèÜ WINNER for API (TargetReturn): TCN with MSE=0.1601
üêõ DEBUG: API - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for API.
üêõ DEBUG: API - Moving model to CPU before return...
üêõ DEBUG [22:58:12.107]: API - Returning result metadata...
üêõ DEBUG: train_worker started for GDXU
üêõ DEBUG [22:58:12.109]: Main received result for API
      ‚è≥ LPLA LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for GDXU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - GDXU: Initiating feature extraction for training.
  [DIAGNOSTIC] GDXU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GDXU: rows after features available: 126
üéØ GDXU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GDXU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GDXU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GDXU: Training LSTM (50 epochs)...
      ‚è≥ LPLA LSTM: Epoch 20/50 (40%)
      ‚è≥ GDXU LSTM: Epoch 10/50 (20%)
      ‚è≥ LPLA LSTM: Epoch 30/50 (60%)
      ‚è≥ GDXU LSTM: Epoch 20/50 (40%)
      ‚è≥ LPLA LSTM: Epoch 40/50 (80%)
      ‚è≥ GDXU LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.437139
         RMSE: 0.661165
         R¬≤ Score: -1.0095 (Poor - 101.0% variance explained)
      üîπ LPLA: Training TCN (50 epochs)...
       ‚úÖ DOUG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=182.5733 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.5639
    - TCN: MSE=0.3552
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3552
        ‚Ä¢ LSTM: MSE=0.5639
        ‚Ä¢ LightGBM Regressor (CPU): MSE=83.4999
        ‚Ä¢ Random Forest: MSE=91.1341
        ‚Ä¢ XGBoost: MSE=182.5733
   ‚úÖ DOUG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DOUG (TargetReturn): TCN with MSE=0.3552
üêõ DEBUG: DOUG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DOUG.
üêõ DEBUG: DOUG - Moving model to CPU before return...
üêõ DEBUG [22:58:14.016]: DOUG - Returning result metadata...
üêõ DEBUG: train_worker started for GFI
      ‚è≥ LPLA TCN: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for GFI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - GFI: Initiating feature extraction for training.
  [DIAGNOSTIC] GFI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GFI: rows after features available: 126
üéØ GFI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ GDXU LSTM: Epoch 40/50 (80%)
  [DIAGNOSTIC] GFI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GFI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GFI: Training LSTM (50 epochs)...
      ‚è≥ LPLA TCN: Epoch 20/50 (40%)
      ‚è≥ LPLA TCN: Epoch 30/50 (60%)
      ‚è≥ LPLA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.385128
         RMSE: 0.620587
         R¬≤ Score: -0.7704
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LPLA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LPLA Random Forest: Starting GridSearchCV fit...
      ‚è≥ GFI LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.179738
         RMSE: 0.423955
         R¬≤ Score: -0.3918 (Poor - 39.2% variance explained)
      üîπ GDXU: Training TCN (50 epochs)...
       ‚úÖ VSEC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.8857 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.5s
    - LSTM: MSE=0.1578
    - TCN: MSE=0.0705
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0705
        ‚Ä¢ LSTM: MSE=0.1578
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.8128
        ‚Ä¢ Random Forest: MSE=25.8166
        ‚Ä¢ XGBoost: MSE=25.8857
   ‚úÖ VSEC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VSEC (TargetReturn): TCN with MSE=0.0705
üêõ DEBUG: VSEC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VSEC.
üêõ DEBUG: VSEC - Moving model to CPU before return...
üêõ DEBUG [22:58:14.624]: VSEC - Returning result metadata...
üêõ DEBUG: train_worker started for WBD
üêõ DEBUG [22:58:14.625]: Main received result for VSEC
üêõ DEBUG [22:58:14.626]: Main received result for WEBL
üêõ DEBUG [22:58:14.626]: Main received result for DOUG
üêõ DEBUG: Training progress: 272/959 done
  ‚öôÔ∏è Training models for WBD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - WBD: Initiating feature extraction for training.
  [DIAGNOSTIC] WBD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WBD: rows after features available: 126
üéØ WBD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WBD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WBD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WBD: Training LSTM (50 epochs)...
      ‚è≥ GDXU TCN: Epoch 10/50 (20%)
      ‚è≥ GDXU TCN: Epoch 20/50 (40%)
       ‚úÖ ICL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.3307 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ICL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GDXU TCN: Epoch 30/50 (60%)
      ‚è≥ GFI LSTM: Epoch 20/50 (40%)
      ‚è≥ GDXU TCN: Epoch 40/50 (80%)
      ‚è≥ WBD LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.177845
         RMSE: 0.421716
         R¬≤ Score: -0.3771
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GDXU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GDXU Random Forest: Starting GridSearchCV fit...
      ‚è≥ GFI LSTM: Epoch 30/50 (60%)
       ‚úÖ ICL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.0695 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ICL XGBoost: Starting GridSearchCV fit...
      ‚è≥ WBD LSTM: Epoch 20/50 (40%)
      ‚è≥ GFI LSTM: Epoch 40/50 (80%)
      ‚è≥ WBD LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.111063
         RMSE: 0.333262
         R¬≤ Score: -0.4516 (Poor - 45.2% variance explained)
      üîπ GFI: Training TCN (50 epochs)...
      ‚è≥ GFI TCN: Epoch 10/50 (20%)
      ‚è≥ WBD LSTM: Epoch 40/50 (80%)
      ‚è≥ GFI TCN: Epoch 20/50 (40%)
      ‚è≥ GFI TCN: Epoch 30/50 (60%)
      ‚è≥ GFI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.098615
         RMSE: 0.314030
         R¬≤ Score: -0.2889
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GFI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GFI Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.859629
         RMSE: 0.927162
         R¬≤ Score: -1.1938 (Poor - 119.4% variance explained)
      üîπ WBD: Training TCN (50 epochs)...
      ‚è≥ WBD TCN: Epoch 10/50 (20%)
      ‚è≥ WBD TCN: Epoch 20/50 (40%)
      ‚è≥ WBD TCN: Epoch 30/50 (60%)
      ‚è≥ WBD TCN: Epoch 40/50 (80%)
       ‚úÖ LPLA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.3616 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LPLA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.585650
         RMSE: 0.765278
         R¬≤ Score: -0.4946
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WBD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WBD Random Forest: Starting GridSearchCV fit...
       ‚úÖ GDXU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=268.3982 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GDXU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LPLA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.7268 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LPLA XGBoost: Starting GridSearchCV fit...
       ‚úÖ GDXU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=299.2987 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GDXU XGBoost: Starting GridSearchCV fit...
       ‚úÖ GFI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.8678 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GFI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WBD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.5729 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WBD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GFI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=43.1283 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GFI XGBoost: Starting GridSearchCV fit...
       ‚úÖ WBD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=45.4879 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WBD XGBoost: Starting GridSearchCV fit...
       ‚úÖ RYTM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=35.3804 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.2926
    - TCN: MSE=0.1603
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1603
        ‚Ä¢ LSTM: MSE=0.2926
        ‚Ä¢ XGBoost: MSE=35.3804
        ‚Ä¢ Random Forest: MSE=45.7778
        ‚Ä¢ LightGBM Regressor (CPU): MSE=54.0332
   ‚úÖ RYTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RYTM (TargetReturn): TCN with MSE=0.1603
üêõ DEBUG: RYTM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RYTM.
üêõ DEBUG: RYTM - Moving model to CPU before return...
üêõ DEBUG [22:58:28.854]: RYTM - Returning result metadata...
üêõ DEBUG: train_worker started for CTRN
üêõ DEBUG [22:58:28.858]: Main received result for RYTM
  ‚öôÔ∏è Training models for CTRN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - CTRN: Initiating feature extraction for training.
  [DIAGNOSTIC] CTRN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTRN: rows after features available: 126
üéØ CTRN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTRN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTRN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTRN: Training LSTM (50 epochs)...
      ‚è≥ CTRN LSTM: Epoch 10/50 (20%)
      ‚è≥ CTRN LSTM: Epoch 20/50 (40%)
      ‚è≥ CTRN LSTM: Epoch 30/50 (60%)
      ‚è≥ CTRN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.662005
         RMSE: 0.813637
         R¬≤ Score: -0.9396 (Poor - 94.0% variance explained)
      üîπ CTRN: Training TCN (50 epochs)...
      ‚è≥ CTRN TCN: Epoch 10/50 (20%)
      ‚è≥ CTRN TCN: Epoch 20/50 (40%)
      ‚è≥ CTRN TCN: Epoch 30/50 (60%)
      ‚è≥ CTRN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.649834
         RMSE: 0.806123
         R¬≤ Score: -0.9040
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTRN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTRN Random Forest: Starting GridSearchCV fit...
       ‚úÖ CTRN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.8210 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTRN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CTRN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=46.1196 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTRN XGBoost: Starting GridSearchCV fit...
       ‚úÖ LITE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=80.2835 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.5152
    - TCN: MSE=0.5926
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5152
        ‚Ä¢ TCN: MSE=0.5926
        ‚Ä¢ Random Forest: MSE=64.5332
        ‚Ä¢ LightGBM Regressor (CPU): MSE=68.1737
        ‚Ä¢ XGBoost: MSE=80.2835
   ‚úÖ LITE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LITE (TargetReturn): LSTM with MSE=0.5152
üêõ DEBUG: LITE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LITE.
üêõ DEBUG: LITE - Moving model to CPU before return...
üêõ DEBUG [22:58:35.911]: LITE - Returning result metadata...
üêõ DEBUG: train_worker started for SENEA
üêõ DEBUG [22:58:35.911]: Main received result for LITE
  ‚öôÔ∏è Training models for SENEA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SENEA: Initiating feature extraction for training.
  [DIAGNOSTIC] SENEA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SENEA: rows after features available: 126
üéØ SENEA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SENEA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SENEA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SENEA: Training LSTM (50 epochs)...
      ‚è≥ SENEA LSTM: Epoch 10/50 (20%)
      ‚è≥ SENEA LSTM: Epoch 20/50 (40%)
      ‚è≥ SENEA LSTM: Epoch 30/50 (60%)
      ‚è≥ SENEA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.513637
         RMSE: 0.716684
         R¬≤ Score: -0.7933 (Poor - 79.3% variance explained)
      üîπ SENEA: Training TCN (50 epochs)...
      ‚è≥ SENEA TCN: Epoch 10/50 (20%)
      ‚è≥ SENEA TCN: Epoch 20/50 (40%)
      ‚è≥ SENEA TCN: Epoch 30/50 (60%)
      ‚è≥ SENEA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.301479
         RMSE: 0.549071
         R¬≤ Score: -0.0526
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SENEA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SENEA Random Forest: Starting GridSearchCV fit...
       ‚úÖ SENEA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.5884 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SENEA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SENEA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.2109 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SENEA XGBoost: Starting GridSearchCV fit...
       ‚úÖ CAKE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.4747 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.6047
    - TCN: MSE=0.5938
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5938
        ‚Ä¢ LSTM: MSE=0.6047
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.7738
        ‚Ä¢ XGBoost: MSE=17.4747
        ‚Ä¢ Random Forest: MSE=22.6339
   ‚úÖ CAKE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CAKE (TargetReturn): TCN with MSE=0.5938
üêõ DEBUG: CAKE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CAKE.
üêõ DEBUG: CAKE - Moving model to CPU before return...
üêõ DEBUG [22:58:59.873]: CAKE - Returning result metadata...
üêõ DEBUG: train_worker started for CUK
üêõ DEBUG [22:58:59.873]: Main received result for CAKE
  ‚öôÔ∏è Training models for CUK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CUK: Initiating feature extraction for training.
  [DIAGNOSTIC] CUK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CUK: rows after features available: 126
üéØ CUK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CUK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CUK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CUK: Training LSTM (50 epochs)...
      ‚è≥ CUK LSTM: Epoch 10/50 (20%)
      ‚è≥ CUK LSTM: Epoch 20/50 (40%)
      ‚è≥ CUK LSTM: Epoch 30/50 (60%)
      ‚è≥ CUK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.682484
         RMSE: 0.826126
         R¬≤ Score: -0.8681 (Poor - 86.8% variance explained)
      üîπ CUK: Training TCN (50 epochs)...
      ‚è≥ CUK TCN: Epoch 10/50 (20%)
      ‚è≥ CUK TCN: Epoch 20/50 (40%)
      ‚è≥ CUK TCN: Epoch 30/50 (60%)
      ‚è≥ CUK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.629997
         RMSE: 0.793723
         R¬≤ Score: -0.7244
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CUK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CUK Random Forest: Starting GridSearchCV fit...
       ‚úÖ CUK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.6728 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CUK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CUK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=32.9737 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CUK XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARKX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.3429 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.5357
    - TCN: MSE=0.5913
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5357
        ‚Ä¢ TCN: MSE=0.5913
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.0077
        ‚Ä¢ Random Forest: MSE=16.5240
        ‚Ä¢ XGBoost: MSE=19.3429
   ‚úÖ ARKX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKX (TargetReturn): LSTM with MSE=0.5357
üêõ DEBUG: ARKX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKX.
üêõ DEBUG: ARKX - Moving model to CPU before return...
üêõ DEBUG [22:59:10.411]: ARKX - Returning result metadata...
üêõ DEBUG [22:59:10.412]: Main received result for ARKX
üêõ DEBUG: Training progress: 276/959 done
üêõ DEBUG: train_worker started for MSB
  ‚öôÔ∏è Training models for MSB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - MSB: Initiating feature extraction for training.
  [DIAGNOSTIC] MSB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MSB: rows after features available: 126
üéØ MSB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MSB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MSB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MSB: Training LSTM (50 epochs)...
      ‚è≥ MSB LSTM: Epoch 10/50 (20%)
      ‚è≥ MSB LSTM: Epoch 20/50 (40%)
      ‚è≥ MSB LSTM: Epoch 30/50 (60%)
      ‚è≥ MSB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.091441
         RMSE: 0.302392
         R¬≤ Score: -0.4963 (Poor - 49.6% variance explained)
      üîπ MSB: Training TCN (50 epochs)...
      ‚è≥ MSB TCN: Epoch 10/50 (20%)
      ‚è≥ MSB TCN: Epoch 20/50 (40%)
      ‚è≥ MSB TCN: Epoch 30/50 (60%)
      ‚è≥ MSB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.071184
         RMSE: 0.266804
         R¬≤ Score: -0.1649
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MSB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MSB Random Forest: Starting GridSearchCV fit...
       ‚úÖ MSB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=36.6614 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MSB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MSB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=35.4141 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MSB XGBoost: Starting GridSearchCV fit...
       ‚úÖ AFRM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=101.5195 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.5s
    - LSTM: MSE=0.5507
    - TCN: MSE=0.5514
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5507
        ‚Ä¢ TCN: MSE=0.5514
        ‚Ä¢ Random Forest: MSE=56.3530
        ‚Ä¢ LightGBM Regressor (CPU): MSE=65.3896
        ‚Ä¢ XGBoost: MSE=101.5195
   ‚úÖ AFRM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AFRM (TargetReturn): LSTM with MSE=0.5507
üêõ DEBUG: AFRM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AFRM.
üêõ DEBUG: AFRM - Moving model to CPU before return...
üêõ DEBUG [22:59:22.531]: AFRM - Returning result metadata...
üêõ DEBUG: train_worker started for VST
  ‚öôÔ∏è Training models for VST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - VST: Initiating feature extraction for training.
  [DIAGNOSTIC] VST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VST: rows after features available: 126
üéØ VST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VST: Training LSTM (50 epochs)...
       ‚úÖ LTM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.6098 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.4694
    - TCN: MSE=0.4751
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4694
        ‚Ä¢ TCN: MSE=0.4751
        ‚Ä¢ XGBoost: MSE=10.6098
        ‚Ä¢ Random Forest: MSE=10.7943
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.5431
   ‚úÖ LTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LTM (TargetReturn): LSTM with MSE=0.4694
üêõ DEBUG: LTM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LTM.
üêõ DEBUG: LTM - Moving model to CPU before return...
üêõ DEBUG [22:59:22.885]: LTM - Returning result metadata...
üêõ DEBUG: train_worker started for DLTR
üêõ DEBUG [22:59:22.886]: Main received result for LTM
üêõ DEBUG [22:59:22.886]: Main received result for AFRM
  ‚öôÔ∏è Training models for DLTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - DLTR: Initiating feature extraction for training.
  [DIAGNOSTIC] DLTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DLTR: rows after features available: 126
üéØ DLTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DLTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DLTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DLTR: Training LSTM (50 epochs)...
      ‚è≥ VST LSTM: Epoch 10/50 (20%)
      ‚è≥ DLTR LSTM: Epoch 10/50 (20%)
      ‚è≥ VST LSTM: Epoch 20/50 (40%)
      ‚è≥ DLTR LSTM: Epoch 20/50 (40%)
      ‚è≥ VST LSTM: Epoch 30/50 (60%)
      ‚è≥ DLTR LSTM: Epoch 30/50 (60%)
      ‚è≥ VST LSTM: Epoch 40/50 (80%)
      ‚è≥ DLTR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.658246
         RMSE: 0.811324
         R¬≤ Score: -1.4268 (Poor - 142.7% variance explained)
      üîπ VST: Training TCN (50 epochs)...
      ‚è≥ VST TCN: Epoch 10/50 (20%)
      ‚è≥ VST TCN: Epoch 20/50 (40%)
      ‚è≥ VST TCN: Epoch 30/50 (60%)
      ‚è≥ VST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.482629
         RMSE: 0.694715
         R¬≤ Score: -0.7794
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VST Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.312673
         RMSE: 0.559172
         R¬≤ Score: -0.7139 (Poor - 71.4% variance explained)
      üîπ DLTR: Training TCN (50 epochs)...
      ‚è≥ DLTR TCN: Epoch 10/50 (20%)
      ‚è≥ DLTR TCN: Epoch 20/50 (40%)
      ‚è≥ DLTR TCN: Epoch 30/50 (60%)
      ‚è≥ DLTR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.239952
         RMSE: 0.489849
         R¬≤ Score: -0.3153
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DLTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DLTR Random Forest: Starting GridSearchCV fit...
       ‚úÖ VST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=74.6141 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VST LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DLTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=47.7576 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DLTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=72.0711 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VST XGBoost: Starting GridSearchCV fit...
       ‚úÖ DLTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=56.9632 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DLTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ BCS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.4066 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.2822
    - TCN: MSE=0.2464
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2464
        ‚Ä¢ LSTM: MSE=0.2822
        ‚Ä¢ XGBoost: MSE=14.4066
        ‚Ä¢ Random Forest: MSE=16.9322
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6485
   ‚úÖ BCS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BCS (TargetReturn): TCN with MSE=0.2464
üêõ DEBUG: BCS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BCS.
üêõ DEBUG: BCS - Moving model to CPU before return...
üêõ DEBUG [22:59:46.914]: BCS - Returning result metadata...
üêõ DEBUG [22:59:46.914]: Main received result for BCS
üêõ DEBUG: train_worker started for TIGO
  ‚öôÔ∏è Training models for TIGO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - TIGO: Initiating feature extraction for training.
  [DIAGNOSTIC] TIGO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TIGO: rows after features available: 126
üéØ TIGO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TIGO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TIGO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TIGO: Training LSTM (50 epochs)...
      ‚è≥ TIGO LSTM: Epoch 10/50 (20%)
      ‚è≥ TIGO LSTM: Epoch 20/50 (40%)
       ‚úÖ MPAA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=45.6677 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.3984
    - TCN: MSE=0.2293
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2293
        ‚Ä¢ LSTM: MSE=0.3984
        ‚Ä¢ XGBoost: MSE=45.6677
        ‚Ä¢ Random Forest: MSE=56.5295
        ‚Ä¢ LightGBM Regressor (CPU): MSE=63.1901
   ‚úÖ MPAA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MPAA (TargetReturn): TCN with MSE=0.2293
üêõ DEBUG: MPAA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MPAA.
üêõ DEBUG: MPAA - Moving model to CPU before return...
üêõ DEBUG [22:59:48.296]: MPAA - Returning result metadata...
üêõ DEBUG: train_worker started for YBTC
üêõ DEBUG [22:59:48.296]: Main received result for MPAA
üêõ DEBUG: Training progress: 280/959 done
  ‚öôÔ∏è Training models for YBTC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - YBTC: Initiating feature extraction for training.
  [DIAGNOSTIC] YBTC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YBTC: rows after features available: 126
üéØ YBTC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YBTC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YBTC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YBTC: Training LSTM (50 epochs)...
      ‚è≥ TIGO LSTM: Epoch 30/50 (60%)
      ‚è≥ YBTC LSTM: Epoch 10/50 (20%)
      ‚è≥ TIGO LSTM: Epoch 40/50 (80%)
      ‚è≥ YBTC LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.214452
         RMSE: 0.463090
         R¬≤ Score: -1.4746 (Poor - 147.5% variance explained)
      üîπ TIGO: Training TCN (50 epochs)...
      ‚è≥ TIGO TCN: Epoch 10/50 (20%)
      ‚è≥ TIGO TCN: Epoch 20/50 (40%)
      ‚è≥ YBTC LSTM: Epoch 30/50 (60%)
      ‚è≥ TIGO TCN: Epoch 30/50 (60%)
      ‚è≥ TIGO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.091839
         RMSE: 0.303049
         R¬≤ Score: -0.0597
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TIGO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TIGO Random Forest: Starting GridSearchCV fit...
      ‚è≥ YBTC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.467004
         RMSE: 0.683377
         R¬≤ Score: -1.0731 (Poor - 107.3% variance explained)
      üîπ YBTC: Training TCN (50 epochs)...
      ‚è≥ YBTC TCN: Epoch 10/50 (20%)
      ‚è≥ YBTC TCN: Epoch 20/50 (40%)
      ‚è≥ YBTC TCN: Epoch 30/50 (60%)
      ‚è≥ YBTC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.439533
         RMSE: 0.662972
         R¬≤ Score: -0.9512
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YBTC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YBTC Random Forest: Starting GridSearchCV fit...
       ‚úÖ TIGO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.7301 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TIGO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TIGO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.7265 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TIGO XGBoost: Starting GridSearchCV fit...
       ‚úÖ YBTC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.6132 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YBTC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ YBTC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.2810 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YBTC XGBoost: Starting GridSearchCV fit...
       ‚úÖ SLI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=59.7916 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.5023
    - TCN: MSE=0.4358
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4358
        ‚Ä¢ LSTM: MSE=0.5023
        ‚Ä¢ Random Forest: MSE=57.6457
        ‚Ä¢ XGBoost: MSE=59.7916
        ‚Ä¢ LightGBM Regressor (CPU): MSE=62.5310
   ‚úÖ SLI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLI (TargetReturn): TCN with MSE=0.4358
üêõ DEBUG: SLI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLI.
üêõ DEBUG: SLI - Moving model to CPU before return...
üêõ DEBUG [22:59:56.262]: SLI - Returning result metadata...
üêõ DEBUG [22:59:56.262]: Main received result for SLI
üêõ DEBUG: train_worker started for APH
  ‚öôÔ∏è Training models for APH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - APH: Initiating feature extraction for training.
  [DIAGNOSTIC] APH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APH: rows after features available: 126
üéØ APH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APH: Training LSTM (50 epochs)...
      ‚è≥ APH LSTM: Epoch 10/50 (20%)
      ‚è≥ APH LSTM: Epoch 20/50 (40%)
      ‚è≥ APH LSTM: Epoch 30/50 (60%)
      ‚è≥ APH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.350662
         RMSE: 0.592167
         R¬≤ Score: -1.2298 (Poor - 123.0% variance explained)
      üîπ APH: Training TCN (50 epochs)...
      ‚è≥ APH TCN: Epoch 10/50 (20%)
      ‚è≥ APH TCN: Epoch 20/50 (40%)
      ‚è≥ APH TCN: Epoch 30/50 (60%)
      ‚è≥ APH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.285387
         RMSE: 0.534216
         R¬≤ Score: -0.8147
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APH Random Forest: Starting GridSearchCV fit...
       ‚úÖ APH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.6819 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ APH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.0757 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APH XGBoost: Starting GridSearchCV fit...
       ‚úÖ CXW XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.0173 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.3s
    - LSTM: MSE=0.1011
    - TCN: MSE=0.1622
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1011
        ‚Ä¢ TCN: MSE=0.1622
        ‚Ä¢ XGBoost: MSE=21.0173
        ‚Ä¢ Random Forest: MSE=27.9807
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.7628
   ‚úÖ CXW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CXW (TargetReturn): LSTM with MSE=0.1011
üêõ DEBUG: CXW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CXW.
üêõ DEBUG: CXW - Moving model to CPU before return...
üêõ DEBUG [23:00:07.867]: CXW - Returning result metadata...
üêõ DEBUG [23:00:07.868]: Main received result for CXW
üêõ DEBUG: train_worker started for PCT
  ‚öôÔ∏è Training models for PCT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - PCT: Initiating feature extraction for training.
  [DIAGNOSTIC] PCT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PCT: rows after features available: 126
üéØ PCT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PCT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PCT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PCT: Training LSTM (50 epochs)...
      ‚è≥ PCT LSTM: Epoch 10/50 (20%)
      ‚è≥ PCT LSTM: Epoch 20/50 (40%)
      ‚è≥ PCT LSTM: Epoch 30/50 (60%)
      ‚è≥ PCT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.630738
         RMSE: 0.794190
         R¬≤ Score: -0.9599 (Poor - 96.0% variance explained)
      üîπ PCT: Training TCN (50 epochs)...
      ‚è≥ PCT TCN: Epoch 10/50 (20%)
      ‚è≥ PCT TCN: Epoch 20/50 (40%)
      ‚è≥ PCT TCN: Epoch 30/50 (60%)
      ‚è≥ PCT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.661411
         RMSE: 0.813272
         R¬≤ Score: -1.0552
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PCT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PCT Random Forest: Starting GridSearchCV fit...
       ‚úÖ PCT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=75.7483 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PCT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PCT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=168.4201 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PCT XGBoost: Starting GridSearchCV fit...
       ‚úÖ LPLA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=20.6747 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.4371
    - TCN: MSE=0.3851
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3851
        ‚Ä¢ LSTM: MSE=0.4371
        ‚Ä¢ XGBoost: MSE=20.6747
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.7268
        ‚Ä¢ Random Forest: MSE=23.3616
   ‚úÖ LPLA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LPLA (TargetReturn): TCN with MSE=0.3851
üêõ DEBUG: LPLA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LPLA.
üêõ DEBUG: LPLA - Moving model to CPU before return...
üêõ DEBUG [23:00:16.389]: LPLA - Returning result metadata...
üêõ DEBUG: train_worker started for UFO
  ‚öôÔ∏è Training models for UFO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - UFO: Initiating feature extraction for training.
  [DIAGNOSTIC] UFO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UFO: rows after features available: 126
üéØ UFO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UFO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UFO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UFO: Training LSTM (50 epochs)...
       ‚úÖ ICL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.8349 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.1449
    - TCN: MSE=0.0840
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0840
        ‚Ä¢ LSTM: MSE=0.1449
        ‚Ä¢ Random Forest: MSE=11.3307
        ‚Ä¢ XGBoost: MSE=11.8349
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.0695
   ‚úÖ ICL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ICL (TargetReturn): TCN with MSE=0.0840
üêõ DEBUG: ICL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ICL.
üêõ DEBUG: ICL - Moving model to CPU before return...
üêõ DEBUG [23:00:16.700]: ICL - Returning result metadata...
üêõ DEBUG: train_worker started for ZVRA
üêõ DEBUG [23:00:16.703]: Main received result for ICL
üêõ DEBUG [23:00:16.703]: Main received result for LPLA
üêõ DEBUG: Training progress: 284/959 done
  ‚öôÔ∏è Training models for ZVRA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ZVRA: Initiating feature extraction for training.
  [DIAGNOSTIC] ZVRA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ZVRA: rows after features available: 126
üéØ ZVRA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ZVRA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ZVRA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ZVRA: Training LSTM (50 epochs)...
      ‚è≥ UFO LSTM: Epoch 10/50 (20%)
      ‚è≥ ZVRA LSTM: Epoch 10/50 (20%)
      ‚è≥ UFO LSTM: Epoch 20/50 (40%)
      ‚è≥ ZVRA LSTM: Epoch 20/50 (40%)
      ‚è≥ UFO LSTM: Epoch 30/50 (60%)
      ‚è≥ ZVRA LSTM: Epoch 30/50 (60%)
      ‚è≥ UFO LSTM: Epoch 40/50 (80%)
      ‚è≥ ZVRA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.763090
         RMSE: 0.873550
         R¬≤ Score: -1.2403 (Poor - 124.0% variance explained)
      üîπ UFO: Training TCN (50 epochs)...
      ‚è≥ UFO TCN: Epoch 10/50 (20%)
      ‚è≥ UFO TCN: Epoch 20/50 (40%)
      ‚è≥ UFO TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.557504
         RMSE: 0.746662
         R¬≤ Score: -1.1080 (Poor - 110.8% variance explained)
      üîπ ZVRA: Training TCN (50 epochs)...
      ‚è≥ UFO TCN: Epoch 40/50 (80%)
      ‚è≥ ZVRA TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.553301
         RMSE: 0.743842
         R¬≤ Score: -0.6244
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UFO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UFO Random Forest: Starting GridSearchCV fit...
      ‚è≥ ZVRA TCN: Epoch 20/50 (40%)
      ‚è≥ ZVRA TCN: Epoch 30/50 (60%)
       ‚úÖ GFI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=38.7825 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.1111
    - TCN: MSE=0.0986
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0986
        ‚Ä¢ LSTM: MSE=0.1111
        ‚Ä¢ XGBoost: MSE=38.7825
        ‚Ä¢ Random Forest: MSE=42.8678
        ‚Ä¢ LightGBM Regressor (CPU): MSE=43.1283
   ‚úÖ GFI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GFI (TargetReturn): TCN with MSE=0.0986
üêõ DEBUG: GFI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GFI.
üêõ DEBUG: GFI - Moving model to CPU before return...
üêõ DEBUG [23:00:20.054]: GFI - Returning result metadata...
üêõ DEBUG: train_worker started for MRX
  ‚öôÔ∏è Training models for MRX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - MRX: Initiating feature extraction for training.
  [DIAGNOSTIC] MRX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MRX: rows after features available: 126
üéØ MRX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ ZVRA TCN: Epoch 40/50 (80%)
  [DIAGNOSTIC] MRX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MRX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MRX: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.471283
         RMSE: 0.686501
         R¬≤ Score: -0.7820
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ZVRA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ZVRA Random Forest: Starting GridSearchCV fit...
      ‚è≥ MRX LSTM: Epoch 10/50 (20%)
      ‚è≥ MRX LSTM: Epoch 20/50 (40%)
      ‚è≥ MRX LSTM: Epoch 30/50 (60%)
      ‚è≥ MRX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.472972
         RMSE: 0.687730
         R¬≤ Score: -1.5994 (Poor - 159.9% variance explained)
      üîπ MRX: Training TCN (50 epochs)...
      ‚è≥ MRX TCN: Epoch 10/50 (20%)
      ‚è≥ MRX TCN: Epoch 20/50 (40%)
       ‚úÖ UFO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.7826 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UFO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MRX TCN: Epoch 30/50 (60%)
      ‚è≥ MRX TCN: Epoch 40/50 (80%)
       ‚úÖ ZVRA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.6604 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ZVRA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.302525
         RMSE: 0.550023
         R¬≤ Score: -0.6627
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MRX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MRX Random Forest: Starting GridSearchCV fit...
       ‚úÖ UFO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.3637 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UFO XGBoost: Starting GridSearchCV fit...
       ‚úÖ WBD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.2957 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.8596
    - TCN: MSE=0.5856
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5856
        ‚Ä¢ LSTM: MSE=0.8596
        ‚Ä¢ Random Forest: MSE=31.5729
        ‚Ä¢ XGBoost: MSE=44.2957
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.4879
   ‚úÖ WBD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WBD (TargetReturn): TCN with MSE=0.5856
üêõ DEBUG: WBD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WBD.
üêõ DEBUG: WBD - Moving model to CPU before return...
üêõ DEBUG [23:00:23.522]: WBD - Returning result metadata...
üêõ DEBUG: train_worker started for SMLR
  ‚öôÔ∏è Training models for SMLR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - SMLR: Initiating feature extraction for training.
  [DIAGNOSTIC] SMLR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMLR: rows after features available: 126
üéØ SMLR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SMLR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMLR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMLR: Training LSTM (50 epochs)...
       ‚úÖ ZVRA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=58.3346 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ZVRA XGBoost: Starting GridSearchCV fit...
       ‚úÖ GDXU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=325.5901 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 125.1s
    - LSTM: MSE=0.1797
    - TCN: MSE=0.1778
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 128.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1778
        ‚Ä¢ LSTM: MSE=0.1797
        ‚Ä¢ Random Forest: MSE=268.3982
        ‚Ä¢ LightGBM Regressor (CPU): MSE=299.2987
        ‚Ä¢ XGBoost: MSE=325.5901
   ‚úÖ GDXU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GDXU (TargetReturn): TCN with MSE=0.1778
üêõ DEBUG: GDXU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GDXU.
üêõ DEBUG: GDXU - Moving model to CPU before return...
üêõ DEBUG [23:00:23.837]: GDXU - Returning result metadata...
üêõ DEBUG: train_worker started for ARKQ
üêõ DEBUG [23:00:23.838]: Main received result for GDXU
üêõ DEBUG [23:00:23.838]: Main received result for GFI
üêõ DEBUG [23:00:23.838]: Main received result for WBD
  ‚öôÔ∏è Training models for ARKQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - ARKQ: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKQ: rows after features available: 126
üéØ ARKQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKQ: Training LSTM (50 epochs)...
      ‚è≥ SMLR LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKQ LSTM: Epoch 10/50 (20%)
      ‚è≥ SMLR LSTM: Epoch 20/50 (40%)
      ‚è≥ ARKQ LSTM: Epoch 20/50 (40%)
      ‚è≥ SMLR LSTM: Epoch 30/50 (60%)
      ‚è≥ ARKQ LSTM: Epoch 30/50 (60%)
      ‚è≥ SMLR LSTM: Epoch 40/50 (80%)
      ‚è≥ ARKQ LSTM: Epoch 40/50 (80%)
       ‚úÖ MRX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.6926 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MRX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.174623
         RMSE: 0.417879
         R¬≤ Score: -1.0732 (Poor - 107.3% variance explained)
      üîπ SMLR: Training TCN (50 epochs)...
      ‚è≥ SMLR TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.518207
         RMSE: 0.719866
         R¬≤ Score: -1.0299 (Poor - 103.0% variance explained)
      üîπ ARKQ: Training TCN (50 epochs)...
      ‚è≥ SMLR TCN: Epoch 20/50 (40%)
      ‚è≥ ARKQ TCN: Epoch 10/50 (20%)
      ‚è≥ SMLR TCN: Epoch 30/50 (60%)
      ‚è≥ ARKQ TCN: Epoch 20/50 (40%)
      ‚è≥ SMLR TCN: Epoch 40/50 (80%)
      ‚è≥ ARKQ TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.093408
         RMSE: 0.305627
         R¬≤ Score: -0.1090
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMLR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMLR Random Forest: Starting GridSearchCV fit...
      ‚è≥ ARKQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.497909
         RMSE: 0.705627
         R¬≤ Score: -0.9504
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKQ Random Forest: Starting GridSearchCV fit...
       ‚úÖ MRX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.4038 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MRX XGBoost: Starting GridSearchCV fit...
       ‚úÖ SMLR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=52.9410 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMLR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARKQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.8527 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SMLR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=76.0513 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMLR XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARKQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.8681 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ CTRN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.5962 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.6620
    - TCN: MSE=0.6498
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6498
        ‚Ä¢ LSTM: MSE=0.6620
        ‚Ä¢ Random Forest: MSE=31.8210
        ‚Ä¢ XGBoost: MSE=41.5962
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.1196
   ‚úÖ CTRN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTRN (TargetReturn): TCN with MSE=0.6498
üêõ DEBUG: CTRN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTRN.
üêõ DEBUG: CTRN - Moving model to CPU before return...
üêõ DEBUG [23:00:33.647]: CTRN - Returning result metadata...
üêõ DEBUG [23:00:33.647]: Main received result for CTRN
üêõ DEBUG: Training progress: 288/959 done
üêõ DEBUG: train_worker started for ARKK
  ‚öôÔ∏è Training models for ARKK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - ARKK: Initiating feature extraction for training.
  [DIAGNOSTIC] ARKK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARKK: rows after features available: 126
üéØ ARKK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARKK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARKK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARKK: Training LSTM (50 epochs)...
      ‚è≥ ARKK LSTM: Epoch 10/50 (20%)
      ‚è≥ ARKK LSTM: Epoch 20/50 (40%)
      ‚è≥ ARKK LSTM: Epoch 30/50 (60%)
      ‚è≥ ARKK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.676195
         RMSE: 0.822311
         R¬≤ Score: -0.8524 (Poor - 85.2% variance explained)
      üîπ ARKK: Training TCN (50 epochs)...
      ‚è≥ ARKK TCN: Epoch 10/50 (20%)
      ‚è≥ ARKK TCN: Epoch 20/50 (40%)
      ‚è≥ ARKK TCN: Epoch 30/50 (60%)
      ‚è≥ ARKK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.629073
         RMSE: 0.793141
         R¬≤ Score: -0.7233
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARKK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARKK Random Forest: Starting GridSearchCV fit...
       ‚úÖ SENEA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.7481 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.4s
    - LSTM: MSE=0.5136
    - TCN: MSE=0.3015
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3015
        ‚Ä¢ LSTM: MSE=0.5136
        ‚Ä¢ Random Forest: MSE=7.5884
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.2109
        ‚Ä¢ XGBoost: MSE=9.7481
   ‚úÖ SENEA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SENEA (TargetReturn): TCN with MSE=0.3015
üêõ DEBUG: SENEA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SENEA.
üêõ DEBUG: SENEA - Moving model to CPU before return...
üêõ DEBUG [23:00:38.869]: SENEA - Returning result metadata...
üêõ DEBUG [23:00:38.869]: Main received result for SENEA
üêõ DEBUG: train_worker started for VRSN
  ‚öôÔ∏è Training models for VRSN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - VRSN: Initiating feature extraction for training.
  [DIAGNOSTIC] VRSN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VRSN: rows after features available: 126
üéØ VRSN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VRSN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VRSN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VRSN: Training LSTM (50 epochs)...
       ‚úÖ ARKK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.0117 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARKK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ VRSN LSTM: Epoch 10/50 (20%)
      ‚è≥ VRSN LSTM: Epoch 20/50 (40%)
       ‚úÖ ARKK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.1652 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARKK XGBoost: Starting GridSearchCV fit...
      ‚è≥ VRSN LSTM: Epoch 30/50 (60%)
      ‚è≥ VRSN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.380532
         RMSE: 0.616873
         R¬≤ Score: -1.1484 (Poor - 114.8% variance explained)
      üîπ VRSN: Training TCN (50 epochs)...
      ‚è≥ VRSN TCN: Epoch 10/50 (20%)
      ‚è≥ VRSN TCN: Epoch 20/50 (40%)
      ‚è≥ VRSN TCN: Epoch 30/50 (60%)
      ‚è≥ VRSN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.264490
         RMSE: 0.514286
         R¬≤ Score: -0.4933
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VRSN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VRSN Random Forest: Starting GridSearchCV fit...
       ‚úÖ VRSN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9655 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VRSN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VRSN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.6739 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VRSN XGBoost: Starting GridSearchCV fit...
       ‚úÖ CUK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.0359 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.6825
    - TCN: MSE=0.6300
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6300
        ‚Ä¢ LSTM: MSE=0.6825
        ‚Ä¢ Random Forest: MSE=22.6728
        ‚Ä¢ XGBoost: MSE=29.0359
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.9737
   ‚úÖ CUK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CUK (TargetReturn): TCN with MSE=0.6300
üêõ DEBUG: CUK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CUK.
üêõ DEBUG: CUK - Moving model to CPU before return...
üêõ DEBUG [23:01:04.701]: CUK - Returning result metadata...
üêõ DEBUG [23:01:04.701]: Main received result for CUK
üêõ DEBUG: train_worker started for WF
  ‚öôÔ∏è Training models for WF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - WF: Initiating feature extraction for training.
  [DIAGNOSTIC] WF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WF: rows after features available: 126
üéØ WF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WF: Training LSTM (50 epochs)...
      ‚è≥ WF LSTM: Epoch 10/50 (20%)
      ‚è≥ WF LSTM: Epoch 20/50 (40%)
      ‚è≥ WF LSTM: Epoch 30/50 (60%)
      ‚è≥ WF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.374769
         RMSE: 0.612184
         R¬≤ Score: -0.8207 (Poor - 82.1% variance explained)
      üîπ WF: Training TCN (50 epochs)...
      ‚è≥ WF TCN: Epoch 10/50 (20%)
      ‚è≥ WF TCN: Epoch 20/50 (40%)
      ‚è≥ WF TCN: Epoch 30/50 (60%)
      ‚è≥ WF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.361577
         RMSE: 0.601312
         R¬≤ Score: -0.7566
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WF Random Forest: Starting GridSearchCV fit...
       ‚úÖ WF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.4212 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.9992 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WF XGBoost: Starting GridSearchCV fit...
       ‚úÖ MSB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.2504 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.4s
    - LSTM: MSE=0.0914
    - TCN: MSE=0.0712
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0712
        ‚Ä¢ LSTM: MSE=0.0914
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.4141
        ‚Ä¢ Random Forest: MSE=36.6614
        ‚Ä¢ XGBoost: MSE=40.2504
   ‚úÖ MSB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MSB (TargetReturn): TCN with MSE=0.0712
üêõ DEBUG: MSB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MSB.
üêõ DEBUG: MSB - Moving model to CPU before return...
üêõ DEBUG [23:01:12.857]: MSB - Returning result metadata...
üêõ DEBUG: train_worker started for DAPP
üêõ DEBUG [23:01:12.858]: Main received result for MSB
  ‚öôÔ∏è Training models for DAPP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - DAPP: Initiating feature extraction for training.
  [DIAGNOSTIC] DAPP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAPP: rows after features available: 126
üéØ DAPP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAPP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAPP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAPP: Training LSTM (50 epochs)...
      ‚è≥ DAPP LSTM: Epoch 10/50 (20%)
      ‚è≥ DAPP LSTM: Epoch 20/50 (40%)
      ‚è≥ DAPP LSTM: Epoch 30/50 (60%)
      ‚è≥ DAPP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.614697
         RMSE: 0.784026
         R¬≤ Score: -0.8119 (Poor - 81.2% variance explained)
      üîπ DAPP: Training TCN (50 epochs)...
      ‚è≥ DAPP TCN: Epoch 10/50 (20%)
      ‚è≥ DAPP TCN: Epoch 20/50 (40%)
      ‚è≥ DAPP TCN: Epoch 30/50 (60%)
      ‚è≥ DAPP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.518216
         RMSE: 0.719872
         R¬≤ Score: -0.5275
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAPP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAPP Random Forest: Starting GridSearchCV fit...
       ‚úÖ DAPP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.2434 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAPP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DAPP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=82.0518 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAPP XGBoost: Starting GridSearchCV fit...
       ‚úÖ VST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=95.2545 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 117.3s
    - LSTM: MSE=0.6582
    - TCN: MSE=0.4826
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4826
        ‚Ä¢ LSTM: MSE=0.6582
        ‚Ä¢ LightGBM Regressor (CPU): MSE=72.0711
        ‚Ä¢ Random Forest: MSE=74.6141
        ‚Ä¢ XGBoost: MSE=95.2545
   ‚úÖ VST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VST (TargetReturn): TCN with MSE=0.4826
üêõ DEBUG: VST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VST.
üêõ DEBUG: VST - Moving model to CPU before return...
üêõ DEBUG [23:01:25.959]: VST - Returning result metadata...
üêõ DEBUG: train_worker started for PEGA
üêõ DEBUG [23:01:25.960]: Main received result for VST
üêõ DEBUG: Training progress: 292/959 done
  ‚öôÔ∏è Training models for PEGA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - PEGA: Initiating feature extraction for training.
  [DIAGNOSTIC] PEGA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PEGA: rows after features available: 126
üéØ PEGA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PEGA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PEGA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PEGA: Training LSTM (50 epochs)...
      ‚è≥ PEGA LSTM: Epoch 10/50 (20%)
      ‚è≥ PEGA LSTM: Epoch 20/50 (40%)
       ‚úÖ DLTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.7411 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 118.1s
    - LSTM: MSE=0.3127
    - TCN: MSE=0.2400
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2400
        ‚Ä¢ LSTM: MSE=0.3127
        ‚Ä¢ Random Forest: MSE=47.7576
        ‚Ä¢ XGBoost: MSE=54.7411
        ‚Ä¢ LightGBM Regressor (CPU): MSE=56.9632
   ‚úÖ DLTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DLTR (TargetReturn): TCN with MSE=0.2400
üêõ DEBUG: DLTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DLTR.
üêõ DEBUG: DLTR - Moving model to CPU before return...
üêõ DEBUG [23:01:27.355]: DLTR - Returning result metadata...
üêõ DEBUG: train_worker started for ORN
üêõ DEBUG [23:01:27.356]: Main received result for DLTR
      ‚è≥ PEGA LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for ORN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ORN: Initiating feature extraction for training.
  [DIAGNOSTIC] ORN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ORN: rows after features available: 126
üéØ ORN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ORN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ORN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ORN: Training LSTM (50 epochs)...
      ‚è≥ ORN LSTM: Epoch 10/50 (20%)
      ‚è≥ PEGA LSTM: Epoch 40/50 (80%)
      ‚è≥ ORN LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.348332
         RMSE: 0.590196
         R¬≤ Score: -0.7377 (Poor - 73.8% variance explained)
      üîπ PEGA: Training TCN (50 epochs)...
      ‚è≥ PEGA TCN: Epoch 10/50 (20%)
      ‚è≥ PEGA TCN: Epoch 20/50 (40%)
      ‚è≥ PEGA TCN: Epoch 30/50 (60%)
      ‚è≥ PEGA TCN: Epoch 40/50 (80%)
      ‚è≥ ORN LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.250546
         RMSE: 0.500546
         R¬≤ Score: -0.2499
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PEGA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PEGA Random Forest: Starting GridSearchCV fit...
      ‚è≥ ORN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.461068
         RMSE: 0.679020
         R¬≤ Score: -0.8142 (Poor - 81.4% variance explained)
      üîπ ORN: Training TCN (50 epochs)...
      ‚è≥ ORN TCN: Epoch 10/50 (20%)
      ‚è≥ ORN TCN: Epoch 20/50 (40%)
      ‚è≥ ORN TCN: Epoch 30/50 (60%)
      ‚è≥ ORN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.367872
         RMSE: 0.606525
         R¬≤ Score: -0.4475
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ORN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ORN Random Forest: Starting GridSearchCV fit...
       ‚úÖ PEGA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.3911 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PEGA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PEGA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=34.1721 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PEGA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ORN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=98.0416 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ORN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ORN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=61.1105 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ORN XGBoost: Starting GridSearchCV fit...
       ‚úÖ TIGO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.2749 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.2145
    - TCN: MSE=0.0918
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0918
        ‚Ä¢ LSTM: MSE=0.2145
        ‚Ä¢ XGBoost: MSE=11.2749
        ‚Ä¢ Random Forest: MSE=15.7301
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.7265
   ‚úÖ TIGO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TIGO (TargetReturn): TCN with MSE=0.0918
üêõ DEBUG: TIGO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TIGO.
üêõ DEBUG: TIGO - Moving model to CPU before return...
üêõ DEBUG [23:01:52.858]: TIGO - Returning result metadata...
üêõ DEBUG [23:01:52.858]: Main received result for TIGO
üêõ DEBUG: train_worker started for BITQ
  ‚öôÔ∏è Training models for BITQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - BITQ: Initiating feature extraction for training.
  [DIAGNOSTIC] BITQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BITQ: rows after features available: 126
üéØ BITQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BITQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BITQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BITQ: Training LSTM (50 epochs)...
      ‚è≥ BITQ LSTM: Epoch 10/50 (20%)
       ‚úÖ YBTC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=19.5170 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.4670
    - TCN: MSE=0.4395
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4395
        ‚Ä¢ LSTM: MSE=0.4670
        ‚Ä¢ XGBoost: MSE=19.5170
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.2810
        ‚Ä¢ Random Forest: MSE=20.6132
   ‚úÖ YBTC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YBTC (TargetReturn): TCN with MSE=0.4395
üêõ DEBUG: YBTC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YBTC.
üêõ DEBUG: YBTC - Moving model to CPU before return...
üêõ DEBUG [23:01:53.686]: YBTC - Returning result metadata...
üêõ DEBUG [23:01:53.686]: Main received result for YBTC
üêõ DEBUG: train_worker started for CCL
  ‚öôÔ∏è Training models for CCL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CCL: Initiating feature extraction for training.
  [DIAGNOSTIC] CCL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CCL: rows after features available: 126
üéØ CCL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CCL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CCL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CCL: Training LSTM (50 epochs)...
      ‚è≥ BITQ LSTM: Epoch 20/50 (40%)
      ‚è≥ CCL LSTM: Epoch 10/50 (20%)
      ‚è≥ BITQ LSTM: Epoch 30/50 (60%)
      ‚è≥ CCL LSTM: Epoch 20/50 (40%)
      ‚è≥ BITQ LSTM: Epoch 40/50 (80%)
      ‚è≥ CCL LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.642110
         RMSE: 0.801318
         R¬≤ Score: -1.0949 (Poor - 109.5% variance explained)
      üîπ BITQ: Training TCN (50 epochs)...
      ‚è≥ BITQ TCN: Epoch 10/50 (20%)
      ‚è≥ BITQ TCN: Epoch 20/50 (40%)
      ‚è≥ BITQ TCN: Epoch 30/50 (60%)
      ‚è≥ CCL LSTM: Epoch 40/50 (80%)
      ‚è≥ BITQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.452085
         RMSE: 0.672373
         R¬≤ Score: -0.4749
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BITQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BITQ Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.742349
         RMSE: 0.861597
         R¬≤ Score: -1.0340 (Poor - 103.4% variance explained)
      üîπ CCL: Training TCN (50 epochs)...
      ‚è≥ CCL TCN: Epoch 10/50 (20%)
      ‚è≥ CCL TCN: Epoch 20/50 (40%)
      ‚è≥ CCL TCN: Epoch 30/50 (60%)
      ‚è≥ CCL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.614483
         RMSE: 0.783890
         R¬≤ Score: -0.6836
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CCL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CCL Random Forest: Starting GridSearchCV fit...
       ‚úÖ BITQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=33.3065 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BITQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BITQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=48.5829 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BITQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ CCL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.9300 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CCL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CCL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.7381 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CCL XGBoost: Starting GridSearchCV fit...
       ‚úÖ APH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.7128 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.3507
    - TCN: MSE=0.2854
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2854
        ‚Ä¢ LSTM: MSE=0.3507
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.0757
        ‚Ä¢ XGBoost: MSE=31.7128
        ‚Ä¢ Random Forest: MSE=39.6819
   ‚úÖ APH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APH (TargetReturn): TCN with MSE=0.2854
üêõ DEBUG: APH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APH.
üêõ DEBUG: APH - Moving model to CPU before return...
üêõ DEBUG [23:02:01.541]: APH - Returning result metadata...
üêõ DEBUG: train_worker started for NUGT
üêõ DEBUG [23:02:01.544]: Main received result for APH
üêõ DEBUG: Training progress: 296/959 done
  ‚öôÔ∏è Training models for NUGT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - NUGT: Initiating feature extraction for training.
  [DIAGNOSTIC] NUGT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NUGT: rows after features available: 126
üéØ NUGT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NUGT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NUGT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NUGT: Training LSTM (50 epochs)...
      ‚è≥ NUGT LSTM: Epoch 10/50 (20%)
      ‚è≥ NUGT LSTM: Epoch 20/50 (40%)
      ‚è≥ NUGT LSTM: Epoch 30/50 (60%)
      ‚è≥ NUGT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.227427
         RMSE: 0.476893
         R¬≤ Score: -0.7812 (Poor - 78.1% variance explained)
      üîπ NUGT: Training TCN (50 epochs)...
      ‚è≥ NUGT TCN: Epoch 10/50 (20%)
      ‚è≥ NUGT TCN: Epoch 20/50 (40%)
      ‚è≥ NUGT TCN: Epoch 30/50 (60%)
      ‚è≥ NUGT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.160963
         RMSE: 0.401202
         R¬≤ Score: -0.2607
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NUGT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NUGT Random Forest: Starting GridSearchCV fit...
       ‚úÖ NUGT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=108.6507 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NUGT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NUGT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=125.1161 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NUGT XGBoost: Starting GridSearchCV fit...
       ‚úÖ PCT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=87.4633 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.6307
    - TCN: MSE=0.6614
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6307
        ‚Ä¢ TCN: MSE=0.6614
        ‚Ä¢ Random Forest: MSE=75.7483
        ‚Ä¢ XGBoost: MSE=87.4633
        ‚Ä¢ LightGBM Regressor (CPU): MSE=168.4201
   ‚úÖ PCT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PCT (TargetReturn): LSTM with MSE=0.6307
üêõ DEBUG: PCT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PCT.
üêõ DEBUG: PCT - Moving model to CPU before return...
üêõ DEBUG [23:02:14.123]: PCT - Returning result metadata...
üêõ DEBUG [23:02:14.124]: Main received result for PCT
üêõ DEBUG: train_worker started for PRIM
  ‚öôÔ∏è Training models for PRIM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - PRIM: Initiating feature extraction for training.
  [DIAGNOSTIC] PRIM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRIM: rows after features available: 126
üéØ PRIM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRIM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRIM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRIM: Training LSTM (50 epochs)...
      ‚è≥ PRIM LSTM: Epoch 10/50 (20%)
      ‚è≥ PRIM LSTM: Epoch 20/50 (40%)
      ‚è≥ PRIM LSTM: Epoch 30/50 (60%)
      ‚è≥ PRIM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.530398
         RMSE: 0.728284
         R¬≤ Score: -1.0288 (Poor - 102.9% variance explained)
      üîπ PRIM: Training TCN (50 epochs)...
      ‚è≥ PRIM TCN: Epoch 10/50 (20%)
      ‚è≥ PRIM TCN: Epoch 20/50 (40%)
      ‚è≥ PRIM TCN: Epoch 30/50 (60%)
      ‚è≥ PRIM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.418863
         RMSE: 0.647196
         R¬≤ Score: -0.6022
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRIM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRIM Random Forest: Starting GridSearchCV fit...
       ‚úÖ PRIM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.6035 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRIM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PRIM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=32.2649 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRIM XGBoost: Starting GridSearchCV fit...
       ‚úÖ UFO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.2209 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.7631
    - TCN: MSE=0.5533
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5533
        ‚Ä¢ LSTM: MSE=0.7631
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.3637
        ‚Ä¢ Random Forest: MSE=22.7826
        ‚Ä¢ XGBoost: MSE=26.2209
   ‚úÖ UFO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UFO (TargetReturn): TCN with MSE=0.5533
üêõ DEBUG: UFO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UFO.
üêõ DEBUG: UFO - Moving model to CPU before return...
üêõ DEBUG [23:02:21.493]: UFO - Returning result metadata...
üêõ DEBUG: train_worker started for IDCC
üêõ DEBUG [23:02:21.494]: Main received result for UFO
  ‚öôÔ∏è Training models for IDCC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - IDCC: Initiating feature extraction for training.
  [DIAGNOSTIC] IDCC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IDCC: rows after features available: 126
üéØ IDCC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IDCC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IDCC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IDCC: Training LSTM (50 epochs)...
      ‚è≥ IDCC LSTM: Epoch 10/50 (20%)
      ‚è≥ IDCC LSTM: Epoch 20/50 (40%)
      ‚è≥ IDCC LSTM: Epoch 30/50 (60%)
      ‚è≥ IDCC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.441565
         RMSE: 0.664504
         R¬≤ Score: -1.0335 (Poor - 103.4% variance explained)
      üîπ IDCC: Training TCN (50 epochs)...
      ‚è≥ IDCC TCN: Epoch 10/50 (20%)
      ‚è≥ IDCC TCN: Epoch 20/50 (40%)
      ‚è≥ IDCC TCN: Epoch 30/50 (60%)
      ‚è≥ IDCC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.220328
         RMSE: 0.469391
         R¬≤ Score: -0.0147
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IDCC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IDCC Random Forest: Starting GridSearchCV fit...
       ‚úÖ MRX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=18.5751 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.4730
    - TCN: MSE=0.3025
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3025
        ‚Ä¢ LSTM: MSE=0.4730
        ‚Ä¢ XGBoost: MSE=18.5751
        ‚Ä¢ Random Forest: MSE=21.6926
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.4038
   ‚úÖ MRX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MRX (TargetReturn): TCN with MSE=0.3025
üêõ DEBUG: MRX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MRX.
üêõ DEBUG: MRX - Moving model to CPU before return...
üêõ DEBUG [23:02:25.098]: MRX - Returning result metadata...
üêõ DEBUG: train_worker started for GTX
  ‚öôÔ∏è Training models for GTX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - GTX: Initiating feature extraction for training.
  [DIAGNOSTIC] GTX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GTX: rows after features available: 126
üéØ GTX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GTX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GTX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GTX: Training LSTM (50 epochs)...
       ‚úÖ ZVRA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=34.3349 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.5575
    - TCN: MSE=0.4713
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4713
        ‚Ä¢ LSTM: MSE=0.5575
        ‚Ä¢ Random Forest: MSE=25.6604
        ‚Ä¢ XGBoost: MSE=34.3349
        ‚Ä¢ LightGBM Regressor (CPU): MSE=58.3346
   ‚úÖ ZVRA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ZVRA (TargetReturn): TCN with MSE=0.4713
üêõ DEBUG: ZVRA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ZVRA.
üêõ DEBUG: ZVRA - Moving model to CPU before return...
üêõ DEBUG [23:02:25.432]: ZVRA - Returning result metadata...
üêõ DEBUG [23:02:25.433]: Main received result for ZVRA
üêõ DEBUG [23:02:25.433]: Main received result for MRX
üêõ DEBUG: Training progress: 300/959 done
üêõ DEBUG: train_worker started for ITRN
  ‚öôÔ∏è Training models for ITRN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ITRN: Initiating feature extraction for training.
  [DIAGNOSTIC] ITRN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ITRN: rows after features available: 126
üéØ ITRN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ITRN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ITRN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ITRN: Training LSTM (50 epochs)...
      ‚è≥ GTX LSTM: Epoch 10/50 (20%)
      ‚è≥ ITRN LSTM: Epoch 10/50 (20%)
      ‚è≥ GTX LSTM: Epoch 20/50 (40%)
      ‚è≥ ITRN LSTM: Epoch 20/50 (40%)
      ‚è≥ GTX LSTM: Epoch 30/50 (60%)
      ‚è≥ ITRN LSTM: Epoch 30/50 (60%)
      ‚è≥ GTX LSTM: Epoch 40/50 (80%)
       ‚úÖ IDCC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.3191 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IDCC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ITRN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.259017
         RMSE: 0.508937
         R¬≤ Score: -1.0729 (Poor - 107.3% variance explained)
      üîπ GTX: Training TCN (50 epochs)...
      ‚è≥ GTX TCN: Epoch 10/50 (20%)
      ‚è≥ GTX TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.119393
         RMSE: 0.345532
         R¬≤ Score: -0.5728 (Poor - 57.3% variance explained)
      üîπ ITRN: Training TCN (50 epochs)...
      ‚è≥ GTX TCN: Epoch 30/50 (60%)
      ‚è≥ ITRN TCN: Epoch 10/50 (20%)
      ‚è≥ GTX TCN: Epoch 40/50 (80%)
       ‚úÖ IDCC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.3939 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IDCC XGBoost: Starting GridSearchCV fit...
      ‚è≥ ITRN TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.134913
         RMSE: 0.367306
         R¬≤ Score: -0.0797
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GTX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GTX Random Forest: Starting GridSearchCV fit...
      ‚è≥ ITRN TCN: Epoch 30/50 (60%)
      ‚è≥ ITRN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.103980
         RMSE: 0.322459
         R¬≤ Score: -0.3698
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ITRN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ITRN Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARKQ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.1004 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.5182
    - TCN: MSE=0.4979
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4979
        ‚Ä¢ LSTM: MSE=0.5182
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.8681
        ‚Ä¢ XGBoost: MSE=17.1004
        ‚Ä¢ Random Forest: MSE=24.8527
   ‚úÖ ARKQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKQ (TargetReturn): TCN with MSE=0.4979
üêõ DEBUG: ARKQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKQ.
üêõ DEBUG: ARKQ - Moving model to CPU before return...
üêõ DEBUG [23:02:31.655]: ARKQ - Returning result metadata...
üêõ DEBUG: train_worker started for APG
       ‚úÖ GTX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.9944 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GTX LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for APG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - APG: Initiating feature extraction for training.
  [DIAGNOSTIC] APG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APG: rows after features available: 126
üéØ APG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APG: Training LSTM (50 epochs)...
       ‚úÖ ITRN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.1776 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ITRN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ APG LSTM: Epoch 10/50 (20%)
       ‚úÖ SMLR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=84.4991 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.1746
    - TCN: MSE=0.0934
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0934
        ‚Ä¢ LSTM: MSE=0.1746
        ‚Ä¢ Random Forest: MSE=52.9410
        ‚Ä¢ LightGBM Regressor (CPU): MSE=76.0513
        ‚Ä¢ XGBoost: MSE=84.4991
   ‚úÖ SMLR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMLR (TargetReturn): TCN with MSE=0.0934
üêõ DEBUG: SMLR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMLR.
üêõ DEBUG: SMLR - Moving model to CPU before return...
üêõ DEBUG [23:02:32.205]: SMLR - Returning result metadata...
üêõ DEBUG [23:02:32.206]: Main received result for SMLR
üêõ DEBUG [23:02:32.206]: Main received result for ARKQ
üêõ DEBUG: train_worker started for UAL
  ‚öôÔ∏è Training models for UAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - UAL: Initiating feature extraction for training.
  [DIAGNOSTIC] UAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UAL: rows after features available: 126
üéØ UAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UAL: Training LSTM (50 epochs)...
       ‚úÖ GTX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.7649 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GTX XGBoost: Starting GridSearchCV fit...
       ‚úÖ ITRN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.1545 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ITRN XGBoost: Starting GridSearchCV fit...
      ‚è≥ UAL LSTM: Epoch 10/50 (20%)
      ‚è≥ APG LSTM: Epoch 20/50 (40%)
      ‚è≥ UAL LSTM: Epoch 20/50 (40%)
      ‚è≥ APG LSTM: Epoch 30/50 (60%)
      ‚è≥ APG LSTM: Epoch 40/50 (80%)
      ‚è≥ UAL LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.377492
         RMSE: 0.614404
         R¬≤ Score: -0.7450 (Poor - 74.5% variance explained)
      üîπ APG: Training TCN (50 epochs)...
      ‚è≥ UAL LSTM: Epoch 40/50 (80%)
      ‚è≥ APG TCN: Epoch 10/50 (20%)
      ‚è≥ APG TCN: Epoch 20/50 (40%)
      ‚è≥ APG TCN: Epoch 30/50 (60%)
      ‚è≥ APG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.315849
         RMSE: 0.562004
         R¬≤ Score: -0.4600
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APG Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.522178
         RMSE: 0.722619
         R¬≤ Score: -0.6451 (Poor - 64.5% variance explained)
      üîπ UAL: Training TCN (50 epochs)...
      ‚è≥ UAL TCN: Epoch 10/50 (20%)
      ‚è≥ UAL TCN: Epoch 20/50 (40%)
      ‚è≥ UAL TCN: Epoch 30/50 (60%)
      ‚è≥ UAL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.569747
         RMSE: 0.754816
         R¬≤ Score: -0.7950
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UAL Random Forest: Starting GridSearchCV fit...
       ‚úÖ APG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.1044 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ APG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.1000 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APG XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARKK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.1362 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.6762
    - TCN: MSE=0.6291
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6291
        ‚Ä¢ LSTM: MSE=0.6762
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.1652
        ‚Ä¢ XGBoost: MSE=36.1362
        ‚Ä¢ Random Forest: MSE=39.0117
   ‚úÖ ARKK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARKK (TargetReturn): TCN with MSE=0.6291
üêõ DEBUG: ARKK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARKK.
üêõ DEBUG: ARKK - Moving model to CPU before return...
üêõ DEBUG [23:02:38.030]: ARKK - Returning result metadata...
üêõ DEBUG [23:02:38.030]: Main received result for ARKK
üêõ DEBUG: train_worker started for AEM
  ‚öôÔ∏è Training models for AEM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - AEM: Initiating feature extraction for training.
  [DIAGNOSTIC] AEM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AEM: rows after features available: 126
üéØ AEM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AEM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AEM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AEM: Training LSTM (50 epochs)...
       ‚úÖ UAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.2272 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AEM LSTM: Epoch 10/50 (20%)
      ‚è≥ AEM LSTM: Epoch 20/50 (40%)
       ‚úÖ UAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=32.3110 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UAL XGBoost: Starting GridSearchCV fit...
      ‚è≥ AEM LSTM: Epoch 30/50 (60%)
      ‚è≥ AEM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.222733
         RMSE: 0.471946
         R¬≤ Score: -0.4651 (Poor - 46.5% variance explained)
      üîπ AEM: Training TCN (50 epochs)...
      ‚è≥ AEM TCN: Epoch 10/50 (20%)
      ‚è≥ AEM TCN: Epoch 20/50 (40%)
      ‚è≥ AEM TCN: Epoch 30/50 (60%)
      ‚è≥ AEM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.197629
         RMSE: 0.444555
         R¬≤ Score: -0.3000
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AEM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AEM Random Forest: Starting GridSearchCV fit...
       ‚úÖ VRSN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.6126 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.4s
    - LSTM: MSE=0.3805
    - TCN: MSE=0.2645
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2645
        ‚Ä¢ LSTM: MSE=0.3805
        ‚Ä¢ XGBoost: MSE=5.6126
        ‚Ä¢ Random Forest: MSE=6.9655
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.6739
   ‚úÖ VRSN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VRSN (TargetReturn): TCN with MSE=0.2645
üêõ DEBUG: VRSN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VRSN.
üêõ DEBUG: VRSN - Moving model to CPU before return...
üêõ DEBUG [23:02:41.613]: VRSN - Returning result metadata...
üêõ DEBUG [23:02:41.614]: Main received result for VRSN
üêõ DEBUG: Training progress: 304/959 done
üêõ DEBUG: train_worker started for LUNR
  ‚öôÔ∏è Training models for LUNR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - LUNR: Initiating feature extraction for training.
  [DIAGNOSTIC] LUNR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LUNR: rows after features available: 126
üéØ LUNR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LUNR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LUNR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LUNR: Training LSTM (50 epochs)...
      ‚è≥ LUNR LSTM: Epoch 10/50 (20%)
      ‚è≥ LUNR LSTM: Epoch 20/50 (40%)
      ‚è≥ LUNR LSTM: Epoch 30/50 (60%)
      ‚è≥ LUNR LSTM: Epoch 40/50 (80%)
       ‚úÖ AEM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.1215 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AEM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.249299
         RMSE: 0.499299
         R¬≤ Score: -0.9284 (Poor - 92.8% variance explained)
      üîπ LUNR: Training TCN (50 epochs)...
      ‚è≥ LUNR TCN: Epoch 10/50 (20%)
      ‚è≥ LUNR TCN: Epoch 20/50 (40%)
      ‚è≥ LUNR TCN: Epoch 30/50 (60%)
      ‚è≥ LUNR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.142345
         RMSE: 0.377286
         R¬≤ Score: -0.1011
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LUNR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LUNR Random Forest: Starting GridSearchCV fit...
       ‚úÖ AEM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.0316 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AEM XGBoost: Starting GridSearchCV fit...
       ‚úÖ LUNR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=162.7543 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LUNR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LUNR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=210.4624 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LUNR XGBoost: Starting GridSearchCV fit...
       ‚úÖ WF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.7538 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.3748
    - TCN: MSE=0.3616
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3616
        ‚Ä¢ LSTM: MSE=0.3748
        ‚Ä¢ Random Forest: MSE=35.4212
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.9992
        ‚Ä¢ XGBoost: MSE=44.7538
   ‚úÖ WF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WF (TargetReturn): TCN with MSE=0.3616
üêõ DEBUG: WF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WF.
üêõ DEBUG: WF - Moving model to CPU before return...
üêõ DEBUG [23:03:08.882]: WF - Returning result metadata...
üêõ DEBUG: train_worker started for HIPO
üêõ DEBUG [23:03:08.883]: Main received result for WF
  ‚öôÔ∏è Training models for HIPO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - HIPO: Initiating feature extraction for training.
  [DIAGNOSTIC] HIPO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HIPO: rows after features available: 126
üéØ HIPO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HIPO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HIPO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HIPO: Training LSTM (50 epochs)...
      ‚è≥ HIPO LSTM: Epoch 10/50 (20%)
      ‚è≥ HIPO LSTM: Epoch 20/50 (40%)
      ‚è≥ HIPO LSTM: Epoch 30/50 (60%)
      ‚è≥ HIPO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.339657
         RMSE: 0.582801
         R¬≤ Score: -1.0542 (Poor - 105.4% variance explained)
      üîπ HIPO: Training TCN (50 epochs)...
      ‚è≥ HIPO TCN: Epoch 10/50 (20%)
      ‚è≥ HIPO TCN: Epoch 20/50 (40%)
      ‚è≥ HIPO TCN: Epoch 30/50 (60%)
      ‚è≥ HIPO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.171015
         RMSE: 0.413539
         R¬≤ Score: -0.0343
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HIPO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HIPO Random Forest: Starting GridSearchCV fit...
       ‚úÖ DAPP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.1265 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.0s
    - LSTM: MSE=0.6147
    - TCN: MSE=0.5182
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5182
        ‚Ä¢ LSTM: MSE=0.6147
        ‚Ä¢ Random Forest: MSE=34.2434
        ‚Ä¢ XGBoost: MSE=35.1265
        ‚Ä¢ LightGBM Regressor (CPU): MSE=82.0518
   ‚úÖ DAPP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAPP (TargetReturn): TCN with MSE=0.5182
üêõ DEBUG: DAPP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAPP.
üêõ DEBUG: DAPP - Moving model to CPU before return...
üêõ DEBUG [23:03:12.899]: DAPP - Returning result metadata...
üêõ DEBUG [23:03:12.900]: Main received result for DAPP
üêõ DEBUG: train_worker started for PRA
  ‚öôÔ∏è Training models for PRA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - PRA: Initiating feature extraction for training.
  [DIAGNOSTIC] PRA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRA: rows after features available: 126
üéØ PRA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRA: Training LSTM (50 epochs)...
      ‚è≥ PRA LSTM: Epoch 10/50 (20%)
      ‚è≥ PRA LSTM: Epoch 20/50 (40%)
       ‚úÖ HIPO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.8639 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HIPO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PRA LSTM: Epoch 30/50 (60%)
       ‚úÖ HIPO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=29.5120 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HIPO XGBoost: Starting GridSearchCV fit...
      ‚è≥ PRA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.697616
         RMSE: 0.835234
         R¬≤ Score: -0.7118 (Poor - 71.2% variance explained)
      üîπ PRA: Training TCN (50 epochs)...
      ‚è≥ PRA TCN: Epoch 10/50 (20%)
      ‚è≥ PRA TCN: Epoch 20/50 (40%)
      ‚è≥ PRA TCN: Epoch 30/50 (60%)
      ‚è≥ PRA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.618292
         RMSE: 0.786316
         R¬≤ Score: -0.5172
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRA Random Forest: Starting GridSearchCV fit...
       ‚úÖ PRA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.0077 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PRA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.2492 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRA XGBoost: Starting GridSearchCV fit...
       ‚úÖ PEGA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=51.8527 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.3483
    - TCN: MSE=0.2505
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2505
        ‚Ä¢ LSTM: MSE=0.3483
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.1721
        ‚Ä¢ Random Forest: MSE=38.3911
        ‚Ä¢ XGBoost: MSE=51.8527
   ‚úÖ PEGA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PEGA (TargetReturn): TCN with MSE=0.2505
üêõ DEBUG: PEGA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PEGA.
üêõ DEBUG: PEGA - Moving model to CPU before return...
üêõ DEBUG [23:03:29.205]: PEGA - Returning result metadata...
üêõ DEBUG [23:03:29.205]: Main received result for PEGA
üêõ DEBUG: train_worker started for DAN
  ‚öôÔ∏è Training models for DAN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - DAN: Initiating feature extraction for training.
  [DIAGNOSTIC] DAN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAN: rows after features available: 126
üéØ DAN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAN: Training LSTM (50 epochs)...
      ‚è≥ DAN LSTM: Epoch 10/50 (20%)
      ‚è≥ DAN LSTM: Epoch 20/50 (40%)
      ‚è≥ DAN LSTM: Epoch 30/50 (60%)
      ‚è≥ DAN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.230681
         RMSE: 0.480293
         R¬≤ Score: -1.0953 (Poor - 109.5% variance explained)
      üîπ DAN: Training TCN (50 epochs)...
      ‚è≥ DAN TCN: Epoch 10/50 (20%)
      ‚è≥ DAN TCN: Epoch 20/50 (40%)
      ‚è≥ DAN TCN: Epoch 30/50 (60%)
      ‚è≥ DAN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.126595
         RMSE: 0.355802
         R¬≤ Score: -0.1499
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAN Random Forest: Starting GridSearchCV fit...
       ‚úÖ ORN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=91.0248 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.4611
    - TCN: MSE=0.3679
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3679
        ‚Ä¢ LSTM: MSE=0.4611
        ‚Ä¢ LightGBM Regressor (CPU): MSE=61.1105
        ‚Ä¢ XGBoost: MSE=91.0248
        ‚Ä¢ Random Forest: MSE=98.0416
   ‚úÖ ORN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ORN (TargetReturn): TCN with MSE=0.3679
üêõ DEBUG: ORN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ORN.
üêõ DEBUG: ORN - Moving model to CPU before return...
üêõ DEBUG [23:03:32.067]: ORN - Returning result metadata...
üêõ DEBUG: train_worker started for TGTX
üêõ DEBUG [23:03:32.067]: Main received result for ORN
üêõ DEBUG: Training progress: 308/959 done
  ‚öôÔ∏è Training models for TGTX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - TGTX: Initiating feature extraction for training.
  [DIAGNOSTIC] TGTX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TGTX: rows after features available: 126
üéØ TGTX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TGTX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TGTX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TGTX: Training LSTM (50 epochs)...
      ‚è≥ TGTX LSTM: Epoch 10/50 (20%)
      ‚è≥ TGTX LSTM: Epoch 20/50 (40%)
      ‚è≥ TGTX LSTM: Epoch 30/50 (60%)
      ‚è≥ TGTX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.283209
         RMSE: 0.532174
         R¬≤ Score: -0.4411 (Poor - 44.1% variance explained)
      üîπ TGTX: Training TCN (50 epochs)...
      ‚è≥ TGTX TCN: Epoch 10/50 (20%)
      ‚è≥ TGTX TCN: Epoch 20/50 (40%)
       ‚úÖ DAN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.7716 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TGTX TCN: Epoch 30/50 (60%)
      ‚è≥ TGTX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.212103
         RMSE: 0.460546
         R¬≤ Score: -0.0793
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TGTX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TGTX Random Forest: Starting GridSearchCV fit...
       ‚úÖ DAN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.4683 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAN XGBoost: Starting GridSearchCV fit...
       ‚úÖ TGTX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.0231 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TGTX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TGTX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=28.4168 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TGTX XGBoost: Starting GridSearchCV fit...
       ‚úÖ BITQ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=32.9033 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.6421
    - TCN: MSE=0.4521
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4521
        ‚Ä¢ LSTM: MSE=0.6421
        ‚Ä¢ XGBoost: MSE=32.9033
        ‚Ä¢ Random Forest: MSE=33.3065
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.5829
   ‚úÖ BITQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BITQ (TargetReturn): TCN with MSE=0.4521
üêõ DEBUG: BITQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BITQ.
üêõ DEBUG: BITQ - Moving model to CPU before return...
üêõ DEBUG [23:03:59.141]: BITQ - Returning result metadata...
üêõ DEBUG: train_worker started for AVPT
üêõ DEBUG [23:03:59.142]: Main received result for BITQ
  ‚öôÔ∏è Training models for AVPT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - AVPT: Initiating feature extraction for training.
  [DIAGNOSTIC] AVPT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AVPT: rows after features available: 126
üéØ AVPT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AVPT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AVPT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AVPT: Training LSTM (50 epochs)...
      ‚è≥ AVPT LSTM: Epoch 10/50 (20%)
       ‚úÖ CCL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.9300 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.7423
    - TCN: MSE=0.6145
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6145
        ‚Ä¢ LSTM: MSE=0.7423
        ‚Ä¢ Random Forest: MSE=24.9300
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.7381
        ‚Ä¢ XGBoost: MSE=36.9300
   ‚úÖ CCL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CCL (TargetReturn): TCN with MSE=0.6145
üêõ DEBUG: CCL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CCL.
üêõ DEBUG: CCL - Moving model to CPU before return...
üêõ DEBUG [23:03:59.778]: CCL - Returning result metadata...
üêõ DEBUG [23:03:59.779]: Main received result for CCL
üêõ DEBUG: train_worker started for RDVT
  ‚öôÔ∏è Training models for RDVT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - RDVT: Initiating feature extraction for training.
  [DIAGNOSTIC] RDVT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RDVT: rows after features available: 126
üéØ RDVT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RDVT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RDVT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RDVT: Training LSTM (50 epochs)...
      ‚è≥ AVPT LSTM: Epoch 20/50 (40%)
      ‚è≥ RDVT LSTM: Epoch 10/50 (20%)
      ‚è≥ AVPT LSTM: Epoch 30/50 (60%)
      ‚è≥ RDVT LSTM: Epoch 20/50 (40%)
      ‚è≥ AVPT LSTM: Epoch 40/50 (80%)
      ‚è≥ RDVT LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.298383
         RMSE: 0.546244
         R¬≤ Score: -0.5272 (Poor - 52.7% variance explained)
      üîπ AVPT: Training TCN (50 epochs)...
      ‚è≥ AVPT TCN: Epoch 10/50 (20%)
      ‚è≥ AVPT TCN: Epoch 20/50 (40%)
      ‚è≥ AVPT TCN: Epoch 30/50 (60%)
      ‚è≥ AVPT TCN: Epoch 40/50 (80%)
      ‚è≥ RDVT LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.312499
         RMSE: 0.559016
         R¬≤ Score: -0.5995
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AVPT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AVPT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.299710
         RMSE: 0.547458
         R¬≤ Score: -0.7024 (Poor - 70.2% variance explained)
      üîπ RDVT: Training TCN (50 epochs)...
      ‚è≥ RDVT TCN: Epoch 10/50 (20%)
      ‚è≥ RDVT TCN: Epoch 20/50 (40%)
      ‚è≥ RDVT TCN: Epoch 30/50 (60%)
      ‚è≥ RDVT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.200278
         RMSE: 0.447524
         R¬≤ Score: -0.1376
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RDVT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RDVT Random Forest: Starting GridSearchCV fit...
       ‚úÖ AVPT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.9879 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AVPT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AVPT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.7090 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AVPT XGBoost: Starting GridSearchCV fit...
       ‚úÖ RDVT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.0736 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RDVT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RDVT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.9022 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RDVT XGBoost: Starting GridSearchCV fit...
       ‚úÖ NUGT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=119.8413 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.2274
    - TCN: MSE=0.1610
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1610
        ‚Ä¢ LSTM: MSE=0.2274
        ‚Ä¢ Random Forest: MSE=108.6507
        ‚Ä¢ XGBoost: MSE=119.8413
        ‚Ä¢ LightGBM Regressor (CPU): MSE=125.1161
   ‚úÖ NUGT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NUGT (TargetReturn): TCN with MSE=0.1610
üêõ DEBUG: NUGT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NUGT.
üêõ DEBUG: NUGT - Moving model to CPU before return...
üêõ DEBUG [23:04:09.264]: NUGT - Returning result metadata...
üêõ DEBUG: train_worker started for VSTA
üêõ DEBUG [23:04:09.266]: Main received result for NUGT
  ‚öôÔ∏è Training models for VSTA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - VSTA: Initiating feature extraction for training.
  [DIAGNOSTIC] VSTA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VSTA: rows after features available: 126
üéØ VSTA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VSTA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VSTA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VSTA: Training LSTM (50 epochs)...
      ‚è≥ VSTA LSTM: Epoch 10/50 (20%)
      ‚è≥ VSTA LSTM: Epoch 20/50 (40%)
      ‚è≥ VSTA LSTM: Epoch 30/50 (60%)
      ‚è≥ VSTA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.245406
         RMSE: 0.495384
         R¬≤ Score: -0.5002 (Poor - 50.0% variance explained)
      üîπ VSTA: Training TCN (50 epochs)...
      ‚è≥ VSTA TCN: Epoch 10/50 (20%)
      ‚è≥ VSTA TCN: Epoch 20/50 (40%)
      ‚è≥ VSTA TCN: Epoch 30/50 (60%)
      ‚è≥ VSTA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.212085
         RMSE: 0.460527
         R¬≤ Score: -0.2965
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VSTA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VSTA Random Forest: Starting GridSearchCV fit...
       ‚úÖ VSTA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=73.9161 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VSTA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VSTA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=95.8796 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VSTA XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRIM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.7109 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.5304
    - TCN: MSE=0.4189
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4189
        ‚Ä¢ LSTM: MSE=0.5304
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.2649
        ‚Ä¢ XGBoost: MSE=33.7109
        ‚Ä¢ Random Forest: MSE=34.6035
   ‚úÖ PRIM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRIM (TargetReturn): TCN with MSE=0.4189
üêõ DEBUG: PRIM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRIM.
üêõ DEBUG: PRIM - Moving model to CPU before return...
üêõ DEBUG [23:04:20.520]: PRIM - Returning result metadata...
üêõ DEBUG [23:04:20.520]: Main received result for PRIM
üêõ DEBUG: Training progress: 312/959 done
üêõ DEBUG: train_worker started for AAMI
  ‚öôÔ∏è Training models for AAMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AAMI: Initiating feature extraction for training.
  [DIAGNOSTIC] AAMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AAMI: rows after features available: 126
üéØ AAMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AAMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AAMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AAMI: Training LSTM (50 epochs)...
      ‚è≥ AAMI LSTM: Epoch 10/50 (20%)
      ‚è≥ AAMI LSTM: Epoch 20/50 (40%)
      ‚è≥ AAMI LSTM: Epoch 30/50 (60%)
      ‚è≥ AAMI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.249233
         RMSE: 0.499232
         R¬≤ Score: -0.9247 (Poor - 92.5% variance explained)
      üîπ AAMI: Training TCN (50 epochs)...
      ‚è≥ AAMI TCN: Epoch 10/50 (20%)
      ‚è≥ AAMI TCN: Epoch 20/50 (40%)
      ‚è≥ AAMI TCN: Epoch 30/50 (60%)
      ‚è≥ AAMI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.213898
         RMSE: 0.462491
         R¬≤ Score: -0.6518
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AAMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AAMI Random Forest: Starting GridSearchCV fit...
       ‚úÖ IDCC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.0989 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.4416
    - TCN: MSE=0.2203
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2203
        ‚Ä¢ LSTM: MSE=0.4416
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.3939
        ‚Ä¢ XGBoost: MSE=9.0989
        ‚Ä¢ Random Forest: MSE=10.3191
   ‚úÖ IDCC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IDCC (TargetReturn): TCN with MSE=0.2203
üêõ DEBUG: IDCC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IDCC.
üêõ DEBUG: IDCC - Moving model to CPU before return...
üêõ DEBUG [23:04:25.723]: IDCC - Returning result metadata...
üêõ DEBUG [23:04:25.723]: Main received result for IDCC
üêõ DEBUG: train_worker started for ENVX
  ‚öôÔ∏è Training models for ENVX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ENVX: Initiating feature extraction for training.
  [DIAGNOSTIC] ENVX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ENVX: rows after features available: 126
üéØ ENVX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ENVX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ENVX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ENVX: Training LSTM (50 epochs)...
       ‚úÖ AAMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=52.0992 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AAMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ENVX LSTM: Epoch 10/50 (20%)
      ‚è≥ ENVX LSTM: Epoch 20/50 (40%)
       ‚úÖ AAMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=35.9917 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AAMI XGBoost: Starting GridSearchCV fit...
      ‚è≥ ENVX LSTM: Epoch 30/50 (60%)
      ‚è≥ ENVX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.767122
         RMSE: 0.875855
         R¬≤ Score: -1.0373 (Poor - 103.7% variance explained)
      üîπ ENVX: Training TCN (50 epochs)...
      ‚è≥ ENVX TCN: Epoch 10/50 (20%)
      ‚è≥ ENVX TCN: Epoch 20/50 (40%)
      ‚è≥ ENVX TCN: Epoch 30/50 (60%)
      ‚è≥ ENVX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.405926
         RMSE: 0.637123
         R¬≤ Score: -0.0781
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ENVX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ENVX Random Forest: Starting GridSearchCV fit...
       ‚úÖ GTX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.8285 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.2590
    - TCN: MSE=0.1349
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1349
        ‚Ä¢ LSTM: MSE=0.2590
        ‚Ä¢ Random Forest: MSE=20.9944
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.7649
        ‚Ä¢ XGBoost: MSE=30.8285
   ‚úÖ GTX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GTX (TargetReturn): TCN with MSE=0.1349
üêõ DEBUG: GTX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GTX.
üêõ DEBUG: GTX - Moving model to CPU before return...
üêõ DEBUG [23:04:29.941]: GTX - Returning result metadata...
üêõ DEBUG [23:04:29.941]: Main received result for GTX
üêõ DEBUG: train_worker started for ASA
  ‚öôÔ∏è Training models for ASA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - ASA: Initiating feature extraction for training.
  [DIAGNOSTIC] ASA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ASA: rows after features available: 126
üéØ ASA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ASA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ASA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ASA: Training LSTM (50 epochs)...
      ‚è≥ ASA LSTM: Epoch 10/50 (20%)
      ‚è≥ ASA LSTM: Epoch 20/50 (40%)
       ‚úÖ ENVX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=121.9207 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ENVX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ASA LSTM: Epoch 30/50 (60%)
      ‚è≥ ASA LSTM: Epoch 40/50 (80%)
       ‚úÖ ENVX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=190.7615 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ENVX XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.167697
         RMSE: 0.409508
         R¬≤ Score: -0.2519 (Poor - 25.2% variance explained)
      üîπ ASA: Training TCN (50 epochs)...
      ‚è≥ ASA TCN: Epoch 10/50 (20%)
      ‚è≥ ASA TCN: Epoch 20/50 (40%)
      ‚è≥ ASA TCN: Epoch 30/50 (60%)
      ‚è≥ ASA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.140376
         RMSE: 0.374667
         R¬≤ Score: -0.0479
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ASA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ASA Random Forest: Starting GridSearchCV fit...
       ‚úÖ ITRN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.0876 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.1194
    - TCN: MSE=0.1040
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1040
        ‚Ä¢ LSTM: MSE=0.1194
        ‚Ä¢ Random Forest: MSE=24.1776
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.1545
        ‚Ä¢ XGBoost: MSE=26.0876
   ‚úÖ ITRN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ITRN (TargetReturn): TCN with MSE=0.1040
üêõ DEBUG: ITRN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ITRN.
üêõ DEBUG: ITRN - Moving model to CPU before return...
üêõ DEBUG [23:04:33.927]: ITRN - Returning result metadata...
üêõ DEBUG [23:04:33.927]: Main received result for ITRN
üêõ DEBUG: train_worker started for C
  ‚öôÔ∏è Training models for C (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - C: Initiating feature extraction for training.
  [DIAGNOSTIC] C: fetch_training_data - Initial data rows: 205
   ‚Ü≥ C: rows after features available: 126
üéØ C: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] C: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö C: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ C: Training LSTM (50 epochs)...
      ‚è≥ C LSTM: Epoch 10/50 (20%)
      ‚è≥ C LSTM: Epoch 20/50 (40%)
      ‚è≥ C LSTM: Epoch 30/50 (60%)
      ‚è≥ C LSTM: Epoch 40/50 (80%)
       ‚úÖ ASA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.6084 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ASA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.650319
         RMSE: 0.806424
         R¬≤ Score: -0.9104 (Poor - 91.0% variance explained)
      üîπ C: Training TCN (50 epochs)...
      ‚è≥ C TCN: Epoch 10/50 (20%)
      ‚è≥ C TCN: Epoch 20/50 (40%)
      ‚è≥ C TCN: Epoch 30/50 (60%)
       ‚úÖ ASA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=32.4920 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ASA XGBoost: Starting GridSearchCV fit...
      ‚è≥ C TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.639269
         RMSE: 0.799543
         R¬≤ Score: -0.8779
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä C: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ C Random Forest: Starting GridSearchCV fit...
       ‚úÖ UAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.2011 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.8s
    - LSTM: MSE=0.5222
    - TCN: MSE=0.5697
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5222
        ‚Ä¢ TCN: MSE=0.5697
        ‚Ä¢ Random Forest: MSE=23.2272
        ‚Ä¢ XGBoost: MSE=29.2011
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.3110
   ‚úÖ UAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UAL (TargetReturn): LSTM with MSE=0.5222
üêõ DEBUG: UAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UAL.
üêõ DEBUG: UAL - Moving model to CPU before return...
üêõ DEBUG [23:04:37.993]: UAL - Returning result metadata...
üêõ DEBUG: train_worker started for FNGO
  ‚öôÔ∏è Training models for FNGO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - FNGO: Initiating feature extraction for training.
  [DIAGNOSTIC] FNGO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FNGO: rows after features available: 126
üéØ FNGO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FNGO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FNGO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FNGO: Training LSTM (50 epochs)...
      ‚è≥ FNGO LSTM: Epoch 10/50 (20%)
       ‚úÖ APG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.6387 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.3775
    - TCN: MSE=0.3158
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3158
        ‚Ä¢ LSTM: MSE=0.3775
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.1000
        ‚Ä¢ XGBoost: MSE=24.6387
        ‚Ä¢ Random Forest: MSE=35.1044
   ‚úÖ APG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APG (TargetReturn): TCN with MSE=0.3158
üêõ DEBUG: APG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APG.
üêõ DEBUG: APG - Moving model to CPU before return...
üêõ DEBUG [23:04:38.558]: APG - Returning result metadata...
üêõ DEBUG [23:04:38.559]: Main received result for APG
üêõ DEBUG: Training progress: 316/959 done
üêõ DEBUG [23:04:38.559]: Main received result for UAL
üêõ DEBUG: train_worker started for FIVE
  ‚öôÔ∏è Training models for FIVE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FIVE: Initiating feature extraction for training.
  [DIAGNOSTIC] FIVE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FIVE: rows after features available: 126
üéØ FIVE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FIVE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FIVE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FIVE: Training LSTM (50 epochs)...
      ‚è≥ FNGO LSTM: Epoch 20/50 (40%)
       ‚úÖ C Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.1608 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ C LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FIVE LSTM: Epoch 10/50 (20%)
      ‚è≥ FNGO LSTM: Epoch 30/50 (60%)
      ‚è≥ FIVE LSTM: Epoch 20/50 (40%)
      ‚è≥ FNGO LSTM: Epoch 40/50 (80%)
       ‚úÖ C LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.1576 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ C XGBoost: Starting GridSearchCV fit...
      ‚è≥ FIVE LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.545062
         RMSE: 0.738283
         R¬≤ Score: -1.0485 (Poor - 104.8% variance explained)
      üîπ FNGO: Training TCN (50 epochs)...
      ‚è≥ FNGO TCN: Epoch 10/50 (20%)
      ‚è≥ FNGO TCN: Epoch 20/50 (40%)
      ‚è≥ FNGO TCN: Epoch 30/50 (60%)
      ‚è≥ FNGO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.459280
         RMSE: 0.677702
         R¬≤ Score: -0.7261
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FNGO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FNGO Random Forest: Starting GridSearchCV fit...
      ‚è≥ FIVE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.582207
         RMSE: 0.763025
         R¬≤ Score: -1.2544 (Poor - 125.4% variance explained)
      üîπ FIVE: Training TCN (50 epochs)...
      ‚è≥ FIVE TCN: Epoch 10/50 (20%)
      ‚è≥ FIVE TCN: Epoch 20/50 (40%)
      ‚è≥ FIVE TCN: Epoch 30/50 (60%)
      ‚è≥ FIVE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.381204
         RMSE: 0.617418
         R¬≤ Score: -0.4761
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FIVE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FIVE Random Forest: Starting GridSearchCV fit...
       ‚úÖ AEM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.4408 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.2227
    - TCN: MSE=0.1976
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1976
        ‚Ä¢ LSTM: MSE=0.2227
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.0316
        ‚Ä¢ Random Forest: MSE=28.1215
        ‚Ä¢ XGBoost: MSE=28.4408
   ‚úÖ AEM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AEM (TargetReturn): TCN with MSE=0.1976
üêõ DEBUG: AEM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AEM.
üêõ DEBUG: AEM - Moving model to CPU before return...
üêõ DEBUG [23:04:43.702]: AEM - Returning result metadata...
üêõ DEBUG: train_worker started for YPF
üêõ DEBUG [23:04:43.703]: Main received result for AEM
  ‚öôÔ∏è Training models for YPF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - YPF: Initiating feature extraction for training.
  [DIAGNOSTIC] YPF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YPF: rows after features available: 126
üéØ YPF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YPF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YPF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YPF: Training LSTM (50 epochs)...
       ‚úÖ FNGO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=55.6482 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FNGO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ YPF LSTM: Epoch 10/50 (20%)
      ‚è≥ YPF LSTM: Epoch 20/50 (40%)
       ‚úÖ FNGO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=80.4721 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FNGO XGBoost: Starting GridSearchCV fit...
       ‚úÖ FIVE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.7620 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FIVE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ YPF LSTM: Epoch 30/50 (60%)
      ‚è≥ YPF LSTM: Epoch 40/50 (80%)
       ‚úÖ FIVE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=68.1772 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FIVE XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.076974
         RMSE: 0.277443
         R¬≤ Score: -1.0176 (Poor - 101.8% variance explained)
      üîπ YPF: Training TCN (50 epochs)...
      ‚è≥ YPF TCN: Epoch 10/50 (20%)
      ‚è≥ YPF TCN: Epoch 20/50 (40%)
      ‚è≥ YPF TCN: Epoch 30/50 (60%)
      ‚è≥ YPF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.055035
         RMSE: 0.234596
         R¬≤ Score: -0.4426
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YPF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YPF Random Forest: Starting GridSearchCV fit...
       ‚úÖ LUNR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=347.1486 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.2493
    - TCN: MSE=0.1423
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1423
        ‚Ä¢ LSTM: MSE=0.2493
        ‚Ä¢ Random Forest: MSE=162.7543
        ‚Ä¢ LightGBM Regressor (CPU): MSE=210.4624
        ‚Ä¢ XGBoost: MSE=347.1486
   ‚úÖ LUNR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LUNR (TargetReturn): TCN with MSE=0.1423
üêõ DEBUG: LUNR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LUNR.
üêõ DEBUG: LUNR - Moving model to CPU before return...
üêõ DEBUG [23:04:47.741]: LUNR - Returning result metadata...
üêõ DEBUG [23:04:47.741]: Main received result for LUNR
üêõ DEBUG: train_worker started for NWG
  ‚öôÔ∏è Training models for NWG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - NWG: Initiating feature extraction for training.
  [DIAGNOSTIC] NWG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NWG: rows after features available: 126
üéØ NWG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NWG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NWG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NWG: Training LSTM (50 epochs)...
      ‚è≥ NWG LSTM: Epoch 10/50 (20%)
      ‚è≥ NWG LSTM: Epoch 20/50 (40%)
      ‚è≥ NWG LSTM: Epoch 30/50 (60%)
       ‚úÖ YPF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.7676 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YPF LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NWG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.414239
         RMSE: 0.643614
         R¬≤ Score: -0.7546 (Poor - 75.5% variance explained)
      üîπ NWG: Training TCN (50 epochs)...
      ‚è≥ NWG TCN: Epoch 10/50 (20%)
      ‚è≥ NWG TCN: Epoch 20/50 (40%)
       ‚úÖ YPF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=34.7164 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YPF XGBoost: Starting GridSearchCV fit...
      ‚è≥ NWG TCN: Epoch 30/50 (60%)
      ‚è≥ NWG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.304897
         RMSE: 0.552175
         R¬≤ Score: -0.2915
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NWG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NWG Random Forest: Starting GridSearchCV fit...
       ‚úÖ NWG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.9763 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NWG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NWG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.3073 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NWG XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.3806 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 111.3s
    - LSTM: MSE=0.6976
    - TCN: MSE=0.6183
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 114.3 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6183
        ‚Ä¢ LSTM: MSE=0.6976
        ‚Ä¢ Random Forest: MSE=21.0077
        ‚Ä¢ XGBoost: MSE=22.3806
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.2492
   ‚úÖ PRA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRA (TargetReturn): TCN with MSE=0.6183
üêõ DEBUG: PRA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRA.
üêõ DEBUG: PRA - Moving model to CPU before return...
üêõ DEBUG [23:05:09.967]: PRA - Returning result metadata...
üêõ DEBUG: train_worker started for PW
  ‚öôÔ∏è Training models for PW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - PW: Initiating feature extraction for training.
  [DIAGNOSTIC] PW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PW: rows after features available: 126
üéØ PW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PW: Training LSTM (50 epochs)...
       ‚úÖ HIPO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.0569 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.5s
    - LSTM: MSE=0.3397
    - TCN: MSE=0.1710
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1710
        ‚Ä¢ LSTM: MSE=0.3397
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.5120
        ‚Ä¢ Random Forest: MSE=30.8639
        ‚Ä¢ XGBoost: MSE=31.0569
   ‚úÖ HIPO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HIPO (TargetReturn): TCN with MSE=0.1710
üêõ DEBUG: HIPO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HIPO.
üêõ DEBUG: HIPO - Moving model to CPU before return...
üêõ DEBUG [23:05:10.270]: HIPO - Returning result metadata...
üêõ DEBUG: train_worker started for UPXI
üêõ DEBUG [23:05:10.271]: Main received result for HIPO
üêõ DEBUG: Training progress: 320/959 done
üêõ DEBUG [23:05:10.271]: Main received result for PRA
  ‚öôÔ∏è Training models for UPXI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - UPXI: Initiating feature extraction for training.
  [DIAGNOSTIC] UPXI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UPXI: rows after features available: 126
üéØ UPXI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UPXI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UPXI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UPXI: Training LSTM (50 epochs)...
      ‚è≥ PW LSTM: Epoch 10/50 (20%)
      ‚è≥ UPXI LSTM: Epoch 10/50 (20%)
      ‚è≥ PW LSTM: Epoch 20/50 (40%)
      ‚è≥ UPXI LSTM: Epoch 20/50 (40%)
      ‚è≥ PW LSTM: Epoch 30/50 (60%)
      ‚è≥ UPXI LSTM: Epoch 30/50 (60%)
      ‚è≥ PW LSTM: Epoch 40/50 (80%)
      ‚è≥ UPXI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.025796
         RMSE: 0.160610
         R¬≤ Score: -0.2067 (Poor - 20.7% variance explained)
      üîπ PW: Training TCN (50 epochs)...
      ‚è≥ PW TCN: Epoch 10/50 (20%)
      ‚è≥ PW TCN: Epoch 20/50 (40%)
      ‚è≥ PW TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.500826
         RMSE: 0.707690
         R¬≤ Score: -0.6154 (Poor - 61.5% variance explained)
      üîπ UPXI: Training TCN (50 epochs)...
      ‚è≥ PW TCN: Epoch 40/50 (80%)
      ‚è≥ UPXI TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.022493
         RMSE: 0.149978
         R¬≤ Score: -0.0522
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PW Random Forest: Starting GridSearchCV fit...
      ‚è≥ UPXI TCN: Epoch 20/50 (40%)
      ‚è≥ UPXI TCN: Epoch 30/50 (60%)
      ‚è≥ UPXI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.315059
         RMSE: 0.561301
         R¬≤ Score: -0.0162
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UPXI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UPXI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.1462 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UPXI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2461.5798 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UPXI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.9324 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PW XGBoost: Starting GridSearchCV fit...
       ‚úÖ UPXI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=2156.1910 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UPXI XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=42.4736 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 117.1s
    - LSTM: MSE=0.2307
    - TCN: MSE=0.1266
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1266
        ‚Ä¢ LSTM: MSE=0.2307
        ‚Ä¢ XGBoost: MSE=42.4736
        ‚Ä¢ Random Forest: MSE=45.7716
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.4683
   ‚úÖ DAN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAN (TargetReturn): TCN with MSE=0.1266
üêõ DEBUG: DAN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAN.
üêõ DEBUG: DAN - Moving model to CPU before return...
üêõ DEBUG [23:05:32.408]: DAN - Returning result metadata...
üêõ DEBUG: train_worker started for CD
üêõ DEBUG [23:05:32.410]: Main received result for DAN
  ‚öôÔ∏è Training models for CD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - CD: Initiating feature extraction for training.
  [DIAGNOSTIC] CD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CD: rows after features available: 126
üéØ CD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CD: Training LSTM (50 epochs)...
      ‚è≥ CD LSTM: Epoch 10/50 (20%)
      ‚è≥ CD LSTM: Epoch 20/50 (40%)
      ‚è≥ CD LSTM: Epoch 30/50 (60%)
      ‚è≥ CD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.009323
         RMSE: 0.096558
         R¬≤ Score: -1.7442 (Poor - 174.4% variance explained)
      üîπ CD: Training TCN (50 epochs)...
      ‚è≥ CD TCN: Epoch 10/50 (20%)
      ‚è≥ CD TCN: Epoch 20/50 (40%)
      ‚è≥ CD TCN: Epoch 30/50 (60%)
      ‚è≥ CD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.005106
         RMSE: 0.071456
         R¬≤ Score: -0.5029
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CD Random Forest: Starting GridSearchCV fit...
       ‚úÖ TGTX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.5910 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.2832
    - TCN: MSE=0.2121
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2121
        ‚Ä¢ LSTM: MSE=0.2832
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.4168
        ‚Ä¢ Random Forest: MSE=44.0231
        ‚Ä¢ XGBoost: MSE=47.5910
   ‚úÖ TGTX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TGTX (TargetReturn): TCN with MSE=0.2121
üêõ DEBUG: TGTX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TGTX.
üêõ DEBUG: TGTX - Moving model to CPU before return...
üêõ DEBUG [23:05:37.287]: TGTX - Returning result metadata...
üêõ DEBUG: train_worker started for ADTN
üêõ DEBUG [23:05:37.288]: Main received result for TGTX
  ‚öôÔ∏è Training models for ADTN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ADTN: Initiating feature extraction for training.
  [DIAGNOSTIC] ADTN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ADTN: rows after features available: 126
üéØ ADTN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ADTN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ADTN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ADTN: Training LSTM (50 epochs)...
      ‚è≥ ADTN LSTM: Epoch 10/50 (20%)
       ‚úÖ CD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=606.6510 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ADTN LSTM: Epoch 20/50 (40%)
      ‚è≥ ADTN LSTM: Epoch 30/50 (60%)
       ‚úÖ CD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=1377.8475 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CD XGBoost: Starting GridSearchCV fit...
      ‚è≥ ADTN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.307004
         RMSE: 0.554080
         R¬≤ Score: -0.9868 (Poor - 98.7% variance explained)
      üîπ ADTN: Training TCN (50 epochs)...
      ‚è≥ ADTN TCN: Epoch 10/50 (20%)
      ‚è≥ ADTN TCN: Epoch 20/50 (40%)
      ‚è≥ ADTN TCN: Epoch 30/50 (60%)
      ‚è≥ ADTN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.163497
         RMSE: 0.404347
         R¬≤ Score: -0.0581
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ADTN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ADTN Random Forest: Starting GridSearchCV fit...
       ‚úÖ ADTN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.9719 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ADTN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ADTN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.1210 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ADTN XGBoost: Starting GridSearchCV fit...
       ‚úÖ AVPT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.8577 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.2984
    - TCN: MSE=0.3125
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2984
        ‚Ä¢ TCN: MSE=0.3125
        ‚Ä¢ Random Forest: MSE=19.9879
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.7090
        ‚Ä¢ XGBoost: MSE=41.8577
   ‚úÖ AVPT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AVPT (TargetReturn): LSTM with MSE=0.2984
üêõ DEBUG: AVPT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AVPT.
üêõ DEBUG: AVPT - Moving model to CPU before return...
üêõ DEBUG [23:06:04.656]: AVPT - Returning result metadata...
üêõ DEBUG [23:06:04.656]: Main received result for AVPTüêõ DEBUG: train_worker started for RL

üêõ DEBUG: Training progress: 324/959 done
  ‚öôÔ∏è Training models for RL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - RL: Initiating feature extraction for training.
  [DIAGNOSTIC] RL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RL: rows after features available: 126
üéØ RL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RL: Training LSTM (50 epochs)...
      ‚è≥ RL LSTM: Epoch 10/50 (20%)
      ‚è≥ RL LSTM: Epoch 20/50 (40%)
       ‚úÖ RDVT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.5088 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.2997
    - TCN: MSE=0.2003
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2003
        ‚Ä¢ LSTM: MSE=0.2997
        ‚Ä¢ Random Forest: MSE=18.0736
        ‚Ä¢ XGBoost: MSE=19.5088
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.9022
   ‚úÖ RDVT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RDVT (TargetReturn): TCN with MSE=0.2003
üêõ DEBUG: RDVT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RDVT.
üêõ DEBUG: RDVT - Moving model to CPU before return...
üêõ DEBUG [23:06:05.964]: RDVT - Returning result metadata...
üêõ DEBUG: train_worker started for SHOP
üêõ DEBUG [23:06:05.965]: Main received result for RDVT
  ‚öôÔ∏è Training models for SHOP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - SHOP: Initiating feature extraction for training.
  [DIAGNOSTIC] SHOP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SHOP: rows after features available: 126
üéØ SHOP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SHOP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SHOP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SHOP: Training LSTM (50 epochs)...
      ‚è≥ RL LSTM: Epoch 30/50 (60%)
      ‚è≥ RL LSTM: Epoch 40/50 (80%)
      ‚è≥ SHOP LSTM: Epoch 10/50 (20%)
      ‚è≥ SHOP LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.500113
         RMSE: 0.707187
         R¬≤ Score: -1.0287 (Poor - 102.9% variance explained)
      üîπ RL: Training TCN (50 epochs)...
      ‚è≥ RL TCN: Epoch 10/50 (20%)
      ‚è≥ RL TCN: Epoch 20/50 (40%)
      ‚è≥ RL TCN: Epoch 30/50 (60%)
      ‚è≥ RL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.412931
         RMSE: 0.642597
         R¬≤ Score: -0.6750
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RL Random Forest: Starting GridSearchCV fit...
      ‚è≥ SHOP LSTM: Epoch 30/50 (60%)
      ‚è≥ SHOP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.474221
         RMSE: 0.688637
         R¬≤ Score: -0.6253 (Poor - 62.5% variance explained)
      üîπ SHOP: Training TCN (50 epochs)...
      ‚è≥ SHOP TCN: Epoch 10/50 (20%)
      ‚è≥ SHOP TCN: Epoch 20/50 (40%)
      ‚è≥ SHOP TCN: Epoch 30/50 (60%)
      ‚è≥ SHOP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.466819
         RMSE: 0.683241
         R¬≤ Score: -0.5999
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SHOP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SHOP Random Forest: Starting GridSearchCV fit...
       ‚úÖ RL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.3797 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.0006 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RL XGBoost: Starting GridSearchCV fit...
       ‚úÖ SHOP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.5328 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SHOP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SHOP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=42.1435 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SHOP XGBoost: Starting GridSearchCV fit...
       ‚úÖ VSTA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=67.3420 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.2454
    - TCN: MSE=0.2121
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2121
        ‚Ä¢ LSTM: MSE=0.2454
        ‚Ä¢ XGBoost: MSE=67.3420
        ‚Ä¢ Random Forest: MSE=73.9161
        ‚Ä¢ LightGBM Regressor (CPU): MSE=95.8796
   ‚úÖ VSTA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VSTA (TargetReturn): TCN with MSE=0.2121
üêõ DEBUG: VSTA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VSTA.
üêõ DEBUG: VSTA - Moving model to CPU before return...
üêõ DEBUG [23:06:14.628]: VSTA - Returning result metadata...
üêõ DEBUG [23:06:14.628]: Main received result for VSTA
üêõ DEBUG: train_worker started for WPM
  ‚öôÔ∏è Training models for WPM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - WPM: Initiating feature extraction for training.
  [DIAGNOSTIC] WPM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WPM: rows after features available: 126
üéØ WPM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WPM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WPM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WPM: Training LSTM (50 epochs)...
      ‚è≥ WPM LSTM: Epoch 10/50 (20%)
      ‚è≥ WPM LSTM: Epoch 20/50 (40%)
      ‚è≥ WPM LSTM: Epoch 30/50 (60%)
      ‚è≥ WPM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.137526
         RMSE: 0.370845
         R¬≤ Score: -0.6346 (Poor - 63.5% variance explained)
      üîπ WPM: Training TCN (50 epochs)...
      ‚è≥ WPM TCN: Epoch 10/50 (20%)
      ‚è≥ WPM TCN: Epoch 20/50 (40%)
      ‚è≥ WPM TCN: Epoch 30/50 (60%)
      ‚è≥ WPM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.088847
         RMSE: 0.298073
         R¬≤ Score: -0.0560
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WPM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WPM Random Forest: Starting GridSearchCV fit...
       ‚úÖ WPM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.7039 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WPM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WPM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.6286 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WPM XGBoost: Starting GridSearchCV fit...
       ‚úÖ AAMI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=29.0111 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.2492
    - TCN: MSE=0.2139
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2139
        ‚Ä¢ LSTM: MSE=0.2492
        ‚Ä¢ XGBoost: MSE=29.0111
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.9917
        ‚Ä¢ Random Forest: MSE=52.0992
   ‚úÖ AAMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AAMI (TargetReturn): TCN with MSE=0.2139
üêõ DEBUG: AAMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AAMI.
üêõ DEBUG: AAMI - Moving model to CPU before return...
üêõ DEBUG [23:06:27.333]: AAMI - Returning result metadata...
üêõ DEBUG: train_worker started for AAP
üêõ DEBUG [23:06:27.341]: Main received result for AAMI
  ‚öôÔ∏è Training models for AAP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AAP: Initiating feature extraction for training.
  [DIAGNOSTIC] AAP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AAP: rows after features available: 126
üéØ AAP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AAP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AAP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AAP: Training LSTM (50 epochs)...
      ‚è≥ AAP LSTM: Epoch 10/50 (20%)
      ‚è≥ AAP LSTM: Epoch 20/50 (40%)
      ‚è≥ AAP LSTM: Epoch 30/50 (60%)
      ‚è≥ AAP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.704204
         RMSE: 0.839169
         R¬≤ Score: -0.6377 (Poor - 63.8% variance explained)
      üîπ AAP: Training TCN (50 epochs)...
      ‚è≥ AAP TCN: Epoch 10/50 (20%)
      ‚è≥ AAP TCN: Epoch 20/50 (40%)
      ‚è≥ AAP TCN: Epoch 30/50 (60%)
      ‚è≥ AAP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.475046
         RMSE: 0.689236
         R¬≤ Score: -0.1048
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AAP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AAP Random Forest: Starting GridSearchCV fit...
       ‚úÖ ENVX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=197.6894 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.7671
    - TCN: MSE=0.4059
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4059
        ‚Ä¢ LSTM: MSE=0.7671
        ‚Ä¢ Random Forest: MSE=121.9207
        ‚Ä¢ LightGBM Regressor (CPU): MSE=190.7615
        ‚Ä¢ XGBoost: MSE=197.6894
   ‚úÖ ENVX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ENVX (TargetReturn): TCN with MSE=0.4059
üêõ DEBUG: ENVX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ENVX.
üêõ DEBUG: ENVX - Moving model to CPU before return...
üêõ DEBUG [23:06:30.350]: ENVX - Returning result metadata...
üêõ DEBUG: train_worker started for ZLAB
üêõ DEBUG [23:06:30.351]: Main received result for ENVX
üêõ DEBUG: Training progress: 328/959 done
  ‚öôÔ∏è Training models for ZLAB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ZLAB: Initiating feature extraction for training.
  [DIAGNOSTIC] ZLAB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ZLAB: rows after features available: 126
üéØ ZLAB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ZLAB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ZLAB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ZLAB: Training LSTM (50 epochs)...
      ‚è≥ ZLAB LSTM: Epoch 10/50 (20%)
      ‚è≥ ZLAB LSTM: Epoch 20/50 (40%)
      ‚è≥ ZLAB LSTM: Epoch 30/50 (60%)
      ‚è≥ ZLAB LSTM: Epoch 40/50 (80%)
       ‚úÖ AAP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.9718 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AAP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.194143
         RMSE: 0.440617
         R¬≤ Score: -0.5629 (Poor - 56.3% variance explained)
      üîπ ZLAB: Training TCN (50 epochs)...
      ‚è≥ ZLAB TCN: Epoch 10/50 (20%)
      ‚è≥ ZLAB TCN: Epoch 20/50 (40%)
      ‚è≥ ZLAB TCN: Epoch 30/50 (60%)
      ‚è≥ ZLAB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.133957
         RMSE: 0.366002
         R¬≤ Score: -0.0784
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ZLAB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ZLAB Random Forest: Starting GridSearchCV fit...
       ‚úÖ AAP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.8924 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AAP XGBoost: Starting GridSearchCV fit...
       ‚úÖ ASA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.5201 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.1677
    - TCN: MSE=0.1404
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1404
        ‚Ä¢ LSTM: MSE=0.1677
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.4920
        ‚Ä¢ Random Forest: MSE=39.6084
        ‚Ä¢ XGBoost: MSE=47.5201
   ‚úÖ ASA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ASA (TargetReturn): TCN with MSE=0.1404
üêõ DEBUG: ASA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ASA.
üêõ DEBUG: ASA - Moving model to CPU before return...
üêõ DEBUG [23:06:34.290]: ASA - Returning result metadata...
üêõ DEBUG: train_worker started for RRGB
üêõ DEBUG [23:06:34.290]: Main received result for ASA
  ‚öôÔ∏è Training models for RRGB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - RRGB: Initiating feature extraction for training.
  [DIAGNOSTIC] RRGB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RRGB: rows after features available: 126
üéØ RRGB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RRGB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RRGB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RRGB: Training LSTM (50 epochs)...
      ‚è≥ RRGB LSTM: Epoch 10/50 (20%)
      ‚è≥ RRGB LSTM: Epoch 20/50 (40%)
      ‚è≥ RRGB LSTM: Epoch 30/50 (60%)
       ‚úÖ ZLAB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.9847 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ZLAB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ RRGB LSTM: Epoch 40/50 (80%)
       ‚úÖ ZLAB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=41.4999 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ZLAB XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.827173
         RMSE: 0.909491
         R¬≤ Score: -1.2142 (Poor - 121.4% variance explained)
      üîπ RRGB: Training TCN (50 epochs)...
      ‚è≥ RRGB TCN: Epoch 10/50 (20%)
      ‚è≥ RRGB TCN: Epoch 20/50 (40%)
      ‚è≥ RRGB TCN: Epoch 30/50 (60%)
      ‚è≥ RRGB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.836380
         RMSE: 0.914538
         R¬≤ Score: -1.2389
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RRGB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RRGB Random Forest: Starting GridSearchCV fit...
       ‚úÖ RRGB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=129.2734 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RRGB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RRGB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=198.1316 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RRGB XGBoost: Starting GridSearchCV fit...
       ‚úÖ C XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.2070 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.2s
    - LSTM: MSE=0.6503
    - TCN: MSE=0.6393
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6393
        ‚Ä¢ LSTM: MSE=0.6503
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.1576
        ‚Ä¢ Random Forest: MSE=21.1608
        ‚Ä¢ XGBoost: MSE=23.2070
   ‚úÖ C: Phase 3/3 - Model selection complete!
  üèÜ WINNER for C (TargetReturn): TCN with MSE=0.6393
üêõ DEBUG: C - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for C.
üêõ DEBUG: C - Moving model to CPU before return...
üêõ DEBUG [23:06:43.075]: C - Returning result metadata...
üêõ DEBUG [23:06:43.076]: Main received result for C
üêõ DEBUG: train_worker started for GRND
  ‚öôÔ∏è Training models for GRND (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - GRND: Initiating feature extraction for training.
  [DIAGNOSTIC] GRND: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRND: rows after features available: 126
üéØ GRND: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRND: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRND: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRND: Training LSTM (50 epochs)...
      ‚è≥ GRND LSTM: Epoch 10/50 (20%)
      ‚è≥ GRND LSTM: Epoch 20/50 (40%)
      ‚è≥ GRND LSTM: Epoch 30/50 (60%)
      ‚è≥ GRND LSTM: Epoch 40/50 (80%)
       ‚úÖ FNGO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=38.7001 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.5451
    - TCN: MSE=0.4593
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4593
        ‚Ä¢ LSTM: MSE=0.5451
        ‚Ä¢ XGBoost: MSE=38.7001
        ‚Ä¢ Random Forest: MSE=55.6482
        ‚Ä¢ LightGBM Regressor (CPU): MSE=80.4721
   ‚úÖ FNGO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FNGO (TargetReturn): TCN with MSE=0.4593
üêõ DEBUG: FNGO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FNGO.
üêõ DEBUG: FNGO - Moving model to CPU before return...
üêõ DEBUG [23:06:45.169]: FNGO - Returning result metadata...
üêõ DEBUG [23:06:45.169]: Main received result for FNGO
üêõ DEBUG: train_worker started for BA
  ‚öôÔ∏è Training models for BA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BA: Initiating feature extraction for training.
  [DIAGNOSTIC] BA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BA: rows after features available: 126
üéØ BA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BA: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.337526
         RMSE: 0.580970
         R¬≤ Score: -1.2290 (Poor - 122.9% variance explained)
      üîπ GRND: Training TCN (50 epochs)...
      ‚è≥ GRND TCN: Epoch 10/50 (20%)
      ‚è≥ GRND TCN: Epoch 20/50 (40%)
      ‚è≥ BA LSTM: Epoch 10/50 (20%)
      ‚è≥ GRND TCN: Epoch 30/50 (60%)
      ‚è≥ GRND TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.149764
         RMSE: 0.386994
         R¬≤ Score: 0.0110
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRND: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRND Random Forest: Starting GridSearchCV fit...
      ‚è≥ BA LSTM: Epoch 20/50 (40%)
      ‚è≥ BA LSTM: Epoch 30/50 (60%)
       ‚úÖ FIVE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=62.3831 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.5822
    - TCN: MSE=0.3812
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3812
        ‚Ä¢ LSTM: MSE=0.5822
        ‚Ä¢ Random Forest: MSE=43.7620
        ‚Ä¢ XGBoost: MSE=62.3831
        ‚Ä¢ LightGBM Regressor (CPU): MSE=68.1772
   ‚úÖ FIVE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FIVE (TargetReturn): TCN with MSE=0.3812
üêõ DEBUG: FIVE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FIVE.
üêõ DEBUG: FIVE - Moving model to CPU before return...
üêõ DEBUG [23:06:47.081]: FIVE - Returning result metadata...
üêõ DEBUG [23:06:47.082]: Main received result for FIVE
üêõ DEBUG: Training progress: 332/959 done
üêõ DEBUG: train_worker started for BBAR
  ‚öôÔ∏è Training models for BBAR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - BBAR: Initiating feature extraction for training.
  [DIAGNOSTIC] BBAR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBAR: rows after features available: 126
üéØ BBAR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ BA LSTM: Epoch 40/50 (80%)
  [DIAGNOSTIC] BBAR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBAR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBAR: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.318683
         RMSE: 0.564520
         R¬≤ Score: -0.7969 (Poor - 79.7% variance explained)
      üîπ BA: Training TCN (50 epochs)...
      ‚è≥ BA TCN: Epoch 10/50 (20%)
      ‚è≥ BBAR LSTM: Epoch 10/50 (20%)
      ‚è≥ BA TCN: Epoch 20/50 (40%)
      ‚è≥ BA TCN: Epoch 30/50 (60%)
      ‚è≥ BA TCN: Epoch 40/50 (80%)
       ‚úÖ YPF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.5196 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.0770
    - TCN: MSE=0.0550
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0550
        ‚Ä¢ LSTM: MSE=0.0770
        ‚Ä¢ Random Forest: MSE=13.7676
        ‚Ä¢ XGBoost: MSE=15.5196
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.7164
   ‚úÖ YPF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YPF (TargetReturn): TCN with MSE=0.0550
üêõ DEBUG: YPF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YPF.
üêõ DEBUG: YPF - Moving model to CPU before return...
üêõ DEBUG [23:06:48.063]: YPF - Returning result metadata...
üêõ DEBUG [23:06:48.064]: Main received result for YPF
üêõ DEBUG: train_worker started for BELFB
      üìä TCN Regression Metrics:
         MSE: 0.280532
         RMSE: 0.529653
         R¬≤ Score: -0.5818
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BA Random Forest: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for BELFB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BELFB: Initiating feature extraction for training.
  [DIAGNOSTIC] BELFB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BELFB: rows after features available: 126
üéØ BELFB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BELFB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BELFB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BELFB: Training LSTM (50 epochs)...
      ‚è≥ BBAR LSTM: Epoch 20/50 (40%)
      ‚è≥ BELFB LSTM: Epoch 10/50 (20%)
      ‚è≥ BBAR LSTM: Epoch 30/50 (60%)
       ‚úÖ GRND Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.4715 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRND LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BELFB LSTM: Epoch 20/50 (40%)
      ‚è≥ BBAR LSTM: Epoch 40/50 (80%)
       ‚úÖ GRND LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=29.3743 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRND XGBoost: Starting GridSearchCV fit...
      ‚è≥ BELFB LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.115349
         RMSE: 0.339630
         R¬≤ Score: -0.9668 (Poor - 96.7% variance explained)
      üîπ BBAR: Training TCN (50 epochs)...
      ‚è≥ BBAR TCN: Epoch 10/50 (20%)
      ‚è≥ BBAR TCN: Epoch 20/50 (40%)
      ‚è≥ BBAR TCN: Epoch 30/50 (60%)
      ‚è≥ BBAR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.062167
         RMSE: 0.249332
         R¬≤ Score: -0.0600
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBAR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBAR Random Forest: Starting GridSearchCV fit...
      ‚è≥ BELFB LSTM: Epoch 40/50 (80%)
       ‚úÖ NWG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.3206 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.4142
    - TCN: MSE=0.3049
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3049
        ‚Ä¢ LSTM: MSE=0.4142
        ‚Ä¢ Random Forest: MSE=7.9763
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.3073
        ‚Ä¢ XGBoost: MSE=12.3206
   ‚úÖ NWG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NWG (TargetReturn): TCN with MSE=0.3049
üêõ DEBUG: NWG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NWG.
üêõ DEBUG: NWG - Moving model to CPU before return...
üêõ DEBUG [23:06:50.427]: NWG - Returning result metadata...
üêõ DEBUG: train_worker started for NGD
üêõ DEBUG [23:06:50.428]: Main received result for NWG
  ‚öôÔ∏è Training models for NGD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - NGD: Initiating feature extraction for training.
  [DIAGNOSTIC] NGD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NGD: rows after features available: 126
üéØ NGD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NGD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NGD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NGD: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.643053
         RMSE: 0.801906
         R¬≤ Score: -1.0891 (Poor - 108.9% variance explained)
      üîπ BELFB: Training TCN (50 epochs)...
      ‚è≥ BELFB TCN: Epoch 10/50 (20%)
      ‚è≥ NGD LSTM: Epoch 10/50 (20%)
       ‚úÖ BA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.3599 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BELFB TCN: Epoch 20/50 (40%)
      ‚è≥ BELFB TCN: Epoch 30/50 (60%)
      ‚è≥ BELFB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.336140
         RMSE: 0.579776
         R¬≤ Score: -0.0920
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BELFB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BELFB Random Forest: Starting GridSearchCV fit...
      ‚è≥ NGD LSTM: Epoch 20/50 (40%)
       ‚úÖ BA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.3643 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BA XGBoost: Starting GridSearchCV fit...
      ‚è≥ NGD LSTM: Epoch 30/50 (60%)
      ‚è≥ NGD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.260809
         RMSE: 0.510695
         R¬≤ Score: -0.7275 (Poor - 72.8% variance explained)
      üîπ NGD: Training TCN (50 epochs)...
      ‚è≥ NGD TCN: Epoch 10/50 (20%)
      ‚è≥ NGD TCN: Epoch 20/50 (40%)
      ‚è≥ NGD TCN: Epoch 30/50 (60%)
      ‚è≥ NGD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.151838
         RMSE: 0.389664
         R¬≤ Score: -0.0057
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NGD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NGD Random Forest: Starting GridSearchCV fit...
       ‚úÖ BBAR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.6135 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBAR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BBAR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.3614 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBAR XGBoost: Starting GridSearchCV fit...
       ‚úÖ BELFB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.0539 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BELFB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BELFB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.6993 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BELFB XGBoost: Starting GridSearchCV fit...
       ‚úÖ NGD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=57.3018 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NGD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NGD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=61.0919 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NGD XGBoost: Starting GridSearchCV fit...
       ‚úÖ UPXI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3977.5077 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 117.0s
    - LSTM: MSE=0.5008
    - TCN: MSE=0.3151
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3151
        ‚Ä¢ LSTM: MSE=0.5008
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2156.1910
        ‚Ä¢ Random Forest: MSE=2461.5798
        ‚Ä¢ XGBoost: MSE=3977.5077
   ‚úÖ UPXI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UPXI (TargetReturn): TCN with MSE=0.3151
üêõ DEBUG: UPXI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UPXI.
üêõ DEBUG: UPXI - Moving model to CPU before return...
üêõ DEBUG [23:07:14.489]: UPXI - Returning result metadata...
üêõ DEBUG: train_worker started for HTT
  ‚öôÔ∏è Training models for HTT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - HTT: Initiating feature extraction for training.
  [DIAGNOSTIC] HTT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HTT: rows after features available: 126
üéØ HTT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HTT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HTT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HTT: Training LSTM (50 epochs)...
      ‚è≥ HTT LSTM: Epoch 10/50 (20%)
       ‚úÖ PW XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.0374 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.7s
    - LSTM: MSE=0.0258
    - TCN: MSE=0.0225
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0225
        ‚Ä¢ LSTM: MSE=0.0258
        ‚Ä¢ XGBoost: MSE=10.0374
        ‚Ä¢ Random Forest: MSE=12.1462
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.9324
   ‚úÖ PW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PW (TargetReturn): TCN with MSE=0.0225
üêõ DEBUG: PW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PW.
üêõ DEBUG: PW - Moving model to CPU before return...
üêõ DEBUG [23:07:15.038]: PW - Returning result metadata...
üêõ DEBUG: train_worker started for CAE
üêõ DEBUG [23:07:15.039]: Main received result for PW
üêõ DEBUG [23:07:15.039]: Main received result for UPXI
üêõ DEBUG: Training progress: 336/959 done
  ‚öôÔ∏è Training models for CAE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CAE: Initiating feature extraction for training.
  [DIAGNOSTIC] CAE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CAE: rows after features available: 126
üéØ CAE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CAE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CAE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CAE: Training LSTM (50 epochs)...
      ‚è≥ HTT LSTM: Epoch 20/50 (40%)
      ‚è≥ CAE LSTM: Epoch 10/50 (20%)
      ‚è≥ HTT LSTM: Epoch 30/50 (60%)
      ‚è≥ CAE LSTM: Epoch 20/50 (40%)
      ‚è≥ HTT LSTM: Epoch 40/50 (80%)
      ‚è≥ CAE LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.292817
         RMSE: 0.541125
         R¬≤ Score: -0.4889 (Poor - 48.9% variance explained)
      üîπ HTT: Training TCN (50 epochs)...
      ‚è≥ HTT TCN: Epoch 10/50 (20%)
      ‚è≥ HTT TCN: Epoch 20/50 (40%)
      ‚è≥ CAE LSTM: Epoch 40/50 (80%)
      ‚è≥ HTT TCN: Epoch 30/50 (60%)
      ‚è≥ HTT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.285584
         RMSE: 0.534401
         R¬≤ Score: -0.4521
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HTT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HTT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.265706
         RMSE: 0.515467
         R¬≤ Score: -0.5288 (Poor - 52.9% variance explained)
      üîπ CAE: Training TCN (50 epochs)...
      ‚è≥ CAE TCN: Epoch 10/50 (20%)
      ‚è≥ CAE TCN: Epoch 20/50 (40%)
      ‚è≥ CAE TCN: Epoch 30/50 (60%)
      ‚è≥ CAE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.217998
         RMSE: 0.466903
         R¬≤ Score: -0.2543
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CAE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CAE Random Forest: Starting GridSearchCV fit...
       ‚úÖ HTT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.8003 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HTT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HTT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.4303 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HTT XGBoost: Starting GridSearchCV fit...
       ‚úÖ CAE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.8594 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CAE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CAE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.5589 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CAE XGBoost: Starting GridSearchCV fit...
       ‚úÖ CD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1437.2917 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.0093
    - TCN: MSE=0.0051
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0051
        ‚Ä¢ LSTM: MSE=0.0093
        ‚Ä¢ Random Forest: MSE=606.6510
        ‚Ä¢ LightGBM Regressor (CPU): MSE=1377.8475
        ‚Ä¢ XGBoost: MSE=1437.2917
   ‚úÖ CD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CD (TargetReturn): TCN with MSE=0.0051
üêõ DEBUG: CD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CD.
üêõ DEBUG: CD - Moving model to CPU before return...
üêõ DEBUG [23:07:37.921]: CD - Returning result metadata...
üêõ DEBUG: train_worker started for BTF
üêõ DEBUG [23:07:37.924]: Main received result for CD
  ‚öôÔ∏è Training models for BTF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - BTF: Initiating feature extraction for training.
  [DIAGNOSTIC] BTF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BTF: rows after features available: 126
üéØ BTF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BTF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BTF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BTF: Training LSTM (50 epochs)...
      ‚è≥ BTF LSTM: Epoch 10/50 (20%)
      ‚è≥ BTF LSTM: Epoch 20/50 (40%)
       ‚úÖ ADTN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.4552 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.9s
    - LSTM: MSE=0.3070
    - TCN: MSE=0.1635
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1635
        ‚Ä¢ LSTM: MSE=0.3070
        ‚Ä¢ XGBoost: MSE=21.4552
        ‚Ä¢ Random Forest: MSE=25.9719
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.1210
   ‚úÖ ADTN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ADTN (TargetReturn): TCN with MSE=0.1635
üêõ DEBUG: ADTN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ADTN.
üêõ DEBUG: ADTN - Moving model to CPU before return...
üêõ DEBUG [23:07:39.459]: ADTN - Returning result metadata...
üêõ DEBUG: train_worker started for ATGE
üêõ DEBUG [23:07:39.459]: Main received result for ADTN
  ‚öôÔ∏è Training models for ATGE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ATGE: Initiating feature extraction for training.
  [DIAGNOSTIC] ATGE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATGE: rows after features available: 126
üéØ ATGE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATGE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATGE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATGE: Training LSTM (50 epochs)...
      ‚è≥ BTF LSTM: Epoch 30/50 (60%)
      ‚è≥ ATGE LSTM: Epoch 10/50 (20%)
      ‚è≥ BTF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.474170
         RMSE: 0.688600
         R¬≤ Score: -0.7990 (Poor - 79.9% variance explained)
      üîπ BTF: Training TCN (50 epochs)...
      ‚è≥ ATGE LSTM: Epoch 20/50 (40%)
      ‚è≥ BTF TCN: Epoch 10/50 (20%)
      ‚è≥ BTF TCN: Epoch 20/50 (40%)
      ‚è≥ BTF TCN: Epoch 30/50 (60%)
      ‚è≥ BTF TCN: Epoch 40/50 (80%)
      ‚è≥ ATGE LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.517752
         RMSE: 0.719550
         R¬≤ Score: -0.9643
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BTF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BTF Random Forest: Starting GridSearchCV fit...
      ‚è≥ ATGE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.463878
         RMSE: 0.681086
         R¬≤ Score: -1.0056 (Poor - 100.6% variance explained)
      üîπ ATGE: Training TCN (50 epochs)...
      ‚è≥ ATGE TCN: Epoch 10/50 (20%)
      ‚è≥ ATGE TCN: Epoch 20/50 (40%)
      ‚è≥ ATGE TCN: Epoch 30/50 (60%)
      ‚è≥ ATGE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.234138
         RMSE: 0.483878
         R¬≤ Score: -0.0123
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATGE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATGE Random Forest: Starting GridSearchCV fit...
       ‚úÖ BTF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.2551 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BTF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BTF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=33.0758 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BTF XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATGE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.6306 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATGE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ATGE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.8014 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATGE XGBoost: Starting GridSearchCV fit...
       ‚úÖ RL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.6146 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.5001
    - TCN: MSE=0.4129
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4129
        ‚Ä¢ LSTM: MSE=0.5001
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.0006
        ‚Ä¢ XGBoost: MSE=23.6146
        ‚Ä¢ Random Forest: MSE=25.3797
   ‚úÖ RL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RL (TargetReturn): TCN with MSE=0.4129
üêõ DEBUG: RL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RL.
üêõ DEBUG: RL - Moving model to CPU before return...
üêõ DEBUG [23:08:09.703]: RL - Returning result metadata...
üêõ DEBUG: train_worker started for GREK
üêõ DEBUG [23:08:09.704]: Main received result for RL
  ‚öôÔ∏è Training models for GREK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - GREK: Initiating feature extraction for training.
  [DIAGNOSTIC] GREK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GREK: rows after features available: 126
üéØ GREK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GREK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GREK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GREK: Training LSTM (50 epochs)...
      ‚è≥ GREK LSTM: Epoch 10/50 (20%)
      ‚è≥ GREK LSTM: Epoch 20/50 (40%)
      ‚è≥ GREK LSTM: Epoch 30/50 (60%)
      ‚è≥ GREK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.110038
         RMSE: 0.331720
         R¬≤ Score: -0.3447 (Poor - 34.5% variance explained)
      üîπ GREK: Training TCN (50 epochs)...
      ‚è≥ GREK TCN: Epoch 10/50 (20%)
      ‚è≥ GREK TCN: Epoch 20/50 (40%)
      ‚è≥ GREK TCN: Epoch 30/50 (60%)
       ‚úÖ SHOP XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=34.4924 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.4742
    - TCN: MSE=0.4668
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4668
        ‚Ä¢ LSTM: MSE=0.4742
        ‚Ä¢ XGBoost: MSE=34.4924
        ‚Ä¢ Random Forest: MSE=38.5328
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.1435
   ‚úÖ SHOP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SHOP (TargetReturn): TCN with MSE=0.4668
üêõ DEBUG: SHOP - train_and_evaluate_models completed
      ‚è≥ GREK TCN: Epoch 40/50 (80%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SHOP.
üêõ DEBUG: SHOP - Moving model to CPU before return...
üêõ DEBUG [23:08:12.403]: SHOP - Returning result metadata...
üêõ DEBUG [23:08:12.403]: Main received result for SHOP
üêõ DEBUG: Training progress: 340/959 done
üêõ DEBUG: train_worker started for LRN
  ‚öôÔ∏è Training models for LRN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - LRN: Initiating feature extraction for training.
  [DIAGNOSTIC] LRN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LRN: rows after features available: 126
üéØ LRN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LRN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LRN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LRN: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.112502
         RMSE: 0.335413
         R¬≤ Score: -0.3748
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GREK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GREK Random Forest: Starting GridSearchCV fit...
      ‚è≥ LRN LSTM: Epoch 10/50 (20%)
      ‚è≥ LRN LSTM: Epoch 20/50 (40%)
      ‚è≥ LRN LSTM: Epoch 30/50 (60%)
      ‚è≥ LRN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.161486
         RMSE: 0.401853
         R¬≤ Score: -1.1365 (Poor - 113.7% variance explained)
      üîπ LRN: Training TCN (50 epochs)...
      ‚è≥ LRN TCN: Epoch 10/50 (20%)
      ‚è≥ LRN TCN: Epoch 20/50 (40%)
       ‚úÖ GREK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.4235 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GREK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LRN TCN: Epoch 30/50 (60%)
      ‚è≥ LRN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.087638
         RMSE: 0.296038
         R¬≤ Score: -0.1595
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LRN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LRN Random Forest: Starting GridSearchCV fit...
       ‚úÖ GREK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.4048 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GREK XGBoost: Starting GridSearchCV fit...
       ‚úÖ LRN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6521 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LRN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WPM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.5272 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.1375
    - TCN: MSE=0.0888
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0888
        ‚Ä¢ LSTM: MSE=0.1375
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.6286
        ‚Ä¢ XGBoost: MSE=37.5272
        ‚Ä¢ Random Forest: MSE=43.7039
   ‚úÖ WPM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WPM (TargetReturn): TCN with MSE=0.0888
üêõ DEBUG: WPM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WPM.
üêõ DEBUG: WPM - Moving model to CPU before return...
üêõ DEBUG [23:08:19.954]: WPM - Returning result metadata...
üêõ DEBUG: train_worker started for AVAL
üêõ DEBUG [23:08:19.956]: Main received result for WPM
       ‚úÖ LRN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.8188 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LRN XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for AVAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - AVAL: Initiating feature extraction for training.
  [DIAGNOSTIC] AVAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AVAL: rows after features available: 126
üéØ AVAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AVAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AVAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AVAL: Training LSTM (50 epochs)...
      ‚è≥ AVAL LSTM: Epoch 10/50 (20%)
      ‚è≥ AVAL LSTM: Epoch 20/50 (40%)
      ‚è≥ AVAL LSTM: Epoch 30/50 (60%)
      ‚è≥ AVAL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.111968
         RMSE: 0.334616
         R¬≤ Score: -0.9350 (Poor - 93.5% variance explained)
      üîπ AVAL: Training TCN (50 epochs)...
      ‚è≥ AVAL TCN: Epoch 10/50 (20%)
      ‚è≥ AVAL TCN: Epoch 20/50 (40%)
      ‚è≥ AVAL TCN: Epoch 30/50 (60%)
      ‚è≥ AVAL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.070653
         RMSE: 0.265807
         R¬≤ Score: -0.2210
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AVAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AVAL Random Forest: Starting GridSearchCV fit...
       ‚úÖ AVAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.2537 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AVAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AVAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.3820 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AVAL XGBoost: Starting GridSearchCV fit...
       ‚úÖ AAP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=83.7818 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.7042
    - TCN: MSE=0.4750
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4750
        ‚Ä¢ LSTM: MSE=0.7042
        ‚Ä¢ Random Forest: MSE=38.9718
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.8924
        ‚Ä¢ XGBoost: MSE=83.7818
   ‚úÖ AAP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AAP (TargetReturn): TCN with MSE=0.4750
üêõ DEBUG: AAP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AAP.
üêõ DEBUG: AAP - Moving model to CPU before return...
üêõ DEBUG [23:08:33.099]: AAP - Returning result metadata...
üêõ DEBUG: train_worker started for ESQ
üêõ DEBUG [23:08:33.100]: Main received result for AAP
  ‚öôÔ∏è Training models for ESQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ESQ: Initiating feature extraction for training.
  [DIAGNOSTIC] ESQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ESQ: rows after features available: 126
üéØ ESQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ESQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ESQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ESQ: Training LSTM (50 epochs)...
      ‚è≥ ESQ LSTM: Epoch 10/50 (20%)
      ‚è≥ ESQ LSTM: Epoch 20/50 (40%)
      ‚è≥ ESQ LSTM: Epoch 30/50 (60%)
       ‚úÖ ZLAB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=51.4843 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.1941
    - TCN: MSE=0.1340
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1340
        ‚Ä¢ LSTM: MSE=0.1941
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.4999
        ‚Ä¢ Random Forest: MSE=44.9847
        ‚Ä¢ XGBoost: MSE=51.4843
   ‚úÖ ZLAB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ZLAB (TargetReturn): TCN with MSE=0.1340
üêõ DEBUG: ZLAB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ZLAB.
üêõ DEBUG: ZLAB - Moving model to CPU before return...
üêõ DEBUG [23:08:34.622]: ZLAB - Returning result metadata...
üêõ DEBUG: train_worker started for OR
üêõ DEBUG [23:08:34.626]: Main received result for ZLAB
  ‚öôÔ∏è Training models for OR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - OR: Initiating feature extraction for training.
  [DIAGNOSTIC] OR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OR: rows after features available: 126
üéØ OR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OR: Training LSTM (50 epochs)...
      ‚è≥ ESQ LSTM: Epoch 40/50 (80%)
      ‚è≥ OR LSTM: Epoch 10/50 (20%)
       ‚úÖ RRGB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=205.4088 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.9s
    - LSTM: MSE=0.8272
    - TCN: MSE=0.8364
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.8272
        ‚Ä¢ TCN: MSE=0.8364
        ‚Ä¢ Random Forest: MSE=129.2734
        ‚Ä¢ LightGBM Regressor (CPU): MSE=198.1316
        ‚Ä¢ XGBoost: MSE=205.4088
   ‚úÖ RRGB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RRGB (TargetReturn): LSTM with MSE=0.8272
üêõ DEBUG: RRGB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RRGB.
üêõ DEBUG: RRGB - Moving model to CPU before return...
üêõ DEBUG [23:08:35.348]: RRGB - Returning result metadata...
üêõ DEBUG: train_worker started for WFC
üêõ DEBUG [23:08:35.349]: Main received result for RRGB
üêõ DEBUG: Training progress: 344/959 done
  ‚öôÔ∏è Training models for WFC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - WFC: Initiating feature extraction for training.
  [DIAGNOSTIC] WFC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WFC: rows after features available: 126
üéØ WFC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WFC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WFC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WFC: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.553497
         RMSE: 0.743974
         R¬≤ Score: -1.4648 (Poor - 146.5% variance explained)
      üîπ ESQ: Training TCN (50 epochs)...
      ‚è≥ OR LSTM: Epoch 20/50 (40%)
      ‚è≥ ESQ TCN: Epoch 10/50 (20%)
      ‚è≥ ESQ TCN: Epoch 20/50 (40%)
      ‚è≥ ESQ TCN: Epoch 30/50 (60%)
      ‚è≥ WFC LSTM: Epoch 10/50 (20%)
      ‚è≥ ESQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.479629
         RMSE: 0.692553
         R¬≤ Score: -1.1359
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ESQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ESQ Random Forest: Starting GridSearchCV fit...
      ‚è≥ OR LSTM: Epoch 30/50 (60%)
      ‚è≥ WFC LSTM: Epoch 20/50 (40%)
      ‚è≥ OR LSTM: Epoch 40/50 (80%)
      ‚è≥ WFC LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.119615
         RMSE: 0.345855
         R¬≤ Score: -0.6254 (Poor - 62.5% variance explained)
      üîπ OR: Training TCN (50 epochs)...
      ‚è≥ WFC LSTM: Epoch 40/50 (80%)
      ‚è≥ OR TCN: Epoch 10/50 (20%)
      ‚è≥ OR TCN: Epoch 20/50 (40%)
      ‚è≥ OR TCN: Epoch 30/50 (60%)
      ‚è≥ OR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.073947
         RMSE: 0.271932
         R¬≤ Score: -0.0049
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OR Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.693003
         RMSE: 0.832468
         R¬≤ Score: -1.4911 (Poor - 149.1% variance explained)
      üîπ WFC: Training TCN (50 epochs)...
      ‚è≥ WFC TCN: Epoch 10/50 (20%)
      ‚è≥ WFC TCN: Epoch 20/50 (40%)
      ‚è≥ WFC TCN: Epoch 30/50 (60%)
      ‚è≥ WFC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.606158
         RMSE: 0.778561
         R¬≤ Score: -1.1789
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WFC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WFC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ESQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.8704 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ESQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ESQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.8985 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ESQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ OR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.9120 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WFC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.3633 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WFC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.3946 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OR XGBoost: Starting GridSearchCV fit...
       ‚úÖ WFC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.0486 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WFC XGBoost: Starting GridSearchCV fit...
       ‚úÖ BA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.0962 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.8s
    - LSTM: MSE=0.3187
    - TCN: MSE=0.2805
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2805
        ‚Ä¢ LSTM: MSE=0.3187
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.3643
        ‚Ä¢ Random Forest: MSE=23.3599
        ‚Ä¢ XGBoost: MSE=32.0962
   ‚úÖ BA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BA (TargetReturn): TCN with MSE=0.2805
üêõ DEBUG: BA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BA.
üêõ DEBUG: BA - Moving model to CPU before return...
üêõ DEBUG [23:08:50.616]: BA - Returning result metadata...
üêõ DEBUG: train_worker started for BBW
  ‚öôÔ∏è Training models for BBW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BBW: Initiating feature extraction for training.
  [DIAGNOSTIC] BBW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBW: rows after features available: 126
üéØ BBW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BBW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBW: Training LSTM (50 epochs)...
      ‚è≥ BBW LSTM: Epoch 10/50 (20%)
       ‚úÖ BELFB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.4773 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.6s
    - LSTM: MSE=0.6431
    - TCN: MSE=0.3361
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3361
        ‚Ä¢ LSTM: MSE=0.6431
        ‚Ä¢ Random Forest: MSE=27.0539
        ‚Ä¢ XGBoost: MSE=30.4773
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.6993
   ‚úÖ BELFB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BELFB (TargetReturn): TCN with MSE=0.3361
üêõ DEBUG: BELFB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BELFB.
üêõ DEBUG: BELFB - Moving model to CPU before return...
üêõ DEBUG [23:08:51.489]: BELFB - Returning result metadata...
üêõ DEBUG: train_worker started for SCHW
  ‚öôÔ∏è Training models for SCHW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SCHW: Initiating feature extraction for training.
  [DIAGNOSTIC] SCHW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SCHW: rows after features available: 126
üéØ SCHW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SCHW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SCHW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SCHW: Training LSTM (50 epochs)...
       ‚úÖ GRND XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.2050 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.3375
    - TCN: MSE=0.1498
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1498
        ‚Ä¢ LSTM: MSE=0.3375
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.3743
        ‚Ä¢ Random Forest: MSE=31.4715
        ‚Ä¢ XGBoost: MSE=40.2050
   ‚úÖ GRND: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRND (TargetReturn): TCN with MSE=0.1498
üêõ DEBUG: GRND - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRND.
üêõ DEBUG: GRND - Moving model to CPU before return...
üêõ DEBUG [23:08:51.538]: GRND - Returning result metadata...
üêõ DEBUG [23:08:51.539]: Main received result for GRND
üêõ DEBUG [23:08:51.539]: Main received result for BA
üêõ DEBUG: train_worker started for MTZ
      ‚è≥ BBW LSTM: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for MTZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - MTZ: Initiating feature extraction for training.
  [DIAGNOSTIC] MTZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MTZ: rows after features available: 126
üéØ MTZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MTZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MTZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MTZ: Training LSTM (50 epochs)...
      ‚è≥ BBW LSTM: Epoch 30/50 (60%)
      ‚è≥ SCHW LSTM: Epoch 10/50 (20%)
      ‚è≥ MTZ LSTM: Epoch 10/50 (20%)
      ‚è≥ BBW LSTM: Epoch 40/50 (80%)
      ‚è≥ SCHW LSTM: Epoch 20/50 (40%)
      ‚è≥ MTZ LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.585671
         RMSE: 0.765292
         R¬≤ Score: -0.8270 (Poor - 82.7% variance explained)
      üîπ BBW: Training TCN (50 epochs)...
      ‚è≥ BBW TCN: Epoch 10/50 (20%)
      ‚è≥ SCHW LSTM: Epoch 30/50 (60%)
      ‚è≥ MTZ LSTM: Epoch 30/50 (60%)
      ‚è≥ BBW TCN: Epoch 20/50 (40%)
      ‚è≥ BBW TCN: Epoch 30/50 (60%)
      ‚è≥ BBW TCN: Epoch 40/50 (80%)
       ‚úÖ BBAR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=39.6164 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.8s
    - LSTM: MSE=0.1153
    - TCN: MSE=0.0622
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0622
        ‚Ä¢ LSTM: MSE=0.1153
        ‚Ä¢ Random Forest: MSE=34.6135
        ‚Ä¢ XGBoost: MSE=39.6164
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.3614
   ‚úÖ BBAR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBAR (TargetReturn): TCN with MSE=0.0622
üêõ DEBUG: BBAR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBAR.
üêõ DEBUG: BBAR - Moving model to CPU before return...
üêõ DEBUG [23:08:53.402]: BBAR - Returning result metadata...
üêõ DEBUG: train_worker started for QTUM
üêõ DEBUG [23:08:53.403]: Main received result for BBAR
üêõ DEBUG [23:08:53.403]: Main received result for BELFB
üêõ DEBUG: Training progress: 348/959 done
  ‚öôÔ∏è Training models for QTUM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - QTUM: Initiating feature extraction for training.
  [DIAGNOSTIC] QTUM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QTUM: rows after features available: 126
üéØ QTUM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QTUM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QTUM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QTUM: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.468365
         RMSE: 0.684372
         R¬≤ Score: -0.4611
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBW Random Forest: Starting GridSearchCV fit...
      ‚è≥ SCHW LSTM: Epoch 40/50 (80%)
      ‚è≥ MTZ LSTM: Epoch 40/50 (80%)
      ‚è≥ QTUM LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.425436
         RMSE: 0.652254
         R¬≤ Score: -1.5059 (Poor - 150.6% variance explained)
      üîπ SCHW: Training TCN (50 epochs)...
      ‚è≥ SCHW TCN: Epoch 10/50 (20%)
      ‚è≥ SCHW TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.388219
         RMSE: 0.623072
         R¬≤ Score: -0.8106 (Poor - 81.1% variance explained)
      üîπ MTZ: Training TCN (50 epochs)...
      ‚è≥ QTUM LSTM: Epoch 20/50 (40%)
      ‚è≥ SCHW TCN: Epoch 30/50 (60%)
      ‚è≥ MTZ TCN: Epoch 10/50 (20%)
      ‚è≥ MTZ TCN: Epoch 20/50 (40%)
      ‚è≥ MTZ TCN: Epoch 30/50 (60%)
      ‚è≥ SCHW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.381571
         RMSE: 0.617714
         R¬≤ Score: -1.2476
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SCHW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SCHW Random Forest: Starting GridSearchCV fit...
      ‚è≥ MTZ TCN: Epoch 40/50 (80%)
      ‚è≥ QTUM LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.431080
         RMSE: 0.656567
         R¬≤ Score: -1.0105
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MTZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MTZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ NGD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=52.4033 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.2608
    - TCN: MSE=0.1518
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1518
        ‚Ä¢ LSTM: MSE=0.2608
        ‚Ä¢ XGBoost: MSE=52.4033
        ‚Ä¢ Random Forest: MSE=57.3018
        ‚Ä¢ LightGBM Regressor (CPU): MSE=61.0919
   ‚úÖ NGD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NGD (TargetReturn): TCN with MSE=0.1518
üêõ DEBUG: NGD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NGD.
üêõ DEBUG: NGD - Moving model to CPU before return...
üêõ DEBUG [23:08:55.157]: NGD - Returning result metadata...
üêõ DEBUG [23:08:55.157]: Main received result for NGD
üêõ DEBUG: train_worker started for ACAD
  ‚öôÔ∏è Training models for ACAD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - ACAD: Initiating feature extraction for training.
  [DIAGNOSTIC] ACAD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ACAD: rows after features available: 126
üéØ ACAD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ACAD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ACAD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ACAD: Training LSTM (50 epochs)...
      ‚è≥ QTUM LSTM: Epoch 40/50 (80%)
      ‚è≥ ACAD LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.379390
         RMSE: 0.615946
         R¬≤ Score: -0.7057 (Poor - 70.6% variance explained)
      üîπ QTUM: Training TCN (50 epochs)...
      ‚è≥ QTUM TCN: Epoch 10/50 (20%)
      ‚è≥ QTUM TCN: Epoch 20/50 (40%)
      ‚è≥ ACAD LSTM: Epoch 20/50 (40%)
      ‚è≥ QTUM TCN: Epoch 30/50 (60%)
      ‚è≥ QTUM TCN: Epoch 40/50 (80%)
       ‚úÖ BBW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.2449 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.478698
         RMSE: 0.691880
         R¬≤ Score: -1.1522
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QTUM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QTUM Random Forest: Starting GridSearchCV fit...
      ‚è≥ ACAD LSTM: Epoch 30/50 (60%)
       ‚úÖ BBW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.3920 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBW XGBoost: Starting GridSearchCV fit...
      ‚è≥ ACAD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.901164
         RMSE: 0.949297
         R¬≤ Score: -1.3812 (Poor - 138.1% variance explained)
      üîπ ACAD: Training TCN (50 epochs)...
      ‚è≥ ACAD TCN: Epoch 10/50 (20%)
       ‚úÖ SCHW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.4003 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SCHW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ACAD TCN: Epoch 20/50 (40%)
      ‚è≥ ACAD TCN: Epoch 30/50 (60%)
      ‚è≥ ACAD TCN: Epoch 40/50 (80%)
       ‚úÖ MTZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=33.9153 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MTZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.857953
         RMSE: 0.926258
         R¬≤ Score: -1.2671
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ACAD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ACAD Random Forest: Starting GridSearchCV fit...
       ‚úÖ SCHW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.7181 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SCHW XGBoost: Starting GridSearchCV fit...
       ‚úÖ MTZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=31.3461 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MTZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ QTUM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.1528 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QTUM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QTUM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.0674 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QTUM XGBoost: Starting GridSearchCV fit...
       ‚úÖ ACAD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=41.9540 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ACAD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ACAD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=36.2034 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ACAD XGBoost: Starting GridSearchCV fit...
       ‚úÖ HTT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.7326 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 116.1s
    - LSTM: MSE=0.2928
    - TCN: MSE=0.2856
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2856
        ‚Ä¢ LSTM: MSE=0.2928
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.4303
        ‚Ä¢ Random Forest: MSE=26.8003
        ‚Ä¢ XGBoost: MSE=33.7326
   ‚úÖ HTT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HTT (TargetReturn): TCN with MSE=0.2856
üêõ DEBUG: HTT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HTT.
üêõ DEBUG: HTT - Moving model to CPU before return...
üêõ DEBUG [23:09:16.875]: HTT - Returning result metadata...
üêõ DEBUG [23:09:16.875]: Main received result for HTT
üêõ DEBUG: train_worker started for ROAD
  ‚öôÔ∏è Training models for ROAD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - ROAD: Initiating feature extraction for training.
  [DIAGNOSTIC] ROAD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ROAD: rows after features available: 126
üéØ ROAD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ROAD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ROAD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ROAD: Training LSTM (50 epochs)...
      ‚è≥ ROAD LSTM: Epoch 10/50 (20%)
      ‚è≥ ROAD LSTM: Epoch 20/50 (40%)
      ‚è≥ ROAD LSTM: Epoch 30/50 (60%)
      ‚è≥ ROAD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.303427
         RMSE: 0.550842
         R¬≤ Score: -1.0411 (Poor - 104.1% variance explained)
      üîπ ROAD: Training TCN (50 epochs)...
      ‚è≥ ROAD TCN: Epoch 10/50 (20%)
      ‚è≥ ROAD TCN: Epoch 20/50 (40%)
      ‚è≥ ROAD TCN: Epoch 30/50 (60%)
      ‚è≥ ROAD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.201302
         RMSE: 0.448667
         R¬≤ Score: -0.3541
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ROAD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ROAD Random Forest: Starting GridSearchCV fit...
       ‚úÖ CAE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.5917 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.2657
    - TCN: MSE=0.2180
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2180
        ‚Ä¢ LSTM: MSE=0.2657
        ‚Ä¢ XGBoost: MSE=15.5917
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.5589
        ‚Ä¢ Random Forest: MSE=17.8594
   ‚úÖ CAE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CAE (TargetReturn): TCN with MSE=0.2180
üêõ DEBUG: CAE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CAE.
üêõ DEBUG: CAE - Moving model to CPU before return...
üêõ DEBUG [23:09:19.885]: CAE - Returning result metadata...
üêõ DEBUG: train_worker started for HSBC
üêõ DEBUG [23:09:19.885]: Main received result for CAE
  ‚öôÔ∏è Training models for HSBC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - HSBC: Initiating feature extraction for training.
  [DIAGNOSTIC] HSBC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HSBC: rows after features available: 126
üéØ HSBC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HSBC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HSBC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HSBC: Training LSTM (50 epochs)...
      ‚è≥ HSBC LSTM: Epoch 10/50 (20%)
      ‚è≥ HSBC LSTM: Epoch 20/50 (40%)
      ‚è≥ HSBC LSTM: Epoch 30/50 (60%)
      ‚è≥ HSBC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.377561
         RMSE: 0.614460
         R¬≤ Score: -0.9704 (Poor - 97.0% variance explained)
      üîπ HSBC: Training TCN (50 epochs)...
      ‚è≥ HSBC TCN: Epoch 10/50 (20%)
      ‚è≥ HSBC TCN: Epoch 20/50 (40%)
       ‚úÖ ROAD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.7569 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ROAD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ HSBC TCN: Epoch 30/50 (60%)
      ‚è≥ HSBC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.245494
         RMSE: 0.495474
         R¬≤ Score: -0.2812
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HSBC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HSBC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ROAD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=27.8831 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ROAD XGBoost: Starting GridSearchCV fit...
       ‚úÖ HSBC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.5621 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HSBC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HSBC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.6018 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HSBC XGBoost: Starting GridSearchCV fit...
       ‚úÖ BTF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.0830 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.4742
    - TCN: MSE=0.5178
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4742
        ‚Ä¢ TCN: MSE=0.5178
        ‚Ä¢ Random Forest: MSE=21.2551
        ‚Ä¢ XGBoost: MSE=23.0830
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.0758
   ‚úÖ BTF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BTF (TargetReturn): LSTM with MSE=0.4742
üêõ DEBUG: BTF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BTF.
üêõ DEBUG: BTF - Moving model to CPU before return...
üêõ DEBUG [23:09:42.945]: BTF - Returning result metadata...
üêõ DEBUG: train_worker started for TGS
üêõ DEBUG [23:09:42.947]: Main received result for BTF
üêõ DEBUG: Training progress: 352/959 done
  ‚öôÔ∏è Training models for TGS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TGS: Initiating feature extraction for training.
  [DIAGNOSTIC] TGS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TGS: rows after features available: 126
üéØ TGS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TGS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TGS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TGS: Training LSTM (50 epochs)...
      ‚è≥ TGS LSTM: Epoch 10/50 (20%)
      ‚è≥ TGS LSTM: Epoch 20/50 (40%)
      ‚è≥ TGS LSTM: Epoch 30/50 (60%)
       ‚úÖ ATGE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.8804 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.4639
    - TCN: MSE=0.2341
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2341
        ‚Ä¢ LSTM: MSE=0.4639
        ‚Ä¢ XGBoost: MSE=16.8804
        ‚Ä¢ Random Forest: MSE=19.6306
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.8014
   ‚úÖ ATGE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATGE (TargetReturn): TCN with MSE=0.2341
üêõ DEBUG: ATGE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATGE.
üêõ DEBUG: ATGE - Moving model to CPU before return...
üêõ DEBUG [23:09:44.487]: ATGE - Returning result metadata...
üêõ DEBUG: train_worker started for BTI
üêõ DEBUG [23:09:44.487]: Main received result for ATGE
  ‚öôÔ∏è Training models for BTI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - BTI: Initiating feature extraction for training.
  [DIAGNOSTIC] BTI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BTI: rows after features available: 126
üéØ BTI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BTI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BTI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BTI: Training LSTM (50 epochs)...
      ‚è≥ TGS LSTM: Epoch 40/50 (80%)
      ‚è≥ BTI LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.145102
         RMSE: 0.380922
         R¬≤ Score: -0.6042 (Poor - 60.4% variance explained)
      üîπ TGS: Training TCN (50 epochs)...
      ‚è≥ TGS TCN: Epoch 10/50 (20%)
      ‚è≥ TGS TCN: Epoch 20/50 (40%)
      ‚è≥ TGS TCN: Epoch 30/50 (60%)
      ‚è≥ BTI LSTM: Epoch 20/50 (40%)
      ‚è≥ TGS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.093631
         RMSE: 0.305992
         R¬≤ Score: -0.0352
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TGS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TGS Random Forest: Starting GridSearchCV fit...
      ‚è≥ BTI LSTM: Epoch 30/50 (60%)
      ‚è≥ BTI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.568079
         RMSE: 0.753710
         R¬≤ Score: -1.6104 (Poor - 161.0% variance explained)
      üîπ BTI: Training TCN (50 epochs)...
      ‚è≥ BTI TCN: Epoch 10/50 (20%)
      ‚è≥ BTI TCN: Epoch 20/50 (40%)
      ‚è≥ BTI TCN: Epoch 30/50 (60%)
      ‚è≥ BTI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.393584
         RMSE: 0.627362
         R¬≤ Score: -0.8086
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BTI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BTI Random Forest: Starting GridSearchCV fit...
       ‚úÖ TGS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.4491 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TGS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TGS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.3500 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TGS XGBoost: Starting GridSearchCV fit...
       ‚úÖ BTI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.0554 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BTI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BTI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.5823 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BTI XGBoost: Starting GridSearchCV fit...
       ‚úÖ GREK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.4842 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.1100
    - TCN: MSE=0.1125
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1100
        ‚Ä¢ TCN: MSE=0.1125
        ‚Ä¢ Random Forest: MSE=11.4235
        ‚Ä¢ XGBoost: MSE=11.4842
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.4048
   ‚úÖ GREK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GREK (TargetReturn): LSTM with MSE=0.1100
üêõ DEBUG: GREK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GREK.
üêõ DEBUG: GREK - Moving model to CPU before return...
üêõ DEBUG [23:10:16.308]: GREK - Returning result metadata...
üêõ DEBUG [23:10:16.308]: Main received result for GREK
üêõ DEBUG: train_worker started for MFG
  ‚öôÔ∏è Training models for MFG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - MFG: Initiating feature extraction for training.
  [DIAGNOSTIC] MFG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MFG: rows after features available: 126
üéØ MFG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MFG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MFG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MFG: Training LSTM (50 epochs)...
      ‚è≥ MFG LSTM: Epoch 10/50 (20%)
      ‚è≥ MFG LSTM: Epoch 20/50 (40%)
      ‚è≥ MFG LSTM: Epoch 30/50 (60%)
       ‚úÖ LRN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.6510 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.1615
    - TCN: MSE=0.0876
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0876
        ‚Ä¢ LSTM: MSE=0.1615
        ‚Ä¢ XGBoost: MSE=10.6510
        ‚Ä¢ Random Forest: MSE=11.6521
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.8188
   ‚úÖ LRN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LRN (TargetReturn): TCN with MSE=0.0876
üêõ DEBUG: LRN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LRN.
üêõ DEBUG: LRN - Moving model to CPU before return...
üêõ DEBUG [23:10:18.351]: LRN - Returning result metadata...
üêõ DEBUG [23:10:18.352]: Main received result for LRN
üêõ DEBUG: train_worker started for LFCR
  ‚öôÔ∏è Training models for LFCR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - LFCR: Initiating feature extraction for training.
  [DIAGNOSTIC] LFCR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LFCR: rows after features available: 126
üéØ LFCR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LFCR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LFCR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LFCR: Training LSTM (50 epochs)...
      ‚è≥ MFG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.523745
         RMSE: 0.723702
         R¬≤ Score: -1.1381 (Poor - 113.8% variance explained)
      üîπ MFG: Training TCN (50 epochs)...
      ‚è≥ MFG TCN: Epoch 10/50 (20%)
      ‚è≥ LFCR LSTM: Epoch 10/50 (20%)
      ‚è≥ MFG TCN: Epoch 20/50 (40%)
      ‚è≥ MFG TCN: Epoch 30/50 (60%)
      ‚è≥ MFG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.437380
         RMSE: 0.661347
         R¬≤ Score: -0.7855
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MFG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MFG Random Forest: Starting GridSearchCV fit...
      ‚è≥ LFCR LSTM: Epoch 20/50 (40%)
      ‚è≥ LFCR LSTM: Epoch 30/50 (60%)
      ‚è≥ LFCR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.183660
         RMSE: 0.428556
         R¬≤ Score: -0.9825 (Poor - 98.2% variance explained)
      üîπ LFCR: Training TCN (50 epochs)...
      ‚è≥ LFCR TCN: Epoch 10/50 (20%)
      ‚è≥ LFCR TCN: Epoch 20/50 (40%)
      ‚è≥ LFCR TCN: Epoch 30/50 (60%)
      ‚è≥ LFCR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.104172
         RMSE: 0.322757
         R¬≤ Score: -0.1245
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LFCR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LFCR Random Forest: Starting GridSearchCV fit...
       ‚úÖ MFG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.5477 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MFG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MFG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.6168 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MFG XGBoost: Starting GridSearchCV fit...
       ‚úÖ LFCR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.4518 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LFCR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LFCR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.3991 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LFCR XGBoost: Starting GridSearchCV fit...
       ‚úÖ AVAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.3244 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=0.1120
    - TCN: MSE=0.0707
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0707
        ‚Ä¢ LSTM: MSE=0.1120
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.3820
        ‚Ä¢ Random Forest: MSE=28.2537
        ‚Ä¢ XGBoost: MSE=29.3244
   ‚úÖ AVAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AVAL (TargetReturn): TCN with MSE=0.0707
üêõ DEBUG: AVAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AVAL.
üêõ DEBUG: AVAL - Moving model to CPU before return...
üêõ DEBUG [23:10:26.065]: AVAL - Returning result metadata...
üêõ DEBUG [23:10:26.066]: Main received result for AVAL
üêõ DEBUG: Training progress: 356/959 done
üêõ DEBUG: train_worker started for WWD
  ‚öôÔ∏è Training models for WWD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - WWD: Initiating feature extraction for training.
  [DIAGNOSTIC] WWD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WWD: rows after features available: 126
üéØ WWD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WWD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WWD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WWD: Training LSTM (50 epochs)...
      ‚è≥ WWD LSTM: Epoch 10/50 (20%)
      ‚è≥ WWD LSTM: Epoch 20/50 (40%)
      ‚è≥ WWD LSTM: Epoch 30/50 (60%)
      ‚è≥ WWD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.681038
         RMSE: 0.825250
         R¬≤ Score: -1.7048 (Poor - 170.5% variance explained)
      üîπ WWD: Training TCN (50 epochs)...
      ‚è≥ WWD TCN: Epoch 10/50 (20%)
      ‚è≥ WWD TCN: Epoch 20/50 (40%)
      ‚è≥ WWD TCN: Epoch 30/50 (60%)
      ‚è≥ WWD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.587533
         RMSE: 0.766507
         R¬≤ Score: -1.3334
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WWD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WWD Random Forest: Starting GridSearchCV fit...
       ‚úÖ WWD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.8075 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WWD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WWD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.7127 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WWD XGBoost: Starting GridSearchCV fit...
       ‚úÖ WFC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.1076 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.6930
    - TCN: MSE=0.6062
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6062
        ‚Ä¢ LSTM: MSE=0.6930
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.0486
        ‚Ä¢ Random Forest: MSE=14.3633
        ‚Ä¢ XGBoost: MSE=21.1076
   ‚úÖ WFC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WFC (TargetReturn): TCN with MSE=0.6062
üêõ DEBUG: WFC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WFC.
üêõ DEBUG: WFC - Moving model to CPU before return...
üêõ DEBUG [23:10:39.108]: WFC - Returning result metadata...
üêõ DEBUG: train_worker started for DGP
  ‚öôÔ∏è Training models for DGP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DGP: Initiating feature extraction for training.
  [DIAGNOSTIC] DGP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DGP: rows after features available: 126
üéØ DGP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DGP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DGP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DGP: Training LSTM (50 epochs)...
       ‚úÖ OR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.2593 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.1196
    - TCN: MSE=0.0739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0739
        ‚Ä¢ LSTM: MSE=0.1196
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.3946
        ‚Ä¢ Random Forest: MSE=28.9120
        ‚Ä¢ XGBoost: MSE=40.2593
   ‚úÖ OR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OR (TargetReturn): TCN with MSE=0.0739
üêõ DEBUG: OR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OR.
üêõ DEBUG: OR - Moving model to CPU before return...
üêõ DEBUG [23:10:40.244]: OR - Returning result metadata...
üêõ DEBUG: train_worker started for CG
      ‚è≥ DGP LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for CG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CG: Initiating feature extraction for training.
  [DIAGNOSTIC] CG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CG: rows after features available: 126
üéØ CG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CG: Training LSTM (50 epochs)...
      ‚è≥ CG LSTM: Epoch 10/50 (20%)
      ‚è≥ DGP LSTM: Epoch 20/50 (40%)
      ‚è≥ CG LSTM: Epoch 20/50 (40%)
       ‚úÖ ESQ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.4847 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.5535
    - TCN: MSE=0.4796
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4796
        ‚Ä¢ LSTM: MSE=0.5535
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8985
        ‚Ä¢ Random Forest: MSE=16.8704
        ‚Ä¢ XGBoost: MSE=18.4847
   ‚úÖ ESQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ESQ (TargetReturn): TCN with MSE=0.4796
üêõ DEBUG: ESQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ESQ.
üêõ DEBUG: ESQ - Moving model to CPU before return...
üêõ DEBUG [23:10:41.239]: ESQ - Returning result metadata...
üêõ DEBUG: train_worker started for WOR
üêõ DEBUG [23:10:41.240]: Main received result for ESQ
üêõ DEBUG [23:10:41.240]: Main received result for OR
üêõ DEBUG [23:10:41.240]: Main received result for WFC
      ‚è≥ DGP LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for WOR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - WOR: Initiating feature extraction for training.
  [DIAGNOSTIC] WOR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WOR: rows after features available: 126
üéØ WOR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WOR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WOR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WOR: Training LSTM (50 epochs)...
      ‚è≥ CG LSTM: Epoch 30/50 (60%)
      ‚è≥ DGP LSTM: Epoch 40/50 (80%)
      ‚è≥ WOR LSTM: Epoch 10/50 (20%)
      ‚è≥ CG LSTM: Epoch 40/50 (80%)
      ‚è≥ WOR LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.292611
         RMSE: 0.540935
         R¬≤ Score: -0.5680 (Poor - 56.8% variance explained)
      üîπ DGP: Training TCN (50 epochs)...
      ‚è≥ DGP TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.569101
         RMSE: 0.754388
         R¬≤ Score: -0.7775 (Poor - 77.7% variance explained)
      üîπ CG: Training TCN (50 epochs)...
      ‚è≥ DGP TCN: Epoch 20/50 (40%)
      ‚è≥ CG TCN: Epoch 10/50 (20%)
      ‚è≥ DGP TCN: Epoch 30/50 (60%)
      ‚è≥ DGP TCN: Epoch 40/50 (80%)
      ‚è≥ CG TCN: Epoch 20/50 (40%)
      ‚è≥ WOR LSTM: Epoch 30/50 (60%)
      ‚è≥ CG TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.300060
         RMSE: 0.547777
         R¬≤ Score: -0.6079
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DGP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DGP Random Forest: Starting GridSearchCV fit...
      ‚è≥ CG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.573900
         RMSE: 0.757562
         R¬≤ Score: -0.7925
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CG Random Forest: Starting GridSearchCV fit...
      ‚è≥ WOR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.244181
         RMSE: 0.494146
         R¬≤ Score: -0.8730 (Poor - 87.3% variance explained)
      üîπ WOR: Training TCN (50 epochs)...
      ‚è≥ WOR TCN: Epoch 10/50 (20%)
      ‚è≥ WOR TCN: Epoch 20/50 (40%)
      ‚è≥ WOR TCN: Epoch 30/50 (60%)
      ‚è≥ WOR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.137124
         RMSE: 0.370302
         R¬≤ Score: -0.0518
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WOR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WOR Random Forest: Starting GridSearchCV fit...
       ‚úÖ DGP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.3155 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DGP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.9707 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DGP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=27.6286 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DGP XGBoost: Starting GridSearchCV fit...
       ‚úÖ CG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=66.1032 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CG XGBoost: Starting GridSearchCV fit...
       ‚úÖ WOR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.4128 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WOR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WOR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=23.4288 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WOR XGBoost: Starting GridSearchCV fit...
       ‚úÖ SCHW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.1477 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.4254
    - TCN: MSE=0.3816
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3816
        ‚Ä¢ LSTM: MSE=0.4254
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.7181
        ‚Ä¢ XGBoost: MSE=8.1477
        ‚Ä¢ Random Forest: MSE=9.4003
   ‚úÖ SCHW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SCHW (TargetReturn): TCN with MSE=0.3816
üêõ DEBUG: SCHW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SCHW.
üêõ DEBUG: SCHW - Moving model to CPU before return...
üêõ DEBUG [23:10:58.227]: SCHW - Returning result metadata...
üêõ DEBUG: train_worker started for SGDM
  ‚öôÔ∏è Training models for SGDM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SGDM: Initiating feature extraction for training.
  [DIAGNOSTIC] SGDM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SGDM: rows after features available: 126
üéØ SGDM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SGDM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SGDM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SGDM: Training LSTM (50 epochs)...
      ‚è≥ SGDM LSTM: Epoch 10/50 (20%)
      ‚è≥ SGDM LSTM: Epoch 20/50 (40%)
       ‚úÖ BBW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.6699 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.5857
    - TCN: MSE=0.4684
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4684
        ‚Ä¢ LSTM: MSE=0.5857
        ‚Ä¢ Random Forest: MSE=21.2449
        ‚Ä¢ XGBoost: MSE=22.6699
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.3920
   ‚úÖ BBW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBW (TargetReturn): TCN with MSE=0.4684
üêõ DEBUG: BBW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBW.
üêõ DEBUG: BBW - Moving model to CPU before return...
üêõ DEBUG [23:10:59.608]: BBW - Returning result metadata...
üêõ DEBUG: train_worker started for BRNS
üêõ DEBUG [23:10:59.609]: Main received result for BBW
üêõ DEBUG: Training progress: 360/959 done
üêõ DEBUG [23:10:59.609]: Main received result for SCHW
  ‚öôÔ∏è Training models for BRNS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BRNS: Initiating feature extraction for training.
  [DIAGNOSTIC] BRNS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BRNS: rows after features available: 126
üéØ BRNS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BRNS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BRNS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BRNS: Training LSTM (50 epochs)...
      ‚è≥ SGDM LSTM: Epoch 30/50 (60%)
      ‚è≥ BRNS LSTM: Epoch 10/50 (20%)
      ‚è≥ SGDM LSTM: Epoch 40/50 (80%)
      ‚è≥ BRNS LSTM: Epoch 20/50 (40%)
       ‚úÖ ACAD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=57.9454 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.9012
    - TCN: MSE=0.8580
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.8580
        ‚Ä¢ LSTM: MSE=0.9012
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.2034
        ‚Ä¢ Random Forest: MSE=41.9540
        ‚Ä¢ XGBoost: MSE=57.9454
   ‚úÖ ACAD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ACAD (TargetReturn): TCN with MSE=0.8580
üêõ DEBUG: ACAD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ACAD.
üêõ DEBUG: ACAD - Moving model to CPU before return...
üêõ DEBUG [23:11:00.719]: ACAD - Returning result metadata...
üêõ DEBUG: train_worker started for FHN
  ‚öôÔ∏è Training models for FHN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - FHN: Initiating feature extraction for training.
  [DIAGNOSTIC] FHN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FHN: rows after features available: 126
üéØ FHN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FHN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FHN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FHN: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.188478
         RMSE: 0.434140
         R¬≤ Score: -0.3590 (Poor - 35.9% variance explained)
      üîπ SGDM: Training TCN (50 epochs)...
      ‚è≥ BRNS LSTM: Epoch 30/50 (60%)
      ‚è≥ SGDM TCN: Epoch 10/50 (20%)
      ‚è≥ SGDM TCN: Epoch 20/50 (40%)
      ‚è≥ SGDM TCN: Epoch 30/50 (60%)
      ‚è≥ FHN LSTM: Epoch 10/50 (20%)
      ‚è≥ SGDM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.207082
         RMSE: 0.455062
         R¬≤ Score: -0.4931
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SGDM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SGDM Random Forest: Starting GridSearchCV fit...
      ‚è≥ BRNS LSTM: Epoch 40/50 (80%)
      ‚è≥ FHN LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.365973
         RMSE: 0.604957
         R¬≤ Score: -0.7163 (Poor - 71.6% variance explained)
      üîπ BRNS: Training TCN (50 epochs)...
      ‚è≥ BRNS TCN: Epoch 10/50 (20%)
      ‚è≥ BRNS TCN: Epoch 20/50 (40%)
      ‚è≥ FHN LSTM: Epoch 30/50 (60%)
      ‚è≥ BRNS TCN: Epoch 30/50 (60%)
      ‚è≥ BRNS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.236034
         RMSE: 0.485833
         R¬≤ Score: -0.1070
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BRNS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BRNS Random Forest: Starting GridSearchCV fit...
      ‚è≥ FHN LSTM: Epoch 40/50 (80%)
       ‚úÖ MTZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.6664 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.8s
    - LSTM: MSE=0.3882
    - TCN: MSE=0.4311
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3882
        ‚Ä¢ TCN: MSE=0.4311
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.3461
        ‚Ä¢ Random Forest: MSE=33.9153
        ‚Ä¢ XGBoost: MSE=35.6664
   ‚úÖ MTZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MTZ (TargetReturn): LSTM with MSE=0.3882
üêõ DEBUG: MTZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MTZ.
üêõ DEBUG: MTZ - Moving model to CPU before return...
üêõ DEBUG [23:11:02.849]: MTZ - Returning result metadata...
       ‚úÖ QTUM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.7769 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.7s
    - LSTM: MSE=0.3794
    - TCN: MSE=0.4787
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3794
        ‚Ä¢ TCN: MSE=0.4787
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.0674
        ‚Ä¢ XGBoost: MSE=11.7769
        ‚Ä¢ Random Forest: MSE=14.1528
   ‚úÖ QTUM: Phase 3/3 - Model selection complete!
üêõ DEBUG: train_worker started for NFLY
üêõ DEBUG [23:11:02.850]: Main received result for MTZ
  üèÜ WINNER for QTUM (TargetReturn): LSTM with MSE=0.3794
üêõ DEBUG: QTUM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QTUM.
üêõ DEBUG: QTUM - Moving model to CPU before return...
üêõ DEBUG [23:11:02.858]: QTUM - Returning result metadata...
üêõ DEBUG: train_worker started for URA
üêõ DEBUG [23:11:02.859]: Main received result for QTUM
üêõ DEBUG [23:11:02.859]: Main received result for ACAD
üêõ DEBUG: Training progress: 364/959 done
  ‚öôÔ∏è Training models for NFLY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - NFLY: Initiating feature extraction for training.
  [DIAGNOSTIC] NFLY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NFLY: rows after features available: 126
üéØ NFLY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for URA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - URA: Initiating feature extraction for training.
  [DIAGNOSTIC] URA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ URA: rows after features available: 126
üéØ URA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NFLY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NFLY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NFLY: Training LSTM (50 epochs)...
  [DIAGNOSTIC] URA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö URA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ URA: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.559793
         RMSE: 0.748193
         R¬≤ Score: -0.5631 (Poor - 56.3% variance explained)
      üîπ FHN: Training TCN (50 epochs)...
      ‚è≥ FHN TCN: Epoch 10/50 (20%)
      ‚è≥ NFLY LSTM: Epoch 10/50 (20%)
      ‚è≥ FHN TCN: Epoch 20/50 (40%)
      ‚è≥ URA LSTM: Epoch 10/50 (20%)
      ‚è≥ FHN TCN: Epoch 30/50 (60%)
      ‚è≥ FHN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.570818
         RMSE: 0.755525
         R¬≤ Score: -0.5939
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FHN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FHN Random Forest: Starting GridSearchCV fit...
      ‚è≥ NFLY LSTM: Epoch 20/50 (40%)
      ‚è≥ URA LSTM: Epoch 20/50 (40%)
       ‚úÖ SGDM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.6670 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SGDM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NFLY LSTM: Epoch 30/50 (60%)
      ‚è≥ URA LSTM: Epoch 30/50 (60%)
       ‚úÖ SGDM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=29.6762 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SGDM XGBoost: Starting GridSearchCV fit...
      ‚è≥ NFLY LSTM: Epoch 40/50 (80%)
      ‚è≥ URA LSTM: Epoch 40/50 (80%)
       ‚úÖ BRNS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=118.5999 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BRNS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.762030
         RMSE: 0.872943
         R¬≤ Score: -1.3006 (Poor - 130.1% variance explained)
      üîπ URA: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.388717
         RMSE: 0.623472
         R¬≤ Score: -1.1603 (Poor - 116.0% variance explained)
      üîπ NFLY: Training TCN (50 epochs)...
      ‚è≥ URA TCN: Epoch 10/50 (20%)
      ‚è≥ NFLY TCN: Epoch 10/50 (20%)
      ‚è≥ URA TCN: Epoch 20/50 (40%)
      ‚è≥ NFLY TCN: Epoch 20/50 (40%)
      ‚è≥ URA TCN: Epoch 30/50 (60%)
      ‚è≥ NFLY TCN: Epoch 30/50 (60%)
      ‚è≥ URA TCN: Epoch 40/50 (80%)
       ‚úÖ BRNS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=145.4003 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BRNS XGBoost: Starting GridSearchCV fit...
      ‚è≥ NFLY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.583657
         RMSE: 0.763974
         R¬≤ Score: -0.7621
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä URA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ URA Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.277547
         RMSE: 0.526827
         R¬≤ Score: -0.5424
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NFLY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NFLY Random Forest: Starting GridSearchCV fit...
       ‚úÖ FHN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.9500 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FHN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FHN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.2821 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FHN XGBoost: Starting GridSearchCV fit...
       ‚úÖ URA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.1423 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ URA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NFLY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.6966 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NFLY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ URA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.8258 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ URA XGBoost: Starting GridSearchCV fit...
       ‚úÖ NFLY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.7413 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NFLY XGBoost: Starting GridSearchCV fit...
       ‚úÖ ROAD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.1236 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.3034
    - TCN: MSE=0.2013
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2013
        ‚Ä¢ LSTM: MSE=0.3034
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.8831
        ‚Ä¢ Random Forest: MSE=35.7569
        ‚Ä¢ XGBoost: MSE=46.1236
   ‚úÖ ROAD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ROAD (TargetReturn): TCN with MSE=0.2013
üêõ DEBUG: ROAD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ROAD.
üêõ DEBUG: ROAD - Moving model to CPU before return...
üêõ DEBUG [23:11:21.751]: ROAD - Returning result metadata...
üêõ DEBUG: train_worker started for IDT
üêõ DEBUG [23:11:21.752]: Main received result for ROAD
  ‚öôÔ∏è Training models for IDT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - IDT: Initiating feature extraction for training.
  [DIAGNOSTIC] IDT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IDT: rows after features available: 126
üéØ IDT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IDT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IDT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IDT: Training LSTM (50 epochs)...
      ‚è≥ IDT LSTM: Epoch 10/50 (20%)
      ‚è≥ IDT LSTM: Epoch 20/50 (40%)
      ‚è≥ IDT LSTM: Epoch 30/50 (60%)
      ‚è≥ IDT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.171323
         RMSE: 0.413912
         R¬≤ Score: -0.3209 (Poor - 32.1% variance explained)
      üîπ IDT: Training TCN (50 epochs)...
      ‚è≥ IDT TCN: Epoch 10/50 (20%)
       ‚úÖ HSBC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.9412 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.3776
    - TCN: MSE=0.2455
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2455
        ‚Ä¢ LSTM: MSE=0.3776
        ‚Ä¢ Random Forest: MSE=12.5621
        ‚Ä¢ XGBoost: MSE=12.9412
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6018
   ‚úÖ HSBC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HSBC (TargetReturn): TCN with MSE=0.2455
üêõ DEBUG: HSBC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HSBC.
üêõ DEBUG: HSBC - Moving model to CPU before return...
üêõ DEBUG [23:11:24.123]: HSBC - Returning result metadata...
üêõ DEBUG: train_worker started for FINV
üêõ DEBUG [23:11:24.130]: Main received result for HSBC
  ‚öôÔ∏è Training models for FINV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FINV: Initiating feature extraction for training.
  [DIAGNOSTIC] FINV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FINV: rows after features available: 126
üéØ FINV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FINV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FINV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FINV: Training LSTM (50 epochs)...
      ‚è≥ IDT TCN: Epoch 20/50 (40%)
      ‚è≥ IDT TCN: Epoch 30/50 (60%)
      ‚è≥ IDT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.141761
         RMSE: 0.376512
         R¬≤ Score: -0.0930
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IDT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IDT Random Forest: Starting GridSearchCV fit...
      ‚è≥ FINV LSTM: Epoch 10/50 (20%)
      ‚è≥ FINV LSTM: Epoch 20/50 (40%)
      ‚è≥ FINV LSTM: Epoch 30/50 (60%)
      ‚è≥ FINV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.240109
         RMSE: 0.490009
         R¬≤ Score: -0.5628 (Poor - 56.3% variance explained)
      üîπ FINV: Training TCN (50 epochs)...
      ‚è≥ FINV TCN: Epoch 10/50 (20%)
      ‚è≥ FINV TCN: Epoch 20/50 (40%)
      ‚è≥ FINV TCN: Epoch 30/50 (60%)
      ‚è≥ FINV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.202038
         RMSE: 0.449486
         R¬≤ Score: -0.3150
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FINV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FINV Random Forest: Starting GridSearchCV fit...
       ‚úÖ IDT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.1311 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IDT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IDT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.2353 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IDT XGBoost: Starting GridSearchCV fit...
       ‚úÖ FINV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.6131 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FINV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FINV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=44.3175 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FINV XGBoost: Starting GridSearchCV fit...
       ‚úÖ TGS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.0636 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.1451
    - TCN: MSE=0.0936
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0936
        ‚Ä¢ LSTM: MSE=0.1451
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3500
        ‚Ä¢ Random Forest: MSE=22.4491
        ‚Ä¢ XGBoost: MSE=25.0636
   ‚úÖ TGS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TGS (TargetReturn): TCN with MSE=0.0936
üêõ DEBUG: TGS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TGS.
üêõ DEBUG: TGS - Moving model to CPU before return...
üêõ DEBUG [23:11:48.689]: TGS - Returning result metadata...
üêõ DEBUG [23:11:48.690]: Main received result for TGS
üêõ DEBUG: train_worker started for ECG
  ‚öôÔ∏è Training models for ECG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ECG: Initiating feature extraction for training.
  [DIAGNOSTIC] ECG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ECG: rows after features available: 106
üéØ ECG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ECG: train_and_evaluate_models - Rows after dropping NaNs: 106
    - Using MSE loss for regression (predicting returns)
   üìö ECG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ECG: Training LSTM (50 epochs)...
      ‚è≥ ECG LSTM: Epoch 10/50 (20%)
      ‚è≥ ECG LSTM: Epoch 20/50 (40%)
      ‚è≥ ECG LSTM: Epoch 30/50 (60%)
      ‚è≥ ECG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.113547
         RMSE: 0.336968
         R¬≤ Score: -1.4415 (Poor - 144.1% variance explained)
      üîπ ECG: Training TCN (50 epochs)...
      ‚è≥ ECG TCN: Epoch 10/50 (20%)
      ‚è≥ ECG TCN: Epoch 20/50 (40%)
      ‚è≥ ECG TCN: Epoch 30/50 (60%)
      ‚è≥ ECG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.046532
         RMSE: 0.215713
         R¬≤ Score: -0.0005
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ECG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ECG Random Forest: Starting GridSearchCV fit...
       ‚úÖ BTI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.5949 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.5681
    - TCN: MSE=0.3936
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3936
        ‚Ä¢ LSTM: MSE=0.5681
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.5823
        ‚Ä¢ Random Forest: MSE=6.0554
        ‚Ä¢ XGBoost: MSE=6.5949
   ‚úÖ BTI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BTI (TargetReturn): TCN with MSE=0.3936
üêõ DEBUG: BTI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BTI.
üêõ DEBUG: BTI - Moving model to CPU before return...
üêõ DEBUG [23:11:50.971]: BTI - Returning result metadata...
üêõ DEBUG: train_worker started for BWMN
üêõ DEBUG [23:11:50.972]: Main received result for BTI
üêõ DEBUG: Training progress: 368/959 done
  ‚öôÔ∏è Training models for BWMN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - BWMN: Initiating feature extraction for training.
  [DIAGNOSTIC] BWMN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BWMN: rows after features available: 126
üéØ BWMN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BWMN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BWMN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BWMN: Training LSTM (50 epochs)...
      ‚è≥ BWMN LSTM: Epoch 10/50 (20%)
      ‚è≥ BWMN LSTM: Epoch 20/50 (40%)
      ‚è≥ BWMN LSTM: Epoch 30/50 (60%)
      ‚è≥ BWMN LSTM: Epoch 40/50 (80%)
       ‚úÖ ECG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=41.0254 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ECG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.415780
         RMSE: 0.644810
         R¬≤ Score: -1.4768 (Poor - 147.7% variance explained)
      üîπ BWMN: Training TCN (50 epochs)...
      ‚è≥ BWMN TCN: Epoch 10/50 (20%)
      ‚è≥ BWMN TCN: Epoch 20/50 (40%)
      ‚è≥ BWMN TCN: Epoch 30/50 (60%)
      ‚è≥ BWMN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.295573
         RMSE: 0.543667
         R¬≤ Score: -0.7607
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BWMN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BWMN Random Forest: Starting GridSearchCV fit...
       ‚úÖ ECG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=51.0055 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ECG XGBoost: Starting GridSearchCV fit...
       ‚úÖ BWMN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.8513 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BWMN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BWMN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=27.4702 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BWMN XGBoost: Starting GridSearchCV fit...
       ‚úÖ MFG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.0269 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.5237
    - TCN: MSE=0.4374
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4374
        ‚Ä¢ LSTM: MSE=0.5237
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.6168
        ‚Ä¢ Random Forest: MSE=20.5477
        ‚Ä¢ XGBoost: MSE=21.0269
   ‚úÖ MFG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MFG (TargetReturn): TCN with MSE=0.4374
üêõ DEBUG: MFG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MFG.
üêõ DEBUG: MFG - Moving model to CPU before return...
üêõ DEBUG [23:12:22.635]: MFG - Returning result metadata...
üêõ DEBUG [23:12:22.635]: Main received result for MFG
üêõ DEBUG: train_worker started for PCRX
  ‚öôÔ∏è Training models for PCRX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - PCRX: Initiating feature extraction for training.
  [DIAGNOSTIC] PCRX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PCRX: rows after features available: 126
üéØ PCRX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PCRX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PCRX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PCRX: Training LSTM (50 epochs)...
      ‚è≥ PCRX LSTM: Epoch 10/50 (20%)
      ‚è≥ PCRX LSTM: Epoch 20/50 (40%)
      ‚è≥ PCRX LSTM: Epoch 30/50 (60%)
      ‚è≥ PCRX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.056445
         RMSE: 0.237582
         R¬≤ Score: -0.5392 (Poor - 53.9% variance explained)
      üîπ PCRX: Training TCN (50 epochs)...
      ‚è≥ PCRX TCN: Epoch 10/50 (20%)
      ‚è≥ PCRX TCN: Epoch 20/50 (40%)
      ‚è≥ PCRX TCN: Epoch 30/50 (60%)
      ‚è≥ PCRX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.046648
         RMSE: 0.215982
         R¬≤ Score: -0.2721
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PCRX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PCRX Random Forest: Starting GridSearchCV fit...
       ‚úÖ LFCR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.9058 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.1837
    - TCN: MSE=0.1042
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1042
        ‚Ä¢ LSTM: MSE=0.1837
        ‚Ä¢ Random Forest: MSE=40.4518
        ‚Ä¢ XGBoost: MSE=40.9058
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.3991
   ‚úÖ LFCR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LFCR (TargetReturn): TCN with MSE=0.1042
üêõ DEBUG: LFCR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LFCR.
üêõ DEBUG: LFCR - Moving model to CPU before return...
üêõ DEBUG [23:12:26.294]: LFCR - Returning result metadata...
üêõ DEBUG: train_worker started for CTLP
üêõ DEBUG [23:12:26.297]: Main received result for LFCR
  ‚öôÔ∏è Training models for CTLP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CTLP: Initiating feature extraction for training.
  [DIAGNOSTIC] CTLP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTLP: rows after features available: 126
üéØ CTLP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTLP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTLP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTLP: Training LSTM (50 epochs)...
      ‚è≥ CTLP LSTM: Epoch 10/50 (20%)
      ‚è≥ CTLP LSTM: Epoch 20/50 (40%)
       ‚úÖ PCRX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.7579 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PCRX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CTLP LSTM: Epoch 30/50 (60%)
      ‚è≥ CTLP LSTM: Epoch 40/50 (80%)
       ‚úÖ PCRX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.2950 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PCRX XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.876523
         RMSE: 0.936228
         R¬≤ Score: -0.9094 (Poor - 90.9% variance explained)
      üîπ CTLP: Training TCN (50 epochs)...
      ‚è≥ CTLP TCN: Epoch 10/50 (20%)
      ‚è≥ CTLP TCN: Epoch 20/50 (40%)
      ‚è≥ CTLP TCN: Epoch 30/50 (60%)
      ‚è≥ CTLP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.689263
         RMSE: 0.830219
         R¬≤ Score: -0.5015
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTLP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTLP Random Forest: Starting GridSearchCV fit...
       ‚úÖ WWD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.3672 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.6810
    - TCN: MSE=0.5875
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5875
        ‚Ä¢ LSTM: MSE=0.6810
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.7127
        ‚Ä¢ XGBoost: MSE=32.3672
        ‚Ä¢ Random Forest: MSE=35.8075
   ‚úÖ WWD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WWD (TargetReturn): TCN with MSE=0.5875
üêõ DEBUG: WWD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WWD.
üêõ DEBUG: WWD - Moving model to CPU before return...
üêõ DEBUG [23:12:31.593]: WWD - Returning result metadata...
üêõ DEBUG [23:12:31.594]: Main received result for WWD
üêõ DEBUG: train_worker started for IPI
  ‚öôÔ∏è Training models for IPI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - IPI: Initiating feature extraction for training.
  [DIAGNOSTIC] IPI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IPI: rows after features available: 126
üéØ IPI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IPI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IPI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IPI: Training LSTM (50 epochs)...
      ‚è≥ IPI LSTM: Epoch 10/50 (20%)
       ‚úÖ CTLP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.2241 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTLP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IPI LSTM: Epoch 20/50 (40%)
       ‚úÖ CTLP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=34.8573 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTLP XGBoost: Starting GridSearchCV fit...
      ‚è≥ IPI LSTM: Epoch 30/50 (60%)
      ‚è≥ IPI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.269204
         RMSE: 0.518849
         R¬≤ Score: -0.6515 (Poor - 65.1% variance explained)
      üîπ IPI: Training TCN (50 epochs)...
      ‚è≥ IPI TCN: Epoch 10/50 (20%)
      ‚è≥ IPI TCN: Epoch 20/50 (40%)
      ‚è≥ IPI TCN: Epoch 30/50 (60%)
      ‚è≥ IPI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.163291
         RMSE: 0.404093
         R¬≤ Score: -0.0017
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IPI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IPI Random Forest: Starting GridSearchCV fit...
       ‚úÖ IPI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.9065 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IPI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IPI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.3320 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IPI XGBoost: Starting GridSearchCV fit...
       ‚úÖ DGP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=39.4345 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.2926
    - TCN: MSE=0.3001
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2926
        ‚Ä¢ TCN: MSE=0.3001
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.6286
        ‚Ä¢ Random Forest: MSE=37.3155
        ‚Ä¢ XGBoost: MSE=39.4345
   ‚úÖ DGP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DGP (TargetReturn): LSTM with MSE=0.2926
üêõ DEBUG: DGP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DGP.
üêõ DEBUG: DGP - Moving model to CPU before return...
üêõ DEBUG [23:12:43.543]: DGP - Returning result metadata...
üêõ DEBUG: train_worker started for FLEX
üêõ DEBUG [23:12:43.544]: Main received result for DGP
üêõ DEBUG: Training progress: 372/959 done
  ‚öôÔ∏è Training models for FLEX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FLEX: Initiating feature extraction for training.
  [DIAGNOSTIC] FLEX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FLEX: rows after features available: 126
üéØ FLEX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FLEX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FLEX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FLEX: Training LSTM (50 epochs)...
       ‚úÖ CG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.0007 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.5691
    - TCN: MSE=0.5739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5691
        ‚Ä¢ TCN: MSE=0.5739
        ‚Ä¢ Random Forest: MSE=26.9707
        ‚Ä¢ XGBoost: MSE=38.0007
        ‚Ä¢ LightGBM Regressor (CPU): MSE=66.1032
   ‚úÖ CG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CG (TargetReturn): LSTM with MSE=0.5691
üêõ DEBUG: CG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CG.
üêõ DEBUG: CG - Moving model to CPU before return...
üêõ DEBUG [23:12:43.980]: CG - Returning result metadata...
üêõ DEBUG [23:12:43.981]: Main received result for CG
üêõ DEBUG: train_worker started for GS
      ‚è≥ FLEX LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for GS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - GS: Initiating feature extraction for training.
  [DIAGNOSTIC] GS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GS: rows after features available: 126
üéØ GS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GS: Training LSTM (50 epochs)...
      ‚è≥ FLEX LSTM: Epoch 20/50 (40%)
      ‚è≥ GS LSTM: Epoch 10/50 (20%)
      ‚è≥ FLEX LSTM: Epoch 30/50 (60%)
      ‚è≥ GS LSTM: Epoch 20/50 (40%)
      ‚è≥ FLEX LSTM: Epoch 40/50 (80%)
      ‚è≥ GS LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.530039
         RMSE: 0.728038
         R¬≤ Score: -1.0921 (Poor - 109.2% variance explained)
      üîπ FLEX: Training TCN (50 epochs)...
      ‚è≥ FLEX TCN: Epoch 10/50 (20%)
      ‚è≥ FLEX TCN: Epoch 20/50 (40%)
      ‚è≥ GS LSTM: Epoch 40/50 (80%)
      ‚è≥ FLEX TCN: Epoch 30/50 (60%)
      ‚è≥ FLEX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.511340
         RMSE: 0.715081
         R¬≤ Score: -1.0183
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FLEX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FLEX Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.816718
         RMSE: 0.903725
         R¬≤ Score: -1.7479 (Poor - 174.8% variance explained)
      üîπ GS: Training TCN (50 epochs)...
      ‚è≥ GS TCN: Epoch 10/50 (20%)
      ‚è≥ GS TCN: Epoch 20/50 (40%)
      ‚è≥ GS TCN: Epoch 30/50 (60%)
      ‚è≥ GS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.575903
         RMSE: 0.758883
         R¬≤ Score: -0.9377
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GS Random Forest: Starting GridSearchCV fit...
       ‚úÖ WOR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.2302 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.2442
    - TCN: MSE=0.1371
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1371
        ‚Ä¢ LSTM: MSE=0.2442
        ‚Ä¢ Random Forest: MSE=19.4128
        ‚Ä¢ XGBoost: MSE=20.2302
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.4288
   ‚úÖ WOR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WOR (TargetReturn): TCN with MSE=0.1371
üêõ DEBUG: WOR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WOR.
üêõ DEBUG: WOR - Moving model to CPU before return...
üêõ DEBUG [23:12:47.906]: WOR - Returning result metadata...
üêõ DEBUG [23:12:47.906]: Main received result for WOR
üêõ DEBUG: train_worker started for WRBY
  ‚öôÔ∏è Training models for WRBY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - WRBY: Initiating feature extraction for training.
  [DIAGNOSTIC] WRBY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WRBY: rows after features available: 126
üéØ WRBY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WRBY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WRBY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WRBY: Training LSTM (50 epochs)...
      ‚è≥ WRBY LSTM: Epoch 10/50 (20%)
      ‚è≥ WRBY LSTM: Epoch 20/50 (40%)
       ‚úÖ FLEX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.3639 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FLEX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ WRBY LSTM: Epoch 30/50 (60%)
       ‚úÖ GS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.5114 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FLEX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=35.2360 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FLEX XGBoost: Starting GridSearchCV fit...
      ‚è≥ WRBY LSTM: Epoch 40/50 (80%)
       ‚úÖ GS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.0411 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GS XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.704693
         RMSE: 0.839460
         R¬≤ Score: -0.8378 (Poor - 83.8% variance explained)
      üîπ WRBY: Training TCN (50 epochs)...
      ‚è≥ WRBY TCN: Epoch 10/50 (20%)
      ‚è≥ WRBY TCN: Epoch 20/50 (40%)
      ‚è≥ WRBY TCN: Epoch 30/50 (60%)
      ‚è≥ WRBY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.728521
         RMSE: 0.853535
         R¬≤ Score: -0.9000
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WRBY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WRBY Random Forest: Starting GridSearchCV fit...
       ‚úÖ WRBY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.6893 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WRBY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WRBY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.5673 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WRBY XGBoost: Starting GridSearchCV fit...
       ‚úÖ SGDM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=27.3248 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.1885
    - TCN: MSE=0.2071
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1885
        ‚Ä¢ TCN: MSE=0.2071
        ‚Ä¢ XGBoost: MSE=27.3248
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.6762
        ‚Ä¢ Random Forest: MSE=30.6670
   ‚úÖ SGDM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SGDM (TargetReturn): LSTM with MSE=0.1885
üêõ DEBUG: SGDM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SGDM.
üêõ DEBUG: SGDM - Moving model to CPU before return...
üêõ DEBUG [23:13:03.960]: SGDM - Returning result metadata...
üêõ DEBUG: train_worker started for BB
üêõ DEBUG [23:13:03.962]: Main received result for SGDM
  ‚öôÔ∏è Training models for BB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BB: Initiating feature extraction for training.
  [DIAGNOSTIC] BB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BB: rows after features available: 126
üéØ BB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BB: Training LSTM (50 epochs)...
      ‚è≥ BB LSTM: Epoch 10/50 (20%)
      ‚è≥ BB LSTM: Epoch 20/50 (40%)
      ‚è≥ BB LSTM: Epoch 30/50 (60%)
      ‚è≥ BB LSTM: Epoch 40/50 (80%)
       ‚úÖ FHN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.4430 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.5598
    - TCN: MSE=0.5708
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5598
        ‚Ä¢ TCN: MSE=0.5708
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.2821
        ‚Ä¢ XGBoost: MSE=12.4430
        ‚Ä¢ Random Forest: MSE=13.9500
   ‚úÖ FHN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FHN (TargetReturn): LSTM with MSE=0.5598
üêõ DEBUG: FHN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FHN.
üêõ DEBUG: FHN - Moving model to CPU before return...
üêõ DEBUG [23:13:05.941]: FHN - Returning result metadata...
üêõ DEBUG: train_worker started for UGI
  ‚öôÔ∏è Training models for UGI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - UGI: Initiating feature extraction for training.
  [DIAGNOSTIC] UGI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UGI: rows after features available: 126
üéØ UGI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UGI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UGI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UGI: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.100918
         RMSE: 0.317676
         R¬≤ Score: -0.5640 (Poor - 56.4% variance explained)
      üîπ BB: Training TCN (50 epochs)...
      ‚è≥ BB TCN: Epoch 10/50 (20%)
       ‚úÖ BRNS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=188.4917 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 120.3s
    - LSTM: MSE=0.3660
    - TCN: MSE=0.2360
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2360
        ‚Ä¢ LSTM: MSE=0.3660
        ‚Ä¢ Random Forest: MSE=118.5999
        ‚Ä¢ LightGBM Regressor (CPU): MSE=145.4003
        ‚Ä¢ XGBoost: MSE=188.4917
   ‚úÖ BRNS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BRNS (TargetReturn): TCN with MSE=0.2360
üêõ DEBUG: BRNS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BRNS.
üêõ DEBUG: BRNS - Moving model to CPU before return...
üêõ DEBUG [23:13:06.482]: BRNS - Returning result metadata...
üêõ DEBUG [23:13:06.483]: Main received result for BRNS
üêõ DEBUG: Training progress: 376/959 done
üêõ DEBUG [23:13:06.483]: Main received result for FHN
üêõ DEBUG: train_worker started for CALM
      ‚è≥ BB TCN: Epoch 20/50 (40%)
      ‚è≥ UGI LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for CALM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - CALM: Initiating feature extraction for training.
  [DIAGNOSTIC] CALM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CALM: rows after features available: 126
üéØ CALM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CALM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CALM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CALM: Training LSTM (50 epochs)...
      ‚è≥ BB TCN: Epoch 30/50 (60%)
      ‚è≥ BB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.098631
         RMSE: 0.314055
         R¬≤ Score: -0.5285
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BB Random Forest: Starting GridSearchCV fit...
      ‚è≥ UGI LSTM: Epoch 20/50 (40%)
      ‚è≥ CALM LSTM: Epoch 10/50 (20%)
      ‚è≥ CALM LSTM: Epoch 20/50 (40%)
      ‚è≥ UGI LSTM: Epoch 30/50 (60%)
      ‚è≥ UGI LSTM: Epoch 40/50 (80%)
      ‚è≥ CALM LSTM: Epoch 30/50 (60%)
      ‚è≥ CALM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.043884
         RMSE: 0.209485
         R¬≤ Score: -0.9594 (Poor - 95.9% variance explained)
      üîπ UGI: Training TCN (50 epochs)...
      ‚è≥ UGI TCN: Epoch 10/50 (20%)
      ‚è≥ UGI TCN: Epoch 20/50 (40%)
      ‚è≥ UGI TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.289055
         RMSE: 0.537639
         R¬≤ Score: -0.6371 (Poor - 63.7% variance explained)
      üîπ CALM: Training TCN (50 epochs)...
      ‚è≥ UGI TCN: Epoch 40/50 (80%)
      ‚è≥ CALM TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.023301
         RMSE: 0.152647
         R¬≤ Score: -0.0404
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UGI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UGI Random Forest: Starting GridSearchCV fit...
      ‚è≥ CALM TCN: Epoch 20/50 (40%)
      ‚è≥ CALM TCN: Epoch 30/50 (60%)
      ‚è≥ CALM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.292560
         RMSE: 0.540888
         R¬≤ Score: -0.6569
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CALM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CALM Random Forest: Starting GridSearchCV fit...
       ‚úÖ BB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.0453 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ URA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.4742 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.7620
    - TCN: MSE=0.5837
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5837
        ‚Ä¢ LSTM: MSE=0.7620
        ‚Ä¢ Random Forest: MSE=30.1423
        ‚Ä¢ XGBoost: MSE=38.4742
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.8258
   ‚úÖ URA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for URA (TargetReturn): TCN with MSE=0.5837
üêõ DEBUG: URA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for URA.
üêõ DEBUG: URA - Moving model to CPU before return...
üêõ DEBUG [23:13:10.293]: URA - Returning result metadata...
üêõ DEBUG: train_worker started for OSIS
  ‚öôÔ∏è Training models for OSIS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - OSIS: Initiating feature extraction for training.
  [DIAGNOSTIC] OSIS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OSIS: rows after features available: 126
üéØ OSIS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OSIS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OSIS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OSIS: Training LSTM (50 epochs)...
       ‚úÖ BB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=99.0442 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BB XGBoost: Starting GridSearchCV fit...
      ‚è≥ OSIS LSTM: Epoch 10/50 (20%)
      ‚è≥ OSIS LSTM: Epoch 20/50 (40%)
      ‚è≥ OSIS LSTM: Epoch 30/50 (60%)
       ‚úÖ NFLY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.9222 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.3887
    - TCN: MSE=0.2775
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2775
        ‚Ä¢ LSTM: MSE=0.3887
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.7413
        ‚Ä¢ Random Forest: MSE=12.6966
        ‚Ä¢ XGBoost: MSE=21.9222
   ‚úÖ NFLY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NFLY (TargetReturn): TCN with MSE=0.2775
üêõ DEBUG: NFLY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NFLY.
üêõ DEBUG: NFLY - Moving model to CPU before return...
üêõ DEBUG [23:13:11.936]: NFLY - Returning result metadata...
üêõ DEBUG [23:13:11.936]: Main received result for NFLY
üêõ DEBUG [23:13:11.936]: Main received result for URA
üêõ DEBUG: train_worker started for RMBS
  ‚öôÔ∏è Training models for RMBS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - RMBS: Initiating feature extraction for training.
  [DIAGNOSTIC] RMBS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RMBS: rows after features available: 126
üéØ RMBS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RMBS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RMBS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RMBS: Training LSTM (50 epochs)...
       ‚úÖ UGI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.6605 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UGI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CALM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.8064 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CALM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ OSIS LSTM: Epoch 40/50 (80%)
      ‚è≥ RMBS LSTM: Epoch 10/50 (20%)
       ‚úÖ UGI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.7809 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UGI XGBoost: Starting GridSearchCV fit...
      ‚è≥ RMBS LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.139258
         RMSE: 0.373173
         R¬≤ Score: -0.5676 (Poor - 56.8% variance explained)
      üîπ OSIS: Training TCN (50 epochs)...
       ‚úÖ CALM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.7922 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CALM XGBoost: Starting GridSearchCV fit...
      ‚è≥ OSIS TCN: Epoch 10/50 (20%)
      ‚è≥ OSIS TCN: Epoch 20/50 (40%)
      ‚è≥ OSIS TCN: Epoch 30/50 (60%)
      ‚è≥ OSIS TCN: Epoch 40/50 (80%)
      ‚è≥ RMBS LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.090767
         RMSE: 0.301276
         R¬≤ Score: -0.0218
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OSIS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OSIS Random Forest: Starting GridSearchCV fit...
      ‚è≥ RMBS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.610505
         RMSE: 0.781348
         R¬≤ Score: -1.0238 (Poor - 102.4% variance explained)
      üîπ RMBS: Training TCN (50 epochs)...
      ‚è≥ RMBS TCN: Epoch 10/50 (20%)
      ‚è≥ RMBS TCN: Epoch 20/50 (40%)
      ‚è≥ RMBS TCN: Epoch 30/50 (60%)
      ‚è≥ RMBS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.538453
         RMSE: 0.733793
         R¬≤ Score: -0.7850
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RMBS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RMBS Random Forest: Starting GridSearchCV fit...
       ‚úÖ OSIS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.6761 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OSIS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OSIS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.6609 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OSIS XGBoost: Starting GridSearchCV fit...
       ‚úÖ RMBS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.2568 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RMBS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RMBS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.0841 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RMBS XGBoost: Starting GridSearchCV fit...
       ‚úÖ FINV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=55.4663 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.1s
    - LSTM: MSE=0.2401
    - TCN: MSE=0.2020
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2020
        ‚Ä¢ LSTM: MSE=0.2401
        ‚Ä¢ Random Forest: MSE=42.6131
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.3175
        ‚Ä¢ XGBoost: MSE=55.4663
   ‚úÖ FINV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FINV (TargetReturn): TCN with MSE=0.2020
üêõ DEBUG: FINV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FINV.
üêõ DEBUG: FINV - Moving model to CPU before return...
üêõ DEBUG [23:13:25.462]: FINV - Returning result metadata...
üêõ DEBUG: train_worker started for CELH
  ‚öôÔ∏è Training models for CELH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CELH: Initiating feature extraction for training.
  [DIAGNOSTIC] CELH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CELH: rows after features available: 126
üéØ CELH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CELH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CELH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CELH: Training LSTM (50 epochs)...
      ‚è≥ CELH LSTM: Epoch 10/50 (20%)
      ‚è≥ CELH LSTM: Epoch 20/50 (40%)
      ‚è≥ CELH LSTM: Epoch 30/50 (60%)
       ‚úÖ IDT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.7970 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.1713
    - TCN: MSE=0.1418
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1418
        ‚Ä¢ LSTM: MSE=0.1713
        ‚Ä¢ Random Forest: MSE=25.1311
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.2353
        ‚Ä¢ XGBoost: MSE=47.7970
   ‚úÖ IDT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IDT (TargetReturn): TCN with MSE=0.1418
üêõ DEBUG: IDT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IDT.
üêõ DEBUG: IDT - Moving model to CPU before return...
üêõ DEBUG [23:13:27.122]: IDT - Returning result metadata...
üêõ DEBUG: train_worker started for UNTY
üêõ DEBUG [23:13:27.123]: Main received result for IDT
üêõ DEBUG: Training progress: 380/959 done
üêõ DEBUG [23:13:27.123]: Main received result for FINV
  ‚öôÔ∏è Training models for UNTY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - UNTY: Initiating feature extraction for training.
  [DIAGNOSTIC] UNTY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UNTY: rows after features available: 126
üéØ UNTY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UNTY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UNTY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UNTY: Training LSTM (50 epochs)...
      ‚è≥ CELH LSTM: Epoch 40/50 (80%)
      ‚è≥ UNTY LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.122562
         RMSE: 0.350089
         R¬≤ Score: -0.4359 (Poor - 43.6% variance explained)
      üîπ CELH: Training TCN (50 epochs)...
      ‚è≥ CELH TCN: Epoch 10/50 (20%)
      ‚è≥ CELH TCN: Epoch 20/50 (40%)
      ‚è≥ CELH TCN: Epoch 30/50 (60%)
      ‚è≥ CELH TCN: Epoch 40/50 (80%)
      ‚è≥ UNTY LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.088263
         RMSE: 0.297091
         R¬≤ Score: -0.0340
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CELH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CELH Random Forest: Starting GridSearchCV fit...
      ‚è≥ UNTY LSTM: Epoch 30/50 (60%)
      ‚è≥ UNTY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.831404
         RMSE: 0.911814
         R¬≤ Score: -1.3883 (Poor - 138.8% variance explained)
      üîπ UNTY: Training TCN (50 epochs)...
      ‚è≥ UNTY TCN: Epoch 10/50 (20%)
      ‚è≥ UNTY TCN: Epoch 20/50 (40%)
      ‚è≥ UNTY TCN: Epoch 30/50 (60%)
      ‚è≥ UNTY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.508156
         RMSE: 0.712851
         R¬≤ Score: -0.4597
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UNTY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UNTY Random Forest: Starting GridSearchCV fit...
       ‚úÖ CELH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=81.0837 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CELH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CELH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=64.6319 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CELH XGBoost: Starting GridSearchCV fit...
       ‚úÖ UNTY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.8833 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UNTY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UNTY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.8618 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UNTY XGBoost: Starting GridSearchCV fit...
       ‚úÖ ECG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=38.5721 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 113.4s
    - LSTM: MSE=0.1135
    - TCN: MSE=0.0465
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 116.8 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0465
        ‚Ä¢ LSTM: MSE=0.1135
        ‚Ä¢ XGBoost: MSE=38.5721
        ‚Ä¢ Random Forest: MSE=41.0254
        ‚Ä¢ LightGBM Regressor (CPU): MSE=51.0055
   ‚úÖ ECG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ECG (TargetReturn): TCN with MSE=0.0465
üêõ DEBUG: ECG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ECG.
üêõ DEBUG: ECG - Moving model to CPU before return...
üêõ DEBUG [23:13:47.433]: ECG - Returning result metadata...
üêõ DEBUG: train_worker started for ESE
üêõ DEBUG [23:13:47.434]: Main received result for ECG
  ‚öôÔ∏è Training models for ESE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ESE: Initiating feature extraction for training.
  [DIAGNOSTIC] ESE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ESE: rows after features available: 126
üéØ ESE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ESE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ESE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ESE: Training LSTM (50 epochs)...
      ‚è≥ ESE LSTM: Epoch 10/50 (20%)
      ‚è≥ ESE LSTM: Epoch 20/50 (40%)
      ‚è≥ ESE LSTM: Epoch 30/50 (60%)
      ‚è≥ ESE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.292027
         RMSE: 0.540395
         R¬≤ Score: -1.1584 (Poor - 115.8% variance explained)
      üîπ ESE: Training TCN (50 epochs)...
      ‚è≥ ESE TCN: Epoch 10/50 (20%)
      ‚è≥ ESE TCN: Epoch 20/50 (40%)
      ‚è≥ ESE TCN: Epoch 30/50 (60%)
      ‚è≥ ESE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.154509
         RMSE: 0.393076
         R¬≤ Score: -0.1420
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ESE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ESE Random Forest: Starting GridSearchCV fit...
       ‚úÖ BWMN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.9710 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.6s
    - LSTM: MSE=0.4158
    - TCN: MSE=0.2956
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2956
        ‚Ä¢ LSTM: MSE=0.4158
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.4702
        ‚Ä¢ Random Forest: MSE=28.8513
        ‚Ä¢ XGBoost: MSE=53.9710
   ‚úÖ BWMN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BWMN (TargetReturn): TCN with MSE=0.2956
üêõ DEBUG: BWMN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BWMN.
üêõ DEBUG: BWMN - Moving model to CPU before return...
üêõ DEBUG [23:13:52.766]: BWMN - Returning result metadata...
üêõ DEBUG [23:13:52.766]: Main received result for BWMNüêõ DEBUG: train_worker started for SYF

  ‚öôÔ∏è Training models for SYF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - SYF: Initiating feature extraction for training.
  [DIAGNOSTIC] SYF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SYF: rows after features available: 126
üéØ SYF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SYF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SYF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SYF: Training LSTM (50 epochs)...
       ‚úÖ ESE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.7473 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ESE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SYF LSTM: Epoch 10/50 (20%)
       ‚úÖ ESE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.1879 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ESE XGBoost: Starting GridSearchCV fit...
      ‚è≥ SYF LSTM: Epoch 20/50 (40%)
      ‚è≥ SYF LSTM: Epoch 30/50 (60%)
      ‚è≥ SYF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.850137
         RMSE: 0.922029
         R¬≤ Score: -1.8398 (Poor - 184.0% variance explained)
      üîπ SYF: Training TCN (50 epochs)...
      ‚è≥ SYF TCN: Epoch 10/50 (20%)
      ‚è≥ SYF TCN: Epoch 20/50 (40%)
      ‚è≥ SYF TCN: Epoch 30/50 (60%)
      ‚è≥ SYF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.629923
         RMSE: 0.793677
         R¬≤ Score: -1.1042
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SYF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SYF Random Forest: Starting GridSearchCV fit...
       ‚úÖ SYF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2754 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SYF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SYF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.6156 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SYF XGBoost: Starting GridSearchCV fit...
       ‚úÖ PCRX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.4956 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.0564
    - TCN: MSE=0.0466
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0466
        ‚Ä¢ LSTM: MSE=0.0564
        ‚Ä¢ Random Forest: MSE=20.7579
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.2950
        ‚Ä¢ XGBoost: MSE=25.4956
   ‚úÖ PCRX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PCRX (TargetReturn): TCN with MSE=0.0466
üêõ DEBUG: PCRX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PCRX.
üêõ DEBUG: PCRX - Moving model to CPU before return...
üêõ DEBUG [23:14:27.529]: PCRX - Returning result metadata...
üêõ DEBUG: train_worker started for NTES
üêõ DEBUG [23:14:27.530]: Main received result for PCRX
üêõ DEBUG: Training progress: 384/959 done
  ‚öôÔ∏è Training models for NTES (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NTES: Initiating feature extraction for training.
  [DIAGNOSTIC] NTES: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTES: rows after features available: 126
üéØ NTES: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTES: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTES: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTES: Training LSTM (50 epochs)...
      ‚è≥ NTES LSTM: Epoch 10/50 (20%)
      ‚è≥ NTES LSTM: Epoch 20/50 (40%)
      ‚è≥ NTES LSTM: Epoch 30/50 (60%)
      ‚è≥ NTES LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.484774
         RMSE: 0.696257
         R¬≤ Score: -0.9525 (Poor - 95.3% variance explained)
      üîπ NTES: Training TCN (50 epochs)...
      ‚è≥ NTES TCN: Epoch 10/50 (20%)
      ‚è≥ NTES TCN: Epoch 20/50 (40%)
      ‚è≥ NTES TCN: Epoch 30/50 (60%)
      ‚è≥ NTES TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.454108
         RMSE: 0.673875
         R¬≤ Score: -0.8290
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTES: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTES Random Forest: Starting GridSearchCV fit...
       ‚úÖ CTLP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.5329 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.8765
    - TCN: MSE=0.6893
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6893
        ‚Ä¢ LSTM: MSE=0.8765
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.8573
        ‚Ä¢ Random Forest: MSE=39.2241
        ‚Ä¢ XGBoost: MSE=52.5329
   ‚úÖ CTLP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTLP (TargetReturn): TCN with MSE=0.6893
üêõ DEBUG: CTLP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTLP.
üêõ DEBUG: CTLP - Moving model to CPU before return...
üêõ DEBUG [23:14:32.788]: CTLP - Returning result metadata...
üêõ DEBUG [23:14:32.788]: Main received result for CTLP
üêõ DEBUG: train_worker started for CHEF
  ‚öôÔ∏è Training models for CHEF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CHEF: Initiating feature extraction for training.
  [DIAGNOSTIC] CHEF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CHEF: rows after features available: 126
üéØ CHEF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CHEF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CHEF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CHEF: Training LSTM (50 epochs)...
       ‚úÖ NTES Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.3625 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTES LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CHEF LSTM: Epoch 10/50 (20%)
      ‚è≥ CHEF LSTM: Epoch 20/50 (40%)
       ‚úÖ NTES LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.3712 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTES XGBoost: Starting GridSearchCV fit...
      ‚è≥ CHEF LSTM: Epoch 30/50 (60%)
      ‚è≥ CHEF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.203864
         RMSE: 0.451513
         R¬≤ Score: -1.4420 (Poor - 144.2% variance explained)
      üîπ CHEF: Training TCN (50 epochs)...
      ‚è≥ CHEF TCN: Epoch 10/50 (20%)
      ‚è≥ CHEF TCN: Epoch 20/50 (40%)
      ‚è≥ CHEF TCN: Epoch 30/50 (60%)
      ‚è≥ CHEF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.128102
         RMSE: 0.357913
         R¬≤ Score: -0.5345
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CHEF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CHEF Random Forest: Starting GridSearchCV fit...
       ‚úÖ IPI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=26.8831 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.2692
    - TCN: MSE=0.1633
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1633
        ‚Ä¢ LSTM: MSE=0.2692
        ‚Ä¢ XGBoost: MSE=26.8831
        ‚Ä¢ Random Forest: MSE=28.9065
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.3320
   ‚úÖ IPI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IPI (TargetReturn): TCN with MSE=0.1633
üêõ DEBUG: IPI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IPI.
üêõ DEBUG: IPI - Moving model to CPU before return...
üêõ DEBUG [23:14:38.654]: IPI - Returning result metadata...
üêõ DEBUG [23:14:38.655]: Main received result for IPI
üêõ DEBUG: train_worker started for CNM
       ‚úÖ CHEF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.1180 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CHEF LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for CNM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - CNM: Initiating feature extraction for training.
  [DIAGNOSTIC] CNM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CNM: rows after features available: 126
üéØ CNM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CNM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CNM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CNM: Training LSTM (50 epochs)...
      ‚è≥ CNM LSTM: Epoch 10/50 (20%)
       ‚úÖ CHEF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.9850 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CHEF XGBoost: Starting GridSearchCV fit...
      ‚è≥ CNM LSTM: Epoch 20/50 (40%)
      ‚è≥ CNM LSTM: Epoch 30/50 (60%)
      ‚è≥ CNM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.584800
         RMSE: 0.764722
         R¬≤ Score: -1.3645 (Poor - 136.5% variance explained)
      üîπ CNM: Training TCN (50 epochs)...
      ‚è≥ CNM TCN: Epoch 10/50 (20%)
      ‚è≥ CNM TCN: Epoch 20/50 (40%)
      ‚è≥ CNM TCN: Epoch 30/50 (60%)
      ‚è≥ CNM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.462929
         RMSE: 0.680389
         R¬≤ Score: -0.8717
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CNM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CNM Random Forest: Starting GridSearchCV fit...
       ‚úÖ CNM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.9842 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CNM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CNM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.0239 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CNM XGBoost: Starting GridSearchCV fit...
       ‚úÖ FLEX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=35.1651 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.5300
    - TCN: MSE=0.5113
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5113
        ‚Ä¢ LSTM: MSE=0.5300
        ‚Ä¢ XGBoost: MSE=35.1651
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.2360
        ‚Ä¢ Random Forest: MSE=38.3639
   ‚úÖ FLEX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FLEX (TargetReturn): TCN with MSE=0.5113
üêõ DEBUG: FLEX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FLEX.
üêõ DEBUG: FLEX - Moving model to CPU before return...
üêõ DEBUG [23:14:46.549]: FLEX - Returning result metadata...
üêõ DEBUG: train_worker started for SSP
üêõ DEBUG [23:14:46.552]: Main received result for FLEX
  ‚öôÔ∏è Training models for SSP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - SSP: Initiating feature extraction for training.
  [DIAGNOSTIC] SSP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SSP: rows after features available: 126
üéØ SSP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SSP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SSP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SSP: Training LSTM (50 epochs)...
      ‚è≥ SSP LSTM: Epoch 10/50 (20%)
      ‚è≥ SSP LSTM: Epoch 20/50 (40%)
      ‚è≥ SSP LSTM: Epoch 30/50 (60%)
      ‚è≥ SSP LSTM: Epoch 40/50 (80%)
       ‚úÖ GS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.6698 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.8167
    - TCN: MSE=0.5759
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5759
        ‚Ä¢ LSTM: MSE=0.8167
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.0411
        ‚Ä¢ Random Forest: MSE=20.5114
        ‚Ä¢ XGBoost: MSE=28.6698
   ‚úÖ GS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GS (TargetReturn): TCN with MSE=0.5759
üêõ DEBUG: GS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GS.
üêõ DEBUG: GS - Moving model to CPU before return...
üêõ DEBUG [23:14:48.438]: GS - Returning result metadata...
üêõ DEBUG [23:14:48.438]: Main received result for GS
üêõ DEBUG: Training progress: 388/959 done
üêõ DEBUG: train_worker started for CBRL
  ‚öôÔ∏è Training models for CBRL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CBRL: Initiating feature extraction for training.
  [DIAGNOSTIC] CBRL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CBRL: rows after features available: 126
üéØ CBRL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CBRL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CBRL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CBRL: Training LSTM (50 epochs)...
      ‚è≥ CBRL LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.192390
         RMSE: 0.438622
         R¬≤ Score: -0.4126 (Poor - 41.3% variance explained)
      üîπ SSP: Training TCN (50 epochs)...
      ‚è≥ SSP TCN: Epoch 10/50 (20%)
      ‚è≥ SSP TCN: Epoch 20/50 (40%)
      ‚è≥ SSP TCN: Epoch 30/50 (60%)
      ‚è≥ SSP TCN: Epoch 40/50 (80%)
      ‚è≥ CBRL LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.144015
         RMSE: 0.379493
         R¬≤ Score: -0.0574
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SSP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SSP Random Forest: Starting GridSearchCV fit...
      ‚è≥ CBRL LSTM: Epoch 30/50 (60%)
      ‚è≥ CBRL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.708542
         RMSE: 0.841750
         R¬≤ Score: -1.5181 (Poor - 151.8% variance explained)
      üîπ CBRL: Training TCN (50 epochs)...
      ‚è≥ CBRL TCN: Epoch 10/50 (20%)
      ‚è≥ CBRL TCN: Epoch 20/50 (40%)
      ‚è≥ CBRL TCN: Epoch 30/50 (60%)
      ‚è≥ CBRL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.286406
         RMSE: 0.535169
         R¬≤ Score: -0.0179
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CBRL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CBRL Random Forest: Starting GridSearchCV fit...
       ‚úÖ SSP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=198.2288 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SSP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WRBY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.6344 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.8s
    - LSTM: MSE=0.7047
    - TCN: MSE=0.7285
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7047
        ‚Ä¢ TCN: MSE=0.7285
        ‚Ä¢ Random Forest: MSE=23.6893
        ‚Ä¢ XGBoost: MSE=24.6344
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.5673
   ‚úÖ WRBY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WRBY (TargetReturn): LSTM with MSE=0.7047
üêõ DEBUG: WRBY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WRBY.
üêõ DEBUG: WRBY - Moving model to CPU before return...
üêõ DEBUG [23:14:52.651]: WRBY - Returning result metadata...
üêõ DEBUG: train_worker started for VIRT
üêõ DEBUG [23:14:52.652]: Main received result for WRBY
  ‚öôÔ∏è Training models for VIRT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - VIRT: Initiating feature extraction for training.
  [DIAGNOSTIC] VIRT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VIRT: rows after features available: 126
üéØ VIRT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VIRT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VIRT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VIRT: Training LSTM (50 epochs)...
       ‚úÖ SSP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=173.3028 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SSP XGBoost: Starting GridSearchCV fit...
      ‚è≥ VIRT LSTM: Epoch 10/50 (20%)
      ‚è≥ VIRT LSTM: Epoch 20/50 (40%)
       ‚úÖ CBRL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.9099 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CBRL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ VIRT LSTM: Epoch 30/50 (60%)
       ‚úÖ CBRL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=48.4936 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CBRL XGBoost: Starting GridSearchCV fit...
      ‚è≥ VIRT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.162334
         RMSE: 0.402907
         R¬≤ Score: -0.4482 (Poor - 44.8% variance explained)
      üîπ VIRT: Training TCN (50 epochs)...
      ‚è≥ VIRT TCN: Epoch 10/50 (20%)
      ‚è≥ VIRT TCN: Epoch 20/50 (40%)
      ‚è≥ VIRT TCN: Epoch 30/50 (60%)
      ‚è≥ VIRT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.139480
         RMSE: 0.373470
         R¬≤ Score: -0.2443
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VIRT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VIRT Random Forest: Starting GridSearchCV fit...
       ‚úÖ VIRT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.9230 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VIRT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VIRT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.3776 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VIRT XGBoost: Starting GridSearchCV fit...
       ‚úÖ UGI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.2795 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.0439
    - TCN: MSE=0.0233
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0233
        ‚Ä¢ LSTM: MSE=0.0439
        ‚Ä¢ Random Forest: MSE=3.6605
        ‚Ä¢ XGBoost: MSE=4.2795
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7809
   ‚úÖ UGI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UGI (TargetReturn): TCN with MSE=0.0233
üêõ DEBUG: UGI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UGI.
üêõ DEBUG: UGI - Moving model to CPU before return...
üêõ DEBUG [23:15:10.082]: UGI - Returning result metadata...
üêõ DEBUG: train_worker started for KMDA
  ‚öôÔ∏è Training models for KMDA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - KMDA: Initiating feature extraction for training.
  [DIAGNOSTIC] KMDA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KMDA: rows after features available: 126
üéØ KMDA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KMDA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KMDA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KMDA: Training LSTM (50 epochs)...
      ‚è≥ KMDA LSTM: Epoch 10/50 (20%)
       ‚úÖ BB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=69.2492 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.1009
    - TCN: MSE=0.0986
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0986
        ‚Ä¢ LSTM: MSE=0.1009
        ‚Ä¢ Random Forest: MSE=60.0453
        ‚Ä¢ XGBoost: MSE=69.2492
        ‚Ä¢ LightGBM Regressor (CPU): MSE=99.0442
   ‚úÖ BB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BB (TargetReturn): TCN with MSE=0.0986
üêõ DEBUG: BB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BB.
üêõ DEBUG: BB - Moving model to CPU before return...
üêõ DEBUG [23:15:11.161]: BB - Returning result metadata...
üêõ DEBUG: train_worker started for EME
      ‚è≥ KMDA LSTM: Epoch 20/50 (40%)
üêõ DEBUG [23:15:11.163]: Main received result for BB
üêõ DEBUG [23:15:11.163]: Main received result for UGI
  ‚öôÔ∏è Training models for EME (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - EME: Initiating feature extraction for training.
  [DIAGNOSTIC] EME: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EME: rows after features available: 126
üéØ EME: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EME: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EME: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EME: Training LSTM (50 epochs)...
      ‚è≥ EME LSTM: Epoch 10/50 (20%)
      ‚è≥ KMDA LSTM: Epoch 30/50 (60%)
      ‚è≥ EME LSTM: Epoch 20/50 (40%)
      ‚è≥ KMDA LSTM: Epoch 40/50 (80%)
      ‚è≥ EME LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.334888
         RMSE: 0.578695
         R¬≤ Score: -0.6488 (Poor - 64.9% variance explained)
      üîπ KMDA: Training TCN (50 epochs)...
      ‚è≥ KMDA TCN: Epoch 10/50 (20%)
      ‚è≥ KMDA TCN: Epoch 20/50 (40%)
      ‚è≥ EME LSTM: Epoch 40/50 (80%)
      ‚è≥ KMDA TCN: Epoch 30/50 (60%)
      ‚è≥ KMDA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.424854
         RMSE: 0.651808
         R¬≤ Score: -1.0917
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KMDA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KMDA Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.521796
         RMSE: 0.722354
         R¬≤ Score: -1.4707 (Poor - 147.1% variance explained)
      üîπ EME: Training TCN (50 epochs)...
      ‚è≥ EME TCN: Epoch 10/50 (20%)
      ‚è≥ EME TCN: Epoch 20/50 (40%)
      ‚è≥ EME TCN: Epoch 30/50 (60%)
      ‚è≥ EME TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.241236
         RMSE: 0.491158
         R¬≤ Score: -0.1422
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EME: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EME Random Forest: Starting GridSearchCV fit...
       ‚úÖ CALM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.7809 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.2891
    - TCN: MSE=0.2926
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2891
        ‚Ä¢ TCN: MSE=0.2926
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.7922
        ‚Ä¢ Random Forest: MSE=22.8064
        ‚Ä¢ XGBoost: MSE=26.7809
   ‚úÖ CALM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CALM (TargetReturn): LSTM with MSE=0.2891
üêõ DEBUG: CALM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CALM.
üêõ DEBUG: CALM - Moving model to CPU before return...
üêõ DEBUG [23:15:14.811]: CALM - Returning result metadata...
üêõ DEBUG [23:15:14.811]: Main received result for CALM
üêõ DEBUG: Training progress: 392/959 done
üêõ DEBUG: train_worker started for CW
  ‚öôÔ∏è Training models for CW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - CW: Initiating feature extraction for training.
  [DIAGNOSTIC] CW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CW: rows after features available: 126
üéØ CW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CW: Training LSTM (50 epochs)...
      ‚è≥ CW LSTM: Epoch 10/50 (20%)
      ‚è≥ CW LSTM: Epoch 20/50 (40%)
      ‚è≥ CW LSTM: Epoch 30/50 (60%)
       ‚úÖ KMDA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.8151 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KMDA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CW LSTM: Epoch 40/50 (80%)
       ‚úÖ EME Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.4139 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EME LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.467919
         RMSE: 0.684046
         R¬≤ Score: -1.2059 (Poor - 120.6% variance explained)
      üîπ CW: Training TCN (50 epochs)...
       ‚úÖ KMDA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.5310 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KMDA XGBoost: Starting GridSearchCV fit...
      ‚è≥ CW TCN: Epoch 10/50 (20%)
      ‚è≥ CW TCN: Epoch 20/50 (40%)
      ‚è≥ CW TCN: Epoch 30/50 (60%)
      ‚è≥ CW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.362046
         RMSE: 0.601703
         R¬≤ Score: -0.7068
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CW Random Forest: Starting GridSearchCV fit...
       ‚úÖ EME LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=31.3906 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EME XGBoost: Starting GridSearchCV fit...
       ‚úÖ OSIS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.1271 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.1393
    - TCN: MSE=0.0908
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0908
        ‚Ä¢ LSTM: MSE=0.1393
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6609
        ‚Ä¢ Random Forest: MSE=19.6761
        ‚Ä¢ XGBoost: MSE=25.1271
   ‚úÖ OSIS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OSIS (TargetReturn): TCN with MSE=0.0908
üêõ DEBUG: OSIS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OSIS.
üêõ DEBUG: OSIS - Moving model to CPU before return...
üêõ DEBUG [23:15:18.642]: OSIS - Returning result metadata...
üêõ DEBUG: train_worker started for NTRS
üêõ DEBUG [23:15:18.643]: Main received result for OSIS
  ‚öôÔ∏è Training models for NTRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - NTRS: Initiating feature extraction for training.
  [DIAGNOSTIC] NTRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTRS: rows after features available: 126
üéØ NTRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTRS: Training LSTM (50 epochs)...
      ‚è≥ NTRS LSTM: Epoch 10/50 (20%)
      ‚è≥ NTRS LSTM: Epoch 20/50 (40%)
      ‚è≥ NTRS LSTM: Epoch 30/50 (60%)
       ‚úÖ CW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2104 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NTRS LSTM: Epoch 40/50 (80%)
       ‚úÖ RMBS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=24.7285 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.6105
    - TCN: MSE=0.5385
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5385
        ‚Ä¢ LSTM: MSE=0.6105
        ‚Ä¢ XGBoost: MSE=24.7285
        ‚Ä¢ Random Forest: MSE=29.2568
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.0841
   ‚úÖ RMBS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RMBS (TargetReturn): TCN with MSE=0.5385
üêõ DEBUG: RMBS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RMBS.
üêõ DEBUG: RMBS - Moving model to CPU before return...
üêõ DEBUG [23:15:20.903]: RMBS - Returning result metadata...
üêõ DEBUG [23:15:20.903]: Main received result for RMBS
üêõ DEBUG: train_worker started for CORT
  ‚öôÔ∏è Training models for CORT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - CORT: Initiating feature extraction for training.
  [DIAGNOSTIC] CORT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CORT: rows after features available: 126
üéØ CORT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CORT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CORT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CORT: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.807064
         RMSE: 0.898367
         R¬≤ Score: -1.3009 (Poor - 130.1% variance explained)
      üîπ NTRS: Training TCN (50 epochs)...
      ‚è≥ NTRS TCN: Epoch 10/50 (20%)
      ‚è≥ NTRS TCN: Epoch 20/50 (40%)
      ‚è≥ CORT LSTM: Epoch 10/50 (20%)
       ‚úÖ CW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.1044 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CW XGBoost: Starting GridSearchCV fit...
      ‚è≥ NTRS TCN: Epoch 30/50 (60%)
      ‚è≥ NTRS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.689455
         RMSE: 0.830334
         R¬≤ Score: -0.9656
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTRS Random Forest: Starting GridSearchCV fit...
      ‚è≥ CORT LSTM: Epoch 20/50 (40%)
      ‚è≥ CORT LSTM: Epoch 30/50 (60%)
      ‚è≥ CORT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.067737
         RMSE: 0.260263
         R¬≤ Score: -0.9243 (Poor - 92.4% variance explained)
      üîπ CORT: Training TCN (50 epochs)...
      ‚è≥ CORT TCN: Epoch 10/50 (20%)
      ‚è≥ CORT TCN: Epoch 20/50 (40%)
      ‚è≥ CORT TCN: Epoch 30/50 (60%)
      ‚è≥ CORT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.043838
         RMSE: 0.209375
         R¬≤ Score: -0.2454
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CORT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CORT Random Forest: Starting GridSearchCV fit...
       ‚úÖ NTRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.2626 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NTRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.9710 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CORT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=205.0387 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CORT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CORT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=120.5318 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CORT XGBoost: Starting GridSearchCV fit...
       ‚úÖ CELH XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=59.4160 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.1226
    - TCN: MSE=0.0883
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0883
        ‚Ä¢ LSTM: MSE=0.1226
        ‚Ä¢ XGBoost: MSE=59.4160
        ‚Ä¢ LightGBM Regressor (CPU): MSE=64.6319
        ‚Ä¢ Random Forest: MSE=81.0837
   ‚úÖ CELH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CELH (TargetReturn): TCN with MSE=0.0883
üêõ DEBUG: CELH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CELH.
üêõ DEBUG: CELH - Moving model to CPU before return...
üêõ DEBUG [23:15:29.289]: CELH - Returning result metadata...
üêõ DEBUG: train_worker started for TFPM
üêõ DEBUG [23:15:29.296]: Main received result for CELH
  ‚öôÔ∏è Training models for TFPM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - TFPM: Initiating feature extraction for training.
  [DIAGNOSTIC] TFPM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TFPM: rows after features available: 126
üéØ TFPM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TFPM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TFPM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TFPM: Training LSTM (50 epochs)...
      ‚è≥ TFPM LSTM: Epoch 10/50 (20%)
       ‚úÖ UNTY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.0354 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 116.5s
    - LSTM: MSE=0.8314
    - TCN: MSE=0.5082
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5082
        ‚Ä¢ LSTM: MSE=0.8314
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8618
        ‚Ä¢ Random Forest: MSE=12.8833
        ‚Ä¢ XGBoost: MSE=14.0354
   ‚úÖ UNTY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UNTY (TargetReturn): TCN with MSE=0.5082
üêõ DEBUG: UNTY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UNTY.
üêõ DEBUG: UNTY - Moving model to CPU before return...
üêõ DEBUG [23:15:30.070]: UNTY - Returning result metadata...
üêõ DEBUG [23:15:30.071]: Main received result for UNTY
üêõ DEBUG: Training progress: 396/959 done
üêõ DEBUG: train_worker started for SUPV
  ‚öôÔ∏è Training models for SUPV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - SUPV: Initiating feature extraction for training.
  [DIAGNOSTIC] SUPV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SUPV: rows after features available: 126
üéØ SUPV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SUPV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SUPV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SUPV: Training LSTM (50 epochs)...
      ‚è≥ TFPM LSTM: Epoch 20/50 (40%)
      ‚è≥ SUPV LSTM: Epoch 10/50 (20%)
      ‚è≥ TFPM LSTM: Epoch 30/50 (60%)
      ‚è≥ SUPV LSTM: Epoch 20/50 (40%)
      ‚è≥ TFPM LSTM: Epoch 40/50 (80%)
      ‚è≥ SUPV LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.143899
         RMSE: 0.379340
         R¬≤ Score: -0.3064 (Poor - 30.6% variance explained)
      üîπ TFPM: Training TCN (50 epochs)...
      ‚è≥ TFPM TCN: Epoch 10/50 (20%)
      ‚è≥ SUPV LSTM: Epoch 40/50 (80%)
      ‚è≥ TFPM TCN: Epoch 20/50 (40%)
      ‚è≥ TFPM TCN: Epoch 30/50 (60%)
      ‚è≥ TFPM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.110761
         RMSE: 0.332808
         R¬≤ Score: -0.0056
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TFPM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TFPM Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.100478
         RMSE: 0.316983
         R¬≤ Score: -0.7566 (Poor - 75.7% variance explained)
      üîπ SUPV: Training TCN (50 epochs)...
      ‚è≥ SUPV TCN: Epoch 10/50 (20%)
      ‚è≥ SUPV TCN: Epoch 20/50 (40%)
      ‚è≥ SUPV TCN: Epoch 30/50 (60%)
      ‚è≥ SUPV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.079296
         RMSE: 0.281596
         R¬≤ Score: -0.3863
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SUPV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SUPV Random Forest: Starting GridSearchCV fit...
       ‚úÖ TFPM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.5263 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TFPM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TFPM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.4983 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TFPM XGBoost: Starting GridSearchCV fit...
       ‚úÖ SUPV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.2964 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SUPV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SUPV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=94.1731 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SUPV XGBoost: Starting GridSearchCV fit...
       ‚úÖ ESE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.6480 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.2920
    - TCN: MSE=0.1545
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1545
        ‚Ä¢ LSTM: MSE=0.2920
        ‚Ä¢ Random Forest: MSE=16.7473
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.1879
        ‚Ä¢ XGBoost: MSE=21.6480
   ‚úÖ ESE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ESE (TargetReturn): TCN with MSE=0.1545
üêõ DEBUG: ESE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ESE.
üêõ DEBUG: ESE - Moving model to CPU before return...
üêõ DEBUG [23:15:51.316]: ESE - Returning result metadata...
üêõ DEBUG [23:15:51.316]: Main received result for ESE
üêõ DEBUG: train_worker started for DRD
  ‚öôÔ∏è Training models for DRD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - DRD: Initiating feature extraction for training.
  [DIAGNOSTIC] DRD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DRD: rows after features available: 126
üéØ DRD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DRD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DRD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DRD: Training LSTM (50 epochs)...
      ‚è≥ DRD LSTM: Epoch 10/50 (20%)
      ‚è≥ DRD LSTM: Epoch 20/50 (40%)
      ‚è≥ DRD LSTM: Epoch 30/50 (60%)
      ‚è≥ DRD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.324463
         RMSE: 0.569616
         R¬≤ Score: -0.6400 (Poor - 64.0% variance explained)
      üîπ DRD: Training TCN (50 epochs)...
      ‚è≥ DRD TCN: Epoch 10/50 (20%)
      ‚è≥ DRD TCN: Epoch 20/50 (40%)
      ‚è≥ DRD TCN: Epoch 30/50 (60%)
      ‚è≥ DRD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.290937
         RMSE: 0.539386
         R¬≤ Score: -0.4705
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DRD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DRD Random Forest: Starting GridSearchCV fit...
       ‚úÖ DRD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.9415 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DRD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DRD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=40.5051 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DRD XGBoost: Starting GridSearchCV fit...
       ‚úÖ SYF XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.1865 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.8501
    - TCN: MSE=0.6299
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6299
        ‚Ä¢ LSTM: MSE=0.8501
        ‚Ä¢ XGBoost: MSE=13.1865
        ‚Ä¢ Random Forest: MSE=14.2754
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.6156
   ‚úÖ SYF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SYF (TargetReturn): TCN with MSE=0.6299
üêõ DEBUG: SYF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SYF.
üêõ DEBUG: SYF - Moving model to CPU before return...
üêõ DEBUG [23:15:57.473]: SYF - Returning result metadata...
üêõ DEBUG [23:15:57.474]: Main received result for SYF
üêõ DEBUG: train_worker started for ACMR
  ‚öôÔ∏è Training models for ACMR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ACMR: Initiating feature extraction for training.
  [DIAGNOSTIC] ACMR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ACMR: rows after features available: 126
üéØ ACMR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ACMR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ACMR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ACMR: Training LSTM (50 epochs)...
      ‚è≥ ACMR LSTM: Epoch 10/50 (20%)
      ‚è≥ ACMR LSTM: Epoch 20/50 (40%)
      ‚è≥ ACMR LSTM: Epoch 30/50 (60%)
      ‚è≥ ACMR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.431682
         RMSE: 0.657025
         R¬≤ Score: -1.1803 (Poor - 118.0% variance explained)
      üîπ ACMR: Training TCN (50 epochs)...
      ‚è≥ ACMR TCN: Epoch 10/50 (20%)
      ‚è≥ ACMR TCN: Epoch 20/50 (40%)
      ‚è≥ ACMR TCN: Epoch 30/50 (60%)
      ‚è≥ ACMR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.374161
         RMSE: 0.611687
         R¬≤ Score: -0.8898
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ACMR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ACMR Random Forest: Starting GridSearchCV fit...
       ‚úÖ ACMR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=104.2439 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ACMR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ACMR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=90.2600 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ACMR XGBoost: Starting GridSearchCV fit...
       ‚úÖ NTES XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.3735 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.4848
    - TCN: MSE=0.4541
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4541
        ‚Ä¢ LSTM: MSE=0.4848
        ‚Ä¢ Random Forest: MSE=20.3625
        ‚Ä¢ XGBoost: MSE=24.3735
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.3712
   ‚úÖ NTES: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTES (TargetReturn): TCN with MSE=0.4541
üêõ DEBUG: NTES - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTES.
üêõ DEBUG: NTES - Moving model to CPU before return...
üêõ DEBUG [23:16:33.440]: NTES - Returning result metadata...
üêõ DEBUG: train_worker started for INBX
üêõ DEBUG [23:16:33.441]: Main received result for NTES
  ‚öôÔ∏è Training models for INBX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - INBX: Initiating feature extraction for training.
  [DIAGNOSTIC] INBX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ INBX: rows after features available: 126
üéØ INBX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] INBX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö INBX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ INBX: Training LSTM (50 epochs)...
      ‚è≥ INBX LSTM: Epoch 10/50 (20%)
      ‚è≥ INBX LSTM: Epoch 20/50 (40%)
      ‚è≥ INBX LSTM: Epoch 30/50 (60%)
      ‚è≥ INBX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.735397
         RMSE: 0.857553
         R¬≤ Score: -1.1332 (Poor - 113.3% variance explained)
      üîπ INBX: Training TCN (50 epochs)...
      ‚è≥ INBX TCN: Epoch 10/50 (20%)
      ‚è≥ INBX TCN: Epoch 20/50 (40%)
      ‚è≥ INBX TCN: Epoch 30/50 (60%)
      ‚è≥ INBX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.624370
         RMSE: 0.790171
         R¬≤ Score: -0.8112
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä INBX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ INBX Random Forest: Starting GridSearchCV fit...
       ‚úÖ INBX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.3050 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ INBX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CHEF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.5378 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.2039
    - TCN: MSE=0.1281
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1281
        ‚Ä¢ LSTM: MSE=0.2039
        ‚Ä¢ Random Forest: MSE=19.1180
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.9850
        ‚Ä¢ XGBoost: MSE=23.5378
   ‚úÖ CHEF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CHEF (TargetReturn): TCN with MSE=0.1281
üêõ DEBUG: CHEF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CHEF.
üêõ DEBUG: CHEF - Moving model to CPU before return...
üêõ DEBUG [23:16:39.372]: CHEF - Returning result metadata...
üêõ DEBUG [23:16:39.372]: Main received result for CHEF
üêõ DEBUG: Training progress: 400/959 done
üêõ DEBUG: train_worker started for FSM
  ‚öôÔ∏è Training models for FSM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - FSM: Initiating feature extraction for training.
  [DIAGNOSTIC] FSM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FSM: rows after features available: 126
üéØ FSM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FSM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FSM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FSM: Training LSTM (50 epochs)...
       ‚úÖ INBX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=43.8704 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ INBX XGBoost: Starting GridSearchCV fit...
      ‚è≥ FSM LSTM: Epoch 10/50 (20%)
      ‚è≥ FSM LSTM: Epoch 20/50 (40%)
      ‚è≥ FSM LSTM: Epoch 30/50 (60%)
      ‚è≥ FSM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.276140
         RMSE: 0.525490
         R¬≤ Score: -1.0290 (Poor - 102.9% variance explained)
      üîπ FSM: Training TCN (50 epochs)...
      ‚è≥ FSM TCN: Epoch 10/50 (20%)
      ‚è≥ FSM TCN: Epoch 20/50 (40%)
      ‚è≥ FSM TCN: Epoch 30/50 (60%)
      ‚è≥ FSM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.152151
         RMSE: 0.390065
         R¬≤ Score: -0.1180
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FSM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FSM Random Forest: Starting GridSearchCV fit...
       ‚úÖ FSM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.6157 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FSM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FSM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=47.6364 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FSM XGBoost: Starting GridSearchCV fit...
       ‚úÖ CNM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.2474 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.5848
    - TCN: MSE=0.4629
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4629
        ‚Ä¢ LSTM: MSE=0.5848
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.0239
        ‚Ä¢ Random Forest: MSE=13.9842
        ‚Ä¢ XGBoost: MSE=15.2474
   ‚úÖ CNM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CNM (TargetReturn): TCN with MSE=0.4629
üêõ DEBUG: CNM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CNM.
üêõ DEBUG: CNM - Moving model to CPU before return...
üêõ DEBUG [23:16:46.171]: CNM - Returning result metadata...
üêõ DEBUG: train_worker started for HUYA
üêõ DEBUG [23:16:46.173]: Main received result for CNM
  ‚öôÔ∏è Training models for HUYA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - HUYA: Initiating feature extraction for training.
  [DIAGNOSTIC] HUYA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HUYA: rows after features available: 126
üéØ HUYA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HUYA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HUYA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HUYA: Training LSTM (50 epochs)...
      ‚è≥ HUYA LSTM: Epoch 10/50 (20%)
      ‚è≥ HUYA LSTM: Epoch 20/50 (40%)
      ‚è≥ HUYA LSTM: Epoch 30/50 (60%)
      ‚è≥ HUYA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.582028
         RMSE: 0.762908
         R¬≤ Score: -1.2125 (Poor - 121.3% variance explained)
      üîπ HUYA: Training TCN (50 epochs)...
      ‚è≥ HUYA TCN: Epoch 10/50 (20%)
      ‚è≥ HUYA TCN: Epoch 20/50 (40%)
      ‚è≥ HUYA TCN: Epoch 30/50 (60%)
      ‚è≥ HUYA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.423736
         RMSE: 0.650950
         R¬≤ Score: -0.6108
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HUYA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HUYA Random Forest: Starting GridSearchCV fit...
       ‚úÖ CBRL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=34.2362 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.7085
    - TCN: MSE=0.2864
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2864
        ‚Ä¢ LSTM: MSE=0.7085
        ‚Ä¢ XGBoost: MSE=34.2362
        ‚Ä¢ Random Forest: MSE=44.9099
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.4936
   ‚úÖ CBRL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CBRL (TargetReturn): TCN with MSE=0.2864
üêõ DEBUG: CBRL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CBRL.
üêõ DEBUG: CBRL - Moving model to CPU before return...
üêõ DEBUG [23:16:50.802]: CBRL - Returning result metadata...
üêõ DEBUG: train_worker started for OPLN
  ‚öôÔ∏è Training models for OPLN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - OPLN: Initiating feature extraction for training.
  [DIAGNOSTIC] OPLN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPLN: rows after features available: 126
üéØ OPLN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPLN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPLN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPLN: Training LSTM (50 epochs)...
      ‚è≥ OPLN LSTM: Epoch 10/50 (20%)
       ‚úÖ SSP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=222.6622 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.1924
    - TCN: MSE=0.1440
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1440
        ‚Ä¢ LSTM: MSE=0.1924
        ‚Ä¢ LightGBM Regressor (CPU): MSE=173.3028
        ‚Ä¢ Random Forest: MSE=198.2288
        ‚Ä¢ XGBoost: MSE=222.6622
   ‚úÖ SSP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SSP (TargetReturn): TCN with MSE=0.1440
üêõ DEBUG: SSP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SSP.
üêõ DEBUG: SSP - Moving model to CPU before return...
üêõ DEBUG [23:16:51.555]: SSP - Returning result metadata...
üêõ DEBUG: train_worker started for JOYY
üêõ DEBUG [23:16:51.556]: Main received result for SSP
üêõ DEBUG [23:16:51.556]: Main received result for CBRL
  ‚öôÔ∏è Training models for JOYY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - JOYY: Initiating feature extraction for training.
  [DIAGNOSTIC] JOYY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JOYY: rows after features available: 126
üéØ JOYY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JOYY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JOYY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JOYY: Training LSTM (50 epochs)...
      ‚è≥ OPLN LSTM: Epoch 20/50 (40%)
       ‚úÖ HUYA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=117.9495 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HUYA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ JOYY LSTM: Epoch 10/50 (20%)
      ‚è≥ OPLN LSTM: Epoch 30/50 (60%)
      ‚è≥ JOYY LSTM: Epoch 20/50 (40%)
      ‚è≥ OPLN LSTM: Epoch 40/50 (80%)
       ‚úÖ HUYA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=54.6798 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HUYA XGBoost: Starting GridSearchCV fit...
      ‚è≥ JOYY LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.467494
         RMSE: 0.683735
         R¬≤ Score: -0.4269 (Poor - 42.7% variance explained)
      üîπ OPLN: Training TCN (50 epochs)...
      ‚è≥ OPLN TCN: Epoch 10/50 (20%)
      ‚è≥ OPLN TCN: Epoch 20/50 (40%)
      ‚è≥ OPLN TCN: Epoch 30/50 (60%)
      ‚è≥ OPLN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.454284
         RMSE: 0.674006
         R¬≤ Score: -0.3866
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPLN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPLN Random Forest: Starting GridSearchCV fit...
      ‚è≥ JOYY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.566323
         RMSE: 0.752545
         R¬≤ Score: -0.6928 (Poor - 69.3% variance explained)
      üîπ JOYY: Training TCN (50 epochs)...
      ‚è≥ JOYY TCN: Epoch 10/50 (20%)
      ‚è≥ JOYY TCN: Epoch 20/50 (40%)
      ‚è≥ JOYY TCN: Epoch 30/50 (60%)
      ‚è≥ JOYY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.707350
         RMSE: 0.841041
         R¬≤ Score: -1.1144
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JOYY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JOYY Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPLN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.4696 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPLN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OPLN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.2139 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPLN XGBoost: Starting GridSearchCV fit...
       ‚úÖ JOYY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.7064 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JOYY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JOYY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.6333 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JOYY XGBoost: Starting GridSearchCV fit...
       ‚úÖ VIRT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.5458 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.1623
    - TCN: MSE=0.1395
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1395
        ‚Ä¢ LSTM: MSE=0.1623
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.3776
        ‚Ä¢ Random Forest: MSE=18.9230
        ‚Ä¢ XGBoost: MSE=26.5458
   ‚úÖ VIRT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VIRT (TargetReturn): TCN with MSE=0.1395
üêõ DEBUG: VIRT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VIRT.
üêõ DEBUG: VIRT - Moving model to CPU before return...
üêõ DEBUG [23:16:59.892]: VIRT - Returning result metadata...
üêõ DEBUG: train_worker started for MUFG
üêõ DEBUG [23:16:59.899]: Main received result for VIRT
üêõ DEBUG: Training progress: 404/959 done
  ‚öôÔ∏è Training models for MUFG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - MUFG: Initiating feature extraction for training.
  [DIAGNOSTIC] MUFG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MUFG: rows after features available: 126
üéØ MUFG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MUFG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MUFG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MUFG: Training LSTM (50 epochs)...
      ‚è≥ MUFG LSTM: Epoch 10/50 (20%)
      ‚è≥ MUFG LSTM: Epoch 20/50 (40%)
      ‚è≥ MUFG LSTM: Epoch 30/50 (60%)
      ‚è≥ MUFG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.313716
         RMSE: 0.560104
         R¬≤ Score: -0.9131 (Poor - 91.3% variance explained)
      üîπ MUFG: Training TCN (50 epochs)...
      ‚è≥ MUFG TCN: Epoch 10/50 (20%)
      ‚è≥ MUFG TCN: Epoch 20/50 (40%)
      ‚è≥ MUFG TCN: Epoch 30/50 (60%)
      ‚è≥ MUFG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.232546
         RMSE: 0.482231
         R¬≤ Score: -0.4181
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MUFG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MUFG Random Forest: Starting GridSearchCV fit...
       ‚úÖ MUFG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.2641 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MUFG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MUFG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.2201 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MUFG XGBoost: Starting GridSearchCV fit...
       ‚úÖ KMDA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.1393 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.3349
    - TCN: MSE=0.4249
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3349
        ‚Ä¢ TCN: MSE=0.4249
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.5310
        ‚Ä¢ Random Forest: MSE=27.8151
        ‚Ä¢ XGBoost: MSE=30.1393
   ‚úÖ KMDA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KMDA (TargetReturn): LSTM with MSE=0.3349
üêõ DEBUG: KMDA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KMDA.
üêõ DEBUG: KMDA - Moving model to CPU before return...
üêõ DEBUG [23:17:14.411]: KMDA - Returning result metadata...
üêõ DEBUG [23:17:14.411]: Main received result for KMDAüêõ DEBUG: train_worker started for GILT

  ‚öôÔ∏è Training models for GILT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - GILT: Initiating feature extraction for training.
  [DIAGNOSTIC] GILT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GILT: rows after features available: 126
üéØ GILT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
       ‚úÖ EME XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.8761 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.5218
    - TCN: MSE=0.2412
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2412
        ‚Ä¢ LSTM: MSE=0.5218
        ‚Ä¢ Random Forest: MSE=24.4139
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.3906
        ‚Ä¢ XGBoost: MSE=31.8761
   ‚úÖ EME: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EME (TargetReturn): TCN with MSE=0.2412
üêõ DEBUG: EME - train_and_evaluate_models completed
  [DIAGNOSTIC] GILT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GILT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GILT: Training LSTM (50 epochs)...
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EME.
üêõ DEBUG: EME - Moving model to CPU before return...
üêõ DEBUG [23:17:14.448]: EME - Returning result metadata...
üêõ DEBUG [23:17:14.449]: Main received result for EME
üêõ DEBUG: train_worker started for PFIX
  ‚öôÔ∏è Training models for PFIX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - PFIX: Initiating feature extraction for training.
  [DIAGNOSTIC] PFIX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PFIX: rows after features available: 126
üéØ PFIX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PFIX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PFIX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PFIX: Training LSTM (50 epochs)...
      ‚è≥ GILT LSTM: Epoch 10/50 (20%)
      ‚è≥ PFIX LSTM: Epoch 10/50 (20%)
      ‚è≥ GILT LSTM: Epoch 20/50 (40%)
      ‚è≥ PFIX LSTM: Epoch 20/50 (40%)
      ‚è≥ PFIX LSTM: Epoch 30/50 (60%)
      ‚è≥ GILT LSTM: Epoch 30/50 (60%)
      ‚è≥ GILT LSTM: Epoch 40/50 (80%)
      ‚è≥ PFIX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.388230
         RMSE: 0.623081
         R¬≤ Score: -0.7524 (Poor - 75.2% variance explained)
      üîπ GILT: Training TCN (50 epochs)...
      ‚è≥ GILT TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.219336
         RMSE: 0.468334
         R¬≤ Score: -1.0872 (Poor - 108.7% variance explained)
      üîπ PFIX: Training TCN (50 epochs)...
      ‚è≥ GILT TCN: Epoch 20/50 (40%)
      ‚è≥ PFIX TCN: Epoch 10/50 (20%)
      ‚è≥ GILT TCN: Epoch 30/50 (60%)
      ‚è≥ PFIX TCN: Epoch 20/50 (40%)
      ‚è≥ PFIX TCN: Epoch 30/50 (60%)
      ‚è≥ GILT TCN: Epoch 40/50 (80%)
      ‚è≥ PFIX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.332334
         RMSE: 0.576484
         R¬≤ Score: -0.5001
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GILT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GILT Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.105177
         RMSE: 0.324310
         R¬≤ Score: -0.0008
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PFIX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PFIX Random Forest: Starting GridSearchCV fit...
       ‚úÖ CW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.7663 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.4679
    - TCN: MSE=0.3620
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3620
        ‚Ä¢ LSTM: MSE=0.4679
        ‚Ä¢ Random Forest: MSE=14.2104
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.1044
        ‚Ä¢ XGBoost: MSE=32.7663
   ‚úÖ CW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CW (TargetReturn): TCN with MSE=0.3620
üêõ DEBUG: CW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CW.
üêõ DEBUG: CW - Moving model to CPU before return...
üêõ DEBUG [23:17:20.083]: CW - Returning result metadata...
üêõ DEBUG [23:17:20.083]: Main received result for CW
üêõ DEBUG: train_worker started for AHR
  ‚öôÔ∏è Training models for AHR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - AHR: Initiating feature extraction for training.
  [DIAGNOSTIC] AHR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AHR: rows after features available: 126
üéØ AHR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AHR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AHR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AHR: Training LSTM (50 epochs)...
       ‚úÖ GILT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.0343 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GILT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PFIX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.7906 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PFIX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AHR LSTM: Epoch 10/50 (20%)
       ‚úÖ GILT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.8289 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GILT XGBoost: Starting GridSearchCV fit...
      ‚è≥ AHR LSTM: Epoch 20/50 (40%)
       ‚úÖ PFIX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.2384 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PFIX XGBoost: Starting GridSearchCV fit...
      ‚è≥ AHR LSTM: Epoch 30/50 (60%)
      ‚è≥ AHR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.247581
         RMSE: 0.497575
         R¬≤ Score: -1.2878 (Poor - 128.8% variance explained)
      üîπ AHR: Training TCN (50 epochs)...
      ‚è≥ AHR TCN: Epoch 10/50 (20%)
      ‚è≥ AHR TCN: Epoch 20/50 (40%)
      ‚è≥ AHR TCN: Epoch 30/50 (60%)
      ‚è≥ AHR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.110698
         RMSE: 0.332712
         R¬≤ Score: -0.0229
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AHR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AHR Random Forest: Starting GridSearchCV fit...
       ‚úÖ AHR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.8641 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AHR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AHR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.9570 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AHR XGBoost: Starting GridSearchCV fit...
       ‚úÖ CORT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=266.7443 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 120.2s
    - LSTM: MSE=0.0677
    - TCN: MSE=0.0438
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0438
        ‚Ä¢ LSTM: MSE=0.0677
        ‚Ä¢ LightGBM Regressor (CPU): MSE=120.5318
        ‚Ä¢ Random Forest: MSE=205.0387
        ‚Ä¢ XGBoost: MSE=266.7443
   ‚úÖ CORT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CORT (TargetReturn): TCN with MSE=0.0438
üêõ DEBUG: CORT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CORT.
üêõ DEBUG: CORT - Moving model to CPU before return...
üêõ DEBUG [23:17:27.431]: CORT - Returning result metadata...
üêõ DEBUG: train_worker started for SERV
  ‚öôÔ∏è Training models for SERV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - SERV: Initiating feature extraction for training.
  [DIAGNOSTIC] SERV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SERV: rows after features available: 126
üéØ SERV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SERV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SERV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SERV: Training LSTM (50 epochs)...
       ‚úÖ NTRS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.6846 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.8071
    - TCN: MSE=0.6895
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6895
        ‚Ä¢ LSTM: MSE=0.8071
        ‚Ä¢ XGBoost: MSE=14.6846
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.9710
        ‚Ä¢ Random Forest: MSE=17.2626
   ‚úÖ NTRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTRS (TargetReturn): TCN with MSE=0.6895
üêõ DEBUG: NTRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTRS.
üêõ DEBUG: NTRS - Moving model to CPU before return...
üêõ DEBUG [23:17:27.872]: NTRS - Returning result metadata...
üêõ DEBUG [23:17:27.872]: Main received result for NTRS
üêõ DEBUG: train_worker started for SGDJ
üêõ DEBUG: Training progress: 408/959 done
üêõ DEBUG [23:17:27.875]: Main received result for CORT
  ‚öôÔ∏è Training models for SGDJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - SGDJ: Initiating feature extraction for training.
  [DIAGNOSTIC] SGDJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SGDJ: rows after features available: 126
üéØ SGDJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ SERV LSTM: Epoch 10/50 (20%)
  [DIAGNOSTIC] SGDJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SGDJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SGDJ: Training LSTM (50 epochs)...
      ‚è≥ SGDJ LSTM: Epoch 10/50 (20%)
      ‚è≥ SERV LSTM: Epoch 20/50 (40%)
      ‚è≥ SERV LSTM: Epoch 30/50 (60%)
      ‚è≥ SGDJ LSTM: Epoch 20/50 (40%)
      ‚è≥ SERV LSTM: Epoch 40/50 (80%)
      ‚è≥ SGDJ LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.546026
         RMSE: 0.738936
         R¬≤ Score: -1.5010 (Poor - 150.1% variance explained)
      üîπ SERV: Training TCN (50 epochs)...
      ‚è≥ SERV TCN: Epoch 10/50 (20%)
      ‚è≥ SERV TCN: Epoch 20/50 (40%)
      ‚è≥ SGDJ LSTM: Epoch 40/50 (80%)
      ‚è≥ SERV TCN: Epoch 30/50 (60%)
      ‚è≥ SERV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.345533
         RMSE: 0.587820
         R¬≤ Score: -0.5826
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SERV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SERV Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.198235
         RMSE: 0.445236
         R¬≤ Score: -0.7188 (Poor - 71.9% variance explained)
      üîπ SGDJ: Training TCN (50 epochs)...
      ‚è≥ SGDJ TCN: Epoch 10/50 (20%)
      ‚è≥ SGDJ TCN: Epoch 20/50 (40%)
      ‚è≥ SGDJ TCN: Epoch 30/50 (60%)
      ‚è≥ SGDJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.118282
         RMSE: 0.343922
         R¬≤ Score: -0.0256
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SGDJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SGDJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ SERV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=390.0367 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SERV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SERV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=386.7895 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SERV XGBoost: Starting GridSearchCV fit...
       ‚úÖ SGDJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.3441 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SGDJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TFPM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.9106 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.1439
    - TCN: MSE=0.1108
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1108
        ‚Ä¢ LSTM: MSE=0.1439
        ‚Ä¢ Random Forest: MSE=17.5263
        ‚Ä¢ XGBoost: MSE=17.9106
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.4983
   ‚úÖ TFPM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TFPM (TargetReturn): TCN with MSE=0.1108
üêõ DEBUG: TFPM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TFPM.
üêõ DEBUG: TFPM - Moving model to CPU before return...
üêõ DEBUG [23:17:35.032]: TFPM - Returning result metadata...
üêõ DEBUG [23:17:35.034]: Main received result for TFPM
       ‚úÖ SGDJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.8761 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SGDJ XGBoost: Starting GridSearchCV fit...
üêõ DEBUG: train_worker started for IRS
  ‚öôÔ∏è Training models for IRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - IRS: Initiating feature extraction for training.
  [DIAGNOSTIC] IRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IRS: rows after features available: 126
üéØ IRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IRS: Training LSTM (50 epochs)...
      ‚è≥ IRS LSTM: Epoch 10/50 (20%)
      ‚è≥ IRS LSTM: Epoch 20/50 (40%)
      ‚è≥ IRS LSTM: Epoch 30/50 (60%)
      ‚è≥ IRS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.124895
         RMSE: 0.353405
         R¬≤ Score: -0.5472 (Poor - 54.7% variance explained)
      üîπ IRS: Training TCN (50 epochs)...
      ‚è≥ IRS TCN: Epoch 10/50 (20%)
      ‚è≥ IRS TCN: Epoch 20/50 (40%)
      ‚è≥ IRS TCN: Epoch 30/50 (60%)
      ‚è≥ IRS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.086876
         RMSE: 0.294747
         R¬≤ Score: -0.0763
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IRS Random Forest: Starting GridSearchCV fit...
       ‚úÖ SUPV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=65.4266 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.1005
    - TCN: MSE=0.0793
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0793
        ‚Ä¢ LSTM: MSE=0.1005
        ‚Ä¢ Random Forest: MSE=44.2964
        ‚Ä¢ XGBoost: MSE=65.4266
        ‚Ä¢ LightGBM Regressor (CPU): MSE=94.1731
   ‚úÖ SUPV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SUPV (TargetReturn): TCN with MSE=0.0793
üêõ DEBUG: SUPV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SUPV.
üêõ DEBUG: SUPV - Moving model to CPU before return...
üêõ DEBUG [23:17:38.278]: SUPV - Returning result metadata...
üêõ DEBUG [23:17:38.278]: Main received result for SUPV
üêõ DEBUG: train_worker started for UGL
  ‚öôÔ∏è Training models for UGL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - UGL: Initiating feature extraction for training.
  [DIAGNOSTIC] UGL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UGL: rows after features available: 126
üéØ UGL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UGL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UGL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UGL: Training LSTM (50 epochs)...
      ‚è≥ UGL LSTM: Epoch 10/50 (20%)
      ‚è≥ UGL LSTM: Epoch 20/50 (40%)
      ‚è≥ UGL LSTM: Epoch 30/50 (60%)
      ‚è≥ UGL LSTM: Epoch 40/50 (80%)
       ‚úÖ IRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.7019 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.332846
         RMSE: 0.576928
         R¬≤ Score: -0.8454 (Poor - 84.5% variance explained)
      üîπ UGL: Training TCN (50 epochs)...
      ‚è≥ UGL TCN: Epoch 10/50 (20%)
      ‚è≥ UGL TCN: Epoch 20/50 (40%)
      ‚è≥ UGL TCN: Epoch 30/50 (60%)
      ‚è≥ UGL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.281824
         RMSE: 0.530871
         R¬≤ Score: -0.5625
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UGL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UGL Random Forest: Starting GridSearchCV fit...
       ‚úÖ IRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.8211 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ UGL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.9777 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UGL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UGL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.1236 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UGL XGBoost: Starting GridSearchCV fit...
       ‚úÖ DRD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=57.5671 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.3245
    - TCN: MSE=0.2909
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2909
        ‚Ä¢ LSTM: MSE=0.3245
        ‚Ä¢ Random Forest: MSE=37.9415
        ‚Ä¢ LightGBM Regressor (CPU): MSE=40.5051
        ‚Ä¢ XGBoost: MSE=57.5671
   ‚úÖ DRD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DRD (TargetReturn): TCN with MSE=0.2909
üêõ DEBUG: DRD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DRD.
üêõ DEBUG: DRD - Moving model to CPU before return...
üêõ DEBUG [23:17:55.867]: DRD - Returning result metadata...
üêõ DEBUG: train_worker started for TE
üêõ DEBUG [23:17:55.867]: Main received result for DRD
üêõ DEBUG: Training progress: 412/959 done
  ‚öôÔ∏è Training models for TE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TE: Initiating feature extraction for training.
  [DIAGNOSTIC] TE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TE: rows after features available: 126
üéØ TE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TE: Training LSTM (50 epochs)...
      ‚è≥ TE LSTM: Epoch 10/50 (20%)
      ‚è≥ TE LSTM: Epoch 20/50 (40%)
      ‚è≥ TE LSTM: Epoch 30/50 (60%)
      ‚è≥ TE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.149464
         RMSE: 0.386606
         R¬≤ Score: -0.4191 (Poor - 41.9% variance explained)
      üîπ TE: Training TCN (50 epochs)...
      ‚è≥ TE TCN: Epoch 10/50 (20%)
      ‚è≥ TE TCN: Epoch 20/50 (40%)
      ‚è≥ TE TCN: Epoch 30/50 (60%)
      ‚è≥ TE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.162127
         RMSE: 0.402650
         R¬≤ Score: -0.5393
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TE Random Forest: Starting GridSearchCV fit...
       ‚úÖ TE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.0547 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=71.9128 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TE XGBoost: Starting GridSearchCV fit...
       ‚úÖ ACMR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=97.4986 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.4317
    - TCN: MSE=0.3742
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3742
        ‚Ä¢ LSTM: MSE=0.4317
        ‚Ä¢ LightGBM Regressor (CPU): MSE=90.2600
        ‚Ä¢ XGBoost: MSE=97.4986
        ‚Ä¢ Random Forest: MSE=104.2439
   ‚úÖ ACMR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ACMR (TargetReturn): TCN with MSE=0.3742
üêõ DEBUG: ACMR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ACMR.
üêõ DEBUG: ACMR - Moving model to CPU before return...
üêõ DEBUG [23:18:06.425]: ACMR - Returning result metadata...
üêõ DEBUG: train_worker started for TTWO
üêõ DEBUG [23:18:06.425]: Main received result for ACMR
  ‚öôÔ∏è Training models for TTWO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - TTWO: Initiating feature extraction for training.
  [DIAGNOSTIC] TTWO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TTWO: rows after features available: 126
üéØ TTWO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TTWO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TTWO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TTWO: Training LSTM (50 epochs)...
      ‚è≥ TTWO LSTM: Epoch 10/50 (20%)
      ‚è≥ TTWO LSTM: Epoch 20/50 (40%)
      ‚è≥ TTWO LSTM: Epoch 30/50 (60%)
      ‚è≥ TTWO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.282690
         RMSE: 0.531686
         R¬≤ Score: -0.9392 (Poor - 93.9% variance explained)
      üîπ TTWO: Training TCN (50 epochs)...
      ‚è≥ TTWO TCN: Epoch 10/50 (20%)
      ‚è≥ TTWO TCN: Epoch 20/50 (40%)
      ‚è≥ TTWO TCN: Epoch 30/50 (60%)
      ‚è≥ TTWO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.145020
         RMSE: 0.380815
         R¬≤ Score: 0.0052
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TTWO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TTWO Random Forest: Starting GridSearchCV fit...
       ‚úÖ TTWO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.4590 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TTWO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TTWO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.5701 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TTWO XGBoost: Starting GridSearchCV fit...
       ‚úÖ INBX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.1579 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.4s
    - LSTM: MSE=0.7354
    - TCN: MSE=0.6244
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6244
        ‚Ä¢ LSTM: MSE=0.7354
        ‚Ä¢ Random Forest: MSE=43.3050
        ‚Ä¢ LightGBM Regressor (CPU): MSE=43.8704
        ‚Ä¢ XGBoost: MSE=46.1579
   ‚úÖ INBX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for INBX (TargetReturn): TCN with MSE=0.6244
üêõ DEBUG: INBX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for INBX.
üêõ DEBUG: INBX - Moving model to CPU before return...
üêõ DEBUG [23:18:36.935]: INBX - Returning result metadata...
üêõ DEBUG [23:18:36.936]: Main received result for INBX
üêõ DEBUG: train_worker started for SIL
  ‚öôÔ∏è Training models for SIL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - SIL: Initiating feature extraction for training.
  [DIAGNOSTIC] SIL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SIL: rows after features available: 126
üéØ SIL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SIL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SIL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SIL: Training LSTM (50 epochs)...
      ‚è≥ SIL LSTM: Epoch 10/50 (20%)
      ‚è≥ SIL LSTM: Epoch 20/50 (40%)
      ‚è≥ SIL LSTM: Epoch 30/50 (60%)
      ‚è≥ SIL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.081673
         RMSE: 0.285785
         R¬≤ Score: -0.4722 (Poor - 47.2% variance explained)
      üîπ SIL: Training TCN (50 epochs)...
      ‚è≥ SIL TCN: Epoch 10/50 (20%)
      ‚è≥ SIL TCN: Epoch 20/50 (40%)
      ‚è≥ SIL TCN: Epoch 30/50 (60%)
      ‚è≥ SIL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.062883
         RMSE: 0.250765
         R¬≤ Score: -0.1335
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SIL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SIL Random Forest: Starting GridSearchCV fit...
       ‚úÖ SIL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.4679 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SIL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SIL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.1640 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SIL XGBoost: Starting GridSearchCV fit...
       ‚úÖ FSM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=45.6293 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.2761
    - TCN: MSE=0.1522
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1522
        ‚Ä¢ LSTM: MSE=0.2761
        ‚Ä¢ Random Forest: MSE=45.6157
        ‚Ä¢ XGBoost: MSE=45.6293
        ‚Ä¢ LightGBM Regressor (CPU): MSE=47.6364
   ‚úÖ FSM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FSM (TargetReturn): TCN with MSE=0.1522
üêõ DEBUG: FSM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FSM.
üêõ DEBUG: FSM - Moving model to CPU before return...
üêõ DEBUG [23:18:45.164]: FSM - Returning result metadata...
üêõ DEBUG [23:18:45.165]: Main received result for FSM
üêõ DEBUG: train_worker started for FGM
  ‚öôÔ∏è Training models for FGM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - FGM: Initiating feature extraction for training.
  [DIAGNOSTIC] FGM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FGM: rows after features available: 126
üéØ FGM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FGM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FGM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FGM: Training LSTM (50 epochs)...
      ‚è≥ FGM LSTM: Epoch 10/50 (20%)
      ‚è≥ FGM LSTM: Epoch 20/50 (40%)
      ‚è≥ FGM LSTM: Epoch 30/50 (60%)
      ‚è≥ FGM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.291819
         RMSE: 0.540203
         R¬≤ Score: -1.0419 (Poor - 104.2% variance explained)
      üîπ FGM: Training TCN (50 epochs)...
      ‚è≥ FGM TCN: Epoch 10/50 (20%)
      ‚è≥ FGM TCN: Epoch 20/50 (40%)
      ‚è≥ FGM TCN: Epoch 30/50 (60%)
      ‚è≥ FGM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.140098
         RMSE: 0.374297
         R¬≤ Score: 0.0197
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FGM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FGM Random Forest: Starting GridSearchCV fit...
       ‚úÖ FGM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.3411 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FGM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FGM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.6498 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FGM XGBoost: Starting GridSearchCV fit...
       ‚úÖ OPLN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.7582 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.4675
    - TCN: MSE=0.4543
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4543
        ‚Ä¢ LSTM: MSE=0.4675
        ‚Ä¢ Random Forest: MSE=10.4696
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.2139
        ‚Ä¢ XGBoost: MSE=12.7582
   ‚úÖ OPLN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPLN (TargetReturn): TCN with MSE=0.4543
üêõ DEBUG: OPLN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPLN.
üêõ DEBUG: OPLN - Moving model to CPU before return...
üêõ DEBUG [23:18:54.894]: OPLN - Returning result metadata...
üêõ DEBUG: train_worker started for NVDU
  ‚öôÔ∏è Training models for NVDU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NVDU: Initiating feature extraction for training.
  [DIAGNOSTIC] NVDU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVDU: rows after features available: 126
üéØ NVDU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVDU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVDU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVDU: Training LSTM (50 epochs)...
       ‚úÖ JOYY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.1274 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.5663
    - TCN: MSE=0.7074
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5663
        ‚Ä¢ TCN: MSE=0.7074
        ‚Ä¢ Random Forest: MSE=18.7064
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.6333
        ‚Ä¢ XGBoost: MSE=29.1274
   ‚úÖ JOYY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JOYY (TargetReturn): LSTM with MSE=0.5663
üêõ DEBUG: JOYY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JOYY.
üêõ DEBUG: JOYY - Moving model to CPU before return...
üêõ DEBUG [23:18:55.269]: JOYY - Returning result metadata...
üêõ DEBUG: train_worker started for FDIG
  ‚öôÔ∏è Training models for FDIG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FDIG: Initiating feature extraction for training.
  [DIAGNOSTIC] FDIG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDIG: rows after features available: 126
üéØ FDIG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDIG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDIG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDIG: Training LSTM (50 epochs)...
       ‚úÖ HUYA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=123.7625 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.5820
    - TCN: MSE=0.4237
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4237
        ‚Ä¢ LSTM: MSE=0.5820
        ‚Ä¢ LightGBM Regressor (CPU): MSE=54.6798
        ‚Ä¢ Random Forest: MSE=117.9495
        ‚Ä¢ XGBoost: MSE=123.7625
   ‚úÖ HUYA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HUYA (TargetReturn): TCN with MSE=0.4237
üêõ DEBUG: HUYA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HUYA.
üêõ DEBUG: HUYA - Moving model to CPU before return...
üêõ DEBUG [23:18:55.448]: HUYA - Returning result metadata...
üêõ DEBUG [23:18:55.449]: Main received result for HUYA
üêõ DEBUG: train_worker started for SFM
üêõ DEBUG: Training progress: 416/959 done
üêõ DEBUG [23:18:55.449]: Main received result for OPLN
üêõ DEBUG [23:18:55.449]: Main received result for JOYY
  ‚öôÔ∏è Training models for SFM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - SFM: Initiating feature extraction for training.
  [DIAGNOSTIC] SFM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SFM: rows after features available: 126
üéØ SFM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SFM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SFM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SFM: Training LSTM (50 epochs)...
      ‚è≥ NVDU LSTM: Epoch 10/50 (20%)
      ‚è≥ FDIG LSTM: Epoch 10/50 (20%)
      ‚è≥ NVDU LSTM: Epoch 20/50 (40%)
      ‚è≥ SFM LSTM: Epoch 10/50 (20%)
      ‚è≥ FDIG LSTM: Epoch 20/50 (40%)
      ‚è≥ NVDU LSTM: Epoch 30/50 (60%)
      ‚è≥ SFM LSTM: Epoch 20/50 (40%)
      ‚è≥ NVDU LSTM: Epoch 40/50 (80%)
      ‚è≥ FDIG LSTM: Epoch 30/50 (60%)
      ‚è≥ SFM LSTM: Epoch 30/50 (60%)
      ‚è≥ FDIG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.358878
         RMSE: 0.599064
         R¬≤ Score: -0.3892 (Poor - 38.9% variance explained)
      üîπ NVDU: Training TCN (50 epochs)...
      ‚è≥ NVDU TCN: Epoch 10/50 (20%)
      ‚è≥ NVDU TCN: Epoch 20/50 (40%)
      ‚è≥ SFM LSTM: Epoch 40/50 (80%)
      ‚è≥ NVDU TCN: Epoch 30/50 (60%)
      ‚è≥ NVDU TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.490938
         RMSE: 0.700669
         R¬≤ Score: -0.4907 (Poor - 49.1% variance explained)
      üîπ FDIG: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.420508
         RMSE: 0.648466
         R¬≤ Score: -0.6277
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVDU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVDU Random Forest: Starting GridSearchCV fit...
      ‚è≥ FDIG TCN: Epoch 10/50 (20%)
      ‚è≥ FDIG TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.374258
         RMSE: 0.611766
         R¬≤ Score: -0.8521 (Poor - 85.2% variance explained)
      üîπ SFM: Training TCN (50 epochs)...
      ‚è≥ FDIG TCN: Epoch 30/50 (60%)
      ‚è≥ SFM TCN: Epoch 10/50 (20%)
      ‚è≥ FDIG TCN: Epoch 40/50 (80%)
      ‚è≥ SFM TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.515865
         RMSE: 0.718238
         R¬≤ Score: -0.5663
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDIG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDIG Random Forest: Starting GridSearchCV fit...
      ‚è≥ SFM TCN: Epoch 30/50 (60%)
      ‚è≥ SFM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.202425
         RMSE: 0.449916
         R¬≤ Score: -0.0017
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SFM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SFM Random Forest: Starting GridSearchCV fit...
       ‚úÖ NVDU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=126.3923 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVDU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SFM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.8664 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SFM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDIG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.6089 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDIG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NVDU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=243.7008 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVDU XGBoost: Starting GridSearchCV fit...
       ‚úÖ SFM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.8080 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SFM XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDIG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=31.3496 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDIG XGBoost: Starting GridSearchCV fit...
       ‚úÖ MUFG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.3415 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.6s
    - LSTM: MSE=0.3137
    - TCN: MSE=0.2325
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2325
        ‚Ä¢ LSTM: MSE=0.3137
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.2201
        ‚Ä¢ XGBoost: MSE=18.3415
        ‚Ä¢ Random Forest: MSE=19.2641
   ‚úÖ MUFG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MUFG (TargetReturn): TCN with MSE=0.2325
üêõ DEBUG: MUFG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MUFG.
üêõ DEBUG: MUFG - Moving model to CPU before return...
üêõ DEBUG [23:19:02.458]: MUFG - Returning result metadata...
üêõ DEBUG: train_worker started for NLR
üêõ DEBUG [23:19:02.462]: Main received result for MUFG
  ‚öôÔ∏è Training models for NLR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - NLR: Initiating feature extraction for training.
  [DIAGNOSTIC] NLR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NLR: rows after features available: 126
üéØ NLR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NLR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NLR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NLR: Training LSTM (50 epochs)...
      ‚è≥ NLR LSTM: Epoch 10/50 (20%)
      ‚è≥ NLR LSTM: Epoch 20/50 (40%)
      ‚è≥ NLR LSTM: Epoch 30/50 (60%)
      ‚è≥ NLR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.439207
         RMSE: 0.662727
         R¬≤ Score: -0.3537 (Poor - 35.4% variance explained)
      üîπ NLR: Training TCN (50 epochs)...
      ‚è≥ NLR TCN: Epoch 10/50 (20%)
      ‚è≥ NLR TCN: Epoch 20/50 (40%)
      ‚è≥ NLR TCN: Epoch 30/50 (60%)
      ‚è≥ NLR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.434972
         RMSE: 0.659524
         R¬≤ Score: -0.3407
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NLR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NLR Random Forest: Starting GridSearchCV fit...
       ‚úÖ NLR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.9506 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NLR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NLR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.8629 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NLR XGBoost: Starting GridSearchCV fit...
       ‚úÖ GILT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.4064 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.3882
    - TCN: MSE=0.3323
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3323
        ‚Ä¢ LSTM: MSE=0.3882
        ‚Ä¢ XGBoost: MSE=14.4064
        ‚Ä¢ Random Forest: MSE=17.0343
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.8289
   ‚úÖ GILT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GILT (TargetReturn): TCN with MSE=0.3323
üêõ DEBUG: GILT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GILT.
üêõ DEBUG: GILT - Moving model to CPU before return...
üêõ DEBUG [23:19:17.936]: GILT - Returning result metadata...
üêõ DEBUG [23:19:17.937]: Main received result for GILT
üêõ DEBUG: Training progress: 420/959 done
üêõ DEBUG: train_worker started for JCI
  ‚öôÔ∏è Training models for JCI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - JCI: Initiating feature extraction for training.
  [DIAGNOSTIC] JCI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JCI: rows after features available: 126
üéØ JCI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JCI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JCI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JCI: Training LSTM (50 epochs)...
      ‚è≥ JCI LSTM: Epoch 10/50 (20%)
      ‚è≥ JCI LSTM: Epoch 20/50 (40%)
      ‚è≥ JCI LSTM: Epoch 30/50 (60%)
      ‚è≥ JCI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.427313
         RMSE: 0.653692
         R¬≤ Score: -0.9168 (Poor - 91.7% variance explained)
      üîπ JCI: Training TCN (50 epochs)...
      ‚è≥ JCI TCN: Epoch 10/50 (20%)
      ‚è≥ JCI TCN: Epoch 20/50 (40%)
      ‚è≥ JCI TCN: Epoch 30/50 (60%)
      ‚è≥ JCI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.331955
         RMSE: 0.576155
         R¬≤ Score: -0.4890
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JCI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JCI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PFIX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.0986 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.2193
    - TCN: MSE=0.1052
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1052
        ‚Ä¢ LSTM: MSE=0.2193
        ‚Ä¢ Random Forest: MSE=16.7906
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.2384
        ‚Ä¢ XGBoost: MSE=20.0986
   ‚úÖ PFIX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PFIX (TargetReturn): TCN with MSE=0.1052
üêõ DEBUG: PFIX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PFIX.
üêõ DEBUG: PFIX - Moving model to CPU before return...
üêõ DEBUG [23:19:21.046]: PFIX - Returning result metadata...
üêõ DEBUG [23:19:21.047]: Main received result for PFIX
üêõ DEBUG: train_worker started for EXK
  ‚öôÔ∏è Training models for EXK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - EXK: Initiating feature extraction for training.
  [DIAGNOSTIC] EXK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EXK: rows after features available: 126
üéØ EXK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EXK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EXK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EXK: Training LSTM (50 epochs)...
      ‚è≥ EXK LSTM: Epoch 10/50 (20%)
      ‚è≥ EXK LSTM: Epoch 20/50 (40%)
      ‚è≥ EXK LSTM: Epoch 30/50 (60%)
      ‚è≥ EXK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.316641
         RMSE: 0.562708
         R¬≤ Score: -0.2812 (Poor - 28.1% variance explained)
      üîπ EXK: Training TCN (50 epochs)...
      ‚è≥ EXK TCN: Epoch 10/50 (20%)
      ‚è≥ EXK TCN: Epoch 20/50 (40%)
      ‚è≥ EXK TCN: Epoch 30/50 (60%)
       ‚úÖ JCI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.2515 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JCI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EXK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.372521
         RMSE: 0.610345
         R¬≤ Score: -0.5073
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EXK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EXK Random Forest: Starting GridSearchCV fit...
       ‚úÖ JCI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.4317 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JCI XGBoost: Starting GridSearchCV fit...
       ‚úÖ AHR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=8.1053 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.2476
    - TCN: MSE=0.1107
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1107
        ‚Ä¢ LSTM: MSE=0.2476
        ‚Ä¢ XGBoost: MSE=8.1053
        ‚Ä¢ Random Forest: MSE=8.8641
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.9570
   ‚úÖ AHR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AHR (TargetReturn): TCN with MSE=0.1107
üêõ DEBUG: AHR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AHR.
üêõ DEBUG: AHR - Moving model to CPU before return...
üêõ DEBUG [23:19:27.517]: AHR - Returning result metadata...
üêõ DEBUG [23:19:27.517]: Main received result for AHR
üêõ DEBUG: train_worker started for BFC
       ‚úÖ EXK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=76.9201 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EXK LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for BFC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BFC: Initiating feature extraction for training.
  [DIAGNOSTIC] BFC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BFC: rows after features available: 126
üéØ BFC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BFC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BFC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BFC: Training LSTM (50 epochs)...
      ‚è≥ BFC LSTM: Epoch 10/50 (20%)
       ‚úÖ EXK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=64.3910 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EXK XGBoost: Starting GridSearchCV fit...
      ‚è≥ BFC LSTM: Epoch 20/50 (40%)
      ‚è≥ BFC LSTM: Epoch 30/50 (60%)
      ‚è≥ BFC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.118017
         RMSE: 0.343536
         R¬≤ Score: -0.4407 (Poor - 44.1% variance explained)
      üîπ BFC: Training TCN (50 epochs)...
      ‚è≥ BFC TCN: Epoch 10/50 (20%)
      ‚è≥ BFC TCN: Epoch 20/50 (40%)
      ‚è≥ BFC TCN: Epoch 30/50 (60%)
      ‚è≥ BFC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.117021
         RMSE: 0.342083
         R¬≤ Score: -0.4286
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BFC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BFC Random Forest: Starting GridSearchCV fit...
       ‚úÖ BFC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.3403 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BFC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BFC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8036 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BFC XGBoost: Starting GridSearchCV fit...
       ‚úÖ SGDJ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=19.3498 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.1982
    - TCN: MSE=0.1183
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1183
        ‚Ä¢ LSTM: MSE=0.1982
        ‚Ä¢ XGBoost: MSE=19.3498
        ‚Ä¢ Random Forest: MSE=20.3441
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.8761
   ‚úÖ SGDJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SGDJ (TargetReturn): TCN with MSE=0.1183
üêõ DEBUG: SGDJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SGDJ.
üêõ DEBUG: SGDJ - Moving model to CPU before return...
üêõ DEBUG [23:19:35.535]: SGDJ - Returning result metadata...
üêõ DEBUG: train_worker started for NERD
  ‚öôÔ∏è Training models for NERD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - NERD: Initiating feature extraction for training.
  [DIAGNOSTIC] NERD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NERD: rows after features available: 126
üéØ NERD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NERD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NERD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NERD: Training LSTM (50 epochs)...
      ‚è≥ NERD LSTM: Epoch 10/50 (20%)
      ‚è≥ NERD LSTM: Epoch 20/50 (40%)
       ‚úÖ SERV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=511.3791 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.5460
    - TCN: MSE=0.3455
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3455
        ‚Ä¢ LSTM: MSE=0.5460
        ‚Ä¢ LightGBM Regressor (CPU): MSE=386.7895
        ‚Ä¢ Random Forest: MSE=390.0367
        ‚Ä¢ XGBoost: MSE=511.3791
   ‚úÖ SERV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SERV (TargetReturn): TCN with MSE=0.3455
üêõ DEBUG: SERV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SERV.
üêõ DEBUG: SERV - Moving model to CPU before return...
üêõ DEBUG [23:19:36.776]: SERV - Returning result metadata...
üêõ DEBUG [23:19:36.777]: Main received result for SERV
üêõ DEBUG [23:19:36.777]: Main received result for SGDJ
üêõ DEBUG: Training progress: 424/959 done
üêõ DEBUG: train_worker started for ORCL
  ‚öôÔ∏è Training models for ORCL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ORCL: Initiating feature extraction for training.
  [DIAGNOSTIC] ORCL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ORCL: rows after features available: 126
üéØ ORCL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ORCL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ORCL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ORCL: Training LSTM (50 epochs)...
      ‚è≥ NERD LSTM: Epoch 30/50 (60%)
      ‚è≥ ORCL LSTM: Epoch 10/50 (20%)
      ‚è≥ NERD LSTM: Epoch 40/50 (80%)
      ‚è≥ ORCL LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.364818
         RMSE: 0.604002
         R¬≤ Score: -0.5380 (Poor - 53.8% variance explained)
      üîπ NERD: Training TCN (50 epochs)...
      ‚è≥ NERD TCN: Epoch 10/50 (20%)
      ‚è≥ ORCL LSTM: Epoch 30/50 (60%)
      ‚è≥ NERD TCN: Epoch 20/50 (40%)
      ‚è≥ NERD TCN: Epoch 30/50 (60%)
      ‚è≥ NERD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.345374
         RMSE: 0.587685
         R¬≤ Score: -0.4561
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NERD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NERD Random Forest: Starting GridSearchCV fit...
      ‚è≥ ORCL LSTM: Epoch 40/50 (80%)
       ‚úÖ IRS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=12.6354 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.1249
    - TCN: MSE=0.0869
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0869
        ‚Ä¢ LSTM: MSE=0.1249
        ‚Ä¢ XGBoost: MSE=12.6354
        ‚Ä¢ Random Forest: MSE=13.7019
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.8211
   ‚úÖ IRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IRS (TargetReturn): TCN with MSE=0.0869
üêõ DEBUG: IRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IRS.
üêõ DEBUG: IRS - Moving model to CPU before return...
üêõ DEBUG [23:19:39.189]: IRS - Returning result metadata...
üêõ DEBUG: train_worker started for EPOL
üêõ DEBUG [23:19:39.190]: Main received result for IRS
  ‚öôÔ∏è Training models for EPOL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - EPOL: Initiating feature extraction for training.
  [DIAGNOSTIC] EPOL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EPOL: rows after features available: 126
üéØ EPOL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EPOL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EPOL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EPOL: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.546374
         RMSE: 0.739171
         R¬≤ Score: -0.7485 (Poor - 74.8% variance explained)
      üîπ ORCL: Training TCN (50 epochs)...
      ‚è≥ ORCL TCN: Epoch 10/50 (20%)
      ‚è≥ ORCL TCN: Epoch 20/50 (40%)
      ‚è≥ ORCL TCN: Epoch 30/50 (60%)
      ‚è≥ ORCL TCN: Epoch 40/50 (80%)
      ‚è≥ EPOL LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.596065
         RMSE: 0.772052
         R¬≤ Score: -0.9075
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ORCL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ORCL Random Forest: Starting GridSearchCV fit...
      ‚è≥ EPOL LSTM: Epoch 20/50 (40%)
      ‚è≥ EPOL LSTM: Epoch 30/50 (60%)
      ‚è≥ EPOL LSTM: Epoch 40/50 (80%)
       ‚úÖ NERD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7638 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NERD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.330015
         RMSE: 0.574469
         R¬≤ Score: -0.8068 (Poor - 80.7% variance explained)
      üîπ EPOL: Training TCN (50 epochs)...
      ‚è≥ EPOL TCN: Epoch 10/50 (20%)
      ‚è≥ EPOL TCN: Epoch 20/50 (40%)
      ‚è≥ EPOL TCN: Epoch 30/50 (60%)
      ‚è≥ EPOL TCN: Epoch 40/50 (80%)
       ‚úÖ NERD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.5302 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NERD XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.200730
         RMSE: 0.448029
         R¬≤ Score: -0.0990
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EPOL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EPOL Random Forest: Starting GridSearchCV fit...
       ‚úÖ ORCL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=65.9410 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ORCL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ORCL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=65.5830 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ORCL XGBoost: Starting GridSearchCV fit...
       ‚úÖ UGL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.2377 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.3328
    - TCN: MSE=0.2818
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2818
        ‚Ä¢ LSTM: MSE=0.3328
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.1236
        ‚Ä¢ XGBoost: MSE=30.2377
        ‚Ä¢ Random Forest: MSE=35.9777
   ‚úÖ UGL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UGL (TargetReturn): TCN with MSE=0.2818
üêõ DEBUG: UGL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UGL.
üêõ DEBUG: UGL - Moving model to CPU before return...
üêõ DEBUG [23:19:44.018]: UGL - Returning result metadata...
üêõ DEBUG [23:19:44.019]: Main received result for UGL
üêõ DEBUG: train_worker started for SLM
  ‚öôÔ∏è Training models for SLM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - SLM: Initiating feature extraction for training.
  [DIAGNOSTIC] SLM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLM: rows after features available: 126
üéØ SLM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLM: Training LSTM (50 epochs)...
      ‚è≥ SLM LSTM: Epoch 10/50 (20%)
       ‚úÖ EPOL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.1607 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EPOL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SLM LSTM: Epoch 20/50 (40%)
      ‚è≥ SLM LSTM: Epoch 30/50 (60%)
       ‚úÖ EPOL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.2071 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EPOL XGBoost: Starting GridSearchCV fit...
      ‚è≥ SLM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.273840
         RMSE: 0.523297
         R¬≤ Score: -0.8032 (Poor - 80.3% variance explained)
      üîπ SLM: Training TCN (50 epochs)...
      ‚è≥ SLM TCN: Epoch 10/50 (20%)
      ‚è≥ SLM TCN: Epoch 20/50 (40%)
      ‚è≥ SLM TCN: Epoch 30/50 (60%)
      ‚è≥ SLM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.212820
         RMSE: 0.461324
         R¬≤ Score: -0.4014
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLM Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.3867 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.9280 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLM XGBoost: Starting GridSearchCV fit...
       ‚úÖ TE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=62.5634 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.5s
    - LSTM: MSE=0.1495
    - TCN: MSE=0.1621
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1495
        ‚Ä¢ TCN: MSE=0.1621
        ‚Ä¢ Random Forest: MSE=43.0547
        ‚Ä¢ XGBoost: MSE=62.5634
        ‚Ä¢ LightGBM Regressor (CPU): MSE=71.9128
   ‚úÖ TE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TE (TargetReturn): LSTM with MSE=0.1495
üêõ DEBUG: TE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TE.
üêõ DEBUG: TE - Moving model to CPU before return...
üêõ DEBUG [23:20:01.524]: TE - Returning result metadata...
üêõ DEBUG: train_worker started for LB
üêõ DEBUG [23:20:01.525]: Main received result for TE
  ‚öôÔ∏è Training models for LB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - LB: Initiating feature extraction for training.
  [DIAGNOSTIC] LB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LB: rows after features available: 126
üéØ LB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LB: Training LSTM (50 epochs)...
      ‚è≥ LB LSTM: Epoch 10/50 (20%)
      ‚è≥ LB LSTM: Epoch 20/50 (40%)
      ‚è≥ LB LSTM: Epoch 30/50 (60%)
      ‚è≥ LB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.310382
         RMSE: 0.557120
         R¬≤ Score: -1.2866 (Poor - 128.7% variance explained)
      üîπ LB: Training TCN (50 epochs)...
      ‚è≥ LB TCN: Epoch 10/50 (20%)
      ‚è≥ LB TCN: Epoch 20/50 (40%)
      ‚è≥ LB TCN: Epoch 30/50 (60%)
      ‚è≥ LB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.148750
         RMSE: 0.385681
         R¬≤ Score: -0.0959
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LB Random Forest: Starting GridSearchCV fit...
       ‚úÖ LB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.4731 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=45.6867 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LB XGBoost: Starting GridSearchCV fit...
       ‚úÖ TTWO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.6262 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.2827
    - TCN: MSE=0.1450
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1450
        ‚Ä¢ LSTM: MSE=0.2827
        ‚Ä¢ Random Forest: MSE=10.4590
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.5701
        ‚Ä¢ XGBoost: MSE=11.6262
   ‚úÖ TTWO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TTWO (TargetReturn): TCN with MSE=0.1450
üêõ DEBUG: TTWO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TTWO.
üêõ DEBUG: TTWO - Moving model to CPU before return...
üêõ DEBUG [23:20:11.263]: TTWO - Returning result metadata...
üêõ DEBUG [23:20:11.264]: Main received result for TTWO
üêõ DEBUG: Training progress: 428/959 done
üêõ DEBUG: train_worker started for GE
  ‚öôÔ∏è Training models for GE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - GE: Initiating feature extraction for training.
  [DIAGNOSTIC] GE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GE: rows after features available: 126
üéØ GE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GE: Training LSTM (50 epochs)...
      ‚è≥ GE LSTM: Epoch 10/50 (20%)
      ‚è≥ GE LSTM: Epoch 20/50 (40%)
      ‚è≥ GE LSTM: Epoch 30/50 (60%)
      ‚è≥ GE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.512955
         RMSE: 0.716208
         R¬≤ Score: -0.9865 (Poor - 98.7% variance explained)
      üîπ GE: Training TCN (50 epochs)...
      ‚è≥ GE TCN: Epoch 10/50 (20%)
      ‚è≥ GE TCN: Epoch 20/50 (40%)
      ‚è≥ GE TCN: Epoch 30/50 (60%)
      ‚è≥ GE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.435915
         RMSE: 0.660238
         R¬≤ Score: -0.6882
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GE Random Forest: Starting GridSearchCV fit...
       ‚úÖ GE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.9669 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.9191 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GE XGBoost: Starting GridSearchCV fit...
       ‚úÖ SIL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.4778 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.0817
    - TCN: MSE=0.0629
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0629
        ‚Ä¢ LSTM: MSE=0.0817
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.1640
        ‚Ä¢ XGBoost: MSE=26.4778
        ‚Ä¢ Random Forest: MSE=29.4679
   ‚úÖ SIL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SIL (TargetReturn): TCN with MSE=0.0629
üêõ DEBUG: SIL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SIL.
üêõ DEBUG: SIL - Moving model to CPU before return...
üêõ DEBUG [23:20:42.616]: SIL - Returning result metadata...
üêõ DEBUG [23:20:42.616]: Main received result for SIL
üêõ DEBUG: train_worker started for OCS
  ‚öôÔ∏è Training models for OCS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - OCS: Initiating feature extraction for training.
  [DIAGNOSTIC] OCS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OCS: rows after features available: 126
üéØ OCS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OCS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OCS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OCS: Training LSTM (50 epochs)...
      ‚è≥ OCS LSTM: Epoch 10/50 (20%)
      ‚è≥ OCS LSTM: Epoch 20/50 (40%)
      ‚è≥ OCS LSTM: Epoch 30/50 (60%)
      ‚è≥ OCS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.191955
         RMSE: 0.438127
         R¬≤ Score: -1.1631 (Poor - 116.3% variance explained)
      üîπ OCS: Training TCN (50 epochs)...
      ‚è≥ OCS TCN: Epoch 10/50 (20%)
      ‚è≥ OCS TCN: Epoch 20/50 (40%)
      ‚è≥ OCS TCN: Epoch 30/50 (60%)
      ‚è≥ OCS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.115381
         RMSE: 0.339678
         R¬≤ Score: -0.3002
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OCS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OCS Random Forest: Starting GridSearchCV fit...
       ‚úÖ OCS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.6353 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OCS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OCS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.2297 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OCS XGBoost: Starting GridSearchCV fit...
       ‚úÖ FGM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=8.4017 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.2918
    - TCN: MSE=0.1401
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1401
        ‚Ä¢ LSTM: MSE=0.2918
        ‚Ä¢ XGBoost: MSE=8.4017
        ‚Ä¢ Random Forest: MSE=9.3411
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6498
   ‚úÖ FGM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FGM (TargetReturn): TCN with MSE=0.1401
üêõ DEBUG: FGM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FGM.
üêõ DEBUG: FGM - Moving model to CPU before return...
üêõ DEBUG [23:20:49.658]: FGM - Returning result metadata...
üêõ DEBUG [23:20:49.658]: Main received result for FGM
üêõ DEBUG: train_worker started for TITN
  ‚öôÔ∏è Training models for TITN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - TITN: Initiating feature extraction for training.
  [DIAGNOSTIC] TITN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TITN: rows after features available: 126
üéØ TITN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TITN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TITN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TITN: Training LSTM (50 epochs)...
      ‚è≥ TITN LSTM: Epoch 10/50 (20%)
      ‚è≥ TITN LSTM: Epoch 20/50 (40%)
      ‚è≥ TITN LSTM: Epoch 30/50 (60%)
      ‚è≥ TITN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.417824
         RMSE: 0.646393
         R¬≤ Score: -0.7277 (Poor - 72.8% variance explained)
      üîπ TITN: Training TCN (50 epochs)...
      ‚è≥ TITN TCN: Epoch 10/50 (20%)
      ‚è≥ TITN TCN: Epoch 20/50 (40%)
      ‚è≥ TITN TCN: Epoch 30/50 (60%)
      ‚è≥ TITN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.347191
         RMSE: 0.589229
         R¬≤ Score: -0.4356
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TITN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TITN Random Forest: Starting GridSearchCV fit...
       ‚úÖ TITN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=47.4620 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TITN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TITN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=42.0965 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TITN XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDIG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.0021 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.3s
    - LSTM: MSE=0.4909
    - TCN: MSE=0.5159
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4909
        ‚Ä¢ TCN: MSE=0.5159
        ‚Ä¢ Random Forest: MSE=25.6089
        ‚Ä¢ XGBoost: MSE=26.0021
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.3496
   ‚úÖ FDIG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDIG (TargetReturn): LSTM with MSE=0.4909
üêõ DEBUG: FDIG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDIG.
üêõ DEBUG: FDIG - Moving model to CPU before return...
üêõ DEBUG [23:20:59.455]: FDIG - Returning result metadata...
üêõ DEBUG: train_worker started for MCK
  ‚öôÔ∏è Training models for MCK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - MCK: Initiating feature extraction for training.
  [DIAGNOSTIC] MCK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MCK: rows after features available: 126
üéØ MCK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MCK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MCK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MCK: Training LSTM (50 epochs)...
      ‚è≥ MCK LSTM: Epoch 10/50 (20%)
      ‚è≥ MCK LSTM: Epoch 20/50 (40%)
      ‚è≥ MCK LSTM: Epoch 30/50 (60%)
      ‚è≥ MCK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.324479
         RMSE: 0.569631
         R¬≤ Score: -1.0874 (Poor - 108.7% variance explained)
      üîπ MCK: Training TCN (50 epochs)...
      ‚è≥ MCK TCN: Epoch 10/50 (20%)
       ‚úÖ NVDU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=169.1761 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.3589
    - TCN: MSE=0.4205
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3589
        ‚Ä¢ TCN: MSE=0.4205
        ‚Ä¢ Random Forest: MSE=126.3923
        ‚Ä¢ XGBoost: MSE=169.1761
        ‚Ä¢ LightGBM Regressor (CPU): MSE=243.7008
   ‚úÖ NVDU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVDU (TargetReturn): LSTM with MSE=0.3589
üêõ DEBUG: NVDU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVDU.
üêõ DEBUG: NVDU - Moving model to CPU before return...
üêõ DEBUG [23:21:02.010]: NVDU - Returning result metadata...
üêõ DEBUG: train_worker started for CAH
üêõ DEBUG [23:21:02.012]: Main received result for NVDU
üêõ DEBUG [23:21:02.012]: Main received result for FDIG
üêõ DEBUG: Training progress: 432/959 done
  ‚öôÔ∏è Training models for CAH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CAH: Initiating feature extraction for training.
  [DIAGNOSTIC] CAH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CAH: rows after features available: 126
üéØ CAH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CAH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CAH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CAH: Training LSTM (50 epochs)...
      ‚è≥ MCK TCN: Epoch 20/50 (40%)
      ‚è≥ MCK TCN: Epoch 30/50 (60%)
      ‚è≥ MCK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.266888
         RMSE: 0.516612
         R¬≤ Score: -0.7169
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MCK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MCK Random Forest: Starting GridSearchCV fit...
      ‚è≥ CAH LSTM: Epoch 10/50 (20%)
       ‚úÖ SFM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.2358 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.3743
    - TCN: MSE=0.2024
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2024
        ‚Ä¢ LSTM: MSE=0.3743
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.8080
        ‚Ä¢ Random Forest: MSE=21.8664
        ‚Ä¢ XGBoost: MSE=22.2358
   ‚úÖ SFM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SFM (TargetReturn): TCN with MSE=0.2024
üêõ DEBUG: SFM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SFM.
üêõ DEBUG: SFM - Moving model to CPU before return...
üêõ DEBUG [23:21:03.040]: SFM - Returning result metadata...
üêõ DEBUG: train_worker started for GTES
üêõ DEBUG [23:21:03.041]: Main received result for SFM
  ‚öôÔ∏è Training models for GTES (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - GTES: Initiating feature extraction for training.
  [DIAGNOSTIC] GTES: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GTES: rows after features available: 126
üéØ GTES: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ CAH LSTM: Epoch 20/50 (40%)
  [DIAGNOSTIC] GTES: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GTES: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GTES: Training LSTM (50 epochs)...
      ‚è≥ CAH LSTM: Epoch 30/50 (60%)
      ‚è≥ GTES LSTM: Epoch 10/50 (20%)
      ‚è≥ CAH LSTM: Epoch 40/50 (80%)
      ‚è≥ GTES LSTM: Epoch 20/50 (40%)
      ‚è≥ GTES LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.157015
         RMSE: 0.396251
         R¬≤ Score: -0.4226 (Poor - 42.3% variance explained)
      üîπ CAH: Training TCN (50 epochs)...
      ‚è≥ CAH TCN: Epoch 10/50 (20%)
      ‚è≥ CAH TCN: Epoch 20/50 (40%)
      ‚è≥ GTES LSTM: Epoch 40/50 (80%)
      ‚è≥ CAH TCN: Epoch 30/50 (60%)
      ‚è≥ CAH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.149437
         RMSE: 0.386571
         R¬≤ Score: -0.3539
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CAH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CAH Random Forest: Starting GridSearchCV fit...
       ‚úÖ MCK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.2646 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MCK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.397825
         RMSE: 0.630734
         R¬≤ Score: -0.3331 (Poor - 33.3% variance explained)
      üîπ GTES: Training TCN (50 epochs)...
      ‚è≥ GTES TCN: Epoch 10/50 (20%)
      ‚è≥ GTES TCN: Epoch 20/50 (40%)
      ‚è≥ GTES TCN: Epoch 30/50 (60%)
      ‚è≥ GTES TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.498718
         RMSE: 0.706200
         R¬≤ Score: -0.6712
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GTES: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GTES Random Forest: Starting GridSearchCV fit...
       ‚úÖ MCK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=3.8772 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MCK XGBoost: Starting GridSearchCV fit...
       ‚úÖ CAH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.9818 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CAH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NLR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.8158 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.4392
    - TCN: MSE=0.4350
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4350
        ‚Ä¢ LSTM: MSE=0.4392
        ‚Ä¢ Random Forest: MSE=29.9506
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.8629
        ‚Ä¢ XGBoost: MSE=37.8158
   ‚úÖ NLR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NLR (TargetReturn): TCN with MSE=0.4350
üêõ DEBUG: NLR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NLR.
üêõ DEBUG: NLR - Moving model to CPU before return...
üêõ DEBUG [23:21:08.324]: NLR - Returning result metadata...
üêõ DEBUG [23:21:08.325]: Main received result for NLR
üêõ DEBUG: train_worker started for LOMA
  ‚öôÔ∏è Training models for LOMA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - LOMA: Initiating feature extraction for training.
  [DIAGNOSTIC] LOMA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LOMA: rows after features available: 126
üéØ LOMA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LOMA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LOMA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LOMA: Training LSTM (50 epochs)...
       ‚úÖ CAH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4815 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CAH XGBoost: Starting GridSearchCV fit...
      ‚è≥ LOMA LSTM: Epoch 10/50 (20%)
       ‚úÖ GTES Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.8264 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GTES LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LOMA LSTM: Epoch 20/50 (40%)
       ‚úÖ GTES LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.6141 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GTES XGBoost: Starting GridSearchCV fit...
      ‚è≥ LOMA LSTM: Epoch 30/50 (60%)
      ‚è≥ LOMA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.179112
         RMSE: 0.423216
         R¬≤ Score: -0.6216 (Poor - 62.2% variance explained)
      üîπ LOMA: Training TCN (50 epochs)...
      ‚è≥ LOMA TCN: Epoch 10/50 (20%)
      ‚è≥ LOMA TCN: Epoch 20/50 (40%)
      ‚è≥ LOMA TCN: Epoch 30/50 (60%)
      ‚è≥ LOMA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.113741
         RMSE: 0.337255
         R¬≤ Score: -0.0298
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LOMA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LOMA Random Forest: Starting GridSearchCV fit...
       ‚úÖ LOMA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.7843 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LOMA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LOMA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.6560 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LOMA XGBoost: Starting GridSearchCV fit...
       ‚úÖ JCI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.1606 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.4273
    - TCN: MSE=0.3320
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3320
        ‚Ä¢ LSTM: MSE=0.4273
        ‚Ä¢ XGBoost: MSE=13.1606
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.4317
        ‚Ä¢ Random Forest: MSE=17.2515
   ‚úÖ JCI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JCI (TargetReturn): TCN with MSE=0.3320
üêõ DEBUG: JCI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JCI.
üêõ DEBUG: JCI - Moving model to CPU before return...
üêõ DEBUG [23:21:21.280]: JCI - Returning result metadata...
üêõ DEBUG: train_worker started for NVDL
üêõ DEBUG [23:21:21.284]: Main received result for JCI
  ‚öôÔ∏è Training models for NVDL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - NVDL: Initiating feature extraction for training.
  [DIAGNOSTIC] NVDL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVDL: rows after features available: 126
üéØ NVDL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVDL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVDL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVDL: Training LSTM (50 epochs)...
      ‚è≥ NVDL LSTM: Epoch 10/50 (20%)
      ‚è≥ NVDL LSTM: Epoch 20/50 (40%)
      ‚è≥ NVDL LSTM: Epoch 30/50 (60%)
      ‚è≥ NVDL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.681393
         RMSE: 0.825466
         R¬≤ Score: -1.6245 (Poor - 162.4% variance explained)
      üîπ NVDL: Training TCN (50 epochs)...
      ‚è≥ NVDL TCN: Epoch 10/50 (20%)
      ‚è≥ NVDL TCN: Epoch 20/50 (40%)
      ‚è≥ NVDL TCN: Epoch 30/50 (60%)
      ‚è≥ NVDL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.482721
         RMSE: 0.694781
         R¬≤ Score: -0.8593
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVDL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVDL Random Forest: Starting GridSearchCV fit...
       ‚úÖ EXK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=84.2589 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.3166
    - TCN: MSE=0.3725
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3166
        ‚Ä¢ TCN: MSE=0.3725
        ‚Ä¢ LightGBM Regressor (CPU): MSE=64.3910
        ‚Ä¢ Random Forest: MSE=76.9201
        ‚Ä¢ XGBoost: MSE=84.2589
   ‚úÖ EXK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EXK (TargetReturn): LSTM with MSE=0.3166
üêõ DEBUG: EXK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EXK.
üêõ DEBUG: EXK - Moving model to CPU before return...
üêõ DEBUG [23:21:25.960]: EXK - Returning result metadata...
üêõ DEBUG: train_worker started for KODK
üêõ DEBUG [23:21:25.961]: Main received result for EXK
üêõ DEBUG: Training progress: 436/959 done
  ‚öôÔ∏è Training models for KODK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - KODK: Initiating feature extraction for training.
  [DIAGNOSTIC] KODK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KODK: rows after features available: 126
üéØ KODK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KODK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KODK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KODK: Training LSTM (50 epochs)...
      ‚è≥ KODK LSTM: Epoch 10/50 (20%)
       ‚úÖ NVDL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=127.6794 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVDL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ KODK LSTM: Epoch 20/50 (40%)
      ‚è≥ KODK LSTM: Epoch 30/50 (60%)
       ‚úÖ NVDL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=284.0670 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVDL XGBoost: Starting GridSearchCV fit...
      ‚è≥ KODK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.100176
         RMSE: 0.316505
         R¬≤ Score: -1.2257 (Poor - 122.6% variance explained)
      üîπ KODK: Training TCN (50 epochs)...
      ‚è≥ KODK TCN: Epoch 10/50 (20%)
      ‚è≥ KODK TCN: Epoch 20/50 (40%)
      ‚è≥ KODK TCN: Epoch 30/50 (60%)
      ‚è≥ KODK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.045767
         RMSE: 0.213931
         R¬≤ Score: -0.0169
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KODK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KODK Random Forest: Starting GridSearchCV fit...
       ‚úÖ KODK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.9387 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KODK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KODK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=38.8618 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KODK XGBoost: Starting GridSearchCV fit...
       ‚úÖ BFC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.8911 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.3s
    - LSTM: MSE=0.1180
    - TCN: MSE=0.1170
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1170
        ‚Ä¢ LSTM: MSE=0.1180
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8036
        ‚Ä¢ Random Forest: MSE=11.3403
        ‚Ä¢ XGBoost: MSE=12.8911
   ‚úÖ BFC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BFC (TargetReturn): TCN with MSE=0.1170
üêõ DEBUG: BFC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BFC.
üêõ DEBUG: BFC - Moving model to CPU before return...
üêõ DEBUG [23:21:34.981]: BFC - Returning result metadata...
üêõ DEBUG [23:21:34.982]: Main received result for BFC
üêõ DEBUG: train_worker started for PAYC
  ‚öôÔ∏è Training models for PAYC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - PAYC: Initiating feature extraction for training.
  [DIAGNOSTIC] PAYC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAYC: rows after features available: 126
üéØ PAYC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAYC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAYC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAYC: Training LSTM (50 epochs)...
      ‚è≥ PAYC LSTM: Epoch 10/50 (20%)
      ‚è≥ PAYC LSTM: Epoch 20/50 (40%)
      ‚è≥ PAYC LSTM: Epoch 30/50 (60%)
      ‚è≥ PAYC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.164017
         RMSE: 0.404990
         R¬≤ Score: -0.2535 (Poor - 25.4% variance explained)
      üîπ PAYC: Training TCN (50 epochs)...
      ‚è≥ PAYC TCN: Epoch 10/50 (20%)
      ‚è≥ PAYC TCN: Epoch 20/50 (40%)
      ‚è≥ PAYC TCN: Epoch 30/50 (60%)
      ‚è≥ PAYC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.132805
         RMSE: 0.364424
         R¬≤ Score: -0.0150
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAYC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAYC Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAYC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.1197 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAYC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAYC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.3172 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAYC XGBoost: Starting GridSearchCV fit...
       ‚úÖ NERD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.9809 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.3648
    - TCN: MSE=0.3454
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3454
        ‚Ä¢ LSTM: MSE=0.3648
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.5302
        ‚Ä¢ Random Forest: MSE=7.7638
        ‚Ä¢ XGBoost: MSE=8.9809
   ‚úÖ NERD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NERD (TargetReturn): TCN with MSE=0.3454
üêõ DEBUG: NERD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NERD.
üêõ DEBUG: NERD - Moving model to CPU before return...
üêõ DEBUG [23:21:41.444]: NERD - Returning result metadata...
üêõ DEBUG: train_worker started for SANM
üêõ DEBUG [23:21:41.447]: Main received result for NERD
  ‚öôÔ∏è Training models for SANM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - SANM: Initiating feature extraction for training.
  [DIAGNOSTIC] SANM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SANM: rows after features available: 126
üéØ SANM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SANM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SANM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SANM: Training LSTM (50 epochs)...
      ‚è≥ SANM LSTM: Epoch 10/50 (20%)
      ‚è≥ SANM LSTM: Epoch 20/50 (40%)
      ‚è≥ SANM LSTM: Epoch 30/50 (60%)
      ‚è≥ SANM LSTM: Epoch 40/50 (80%)
       ‚úÖ EPOL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=7.9129 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.3300
    - TCN: MSE=0.2007
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2007
        ‚Ä¢ LSTM: MSE=0.3300
        ‚Ä¢ XGBoost: MSE=7.9129
        ‚Ä¢ Random Forest: MSE=11.1607
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.2071
   ‚úÖ EPOL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EPOL (TargetReturn): TCN with MSE=0.2007
üêõ DEBUG: EPOL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EPOL.
üêõ DEBUG: EPOL - Moving model to CPU before return...
üêõ DEBUG [23:21:44.224]: EPOL - Returning result metadata...
üêõ DEBUG: train_worker started for IZRL
  ‚öôÔ∏è Training models for IZRL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - IZRL: Initiating feature extraction for training.
  [DIAGNOSTIC] IZRL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IZRL: rows after features available: 126
üéØ IZRL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IZRL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IZRL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IZRL: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.527449
         RMSE: 0.726257
         R¬≤ Score: -0.7393 (Poor - 73.9% variance explained)
      üîπ SANM: Training TCN (50 epochs)...
      ‚è≥ SANM TCN: Epoch 10/50 (20%)
      ‚è≥ SANM TCN: Epoch 20/50 (40%)
      ‚è≥ SANM TCN: Epoch 30/50 (60%)
      ‚è≥ SANM TCN: Epoch 40/50 (80%)
      ‚è≥ IZRL LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.339053
         RMSE: 0.582282
         R¬≤ Score: -0.1181
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SANM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SANM Random Forest: Starting GridSearchCV fit...
       ‚úÖ ORCL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=53.4907 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.5464
    - TCN: MSE=0.5961
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5464
        ‚Ä¢ TCN: MSE=0.5961
        ‚Ä¢ XGBoost: MSE=53.4907
        ‚Ä¢ LightGBM Regressor (CPU): MSE=65.5830
        ‚Ä¢ Random Forest: MSE=65.9410
   ‚úÖ ORCL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ORCL (TargetReturn): LSTM with MSE=0.5464
üêõ DEBUG: ORCL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ORCL.
üêõ DEBUG: ORCL - Moving model to CPU before return...
üêõ DEBUG [23:21:45.205]: ORCL - Returning result metadata...
üêõ DEBUG: train_worker started for EQT
üêõ DEBUG [23:21:45.209]: Main received result for ORCL
üêõ DEBUG [23:21:45.209]: Main received result for EPOL
üêõ DEBUG: Training progress: 440/959 done
  ‚öôÔ∏è Training models for EQT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - EQT: Initiating feature extraction for training.
  [DIAGNOSTIC] EQT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EQT: rows after features available: 126
üéØ EQT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EQT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EQT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EQT: Training LSTM (50 epochs)...
      ‚è≥ IZRL LSTM: Epoch 20/50 (40%)
      ‚è≥ EQT LSTM: Epoch 10/50 (20%)
      ‚è≥ IZRL LSTM: Epoch 30/50 (60%)
      ‚è≥ EQT LSTM: Epoch 20/50 (40%)
      ‚è≥ IZRL LSTM: Epoch 40/50 (80%)
      ‚è≥ EQT LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.583573
         RMSE: 0.763920
         R¬≤ Score: -0.8812 (Poor - 88.1% variance explained)
      üîπ IZRL: Training TCN (50 epochs)...
      ‚è≥ IZRL TCN: Epoch 10/50 (20%)
      ‚è≥ IZRL TCN: Epoch 20/50 (40%)
      ‚è≥ IZRL TCN: Epoch 30/50 (60%)
      ‚è≥ IZRL TCN: Epoch 40/50 (80%)
      ‚è≥ EQT LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.519773
         RMSE: 0.720953
         R¬≤ Score: -0.6756
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IZRL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IZRL Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.124470
         RMSE: 0.352803
         R¬≤ Score: -0.8374 (Poor - 83.7% variance explained)
      üîπ EQT: Training TCN (50 epochs)...
      ‚è≥ EQT TCN: Epoch 10/50 (20%)
       ‚úÖ SANM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.8945 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SANM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EQT TCN: Epoch 20/50 (40%)
      ‚è≥ EQT TCN: Epoch 30/50 (60%)
      ‚è≥ EQT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.072478
         RMSE: 0.269218
         R¬≤ Score: -0.0699
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EQT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EQT Random Forest: Starting GridSearchCV fit...
       ‚úÖ SANM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.3115 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SANM XGBoost: Starting GridSearchCV fit...
       ‚úÖ IZRL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.7672 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IZRL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.7512 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.2738
    - TCN: MSE=0.2128
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2128
        ‚Ä¢ LSTM: MSE=0.2738
        ‚Ä¢ Random Forest: MSE=17.3867
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.9280
        ‚Ä¢ XGBoost: MSE=18.7512
   ‚úÖ SLM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLM (TargetReturn): TCN with MSE=0.2128
üêõ DEBUG: SLM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLM.
üêõ DEBUG: SLM - Moving model to CPU before return...
üêõ DEBUG [23:21:49.811]: SLM - Returning result metadata...
üêõ DEBUG [23:21:49.812]: Main received result for SLM
üêõ DEBUG: train_worker started for CIEN
  ‚öôÔ∏è Training models for CIEN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CIEN: Initiating feature extraction for training.
  [DIAGNOSTIC] CIEN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CIEN: rows after features available: 126
üéØ CIEN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CIEN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CIEN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CIEN: Training LSTM (50 epochs)...
      ‚è≥ CIEN LSTM: Epoch 10/50 (20%)
       ‚úÖ IZRL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.8794 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IZRL XGBoost: Starting GridSearchCV fit...
      ‚è≥ CIEN LSTM: Epoch 20/50 (40%)
       ‚úÖ EQT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.4106 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EQT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CIEN LSTM: Epoch 30/50 (60%)
       ‚úÖ EQT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.0907 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EQT XGBoost: Starting GridSearchCV fit...
      ‚è≥ CIEN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.445693
         RMSE: 0.667602
         R¬≤ Score: -0.8443 (Poor - 84.4% variance explained)
      üîπ CIEN: Training TCN (50 epochs)...
      ‚è≥ CIEN TCN: Epoch 10/50 (20%)
      ‚è≥ CIEN TCN: Epoch 20/50 (40%)
      ‚è≥ CIEN TCN: Epoch 30/50 (60%)
      ‚è≥ CIEN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.429416
         RMSE: 0.655299
         R¬≤ Score: -0.7770
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CIEN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CIEN Random Forest: Starting GridSearchCV fit...
       ‚úÖ CIEN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.0767 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CIEN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CIEN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=42.9133 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CIEN XGBoost: Starting GridSearchCV fit...
       ‚úÖ LB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=69.2651 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.6s
    - LSTM: MSE=0.3104
    - TCN: MSE=0.1487
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1487
        ‚Ä¢ LSTM: MSE=0.3104
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.6867
        ‚Ä¢ Random Forest: MSE=60.4731
        ‚Ä¢ XGBoost: MSE=69.2651
   ‚úÖ LB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LB (TargetReturn): TCN with MSE=0.1487
üêõ DEBUG: LB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LB.
üêõ DEBUG: LB - Moving model to CPU before return...
üêõ DEBUG [23:22:03.056]: LB - Returning result metadata...
üêõ DEBUG [23:22:03.058]: Main received result for LB
üêõ DEBUG: train_worker started for BK
  ‚öôÔ∏è Training models for BK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - BK: Initiating feature extraction for training.
  [DIAGNOSTIC] BK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BK: rows after features available: 126
üéØ BK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BK: Training LSTM (50 epochs)...
      ‚è≥ BK LSTM: Epoch 10/50 (20%)
      ‚è≥ BK LSTM: Epoch 20/50 (40%)
      ‚è≥ BK LSTM: Epoch 30/50 (60%)
      ‚è≥ BK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.330883
         RMSE: 0.575224
         R¬≤ Score: -0.2509 (Poor - 25.1% variance explained)
      üîπ BK: Training TCN (50 epochs)...
      ‚è≥ BK TCN: Epoch 10/50 (20%)
      ‚è≥ BK TCN: Epoch 20/50 (40%)
      ‚è≥ BK TCN: Epoch 30/50 (60%)
      ‚è≥ BK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.515142
         RMSE: 0.717734
         R¬≤ Score: -0.9475
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BK Random Forest: Starting GridSearchCV fit...
       ‚úÖ BK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.8836 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.0892 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BK XGBoost: Starting GridSearchCV fit...
       ‚úÖ GE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.4649 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.5130
    - TCN: MSE=0.4359
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4359
        ‚Ä¢ LSTM: MSE=0.5130
        ‚Ä¢ Random Forest: MSE=19.9669
        ‚Ä¢ XGBoost: MSE=20.4649
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.9191
   ‚úÖ GE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GE (TargetReturn): TCN with MSE=0.4359
üêõ DEBUG: GE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GE.
üêõ DEBUG: GE - Moving model to CPU before return...
üêõ DEBUG [23:22:16.689]: GE - Returning result metadata...
üêõ DEBUG [23:22:16.689]: Main received result for GE
üêõ DEBUG: train_worker started for OPY
  ‚öôÔ∏è Training models for OPY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - OPY: Initiating feature extraction for training.
  [DIAGNOSTIC] OPY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPY: rows after features available: 126
üéØ OPY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPY: Training LSTM (50 epochs)...
      ‚è≥ OPY LSTM: Epoch 10/50 (20%)
      ‚è≥ OPY LSTM: Epoch 20/50 (40%)
      ‚è≥ OPY LSTM: Epoch 30/50 (60%)
      ‚è≥ OPY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.487950
         RMSE: 0.698534
         R¬≤ Score: -1.1879 (Poor - 118.8% variance explained)
      üîπ OPY: Training TCN (50 epochs)...
      ‚è≥ OPY TCN: Epoch 10/50 (20%)
      ‚è≥ OPY TCN: Epoch 20/50 (40%)
      ‚è≥ OPY TCN: Epoch 30/50 (60%)
      ‚è≥ OPY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.378753
         RMSE: 0.615429
         R¬≤ Score: -0.6983
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPY Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.2112 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OPY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.8649 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPY XGBoost: Starting GridSearchCV fit...
       ‚úÖ OCS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.1776 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.1920
    - TCN: MSE=0.1154
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1154
        ‚Ä¢ LSTM: MSE=0.1920
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.2297
        ‚Ä¢ Random Forest: MSE=32.6353
        ‚Ä¢ XGBoost: MSE=38.1776
   ‚úÖ OCS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OCS (TargetReturn): TCN with MSE=0.1154
üêõ DEBUG: OCS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OCS.
üêõ DEBUG: OCS - Moving model to CPU before return...
üêõ DEBUG [23:22:49.314]: OCS - Returning result metadata...
üêõ DEBUG: train_worker started for ULS
üêõ DEBUG [23:22:49.315]: Main received result for OCS
üêõ DEBUG: Training progress: 444/959 done
  ‚öôÔ∏è Training models for ULS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - ULS: Initiating feature extraction for training.
  [DIAGNOSTIC] ULS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ULS: rows after features available: 126
üéØ ULS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ULS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ULS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ULS: Training LSTM (50 epochs)...
      ‚è≥ ULS LSTM: Epoch 10/50 (20%)
      ‚è≥ ULS LSTM: Epoch 20/50 (40%)
      ‚è≥ ULS LSTM: Epoch 30/50 (60%)
      ‚è≥ ULS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.359703
         RMSE: 0.599752
         R¬≤ Score: -0.9562 (Poor - 95.6% variance explained)
      üîπ ULS: Training TCN (50 epochs)...
      ‚è≥ ULS TCN: Epoch 10/50 (20%)
      ‚è≥ ULS TCN: Epoch 20/50 (40%)
      ‚è≥ ULS TCN: Epoch 30/50 (60%)
      ‚è≥ ULS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.227827
         RMSE: 0.477312
         R¬≤ Score: -0.2390
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ULS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ULS Random Forest: Starting GridSearchCV fit...
       ‚úÖ ULS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.7321 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ULS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ULS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.0221 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ULS XGBoost: Starting GridSearchCV fit...
       ‚úÖ TITN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.1398 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.8s
    - LSTM: MSE=0.4178
    - TCN: MSE=0.3472
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3472
        ‚Ä¢ LSTM: MSE=0.4178
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.0965
        ‚Ä¢ Random Forest: MSE=47.4620
        ‚Ä¢ XGBoost: MSE=52.1398
   ‚úÖ TITN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TITN (TargetReturn): TCN with MSE=0.3472
üêõ DEBUG: TITN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TITN.
üêõ DEBUG: TITN - Moving model to CPU before return...
üêõ DEBUG [23:22:56.725]: TITN - Returning result metadata...
üêõ DEBUG [23:22:56.725]: Main received result for TITN
üêõ DEBUG: train_worker started for IFS
  ‚öôÔ∏è Training models for IFS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - IFS: Initiating feature extraction for training.
  [DIAGNOSTIC] IFS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IFS: rows after features available: 126
üéØ IFS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IFS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IFS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IFS: Training LSTM (50 epochs)...
      ‚è≥ IFS LSTM: Epoch 10/50 (20%)
      ‚è≥ IFS LSTM: Epoch 20/50 (40%)
      ‚è≥ IFS LSTM: Epoch 30/50 (60%)
      ‚è≥ IFS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.191130
         RMSE: 0.437184
         R¬≤ Score: -0.2974 (Poor - 29.7% variance explained)
      üîπ IFS: Training TCN (50 epochs)...
      ‚è≥ IFS TCN: Epoch 10/50 (20%)
      ‚è≥ IFS TCN: Epoch 20/50 (40%)
      ‚è≥ IFS TCN: Epoch 30/50 (60%)
      ‚è≥ IFS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.149672
         RMSE: 0.386875
         R¬≤ Score: -0.0160
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IFS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IFS Random Forest: Starting GridSearchCV fit...
       ‚úÖ IFS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.1014 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IFS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MCK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.7639 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.1s
    - LSTM: MSE=0.3245
    - TCN: MSE=0.2669
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2669
        ‚Ä¢ LSTM: MSE=0.3245
        ‚Ä¢ Random Forest: MSE=3.2646
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.8772
        ‚Ä¢ XGBoost: MSE=5.7639
   ‚úÖ MCK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MCK (TargetReturn): TCN with MSE=0.2669
üêõ DEBUG: MCK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MCK.
üêõ DEBUG: MCK - Moving model to CPU before return...
üêõ DEBUG [23:23:02.499]: MCK - Returning result metadata...
üêõ DEBUG: train_worker started for COF
üêõ DEBUG [23:23:02.500]: Main received result for MCK
  ‚öôÔ∏è Training models for COF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - COF: Initiating feature extraction for training.
  [DIAGNOSTIC] COF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COF: rows after features available: 126
üéØ COF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COF: Training LSTM (50 epochs)...
      ‚è≥ COF LSTM: Epoch 10/50 (20%)
       ‚úÖ IFS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.7911 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IFS XGBoost: Starting GridSearchCV fit...
      ‚è≥ COF LSTM: Epoch 20/50 (40%)
      ‚è≥ COF LSTM: Epoch 30/50 (60%)
      ‚è≥ COF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.445485
         RMSE: 0.667446
         R¬≤ Score: -0.7517 (Poor - 75.2% variance explained)
      üîπ COF: Training TCN (50 epochs)...
      ‚è≥ COF TCN: Epoch 10/50 (20%)
      ‚è≥ COF TCN: Epoch 20/50 (40%)
      ‚è≥ COF TCN: Epoch 30/50 (60%)
      ‚è≥ COF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.431251
         RMSE: 0.656697
         R¬≤ Score: -0.6958
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COF Random Forest: Starting GridSearchCV fit...
       ‚úÖ CAH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.3270 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.1570
    - TCN: MSE=0.1494
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1494
        ‚Ä¢ LSTM: MSE=0.1570
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4815
        ‚Ä¢ Random Forest: MSE=5.9818
        ‚Ä¢ XGBoost: MSE=7.3270
   ‚úÖ CAH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CAH (TargetReturn): TCN with MSE=0.1494
üêõ DEBUG: CAH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CAH.
üêõ DEBUG: CAH - Moving model to CPU before return...
üêõ DEBUG [23:23:06.445]: CAH - Returning result metadata...
üêõ DEBUG [23:23:06.445]: Main received result for CAH
üêõ DEBUG: train_worker started for UEC
  ‚öôÔ∏è Training models for UEC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - UEC: Initiating feature extraction for training.
  [DIAGNOSTIC] UEC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UEC: rows after features available: 126
üéØ UEC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UEC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UEC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UEC: Training LSTM (50 epochs)...
      ‚è≥ UEC LSTM: Epoch 10/50 (20%)
      ‚è≥ UEC LSTM: Epoch 20/50 (40%)
       ‚úÖ COF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.5046 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COF LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ UEC LSTM: Epoch 30/50 (60%)
      ‚è≥ UEC LSTM: Epoch 40/50 (80%)
       ‚úÖ COF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.3164 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COF XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.547358
         RMSE: 0.739836
         R¬≤ Score: -0.9529 (Poor - 95.3% variance explained)
      üîπ UEC: Training TCN (50 epochs)...
      ‚è≥ UEC TCN: Epoch 10/50 (20%)
      ‚è≥ UEC TCN: Epoch 20/50 (40%)
      ‚è≥ UEC TCN: Epoch 30/50 (60%)
      ‚è≥ UEC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.450909
         RMSE: 0.671498
         R¬≤ Score: -0.6088
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UEC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UEC Random Forest: Starting GridSearchCV fit...
       ‚úÖ GTES XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.0497 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.8s
    - LSTM: MSE=0.3978
    - TCN: MSE=0.4987
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3978
        ‚Ä¢ TCN: MSE=0.4987
        ‚Ä¢ XGBoost: MSE=15.0497
        ‚Ä¢ Random Forest: MSE=15.8264
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.6141
   ‚úÖ GTES: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GTES (TargetReturn): LSTM with MSE=0.3978
üêõ DEBUG: GTES - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GTES.
üêõ DEBUG: GTES - Moving model to CPU before return...
üêõ DEBUG [23:23:10.440]: GTES - Returning result metadata...
üêõ DEBUG: train_worker started for METCB
üêõ DEBUG [23:23:10.440]: Main received result for GTES
üêõ DEBUG: Training progress: 448/959 done
  ‚öôÔ∏è Training models for METCB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - METCB: Initiating feature extraction for training.
  [DIAGNOSTIC] METCB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ METCB: rows after features available: 126
üéØ METCB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] METCB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö METCB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ METCB: Training LSTM (50 epochs)...
      ‚è≥ METCB LSTM: Epoch 10/50 (20%)
      ‚è≥ METCB LSTM: Epoch 20/50 (40%)
      ‚è≥ METCB LSTM: Epoch 30/50 (60%)
      ‚è≥ METCB LSTM: Epoch 40/50 (80%)
       ‚úÖ UEC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.6564 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UEC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.690032
         RMSE: 0.830681
         R¬≤ Score: -1.6122 (Poor - 161.2% variance explained)
      üîπ METCB: Training TCN (50 epochs)...
      ‚è≥ METCB TCN: Epoch 10/50 (20%)
      ‚è≥ METCB TCN: Epoch 20/50 (40%)
      ‚è≥ METCB TCN: Epoch 30/50 (60%)
       ‚úÖ UEC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=69.6037 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UEC XGBoost: Starting GridSearchCV fit...
      ‚è≥ METCB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.426595
         RMSE: 0.653142
         R¬≤ Score: -0.6149
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä METCB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ METCB Random Forest: Starting GridSearchCV fit...
       ‚úÖ LOMA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.6881 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.1791
    - TCN: MSE=0.1137
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1137
        ‚Ä¢ LSTM: MSE=0.1791
        ‚Ä¢ Random Forest: MSE=24.7843
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.6560
        ‚Ä¢ XGBoost: MSE=29.6881
   ‚úÖ LOMA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LOMA (TargetReturn): TCN with MSE=0.1137
üêõ DEBUG: LOMA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LOMA.
üêõ DEBUG: LOMA - Moving model to CPU before return...
üêõ DEBUG [23:23:15.990]: LOMA - Returning result metadata...
üêõ DEBUG: train_worker started for SXT
üêõ DEBUG [23:23:15.991]: Main received result for LOMA
  ‚öôÔ∏è Training models for SXT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - SXT: Initiating feature extraction for training.
  [DIAGNOSTIC] SXT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SXT: rows after features available: 126
üéØ SXT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SXT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SXT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SXT: Training LSTM (50 epochs)...
       ‚úÖ METCB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.6107 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ METCB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SXT LSTM: Epoch 10/50 (20%)
       ‚úÖ METCB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=84.6804 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ METCB XGBoost: Starting GridSearchCV fit...
      ‚è≥ SXT LSTM: Epoch 20/50 (40%)
      ‚è≥ SXT LSTM: Epoch 30/50 (60%)
      ‚è≥ SXT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.180464
         RMSE: 0.424811
         R¬≤ Score: -1.0061 (Poor - 100.6% variance explained)
      üîπ SXT: Training TCN (50 epochs)...
      ‚è≥ SXT TCN: Epoch 10/50 (20%)
      ‚è≥ SXT TCN: Epoch 20/50 (40%)
      ‚è≥ SXT TCN: Epoch 30/50 (60%)
      ‚è≥ SXT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.100151
         RMSE: 0.316466
         R¬≤ Score: -0.1133
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SXT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SXT Random Forest: Starting GridSearchCV fit...
       ‚úÖ SXT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.5638 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SXT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SXT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.7814 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SXT XGBoost: Starting GridSearchCV fit...
       ‚úÖ NVDL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=164.5408 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.6814
    - TCN: MSE=0.4827
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4827
        ‚Ä¢ LSTM: MSE=0.6814
        ‚Ä¢ Random Forest: MSE=127.6794
        ‚Ä¢ XGBoost: MSE=164.5408
        ‚Ä¢ LightGBM Regressor (CPU): MSE=284.0670
   ‚úÖ NVDL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVDL (TargetReturn): TCN with MSE=0.4827
üêõ DEBUG: NVDL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVDL.
üêõ DEBUG: NVDL - Moving model to CPU before return...
üêõ DEBUG [23:23:26.361]: NVDL - Returning result metadata...
üêõ DEBUG: train_worker started for EIS
üêõ DEBUG [23:23:26.367]: Main received result for NVDL
  ‚öôÔ∏è Training models for EIS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - EIS: Initiating feature extraction for training.
  [DIAGNOSTIC] EIS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EIS: rows after features available: 126
üéØ EIS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EIS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EIS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EIS: Training LSTM (50 epochs)...
      ‚è≥ EIS LSTM: Epoch 10/50 (20%)
       ‚úÖ KODK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.7828 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.0s
    - LSTM: MSE=0.1002
    - TCN: MSE=0.0458
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0458
        ‚Ä¢ LSTM: MSE=0.1002
        ‚Ä¢ Random Forest: MSE=18.9387
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.8618
        ‚Ä¢ XGBoost: MSE=53.7828
   ‚úÖ KODK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KODK (TargetReturn): TCN with MSE=0.0458
üêõ DEBUG: KODK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KODK.
üêõ DEBUG: KODK - Moving model to CPU before return...
üêõ DEBUG [23:23:27.031]: KODK - Returning result metadata...
üêõ DEBUG: train_worker started for AG
üêõ DEBUG [23:23:27.031]: Main received result for KODK
  ‚öôÔ∏è Training models for AG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - AG: Initiating feature extraction for training.
  [DIAGNOSTIC] AG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AG: rows after features available: 126
üéØ AG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AG: Training LSTM (50 epochs)...
      ‚è≥ EIS LSTM: Epoch 20/50 (40%)
      ‚è≥ AG LSTM: Epoch 10/50 (20%)
      ‚è≥ EIS LSTM: Epoch 30/50 (60%)
      ‚è≥ AG LSTM: Epoch 20/50 (40%)
      ‚è≥ AG LSTM: Epoch 30/50 (60%)
      ‚è≥ EIS LSTM: Epoch 40/50 (80%)
      ‚è≥ AG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.566869
         RMSE: 0.752907
         R¬≤ Score: -1.1268 (Poor - 112.7% variance explained)
      üîπ EIS: Training TCN (50 epochs)...
      ‚è≥ EIS TCN: Epoch 10/50 (20%)
      ‚è≥ EIS TCN: Epoch 20/50 (40%)
      ‚è≥ EIS TCN: Epoch 30/50 (60%)
      ‚è≥ EIS TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.192389
         RMSE: 0.438622
         R¬≤ Score: -0.9942 (Poor - 99.4% variance explained)
      üîπ AG: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.438240
         RMSE: 0.661997
         R¬≤ Score: -0.6442
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EIS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EIS Random Forest: Starting GridSearchCV fit...
      ‚è≥ AG TCN: Epoch 10/50 (20%)
      ‚è≥ AG TCN: Epoch 20/50 (40%)
      ‚è≥ AG TCN: Epoch 30/50 (60%)
      ‚è≥ AG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.138455
         RMSE: 0.372095
         R¬≤ Score: -0.4351
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AG Random Forest: Starting GridSearchCV fit...
       ‚úÖ EIS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.3206 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EIS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.6861 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EIS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.8022 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EIS XGBoost: Starting GridSearchCV fit...
       ‚úÖ AG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.5786 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AG XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAYC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.5984 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.1640
    - TCN: MSE=0.1328
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1328
        ‚Ä¢ LSTM: MSE=0.1640
        ‚Ä¢ Random Forest: MSE=15.1197
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.3172
        ‚Ä¢ XGBoost: MSE=20.5984
   ‚úÖ PAYC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAYC (TargetReturn): TCN with MSE=0.1328
üêõ DEBUG: PAYC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAYC.
üêõ DEBUG: PAYC - Moving model to CPU before return...
üêõ DEBUG [23:23:41.957]: PAYC - Returning result metadata...
üêõ DEBUG: train_worker started for ATLC
üêõ DEBUG [23:23:41.958]: Main received result for PAYC
üêõ DEBUG: Training progress: 452/959 done
  ‚öôÔ∏è Training models for ATLC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - ATLC: Initiating feature extraction for training.
  [DIAGNOSTIC] ATLC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATLC: rows after features available: 126
üéØ ATLC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATLC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATLC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATLC: Training LSTM (50 epochs)...
      ‚è≥ ATLC LSTM: Epoch 10/50 (20%)
      ‚è≥ ATLC LSTM: Epoch 20/50 (40%)
      ‚è≥ ATLC LSTM: Epoch 30/50 (60%)
      ‚è≥ ATLC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.083773
         RMSE: 0.289436
         R¬≤ Score: -0.8969 (Poor - 89.7% variance explained)
      üîπ ATLC: Training TCN (50 epochs)...
      ‚è≥ ATLC TCN: Epoch 10/50 (20%)
      ‚è≥ ATLC TCN: Epoch 20/50 (40%)
      ‚è≥ ATLC TCN: Epoch 30/50 (60%)
      ‚è≥ ATLC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.054868
         RMSE: 0.234238
         R¬≤ Score: -0.2424
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATLC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATLC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ATLC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.2768 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATLC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ATLC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.2663 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATLC XGBoost: Starting GridSearchCV fit...
       ‚úÖ IZRL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.3484 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.5836
    - TCN: MSE=0.5198
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5198
        ‚Ä¢ LSTM: MSE=0.5836
        ‚Ä¢ XGBoost: MSE=5.3484
        ‚Ä¢ Random Forest: MSE=6.7672
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.8794
   ‚úÖ IZRL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IZRL (TargetReturn): TCN with MSE=0.5198
üêõ DEBUG: IZRL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IZRL.
üêõ DEBUG: IZRL - Moving model to CPU before return...
üêõ DEBUG [23:23:49.255]: IZRL - Returning result metadata...
üêõ DEBUG: train_worker started for MRCY
  ‚öôÔ∏è Training models for MRCY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - MRCY: Initiating feature extraction for training.
  [DIAGNOSTIC] MRCY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MRCY: rows after features available: 126
üéØ MRCY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MRCY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MRCY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MRCY: Training LSTM (50 epochs)...
      ‚è≥ MRCY LSTM: Epoch 10/50 (20%)
      ‚è≥ MRCY LSTM: Epoch 20/50 (40%)
       ‚úÖ SANM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.5482 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.5274
    - TCN: MSE=0.3391
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3391
        ‚Ä¢ LSTM: MSE=0.5274
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.3115
        ‚Ä¢ XGBoost: MSE=17.5482
        ‚Ä¢ Random Forest: MSE=18.8945
   ‚úÖ SANM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SANM (TargetReturn): TCN with MSE=0.3391
üêõ DEBUG: SANM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SANM.
üêõ DEBUG: SANM - Moving model to CPU before return...
üêõ DEBUG [23:23:50.403]: SANM - Returning result metadata...
üêõ DEBUG: train_worker started for FROG
üêõ DEBUG [23:23:50.403]: Main received result for SANM
üêõ DEBUG [23:23:50.404]: Main received result for IZRL
  ‚öôÔ∏è Training models for FROG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FROG: Initiating feature extraction for training.
  [DIAGNOSTIC] FROG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FROG: rows after features available: 126
üéØ FROG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FROG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FROG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FROG: Training LSTM (50 epochs)...
      ‚è≥ MRCY LSTM: Epoch 30/50 (60%)
      ‚è≥ FROG LSTM: Epoch 10/50 (20%)
      ‚è≥ MRCY LSTM: Epoch 40/50 (80%)
      ‚è≥ FROG LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.278146
         RMSE: 0.527395
         R¬≤ Score: -0.7555 (Poor - 75.5% variance explained)
      üîπ MRCY: Training TCN (50 epochs)...
      ‚è≥ MRCY TCN: Epoch 10/50 (20%)
      ‚è≥ FROG LSTM: Epoch 30/50 (60%)
      ‚è≥ MRCY TCN: Epoch 20/50 (40%)
      ‚è≥ MRCY TCN: Epoch 30/50 (60%)
      ‚è≥ MRCY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.159731
         RMSE: 0.399663
         R¬≤ Score: -0.0081
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MRCY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MRCY Random Forest: Starting GridSearchCV fit...
      ‚è≥ FROG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.413872
         RMSE: 0.643328
         R¬≤ Score: -0.8481 (Poor - 84.8% variance explained)
      üîπ FROG: Training TCN (50 epochs)...
      ‚è≥ FROG TCN: Epoch 10/50 (20%)
      ‚è≥ FROG TCN: Epoch 20/50 (40%)
      ‚è≥ FROG TCN: Epoch 30/50 (60%)
      ‚è≥ FROG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.357511
         RMSE: 0.597922
         R¬≤ Score: -0.5964
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FROG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FROG Random Forest: Starting GridSearchCV fit...
       ‚úÖ EQT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.5656 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.1245
    - TCN: MSE=0.0725
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0725
        ‚Ä¢ LSTM: MSE=0.1245
        ‚Ä¢ Random Forest: MSE=14.4106
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.0907
        ‚Ä¢ XGBoost: MSE=15.5656
   ‚úÖ EQT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EQT (TargetReturn): TCN with MSE=0.0725
üêõ DEBUG: EQT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EQT.
üêõ DEBUG: EQT - Moving model to CPU before return...
üêõ DEBUG [23:23:54.106]: EQT - Returning result metadata...
üêõ DEBUG [23:23:54.106]: Main received result for EQT
üêõ DEBUG: train_worker started for LZ
  ‚öôÔ∏è Training models for LZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - LZ: Initiating feature extraction for training.
  [DIAGNOSTIC] LZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LZ: rows after features available: 126
üéØ LZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LZ: Training LSTM (50 epochs)...
      ‚è≥ LZ LSTM: Epoch 10/50 (20%)
       ‚úÖ MRCY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6551 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MRCY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LZ LSTM: Epoch 20/50 (40%)
       ‚úÖ MRCY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.0037 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MRCY XGBoost: Starting GridSearchCV fit...
      ‚è≥ LZ LSTM: Epoch 30/50 (60%)
       ‚úÖ FROG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.3200 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FROG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CIEN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.2773 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.4457
    - TCN: MSE=0.4294
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4294
        ‚Ä¢ LSTM: MSE=0.4457
        ‚Ä¢ Random Forest: MSE=40.0767
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.9133
        ‚Ä¢ XGBoost: MSE=53.2773
   ‚úÖ CIEN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CIEN (TargetReturn): TCN with MSE=0.4294
üêõ DEBUG: CIEN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CIEN.
üêõ DEBUG: CIEN - Moving model to CPU before return...
üêõ DEBUG [23:23:56.018]: CIEN - Returning result metadata...
üêõ DEBUG: train_worker started for PAY
üêõ DEBUG [23:23:56.018]: Main received result for CIEN
üêõ DEBUG: Training progress: 456/959 done
  ‚öôÔ∏è Training models for PAY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - PAY: Initiating feature extraction for training.
  [DIAGNOSTIC] PAY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAY: rows after features available: 126
üéØ PAY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAY: Training LSTM (50 epochs)...
      ‚è≥ LZ LSTM: Epoch 40/50 (80%)
      ‚è≥ PAY LSTM: Epoch 10/50 (20%)
       ‚úÖ FROG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.2076 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FROG XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.675944
         RMSE: 0.822158
         R¬≤ Score: -1.1757 (Poor - 117.6% variance explained)
      üîπ LZ: Training TCN (50 epochs)...
      ‚è≥ LZ TCN: Epoch 10/50 (20%)
      ‚è≥ PAY LSTM: Epoch 20/50 (40%)
      ‚è≥ LZ TCN: Epoch 20/50 (40%)
      ‚è≥ LZ TCN: Epoch 30/50 (60%)
      ‚è≥ LZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.520077
         RMSE: 0.721163
         R¬≤ Score: -0.6740
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LZ Random Forest: Starting GridSearchCV fit...
      ‚è≥ PAY LSTM: Epoch 30/50 (60%)
      ‚è≥ PAY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.268268
         RMSE: 0.517946
         R¬≤ Score: -0.8177 (Poor - 81.8% variance explained)
      üîπ PAY: Training TCN (50 epochs)...
      ‚è≥ PAY TCN: Epoch 10/50 (20%)
      ‚è≥ PAY TCN: Epoch 20/50 (40%)
      ‚è≥ PAY TCN: Epoch 30/50 (60%)
      ‚è≥ PAY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.197440
         RMSE: 0.444342
         R¬≤ Score: -0.3378
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAY Random Forest: Starting GridSearchCV fit...
       ‚úÖ LZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.1503 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=28.2332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.9124 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.7013 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAY XGBoost: Starting GridSearchCV fit...
       ‚úÖ BK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.7638 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.3309
    - TCN: MSE=0.5151
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3309
        ‚Ä¢ TCN: MSE=0.5151
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.0892
        ‚Ä¢ Random Forest: MSE=10.8836
        ‚Ä¢ XGBoost: MSE=11.7638
   ‚úÖ BK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BK (TargetReturn): LSTM with MSE=0.3309
üêõ DEBUG: BK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BK.
üêõ DEBUG: BK - Moving model to CPU before return...
üêõ DEBUG [23:24:07.361]: BK - Returning result metadata...
üêõ DEBUG [23:24:07.361]: Main received result for BK
üêõ DEBUG: train_worker started for TSM
  ‚öôÔ∏è Training models for TSM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TSM: Initiating feature extraction for training.
  [DIAGNOSTIC] TSM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TSM: rows after features available: 126
üéØ TSM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TSM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TSM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TSM: Training LSTM (50 epochs)...
      ‚è≥ TSM LSTM: Epoch 10/50 (20%)
      ‚è≥ TSM LSTM: Epoch 20/50 (40%)
      ‚è≥ TSM LSTM: Epoch 30/50 (60%)
      ‚è≥ TSM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.524949
         RMSE: 0.724533
         R¬≤ Score: -0.8896 (Poor - 89.0% variance explained)
      üîπ TSM: Training TCN (50 epochs)...
      ‚è≥ TSM TCN: Epoch 10/50 (20%)
      ‚è≥ TSM TCN: Epoch 20/50 (40%)
      ‚è≥ TSM TCN: Epoch 30/50 (60%)
      ‚è≥ TSM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.532578
         RMSE: 0.729779
         R¬≤ Score: -0.9171
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TSM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TSM Random Forest: Starting GridSearchCV fit...
       ‚úÖ TSM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.4810 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TSM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TSM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.6070 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TSM XGBoost: Starting GridSearchCV fit...
       ‚úÖ OPY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.2813 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.4879
    - TCN: MSE=0.3788
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3788
        ‚Ä¢ LSTM: MSE=0.4879
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.8649
        ‚Ä¢ Random Forest: MSE=17.2112
        ‚Ä¢ XGBoost: MSE=19.2813
   ‚úÖ OPY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPY (TargetReturn): TCN with MSE=0.3788
üêõ DEBUG: OPY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPY.
üêõ DEBUG: OPY - Moving model to CPU before return...
üêõ DEBUG [23:24:22.072]: OPY - Returning result metadata...
üêõ DEBUG [23:24:22.072]: Main received result for OPY
üêõ DEBUG: train_worker started for NVDA
  ‚öôÔ∏è Training models for NVDA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - NVDA: Initiating feature extraction for training.
  [DIAGNOSTIC] NVDA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVDA: rows after features available: 126
üéØ NVDA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVDA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVDA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVDA: Training LSTM (50 epochs)...
      ‚è≥ NVDA LSTM: Epoch 10/50 (20%)
      ‚è≥ NVDA LSTM: Epoch 20/50 (40%)
      ‚è≥ NVDA LSTM: Epoch 30/50 (60%)
      ‚è≥ NVDA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.457278
         RMSE: 0.676223
         R¬≤ Score: -0.8762 (Poor - 87.6% variance explained)
      üîπ NVDA: Training TCN (50 epochs)...
      ‚è≥ NVDA TCN: Epoch 10/50 (20%)
      ‚è≥ NVDA TCN: Epoch 20/50 (40%)
      ‚è≥ NVDA TCN: Epoch 30/50 (60%)
      ‚è≥ NVDA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.411080
         RMSE: 0.641155
         R¬≤ Score: -0.6866
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVDA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVDA Random Forest: Starting GridSearchCV fit...
       ‚úÖ NVDA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.7797 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVDA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NVDA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.3453 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVDA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ULS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.0867 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.3597
    - TCN: MSE=0.2278
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2278
        ‚Ä¢ LSTM: MSE=0.3597
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.0221
        ‚Ä¢ Random Forest: MSE=16.7321
        ‚Ä¢ XGBoost: MSE=22.0867
   ‚úÖ ULS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ULS (TargetReturn): TCN with MSE=0.2278
üêõ DEBUG: ULS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ULS.
üêõ DEBUG: ULS - Moving model to CPU before return...
üêõ DEBUG [23:24:53.798]: ULS - Returning result metadata...
üêõ DEBUG [23:24:53.799]: Main received result for ULS
üêõ DEBUG: train_worker started for GOAU
  ‚öôÔ∏è Training models for GOAU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - GOAU: Initiating feature extraction for training.
  [DIAGNOSTIC] GOAU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GOAU: rows after features available: 126
üéØ GOAU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GOAU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GOAU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GOAU: Training LSTM (50 epochs)...
      ‚è≥ GOAU LSTM: Epoch 10/50 (20%)
      ‚è≥ GOAU LSTM: Epoch 20/50 (40%)
      ‚è≥ GOAU LSTM: Epoch 30/50 (60%)
      ‚è≥ GOAU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.273848
         RMSE: 0.523305
         R¬≤ Score: -1.0650 (Poor - 106.5% variance explained)
      üîπ GOAU: Training TCN (50 epochs)...
      ‚è≥ GOAU TCN: Epoch 10/50 (20%)
      ‚è≥ GOAU TCN: Epoch 20/50 (40%)
      ‚è≥ GOAU TCN: Epoch 30/50 (60%)
      ‚è≥ GOAU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.129287
         RMSE: 0.359564
         R¬≤ Score: 0.0251
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GOAU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GOAU Random Forest: Starting GridSearchCV fit...
       ‚úÖ GOAU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.5983 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GOAU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GOAU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.5787 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GOAU XGBoost: Starting GridSearchCV fit...
       ‚úÖ IFS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.5217 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.1911
    - TCN: MSE=0.1497
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1497
        ‚Ä¢ LSTM: MSE=0.1911
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.7911
        ‚Ä¢ XGBoost: MSE=10.5217
        ‚Ä¢ Random Forest: MSE=11.1014
   ‚úÖ IFS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IFS (TargetReturn): TCN with MSE=0.1497
üêõ DEBUG: IFS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IFS.
üêõ DEBUG: IFS - Moving model to CPU before return...
üêõ DEBUG [23:25:01.334]: IFS - Returning result metadata...
üêõ DEBUG [23:25:01.334]: Main received result for IFS
üêõ DEBUG: Training progress: 460/959 done
üêõ DEBUG: train_worker started for LYG
  ‚öôÔ∏è Training models for LYG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - LYG: Initiating feature extraction for training.
  [DIAGNOSTIC] LYG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LYG: rows after features available: 126
üéØ LYG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LYG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LYG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LYG: Training LSTM (50 epochs)...
      ‚è≥ LYG LSTM: Epoch 10/50 (20%)
      ‚è≥ LYG LSTM: Epoch 20/50 (40%)
      ‚è≥ LYG LSTM: Epoch 30/50 (60%)
      ‚è≥ LYG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.242140
         RMSE: 0.492078
         R¬≤ Score: -0.7839 (Poor - 78.4% variance explained)
      üîπ LYG: Training TCN (50 epochs)...
      ‚è≥ LYG TCN: Epoch 10/50 (20%)
      ‚è≥ LYG TCN: Epoch 20/50 (40%)
      ‚è≥ LYG TCN: Epoch 30/50 (60%)
      ‚è≥ LYG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.159688
         RMSE: 0.399610
         R¬≤ Score: -0.1765
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LYG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LYG Random Forest: Starting GridSearchCV fit...
       ‚úÖ COF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.2144 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.4455
    - TCN: MSE=0.4313
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4313
        ‚Ä¢ LSTM: MSE=0.4455
        ‚Ä¢ Random Forest: MSE=16.5046
        ‚Ä¢ XGBoost: MSE=17.2144
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.3164
   ‚úÖ COF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COF (TargetReturn): TCN with MSE=0.4313
üêõ DEBUG: COF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COF.
üêõ DEBUG: COF - Moving model to CPU before return...
üêõ DEBUG [23:25:04.638]: COF - Returning result metadata...
üêõ DEBUG [23:25:04.638]: Main received result for COF
üêõ DEBUG: train_worker started for GDXJ
  ‚öôÔ∏è Training models for GDXJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - GDXJ: Initiating feature extraction for training.
  [DIAGNOSTIC] GDXJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GDXJ: rows after features available: 126
üéØ GDXJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GDXJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GDXJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GDXJ: Training LSTM (50 epochs)...
      ‚è≥ GDXJ LSTM: Epoch 10/50 (20%)
      ‚è≥ GDXJ LSTM: Epoch 20/50 (40%)
      ‚è≥ GDXJ LSTM: Epoch 30/50 (60%)
      ‚è≥ GDXJ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.242668
         RMSE: 0.492614
         R¬≤ Score: -0.9225 (Poor - 92.3% variance explained)
      üîπ GDXJ: Training TCN (50 epochs)...
       ‚úÖ LYG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.0637 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LYG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GDXJ TCN: Epoch 10/50 (20%)
      ‚è≥ GDXJ TCN: Epoch 20/50 (40%)
      ‚è≥ GDXJ TCN: Epoch 30/50 (60%)
      ‚è≥ GDXJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.140369
         RMSE: 0.374659
         R¬≤ Score: -0.1121
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GDXJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GDXJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ LYG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.1977 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LYG XGBoost: Starting GridSearchCV fit...
       ‚úÖ GDXJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.9830 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GDXJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UEC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=64.7518 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.5474
    - TCN: MSE=0.4509
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4509
        ‚Ä¢ LSTM: MSE=0.5474
        ‚Ä¢ Random Forest: MSE=43.6564
        ‚Ä¢ XGBoost: MSE=64.7518
        ‚Ä¢ LightGBM Regressor (CPU): MSE=69.6037
   ‚úÖ UEC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UEC (TargetReturn): TCN with MSE=0.4509
üêõ DEBUG: UEC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UEC.
üêõ DEBUG: UEC - Moving model to CPU before return...
üêõ DEBUG [23:25:11.542]: UEC - Returning result metadata...
üêõ DEBUG: train_worker started for ATRA
üêõ DEBUG [23:25:11.548]: Main received result for UEC
       ‚úÖ GDXJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.1747 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GDXJ XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for ATRA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ATRA: Initiating feature extraction for training.
  [DIAGNOSTIC] ATRA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATRA: rows after features available: 126
üéØ ATRA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATRA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATRA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATRA: Training LSTM (50 epochs)...
      ‚è≥ ATRA LSTM: Epoch 10/50 (20%)
      ‚è≥ ATRA LSTM: Epoch 20/50 (40%)
      ‚è≥ ATRA LSTM: Epoch 30/50 (60%)
      ‚è≥ ATRA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.288628
         RMSE: 0.537241
         R¬≤ Score: -0.6761 (Poor - 67.6% variance explained)
      üîπ ATRA: Training TCN (50 epochs)...
      ‚è≥ ATRA TCN: Epoch 10/50 (20%)
      ‚è≥ ATRA TCN: Epoch 20/50 (40%)
      ‚è≥ ATRA TCN: Epoch 30/50 (60%)
      ‚è≥ ATRA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.319535
         RMSE: 0.565274
         R¬≤ Score: -0.8556
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATRA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATRA Random Forest: Starting GridSearchCV fit...
       ‚úÖ METCB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=92.4086 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.6900
    - TCN: MSE=0.4266
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4266
        ‚Ä¢ LSTM: MSE=0.6900
        ‚Ä¢ Random Forest: MSE=60.6107
        ‚Ä¢ LightGBM Regressor (CPU): MSE=84.6804
        ‚Ä¢ XGBoost: MSE=92.4086
   ‚úÖ METCB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for METCB (TargetReturn): TCN with MSE=0.4266
üêõ DEBUG: METCB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for METCB.
üêõ DEBUG: METCB - Moving model to CPU before return...
üêõ DEBUG [23:25:16.353]: METCB - Returning result metadata...
üêõ DEBUG [23:25:16.353]: Main received result for METCB
üêõ DEBUG: train_worker started for UAN
  ‚öôÔ∏è Training models for UAN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - UAN: Initiating feature extraction for training.
  [DIAGNOSTIC] UAN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UAN: rows after features available: 126
üéØ UAN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UAN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UAN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UAN: Training LSTM (50 epochs)...
      ‚è≥ UAN LSTM: Epoch 10/50 (20%)
       ‚úÖ ATRA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=54.7701 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATRA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ UAN LSTM: Epoch 20/50 (40%)
       ‚úÖ ATRA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=66.0935 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATRA XGBoost: Starting GridSearchCV fit...
      ‚è≥ UAN LSTM: Epoch 30/50 (60%)
      ‚è≥ UAN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.244212
         RMSE: 0.494178
         R¬≤ Score: -0.6534 (Poor - 65.3% variance explained)
      üîπ UAN: Training TCN (50 epochs)...
      ‚è≥ UAN TCN: Epoch 10/50 (20%)
      ‚è≥ UAN TCN: Epoch 20/50 (40%)
      ‚è≥ UAN TCN: Epoch 30/50 (60%)
      ‚è≥ UAN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.237450
         RMSE: 0.487289
         R¬≤ Score: -0.6076
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UAN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UAN Random Forest: Starting GridSearchCV fit...
       ‚úÖ SXT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.7129 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.1805
    - TCN: MSE=0.1002
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1002
        ‚Ä¢ LSTM: MSE=0.1805
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.7814
        ‚Ä¢ Random Forest: MSE=27.5638
        ‚Ä¢ XGBoost: MSE=33.7129
   ‚úÖ SXT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SXT (TargetReturn): TCN with MSE=0.1002
üêõ DEBUG: SXT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SXT.
üêõ DEBUG: SXT - Moving model to CPU before return...
üêõ DEBUG [23:25:21.782]: SXT - Returning result metadata...
üêõ DEBUG [23:25:21.783]: Main received result for SXT
üêõ DEBUG: Training progress: 464/959 done
üêõ DEBUG: train_worker started for CRMD
       ‚úÖ UAN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.6310 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UAN LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for CRMD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - CRMD: Initiating feature extraction for training.
  [DIAGNOSTIC] CRMD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRMD: rows after features available: 126
üéØ CRMD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRMD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRMD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRMD: Training LSTM (50 epochs)...
      ‚è≥ CRMD LSTM: Epoch 10/50 (20%)
       ‚úÖ UAN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=10.5937 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UAN XGBoost: Starting GridSearchCV fit...
      ‚è≥ CRMD LSTM: Epoch 20/50 (40%)
      ‚è≥ CRMD LSTM: Epoch 30/50 (60%)
      ‚è≥ CRMD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.360413
         RMSE: 0.600344
         R¬≤ Score: -1.0483 (Poor - 104.8% variance explained)
      üîπ CRMD: Training TCN (50 epochs)...
      ‚è≥ CRMD TCN: Epoch 10/50 (20%)
      ‚è≥ CRMD TCN: Epoch 20/50 (40%)
      ‚è≥ CRMD TCN: Epoch 30/50 (60%)
      ‚è≥ CRMD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.178489
         RMSE: 0.422480
         R¬≤ Score: -0.0144
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRMD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRMD Random Forest: Starting GridSearchCV fit...
       ‚úÖ CRMD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=99.1118 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRMD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRMD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=206.2633 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRMD XGBoost: Starting GridSearchCV fit...
       ‚úÖ EIS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.3096 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.5669
    - TCN: MSE=0.4382
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4382
        ‚Ä¢ LSTM: MSE=0.5669
        ‚Ä¢ XGBoost: MSE=5.3096
        ‚Ä¢ Random Forest: MSE=6.3206
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.8022
   ‚úÖ EIS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EIS (TargetReturn): TCN with MSE=0.4382
üêõ DEBUG: EIS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EIS.
üêõ DEBUG: EIS - Moving model to CPU before return...
üêõ DEBUG [23:25:30.395]: EIS - Returning result metadata...
üêõ DEBUG [23:25:30.396]: Main received result for EIS
üêõ DEBUG: train_worker started for NAMS
  ‚öôÔ∏è Training models for NAMS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - NAMS: Initiating feature extraction for training.
  [DIAGNOSTIC] NAMS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NAMS: rows after features available: 126
üéØ NAMS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NAMS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NAMS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NAMS: Training LSTM (50 epochs)...
      ‚è≥ NAMS LSTM: Epoch 10/50 (20%)
      ‚è≥ NAMS LSTM: Epoch 20/50 (40%)
      ‚è≥ NAMS LSTM: Epoch 30/50 (60%)
      ‚è≥ NAMS LSTM: Epoch 40/50 (80%)
       ‚úÖ AG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=33.0742 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.1924
    - TCN: MSE=0.1385
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1385
        ‚Ä¢ LSTM: MSE=0.1924
        ‚Ä¢ XGBoost: MSE=33.0742
        ‚Ä¢ Random Forest: MSE=35.6861
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.5786
   ‚úÖ AG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AG (TargetReturn): TCN with MSE=0.1385
üêõ DEBUG: AG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AG.
üêõ DEBUG: AG - Moving model to CPU before return...
üêõ DEBUG [23:25:32.361]: AG - Returning result metadata...
üêõ DEBUG [23:25:32.362]: Main received result for AG
üêõ DEBUG: train_worker started for HGV
  ‚öôÔ∏è Training models for HGV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - HGV: Initiating feature extraction for training.
  [DIAGNOSTIC] HGV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HGV: rows after features available: 126
üéØ HGV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HGV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HGV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HGV: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.406207
         RMSE: 0.637344
         R¬≤ Score: -0.7232 (Poor - 72.3% variance explained)
      üîπ NAMS: Training TCN (50 epochs)...
      ‚è≥ NAMS TCN: Epoch 10/50 (20%)
      ‚è≥ HGV LSTM: Epoch 10/50 (20%)
      ‚è≥ NAMS TCN: Epoch 20/50 (40%)
      ‚è≥ NAMS TCN: Epoch 30/50 (60%)
      ‚è≥ NAMS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.294899
         RMSE: 0.543046
         R¬≤ Score: -0.2510
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NAMS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NAMS Random Forest: Starting GridSearchCV fit...
      ‚è≥ HGV LSTM: Epoch 20/50 (40%)
      ‚è≥ HGV LSTM: Epoch 30/50 (60%)
      ‚è≥ HGV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.567875
         RMSE: 0.753575
         R¬≤ Score: -0.6704 (Poor - 67.0% variance explained)
      üîπ HGV: Training TCN (50 epochs)...
      ‚è≥ HGV TCN: Epoch 10/50 (20%)
      ‚è≥ HGV TCN: Epoch 20/50 (40%)
      ‚è≥ HGV TCN: Epoch 30/50 (60%)
      ‚è≥ HGV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.689807
         RMSE: 0.830546
         R¬≤ Score: -1.0291
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HGV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HGV Random Forest: Starting GridSearchCV fit...
       ‚úÖ NAMS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.0678 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NAMS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NAMS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.4379 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NAMS XGBoost: Starting GridSearchCV fit...
       ‚úÖ HGV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.7402 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HGV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HGV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.1610 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HGV XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATLC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.0324 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.0s
    - LSTM: MSE=0.0838
    - TCN: MSE=0.0549
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0549
        ‚Ä¢ LSTM: MSE=0.0838
        ‚Ä¢ Random Forest: MSE=20.2768
        ‚Ä¢ XGBoost: MSE=24.0324
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.2663
   ‚úÖ ATLC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATLC (TargetReturn): TCN with MSE=0.0549
üêõ DEBUG: ATLC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATLC.
üêõ DEBUG: ATLC - Moving model to CPU before return...
üêõ DEBUG [23:25:46.905]: ATLC - Returning result metadata...
üêõ DEBUG: train_worker started for DEC
üêõ DEBUG [23:25:46.906]: Main received result for ATLC
  ‚öôÔ∏è Training models for DEC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - DEC: Initiating feature extraction for training.
  [DIAGNOSTIC] DEC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DEC: rows after features available: 126
üéØ DEC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DEC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DEC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DEC: Training LSTM (50 epochs)...
      ‚è≥ DEC LSTM: Epoch 10/50 (20%)
      ‚è≥ DEC LSTM: Epoch 20/50 (40%)
      ‚è≥ DEC LSTM: Epoch 30/50 (60%)
      ‚è≥ DEC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.535984
         RMSE: 0.732109
         R¬≤ Score: -1.2191 (Poor - 121.9% variance explained)
      üîπ DEC: Training TCN (50 epochs)...
      ‚è≥ DEC TCN: Epoch 10/50 (20%)
      ‚è≥ DEC TCN: Epoch 20/50 (40%)
      ‚è≥ DEC TCN: Epoch 30/50 (60%)
      ‚è≥ DEC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.292293
         RMSE: 0.540641
         R¬≤ Score: -0.2102
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DEC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DEC Random Forest: Starting GridSearchCV fit...
       ‚úÖ DEC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.7449 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DEC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DEC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.9815 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DEC XGBoost: Starting GridSearchCV fit...
       ‚úÖ MRCY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.3370 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.2781
    - TCN: MSE=0.1597
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1597
        ‚Ä¢ LSTM: MSE=0.2781
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.0037
        ‚Ä¢ Random Forest: MSE=11.6551
        ‚Ä¢ XGBoost: MSE=13.3370
   ‚úÖ MRCY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MRCY (TargetReturn): TCN with MSE=0.1597
üêõ DEBUG: MRCY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MRCY.
üêõ DEBUG: MRCY - Moving model to CPU before return...
üêõ DEBUG [23:25:54.164]: MRCY - Returning result metadata...
üêõ DEBUG [23:25:54.164]: Main received result for MRCY
üêõ DEBUG: Training progress: 468/959 done
üêõ DEBUG: train_worker started for PLTM
  ‚öôÔ∏è Training models for PLTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - PLTM: Initiating feature extraction for training.
  [DIAGNOSTIC] PLTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PLTM: rows after features available: 126
üéØ PLTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PLTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PLTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PLTM: Training LSTM (50 epochs)...
      ‚è≥ PLTM LSTM: Epoch 10/50 (20%)
      ‚è≥ PLTM LSTM: Epoch 20/50 (40%)
      ‚è≥ PLTM LSTM: Epoch 30/50 (60%)
      ‚è≥ PLTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.508841
         RMSE: 0.713331
         R¬≤ Score: -0.5611 (Poor - 56.1% variance explained)
      üîπ PLTM: Training TCN (50 epochs)...
      ‚è≥ PLTM TCN: Epoch 10/50 (20%)
      ‚è≥ PLTM TCN: Epoch 20/50 (40%)
      ‚è≥ PLTM TCN: Epoch 30/50 (60%)
      ‚è≥ PLTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.708004
         RMSE: 0.841430
         R¬≤ Score: -1.1721
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PLTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PLTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ FROG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.4571 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.4139
    - TCN: MSE=0.3575
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3575
        ‚Ä¢ LSTM: MSE=0.4139
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.2076
        ‚Ä¢ XGBoost: MSE=19.4571
        ‚Ä¢ Random Forest: MSE=20.3200
   ‚úÖ FROG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FROG (TargetReturn): TCN with MSE=0.3575
üêõ DEBUG: FROG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FROG.
üêõ DEBUG: FROG - Moving model to CPU before return...
üêõ DEBUG [23:25:58.696]: FROG - Returning result metadata...
üêõ DEBUG [23:25:58.696]: Main received result for FROG
üêõ DEBUG: train_worker started for PPLT
  ‚öôÔ∏è Training models for PPLT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - PPLT: Initiating feature extraction for training.
  [DIAGNOSTIC] PPLT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PPLT: rows after features available: 126
üéØ PPLT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PPLT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PPLT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PPLT: Training LSTM (50 epochs)...
      ‚è≥ PPLT LSTM: Epoch 10/50 (20%)
       ‚úÖ PLTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.6926 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PLTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PPLT LSTM: Epoch 20/50 (40%)
      ‚è≥ PPLT LSTM: Epoch 30/50 (60%)
       ‚úÖ PLTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=29.3466 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PLTM XGBoost: Starting GridSearchCV fit...
      ‚è≥ PPLT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.615235
         RMSE: 0.784369
         R¬≤ Score: -0.9028 (Poor - 90.3% variance explained)
      üîπ PPLT: Training TCN (50 epochs)...
      ‚è≥ PPLT TCN: Epoch 10/50 (20%)
      ‚è≥ PPLT TCN: Epoch 20/50 (40%)
      ‚è≥ PPLT TCN: Epoch 30/50 (60%)
      ‚è≥ PPLT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.565780
         RMSE: 0.752184
         R¬≤ Score: -0.7498
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PPLT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PPLT Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.2779 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.2683
    - TCN: MSE=0.1974
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1974
        ‚Ä¢ LSTM: MSE=0.2683
        ‚Ä¢ Random Forest: MSE=26.9124
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.7013
        ‚Ä¢ XGBoost: MSE=36.2779
   ‚úÖ PAY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAY (TargetReturn): TCN with MSE=0.1974
üêõ DEBUG: PAY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAY.
üêõ DEBUG: PAY - Moving model to CPU before return...
üêõ DEBUG [23:26:01.744]: PAY - Returning result metadata...
üêõ DEBUG: train_worker started for DLO
  ‚öôÔ∏è Training models for DLO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - DLO: Initiating feature extraction for training.
  [DIAGNOSTIC] DLO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DLO: rows after features available: 126
üéØ DLO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DLO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DLO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DLO: Training LSTM (50 epochs)...
      ‚è≥ DLO LSTM: Epoch 10/50 (20%)
      ‚è≥ DLO LSTM: Epoch 20/50 (40%)
      ‚è≥ DLO LSTM: Epoch 30/50 (60%)
       ‚úÖ LZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.0756 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.2s
    - LSTM: MSE=0.6759
    - TCN: MSE=0.5201
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5201
        ‚Ä¢ LSTM: MSE=0.6759
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.2332
        ‚Ä¢ Random Forest: MSE=34.1503
        ‚Ä¢ XGBoost: MSE=46.0756
   ‚úÖ LZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LZ (TargetReturn): TCN with MSE=0.5201
üêõ DEBUG: LZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LZ.
üêõ DEBUG: LZ - Moving model to CPU before return...
üêõ DEBUG [23:26:03.415]: LZ - Returning result metadata...
üêõ DEBUG [23:26:03.415]: Main received result for LZ
üêõ DEBUG [23:26:03.415]: Main received result for PAY
üêõ DEBUG: train_worker started for ATI
  ‚öôÔ∏è Training models for ATI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ATI: Initiating feature extraction for training.
  [DIAGNOSTIC] ATI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATI: rows after features available: 126
üéØ ATI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATI: Training LSTM (50 epochs)...
      ‚è≥ DLO LSTM: Epoch 40/50 (80%)
      ‚è≥ ATI LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.668018
         RMSE: 0.817324
         R¬≤ Score: -0.8599 (Poor - 86.0% variance explained)
      üîπ DLO: Training TCN (50 epochs)...
      ‚è≥ DLO TCN: Epoch 10/50 (20%)
      ‚è≥ DLO TCN: Epoch 20/50 (40%)
      ‚è≥ ATI LSTM: Epoch 20/50 (40%)
      ‚è≥ DLO TCN: Epoch 30/50 (60%)
       ‚úÖ PPLT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=67.8728 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PPLT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ DLO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.685914
         RMSE: 0.828199
         R¬≤ Score: -0.9097
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DLO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DLO Random Forest: Starting GridSearchCV fit...
      ‚è≥ ATI LSTM: Epoch 30/50 (60%)
       ‚úÖ PPLT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.0663 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PPLT XGBoost: Starting GridSearchCV fit...
      ‚è≥ ATI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.511111
         RMSE: 0.714921
         R¬≤ Score: -0.9362 (Poor - 93.6% variance explained)
      üîπ ATI: Training TCN (50 epochs)...
      ‚è≥ ATI TCN: Epoch 10/50 (20%)
      ‚è≥ ATI TCN: Epoch 20/50 (40%)
      ‚è≥ ATI TCN: Epoch 30/50 (60%)
      ‚è≥ ATI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.370839
         RMSE: 0.608966
         R¬≤ Score: -0.4048
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATI Random Forest: Starting GridSearchCV fit...
       ‚úÖ DLO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.9534 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DLO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DLO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=32.4889 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DLO XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=53.5895 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ATI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=42.6064 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATI XGBoost: Starting GridSearchCV fit...
       ‚úÖ TSM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=23.4267 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.5249
    - TCN: MSE=0.5326
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5249
        ‚Ä¢ TCN: MSE=0.5326
        ‚Ä¢ XGBoost: MSE=23.4267
        ‚Ä¢ Random Forest: MSE=23.4810
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.6070
   ‚úÖ TSM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TSM (TargetReturn): LSTM with MSE=0.5249
üêõ DEBUG: TSM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TSM.
üêõ DEBUG: TSM - Moving model to CPU before return...
üêõ DEBUG [23:26:12.794]: TSM - Returning result metadata...
üêõ DEBUG [23:26:12.794]: Main received result for TSM
üêõ DEBUG: Training progress: 472/959 done
üêõ DEBUG: train_worker started for ESPO
  ‚öôÔ∏è Training models for ESPO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ESPO: Initiating feature extraction for training.
  [DIAGNOSTIC] ESPO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ESPO: rows after features available: 126
üéØ ESPO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ESPO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ESPO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ESPO: Training LSTM (50 epochs)...
      ‚è≥ ESPO LSTM: Epoch 10/50 (20%)
      ‚è≥ ESPO LSTM: Epoch 20/50 (40%)
      ‚è≥ ESPO LSTM: Epoch 30/50 (60%)
      ‚è≥ ESPO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.451853
         RMSE: 0.672200
         R¬≤ Score: -1.4172 (Poor - 141.7% variance explained)
      üîπ ESPO: Training TCN (50 epochs)...
      ‚è≥ ESPO TCN: Epoch 10/50 (20%)
      ‚è≥ ESPO TCN: Epoch 20/50 (40%)
      ‚è≥ ESPO TCN: Epoch 30/50 (60%)
      ‚è≥ ESPO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.254577
         RMSE: 0.504557
         R¬≤ Score: -0.3619
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ESPO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ESPO Random Forest: Starting GridSearchCV fit...
       ‚úÖ ESPO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.3523 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ESPO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ESPO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.0114 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ESPO XGBoost: Starting GridSearchCV fit...
       ‚úÖ NVDA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.2257 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.4573
    - TCN: MSE=0.4111
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4111
        ‚Ä¢ LSTM: MSE=0.4573
        ‚Ä¢ Random Forest: MSE=32.7797
        ‚Ä¢ XGBoost: MSE=33.2257
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.3453
   ‚úÖ NVDA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVDA (TargetReturn): TCN with MSE=0.4111
üêõ DEBUG: NVDA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVDA.
üêõ DEBUG: NVDA - Moving model to CPU before return...
üêõ DEBUG [23:26:27.463]: NVDA - Returning result metadata...
üêõ DEBUG: train_worker started for VLN
üêõ DEBUG [23:26:27.464]: Main received result for NVDA
  ‚öôÔ∏è Training models for VLN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - VLN: Initiating feature extraction for training.
  [DIAGNOSTIC] VLN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VLN: rows after features available: 126
üéØ VLN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VLN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VLN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VLN: Training LSTM (50 epochs)...
      ‚è≥ VLN LSTM: Epoch 10/50 (20%)
      ‚è≥ VLN LSTM: Epoch 20/50 (40%)
      ‚è≥ VLN LSTM: Epoch 30/50 (60%)
      ‚è≥ VLN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.176429
         RMSE: 0.420034
         R¬≤ Score: -1.0306 (Poor - 103.1% variance explained)
      üîπ VLN: Training TCN (50 epochs)...
      ‚è≥ VLN TCN: Epoch 10/50 (20%)
      ‚è≥ VLN TCN: Epoch 20/50 (40%)
      ‚è≥ VLN TCN: Epoch 30/50 (60%)
      ‚è≥ VLN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.115783
         RMSE: 0.340269
         R¬≤ Score: -0.3326
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VLN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VLN Random Forest: Starting GridSearchCV fit...
       ‚úÖ VLN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=57.7301 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VLN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VLN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=52.4509 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VLN XGBoost: Starting GridSearchCV fit...
       ‚úÖ GOAU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.6680 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.2738
    - TCN: MSE=0.1293
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1293
        ‚Ä¢ LSTM: MSE=0.2738
        ‚Ä¢ Random Forest: MSE=20.5983
        ‚Ä¢ XGBoost: MSE=20.6680
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.5787
   ‚úÖ GOAU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GOAU (TargetReturn): TCN with MSE=0.1293
üêõ DEBUG: GOAU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GOAU.
üêõ DEBUG: GOAU - Moving model to CPU before return...
üêõ DEBUG [23:26:59.878]: GOAU - Returning result metadata...
üêõ DEBUG: train_worker started for AMPL
üêõ DEBUG [23:26:59.880]: Main received result for GOAU
  ‚öôÔ∏è Training models for AMPL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - AMPL: Initiating feature extraction for training.
  [DIAGNOSTIC] AMPL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMPL: rows after features available: 126
üéØ AMPL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMPL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMPL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMPL: Training LSTM (50 epochs)...
      ‚è≥ AMPL LSTM: Epoch 10/50 (20%)
      ‚è≥ AMPL LSTM: Epoch 20/50 (40%)
      ‚è≥ AMPL LSTM: Epoch 30/50 (60%)
      ‚è≥ AMPL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.502397
         RMSE: 0.708800
         R¬≤ Score: -0.7962 (Poor - 79.6% variance explained)
      üîπ AMPL: Training TCN (50 epochs)...
      ‚è≥ AMPL TCN: Epoch 10/50 (20%)
      ‚è≥ AMPL TCN: Epoch 20/50 (40%)
      ‚è≥ AMPL TCN: Epoch 30/50 (60%)
      ‚è≥ AMPL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.598105
         RMSE: 0.773373
         R¬≤ Score: -1.1383
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMPL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMPL Random Forest: Starting GridSearchCV fit...
       ‚úÖ AMPL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=65.8861 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMPL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AMPL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=48.4578 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMPL XGBoost: Starting GridSearchCV fit...
       ‚úÖ LYG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.4796 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.2421
    - TCN: MSE=0.1597
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1597
        ‚Ä¢ LSTM: MSE=0.2421
        ‚Ä¢ Random Forest: MSE=16.0637
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.1977
        ‚Ä¢ XGBoost: MSE=20.4796
   ‚úÖ LYG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LYG (TargetReturn): TCN with MSE=0.1597
üêõ DEBUG: LYG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LYG.
üêõ DEBUG: LYG - Moving model to CPU before return...
üêõ DEBUG [23:27:07.064]: LYG - Returning result metadata...
üêõ DEBUG: train_worker started for GILD
üêõ DEBUG [23:27:07.064]: Main received result for LYG
  ‚öôÔ∏è Training models for GILD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - GILD: Initiating feature extraction for training.
  [DIAGNOSTIC] GILD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GILD: rows after features available: 126
üéØ GILD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GILD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GILD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GILD: Training LSTM (50 epochs)...
      ‚è≥ GILD LSTM: Epoch 10/50 (20%)
      ‚è≥ GILD LSTM: Epoch 20/50 (40%)
       ‚úÖ GDXJ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=20.6108 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.4s
    - LSTM: MSE=0.2427
    - TCN: MSE=0.1404
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1404
        ‚Ä¢ LSTM: MSE=0.2427
        ‚Ä¢ XGBoost: MSE=20.6108
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.1747
        ‚Ä¢ Random Forest: MSE=25.9830
   ‚úÖ GDXJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GDXJ (TargetReturn): TCN with MSE=0.1404
üêõ DEBUG: GDXJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GDXJ.
üêõ DEBUG: GDXJ - Moving model to CPU before return...
üêõ DEBUG [23:27:08.003]: GDXJ - Returning result metadata...
üêõ DEBUG: train_worker started for CTOR
üêõ DEBUG [23:27:08.004]: Main received result for GDXJ
üêõ DEBUG: Training progress: 476/959 done
  ‚öôÔ∏è Training models for CTOR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - CTOR: Initiating feature extraction for training.
  [DIAGNOSTIC] CTOR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTOR: rows after features available: 126
üéØ CTOR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTOR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTOR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTOR: Training LSTM (50 epochs)...
      ‚è≥ GILD LSTM: Epoch 30/50 (60%)
      ‚è≥ CTOR LSTM: Epoch 10/50 (20%)
      ‚è≥ GILD LSTM: Epoch 40/50 (80%)
      ‚è≥ CTOR LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.120206
         RMSE: 0.346708
         R¬≤ Score: -0.2539 (Poor - 25.4% variance explained)
      üîπ GILD: Training TCN (50 epochs)...
      ‚è≥ CTOR LSTM: Epoch 30/50 (60%)
      ‚è≥ GILD TCN: Epoch 10/50 (20%)
      ‚è≥ GILD TCN: Epoch 20/50 (40%)
      ‚è≥ GILD TCN: Epoch 30/50 (60%)
      ‚è≥ GILD TCN: Epoch 40/50 (80%)
      ‚è≥ CTOR LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.100383
         RMSE: 0.316833
         R¬≤ Score: -0.0471
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GILD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GILD Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.500666
         RMSE: 0.707578
         R¬≤ Score: -0.5897 (Poor - 59.0% variance explained)
      üîπ CTOR: Training TCN (50 epochs)...
      ‚è≥ CTOR TCN: Epoch 10/50 (20%)
      ‚è≥ CTOR TCN: Epoch 20/50 (40%)
      ‚è≥ CTOR TCN: Epoch 30/50 (60%)
      ‚è≥ CTOR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.297085
         RMSE: 0.545055
         R¬≤ Score: 0.0567
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTOR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTOR Random Forest: Starting GridSearchCV fit...
       ‚úÖ GILD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0082 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GILD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GILD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.2388 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GILD XGBoost: Starting GridSearchCV fit...
       ‚úÖ CTOR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=1237.1415 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTOR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CTOR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=1678.3221 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTOR XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATRA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=60.9940 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.2886
    - TCN: MSE=0.3195
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2886
        ‚Ä¢ TCN: MSE=0.3195
        ‚Ä¢ Random Forest: MSE=54.7701
        ‚Ä¢ XGBoost: MSE=60.9940
        ‚Ä¢ LightGBM Regressor (CPU): MSE=66.0935
   ‚úÖ ATRA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATRA (TargetReturn): LSTM with MSE=0.2886
üêõ DEBUG: ATRA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATRA.
üêõ DEBUG: ATRA - Moving model to CPU before return...
üêõ DEBUG [23:27:14.540]: ATRA - Returning result metadata...
üêõ DEBUG [23:27:14.540]: Main received result for ATRA
üêõ DEBUG: train_worker started for TNL
  ‚öôÔ∏è Training models for TNL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - TNL: Initiating feature extraction for training.
  [DIAGNOSTIC] TNL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TNL: rows after features available: 126
üéØ TNL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TNL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TNL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TNL: Training LSTM (50 epochs)...
      ‚è≥ TNL LSTM: Epoch 10/50 (20%)
      ‚è≥ TNL LSTM: Epoch 20/50 (40%)
      ‚è≥ TNL LSTM: Epoch 30/50 (60%)
      ‚è≥ TNL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.652043
         RMSE: 0.807492
         R¬≤ Score: -0.6163 (Poor - 61.6% variance explained)
      üîπ TNL: Training TCN (50 epochs)...
      ‚è≥ TNL TCN: Epoch 10/50 (20%)
      ‚è≥ TNL TCN: Epoch 20/50 (40%)
      ‚è≥ TNL TCN: Epoch 30/50 (60%)
      ‚è≥ TNL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.673570
         RMSE: 0.820713
         R¬≤ Score: -0.6697
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TNL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TNL Random Forest: Starting GridSearchCV fit...
       ‚úÖ TNL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.3079 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TNL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TNL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.6095 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TNL XGBoost: Starting GridSearchCV fit...
       ‚úÖ UAN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.9927 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.2442
    - TCN: MSE=0.2375
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2375
        ‚Ä¢ LSTM: MSE=0.2442
        ‚Ä¢ XGBoost: MSE=6.9927
        ‚Ä¢ Random Forest: MSE=7.6310
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.5937
   ‚úÖ UAN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UAN (TargetReturn): TCN with MSE=0.2375
üêõ DEBUG: UAN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UAN.
üêõ DEBUG: UAN - Moving model to CPU before return...
üêõ DEBUG [23:27:21.576]: UAN - Returning result metadata...
üêõ DEBUG: train_worker started for PWR
üêõ DEBUG [23:27:21.577]: Main received result for UAN
  ‚öôÔ∏è Training models for PWR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - PWR: Initiating feature extraction for training.
  [DIAGNOSTIC] PWR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PWR: rows after features available: 126
üéØ PWR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PWR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PWR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PWR: Training LSTM (50 epochs)...
      ‚è≥ PWR LSTM: Epoch 10/50 (20%)
      ‚è≥ PWR LSTM: Epoch 20/50 (40%)
      ‚è≥ PWR LSTM: Epoch 30/50 (60%)
      ‚è≥ PWR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.381577
         RMSE: 0.617719
         R¬≤ Score: -0.9306 (Poor - 93.1% variance explained)
      üîπ PWR: Training TCN (50 epochs)...
      ‚è≥ PWR TCN: Epoch 10/50 (20%)
      ‚è≥ PWR TCN: Epoch 20/50 (40%)
      ‚è≥ PWR TCN: Epoch 30/50 (60%)
      ‚è≥ PWR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.369232
         RMSE: 0.607645
         R¬≤ Score: -0.8682
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PWR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PWR Random Forest: Starting GridSearchCV fit...
       ‚úÖ PWR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.8208 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PWR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRMD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=152.0203 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 119.2s
    - LSTM: MSE=0.3604
    - TCN: MSE=0.1785
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1785
        ‚Ä¢ LSTM: MSE=0.3604
        ‚Ä¢ Random Forest: MSE=99.1118
        ‚Ä¢ XGBoost: MSE=152.0203
        ‚Ä¢ LightGBM Regressor (CPU): MSE=206.2633
   ‚úÖ CRMD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRMD (TargetReturn): TCN with MSE=0.1785
üêõ DEBUG: CRMD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRMD.
üêõ DEBUG: CRMD - Moving model to CPU before return...
üêõ DEBUG [23:27:27.391]: CRMD - Returning result metadata...
üêõ DEBUG [23:27:27.392]: Main received result for CRMD
üêõ DEBUG: train_worker started for EWO
  ‚öôÔ∏è Training models for EWO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - EWO: Initiating feature extraction for training.
  [DIAGNOSTIC] EWO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWO: rows after features available: 126
üéØ EWO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWO: Training LSTM (50 epochs)...
       ‚úÖ PWR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.3237 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PWR XGBoost: Starting GridSearchCV fit...
      ‚è≥ EWO LSTM: Epoch 10/50 (20%)
      ‚è≥ EWO LSTM: Epoch 20/50 (40%)
      ‚è≥ EWO LSTM: Epoch 30/50 (60%)
      ‚è≥ EWO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.151247
         RMSE: 0.388905
         R¬≤ Score: -0.4315 (Poor - 43.2% variance explained)
      üîπ EWO: Training TCN (50 epochs)...
      ‚è≥ EWO TCN: Epoch 10/50 (20%)
      ‚è≥ EWO TCN: Epoch 20/50 (40%)
      ‚è≥ EWO TCN: Epoch 30/50 (60%)
      ‚è≥ EWO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.107508
         RMSE: 0.327883
         R¬≤ Score: -0.0175
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWO Random Forest: Starting GridSearchCV fit...
       ‚úÖ EWO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5081 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4888 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWO XGBoost: Starting GridSearchCV fit...
       ‚úÖ NAMS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.6806 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.4062
    - TCN: MSE=0.2949
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2949
        ‚Ä¢ LSTM: MSE=0.4062
        ‚Ä¢ Random Forest: MSE=26.0678
        ‚Ä¢ XGBoost: MSE=26.6806
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.4379
   ‚úÖ NAMS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NAMS (TargetReturn): TCN with MSE=0.2949
üêõ DEBUG: NAMS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NAMS.
üêõ DEBUG: NAMS - Moving model to CPU before return...
üêõ DEBUG [23:27:34.845]: NAMS - Returning result metadata...
üêõ DEBUG: train_worker started for PAAS
üêõ DEBUG [23:27:34.852]: Main received result for NAMS
üêõ DEBUG: Training progress: 480/959 done
  ‚öôÔ∏è Training models for PAAS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - PAAS: Initiating feature extraction for training.
  [DIAGNOSTIC] PAAS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAAS: rows after features available: 126
üéØ PAAS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAAS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAAS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAAS: Training LSTM (50 epochs)...
      ‚è≥ PAAS LSTM: Epoch 10/50 (20%)
      ‚è≥ PAAS LSTM: Epoch 20/50 (40%)
      ‚è≥ PAAS LSTM: Epoch 30/50 (60%)
       ‚úÖ HGV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.8468 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.5679
    - TCN: MSE=0.6898
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5679
        ‚Ä¢ TCN: MSE=0.6898
        ‚Ä¢ Random Forest: MSE=18.7402
        ‚Ä¢ XGBoost: MSE=18.8468
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.1610
   ‚úÖ HGV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HGV (TargetReturn): LSTM with MSE=0.5679
üêõ DEBUG: HGV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HGV.
üêõ DEBUG: HGV - Moving model to CPU before return...
üêõ DEBUG [23:27:36.403]: HGV - Returning result metadata...
üêõ DEBUG: train_worker started for JBTM
üêõ DEBUG [23:27:36.403]: Main received result for HGV
  ‚öôÔ∏è Training models for JBTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - JBTM: Initiating feature extraction for training.
  [DIAGNOSTIC] JBTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JBTM: rows after features available: 126
üéØ JBTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JBTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JBTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JBTM: Training LSTM (50 epochs)...
      ‚è≥ PAAS LSTM: Epoch 40/50 (80%)
      ‚è≥ JBTM LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.248327
         RMSE: 0.498324
         R¬≤ Score: -0.5743 (Poor - 57.4% variance explained)
      üîπ PAAS: Training TCN (50 epochs)...
      ‚è≥ PAAS TCN: Epoch 10/50 (20%)
      ‚è≥ PAAS TCN: Epoch 20/50 (40%)
      ‚è≥ PAAS TCN: Epoch 30/50 (60%)
      ‚è≥ JBTM LSTM: Epoch 20/50 (40%)
      ‚è≥ PAAS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180976
         RMSE: 0.425412
         R¬≤ Score: -0.1473
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAAS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAAS Random Forest: Starting GridSearchCV fit...
      ‚è≥ JBTM LSTM: Epoch 30/50 (60%)
      ‚è≥ JBTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.567887
         RMSE: 0.753582
         R¬≤ Score: -1.0208 (Poor - 102.1% variance explained)
      üîπ JBTM: Training TCN (50 epochs)...
      ‚è≥ JBTM TCN: Epoch 10/50 (20%)
      ‚è≥ JBTM TCN: Epoch 20/50 (40%)
      ‚è≥ JBTM TCN: Epoch 30/50 (60%)
      ‚è≥ JBTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.538084
         RMSE: 0.733542
         R¬≤ Score: -0.9148
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JBTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JBTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAAS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.0791 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAAS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAAS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.6976 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAAS XGBoost: Starting GridSearchCV fit...
       ‚úÖ JBTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.1883 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JBTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JBTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.8467 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JBTM XGBoost: Starting GridSearchCV fit...
       ‚úÖ DEC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.3561 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.5360
    - TCN: MSE=0.2923
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2923
        ‚Ä¢ LSTM: MSE=0.5360
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.9815
        ‚Ä¢ Random Forest: MSE=15.7449
        ‚Ä¢ XGBoost: MSE=17.3561
   ‚úÖ DEC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DEC (TargetReturn): TCN with MSE=0.2923
üêõ DEBUG: DEC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DEC.
üêõ DEBUG: DEC - Moving model to CPU before return...
üêõ DEBUG [23:27:51.614]: DEC - Returning result metadata...
üêõ DEBUG [23:27:51.615]: Main received result for DEC
üêõ DEBUG: train_worker started for SLVP
  ‚öôÔ∏è Training models for SLVP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - SLVP: Initiating feature extraction for training.
  [DIAGNOSTIC] SLVP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLVP: rows after features available: 126
üéØ SLVP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLVP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLVP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLVP: Training LSTM (50 epochs)...
      ‚è≥ SLVP LSTM: Epoch 10/50 (20%)
      ‚è≥ SLVP LSTM: Epoch 20/50 (40%)
      ‚è≥ SLVP LSTM: Epoch 30/50 (60%)
      ‚è≥ SLVP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.110378
         RMSE: 0.332232
         R¬≤ Score: -0.5615 (Poor - 56.1% variance explained)
      üîπ SLVP: Training TCN (50 epochs)...
      ‚è≥ SLVP TCN: Epoch 10/50 (20%)
      ‚è≥ SLVP TCN: Epoch 20/50 (40%)
      ‚è≥ SLVP TCN: Epoch 30/50 (60%)
      ‚è≥ SLVP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.070018
         RMSE: 0.264609
         R¬≤ Score: 0.0095
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLVP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLVP Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLVP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.8906 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLVP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLVP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.5259 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLVP XGBoost: Starting GridSearchCV fit...
       ‚úÖ PLTM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=22.7676 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.5088
    - TCN: MSE=0.7080
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5088
        ‚Ä¢ TCN: MSE=0.7080
        ‚Ä¢ XGBoost: MSE=22.7676
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.3466
        ‚Ä¢ Random Forest: MSE=32.6926
   ‚úÖ PLTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PLTM (TargetReturn): LSTM with MSE=0.5088
üêõ DEBUG: PLTM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PLTM.
üêõ DEBUG: PLTM - Moving model to CPU before return...
üêõ DEBUG [23:27:59.003]: PLTM - Returning result metadata...
üêõ DEBUG [23:27:59.004]: Main received result for PLTM
üêõ DEBUG: train_worker started for NPK
  ‚öôÔ∏è Training models for NPK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - NPK: Initiating feature extraction for training.
  [DIAGNOSTIC] NPK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NPK: rows after features available: 126
üéØ NPK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NPK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NPK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NPK: Training LSTM (50 epochs)...
      ‚è≥ NPK LSTM: Epoch 10/50 (20%)
      ‚è≥ NPK LSTM: Epoch 20/50 (40%)
      ‚è≥ NPK LSTM: Epoch 30/50 (60%)
      ‚è≥ NPK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.675025
         RMSE: 0.821599
         R¬≤ Score: -0.7472 (Poor - 74.7% variance explained)
      üîπ NPK: Training TCN (50 epochs)...
      ‚è≥ NPK TCN: Epoch 10/50 (20%)
      ‚è≥ NPK TCN: Epoch 20/50 (40%)
      ‚è≥ NPK TCN: Epoch 30/50 (60%)
      ‚è≥ NPK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.730959
         RMSE: 0.854961
         R¬≤ Score: -0.8920
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NPK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NPK Random Forest: Starting GridSearchCV fit...
       ‚úÖ NPK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.9836 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NPK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NPK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.8973 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NPK XGBoost: Starting GridSearchCV fit...
       ‚úÖ PPLT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=84.1171 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.6152
    - TCN: MSE=0.5658
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5658
        ‚Ä¢ LSTM: MSE=0.6152
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.0663
        ‚Ä¢ Random Forest: MSE=67.8728
        ‚Ä¢ XGBoost: MSE=84.1171
   ‚úÖ PPLT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PPLT (TargetReturn): TCN with MSE=0.5658
üêõ DEBUG: PPLT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PPLT.
üêõ DEBUG: PPLT - Moving model to CPU before return...
üêõ DEBUG [23:28:07.344]: PPLT - Returning result metadata...
üêõ DEBUG [23:28:07.344]: Main received result for PPLT
üêõ DEBUG: train_worker started for CENX
üêõ DEBUG: Training progress: 484/959 done
  ‚öôÔ∏è Training models for CENX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - CENX: Initiating feature extraction for training.
  [DIAGNOSTIC] CENX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CENX: rows after features available: 126
üéØ CENX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CENX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CENX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CENX: Training LSTM (50 epochs)...
      ‚è≥ CENX LSTM: Epoch 10/50 (20%)
       ‚úÖ DLO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=39.4513 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.6680
    - TCN: MSE=0.6859
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6680
        ‚Ä¢ TCN: MSE=0.6859
        ‚Ä¢ LightGBM Regressor (CPU): MSE=32.4889
        ‚Ä¢ XGBoost: MSE=39.4513
        ‚Ä¢ Random Forest: MSE=42.9534
   ‚úÖ DLO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DLO (TargetReturn): LSTM with MSE=0.6680
üêõ DEBUG: DLO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DLO.
üêõ DEBUG: DLO - Moving model to CPU before return...
üêõ DEBUG [23:28:08.109]: DLO - Returning result metadata...
üêõ DEBUG [23:28:08.110]: Main received result for DLO
üêõ DEBUG: train_worker started for GRAB
  ‚öôÔ∏è Training models for GRAB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - GRAB: Initiating feature extraction for training.
  [DIAGNOSTIC] GRAB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRAB: rows after features available: 126
üéØ GRAB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRAB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRAB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRAB: Training LSTM (50 epochs)...
      ‚è≥ CENX LSTM: Epoch 20/50 (40%)
      ‚è≥ GRAB LSTM: Epoch 10/50 (20%)
      ‚è≥ CENX LSTM: Epoch 30/50 (60%)
      ‚è≥ GRAB LSTM: Epoch 20/50 (40%)
      ‚è≥ CENX LSTM: Epoch 40/50 (80%)
      ‚è≥ GRAB LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.468488
         RMSE: 0.684462
         R¬≤ Score: -0.4542 (Poor - 45.4% variance explained)
      üîπ CENX: Training TCN (50 epochs)...
      ‚è≥ CENX TCN: Epoch 10/50 (20%)
      ‚è≥ CENX TCN: Epoch 20/50 (40%)
      ‚è≥ CENX TCN: Epoch 30/50 (60%)
      ‚è≥ GRAB LSTM: Epoch 40/50 (80%)
      ‚è≥ CENX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.532975
         RMSE: 0.730051
         R¬≤ Score: -0.6544
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CENX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CENX Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.180608
         RMSE: 0.424980
         R¬≤ Score: -0.6292 (Poor - 62.9% variance explained)
      üîπ GRAB: Training TCN (50 epochs)...
      ‚è≥ GRAB TCN: Epoch 10/50 (20%)
      ‚è≥ GRAB TCN: Epoch 20/50 (40%)
      ‚è≥ GRAB TCN: Epoch 30/50 (60%)
      ‚è≥ GRAB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.134441
         RMSE: 0.366662
         R¬≤ Score: -0.2127
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRAB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRAB Random Forest: Starting GridSearchCV fit...
       ‚úÖ ATI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=40.0259 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.5111
    - TCN: MSE=0.3708
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3708
        ‚Ä¢ LSTM: MSE=0.5111
        ‚Ä¢ XGBoost: MSE=40.0259
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.6064
        ‚Ä¢ Random Forest: MSE=53.5895
   ‚úÖ ATI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATI (TargetReturn): TCN with MSE=0.3708
üêõ DEBUG: ATI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATI.
üêõ DEBUG: ATI - Moving model to CPU before return...
üêõ DEBUG [23:28:11.910]: ATI - Returning result metadata...
üêõ DEBUG: train_worker started for MOS
üêõ DEBUG [23:28:11.913]: Main received result for ATI
  ‚öôÔ∏è Training models for MOS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - MOS: Initiating feature extraction for training.
  [DIAGNOSTIC] MOS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MOS: rows after features available: 126
üéØ MOS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MOS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MOS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MOS: Training LSTM (50 epochs)...
      ‚è≥ MOS LSTM: Epoch 10/50 (20%)
      ‚è≥ MOS LSTM: Epoch 20/50 (40%)
       ‚úÖ CENX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.1125 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CENX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MOS LSTM: Epoch 30/50 (60%)
       ‚úÖ CENX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.4760 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CENX XGBoost: Starting GridSearchCV fit...
      ‚è≥ MOS LSTM: Epoch 40/50 (80%)
       ‚úÖ GRAB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.9934 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRAB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.269967
         RMSE: 0.519583
         R¬≤ Score: -0.9906 (Poor - 99.1% variance explained)
      üîπ MOS: Training TCN (50 epochs)...
      ‚è≥ MOS TCN: Epoch 10/50 (20%)
      ‚è≥ MOS TCN: Epoch 20/50 (40%)
      ‚è≥ MOS TCN: Epoch 30/50 (60%)
       ‚úÖ GRAB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.4481 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRAB XGBoost: Starting GridSearchCV fit...
      ‚è≥ MOS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.136485
         RMSE: 0.369438
         R¬≤ Score: -0.0064
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MOS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MOS Random Forest: Starting GridSearchCV fit...
       ‚úÖ ESPO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.0218 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.4519
    - TCN: MSE=0.2546
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2546
        ‚Ä¢ LSTM: MSE=0.4519
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.0114
        ‚Ä¢ Random Forest: MSE=9.3523
        ‚Ä¢ XGBoost: MSE=14.0218
   ‚úÖ ESPO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ESPO (TargetReturn): TCN with MSE=0.2546
üêõ DEBUG: ESPO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ESPO.
üêõ DEBUG: ESPO - Moving model to CPU before return...
üêõ DEBUG [23:28:15.784]: ESPO - Returning result metadata...
üêõ DEBUG [23:28:15.785]: Main received result for ESPOüêõ DEBUG: train_worker started for CASY

  ‚öôÔ∏è Training models for CASY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - CASY: Initiating feature extraction for training.
  [DIAGNOSTIC] CASY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CASY: rows after features available: 126
üéØ CASY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CASY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CASY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CASY: Training LSTM (50 epochs)...
      ‚è≥ CASY LSTM: Epoch 10/50 (20%)
      ‚è≥ CASY LSTM: Epoch 20/50 (40%)
      ‚è≥ CASY LSTM: Epoch 30/50 (60%)
      ‚è≥ CASY LSTM: Epoch 40/50 (80%)
       ‚úÖ MOS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.2358 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MOS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.200853
         RMSE: 0.448166
         R¬≤ Score: -0.7186 (Poor - 71.9% variance explained)
      üîπ CASY: Training TCN (50 epochs)...
      ‚è≥ CASY TCN: Epoch 10/50 (20%)
      ‚è≥ CASY TCN: Epoch 20/50 (40%)
      ‚è≥ CASY TCN: Epoch 30/50 (60%)
      ‚è≥ CASY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.127927
         RMSE: 0.357668
         R¬≤ Score: -0.0946
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CASY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CASY Random Forest: Starting GridSearchCV fit...
       ‚úÖ MOS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=37.7537 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MOS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CASY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.4004 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CASY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CASY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.0881 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CASY XGBoost: Starting GridSearchCV fit...
       ‚úÖ VLN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=56.9120 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.1764
    - TCN: MSE=0.1158
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1158
        ‚Ä¢ LSTM: MSE=0.1764
        ‚Ä¢ LightGBM Regressor (CPU): MSE=52.4509
        ‚Ä¢ XGBoost: MSE=56.9120
        ‚Ä¢ Random Forest: MSE=57.7301
   ‚úÖ VLN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VLN (TargetReturn): TCN with MSE=0.1158
üêõ DEBUG: VLN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VLN.
üêõ DEBUG: VLN - Moving model to CPU before return...
üêõ DEBUG [23:28:32.535]: VLN - Returning result metadata...
üêõ DEBUG [23:28:32.535]: Main received result for VLN
üêõ DEBUG: Training progress: 488/959 done
üêõ DEBUG: train_worker started for XAR
  ‚öôÔ∏è Training models for XAR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - XAR: Initiating feature extraction for training.
  [DIAGNOSTIC] XAR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XAR: rows after features available: 126
üéØ XAR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XAR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XAR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XAR: Training LSTM (50 epochs)...
      ‚è≥ XAR LSTM: Epoch 10/50 (20%)
      ‚è≥ XAR LSTM: Epoch 20/50 (40%)
      ‚è≥ XAR LSTM: Epoch 30/50 (60%)
      ‚è≥ XAR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.350998
         RMSE: 0.592451
         R¬≤ Score: -0.6134 (Poor - 61.3% variance explained)
      üîπ XAR: Training TCN (50 epochs)...
      ‚è≥ XAR TCN: Epoch 10/50 (20%)
      ‚è≥ XAR TCN: Epoch 20/50 (40%)
      ‚è≥ XAR TCN: Epoch 30/50 (60%)
      ‚è≥ XAR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.254507
         RMSE: 0.504487
         R¬≤ Score: -0.1699
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XAR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XAR Random Forest: Starting GridSearchCV fit...
       ‚úÖ XAR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.5944 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XAR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ XAR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.2264 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XAR XGBoost: Starting GridSearchCV fit...
       ‚úÖ AMPL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=61.9230 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.5024
    - TCN: MSE=0.5981
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5024
        ‚Ä¢ TCN: MSE=0.5981
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.4578
        ‚Ä¢ XGBoost: MSE=61.9230
        ‚Ä¢ Random Forest: MSE=65.8861
   ‚úÖ AMPL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMPL (TargetReturn): LSTM with MSE=0.5024
üêõ DEBUG: AMPL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMPL.
üêõ DEBUG: AMPL - Moving model to CPU before return...
üêõ DEBUG [23:29:06.096]: AMPL - Returning result metadata...
üêõ DEBUG: train_worker started for NIC
üêõ DEBUG [23:29:06.097]: Main received result for AMPL
  ‚öôÔ∏è Training models for NIC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NIC: Initiating feature extraction for training.
  [DIAGNOSTIC] NIC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NIC: rows after features available: 126
üéØ NIC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NIC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NIC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NIC: Training LSTM (50 epochs)...
      ‚è≥ NIC LSTM: Epoch 10/50 (20%)
      ‚è≥ NIC LSTM: Epoch 20/50 (40%)
      ‚è≥ NIC LSTM: Epoch 30/50 (60%)
      ‚è≥ NIC LSTM: Epoch 40/50 (80%)
       ‚úÖ CTOR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6141.1326 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 114.1s
    - LSTM: MSE=0.5007
    - TCN: MSE=0.2971
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2971
        ‚Ä¢ LSTM: MSE=0.5007
        ‚Ä¢ Random Forest: MSE=1237.1415
        ‚Ä¢ LightGBM Regressor (CPU): MSE=1678.3221
        ‚Ä¢ XGBoost: MSE=6141.1326
   ‚úÖ CTOR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTOR (TargetReturn): TCN with MSE=0.2971
üêõ DEBUG: CTOR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTOR.
üêõ DEBUG: CTOR - Moving model to CPU before return...
üêõ DEBUG [23:29:08.460]: CTOR - Returning result metadata...
üêõ DEBUG: train_worker started for QXO
      üìä LSTM Regression Metrics:
         MSE: 0.497510
         RMSE: 0.705344
         R¬≤ Score: -0.9250 (Poor - 92.5% variance explained)
      üîπ NIC: Training TCN (50 epochs)...
  ‚öôÔ∏è Training models for QXO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - QXO: Initiating feature extraction for training.
  [DIAGNOSTIC] QXO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QXO: rows after features available: 126
üéØ QXO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QXO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QXO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QXO: Training LSTM (50 epochs)...
      ‚è≥ NIC TCN: Epoch 10/50 (20%)
      ‚è≥ NIC TCN: Epoch 20/50 (40%)
      ‚è≥ NIC TCN: Epoch 30/50 (60%)
      ‚è≥ NIC TCN: Epoch 40/50 (80%)
      ‚è≥ QXO LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.293706
         RMSE: 0.541946
         R¬≤ Score: -0.1364
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NIC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NIC Random Forest: Starting GridSearchCV fit...
      ‚è≥ QXO LSTM: Epoch 20/50 (40%)
      ‚è≥ QXO LSTM: Epoch 30/50 (60%)
      ‚è≥ QXO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.426058
         RMSE: 0.652731
         R¬≤ Score: -0.8723 (Poor - 87.2% variance explained)
      üîπ QXO: Training TCN (50 epochs)...
      ‚è≥ QXO TCN: Epoch 10/50 (20%)
      ‚è≥ QXO TCN: Epoch 20/50 (40%)
      ‚è≥ QXO TCN: Epoch 30/50 (60%)
      ‚è≥ QXO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.348769
         RMSE: 0.590567
         R¬≤ Score: -0.5327
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QXO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QXO Random Forest: Starting GridSearchCV fit...
       ‚úÖ NIC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.5579 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NIC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NIC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.1860 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NIC XGBoost: Starting GridSearchCV fit...
       ‚úÖ GILD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.2520 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.1202
    - TCN: MSE=0.1004
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1004
        ‚Ä¢ LSTM: MSE=0.1202
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.2388
        ‚Ä¢ Random Forest: MSE=9.0082
        ‚Ä¢ XGBoost: MSE=9.2520
   ‚úÖ GILD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GILD (TargetReturn): TCN with MSE=0.1004
üêõ DEBUG: GILD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GILD.
üêõ DEBUG: GILD - Moving model to CPU before return...
üêõ DEBUG [23:29:13.443]: GILD - Returning result metadata...
üêõ DEBUG [23:29:13.444]: Main received result for GILD
üêõ DEBUG [23:29:13.444]: Main received result for CTOR
üêõ DEBUG: train_worker started for STX
  ‚öôÔ∏è Training models for STX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - STX: Initiating feature extraction for training.
  [DIAGNOSTIC] STX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STX: rows after features available: 126
üéØ STX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STX: Training LSTM (50 epochs)...
      ‚è≥ STX LSTM: Epoch 10/50 (20%)
      ‚è≥ STX LSTM: Epoch 20/50 (40%)
       ‚úÖ QXO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.7067 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QXO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ STX LSTM: Epoch 30/50 (60%)
       ‚úÖ QXO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=33.2426 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QXO XGBoost: Starting GridSearchCV fit...
      ‚è≥ STX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.549419
         RMSE: 0.741228
         R¬≤ Score: -0.9023 (Poor - 90.2% variance explained)
      üîπ STX: Training TCN (50 epochs)...
      ‚è≥ STX TCN: Epoch 10/50 (20%)
      ‚è≥ STX TCN: Epoch 20/50 (40%)
      ‚è≥ STX TCN: Epoch 30/50 (60%)
      ‚è≥ STX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.324078
         RMSE: 0.569279
         R¬≤ Score: -0.1221
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STX Random Forest: Starting GridSearchCV fit...
       ‚úÖ TNL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.9826 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.6520
    - TCN: MSE=0.6736
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6520
        ‚Ä¢ TCN: MSE=0.6736
        ‚Ä¢ Random Forest: MSE=8.3079
        ‚Ä¢ XGBoost: MSE=8.9826
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.6095
   ‚úÖ TNL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TNL (TargetReturn): LSTM with MSE=0.6520
üêõ DEBUG: TNL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TNL.
üêõ DEBUG: TNL - Moving model to CPU before return...
üêõ DEBUG [23:29:18.261]: TNL - Returning result metadata...
üêõ DEBUG [23:29:18.261]: Main received result for TNL
üêõ DEBUG: Training progress: 492/959 done
üêõ DEBUG: train_worker started for ETH
  ‚öôÔ∏è Training models for ETH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - ETH: Initiating feature extraction for training.
  [DIAGNOSTIC] ETH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETH: rows after features available: 126
üéØ ETH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETH: Training LSTM (50 epochs)...
      ‚è≥ ETH LSTM: Epoch 10/50 (20%)
       ‚úÖ STX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.5514 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ETH LSTM: Epoch 20/50 (40%)
      ‚è≥ ETH LSTM: Epoch 30/50 (60%)
       ‚úÖ STX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=53.8527 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STX XGBoost: Starting GridSearchCV fit...
      ‚è≥ ETH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.515674
         RMSE: 0.718104
         R¬≤ Score: -0.7929 (Poor - 79.3% variance explained)
      üîπ ETH: Training TCN (50 epochs)...
      ‚è≥ ETH TCN: Epoch 10/50 (20%)
      ‚è≥ ETH TCN: Epoch 20/50 (40%)
      ‚è≥ ETH TCN: Epoch 30/50 (60%)
      ‚è≥ ETH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.470062
         RMSE: 0.685610
         R¬≤ Score: -0.6343
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETH Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=47.3470 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ETH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=105.6378 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETH XGBoost: Starting GridSearchCV fit...
       ‚úÖ PWR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.0278 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.3816
    - TCN: MSE=0.3692
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3692
        ‚Ä¢ LSTM: MSE=0.3816
        ‚Ä¢ Random Forest: MSE=25.8208
        ‚Ä¢ XGBoost: MSE=30.0278
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.3237
   ‚úÖ PWR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PWR (TargetReturn): TCN with MSE=0.3692
üêõ DEBUG: PWR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PWR.
üêõ DEBUG: PWR - Moving model to CPU before return...
üêõ DEBUG [23:29:27.717]: PWR - Returning result metadata...
üêõ DEBUG [23:29:27.717]: Main received result for PWR
üêõ DEBUG: train_worker started for BKCH
  ‚öôÔ∏è Training models for BKCH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - BKCH: Initiating feature extraction for training.
  [DIAGNOSTIC] BKCH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BKCH: rows after features available: 126
üéØ BKCH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BKCH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BKCH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BKCH: Training LSTM (50 epochs)...
      ‚è≥ BKCH LSTM: Epoch 10/50 (20%)
      ‚è≥ BKCH LSTM: Epoch 20/50 (40%)
      ‚è≥ BKCH LSTM: Epoch 30/50 (60%)
      ‚è≥ BKCH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.499579
         RMSE: 0.706809
         R¬≤ Score: -0.6111 (Poor - 61.1% variance explained)
      üîπ BKCH: Training TCN (50 epochs)...
      ‚è≥ BKCH TCN: Epoch 10/50 (20%)
      ‚è≥ BKCH TCN: Epoch 20/50 (40%)
      ‚è≥ BKCH TCN: Epoch 30/50 (60%)
      ‚è≥ BKCH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.580273
         RMSE: 0.761756
         R¬≤ Score: -0.8713
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BKCH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BKCH Random Forest: Starting GridSearchCV fit...
       ‚úÖ BKCH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.3297 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BKCH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.0624 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.5s
    - LSTM: MSE=0.1512
    - TCN: MSE=0.1075
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1075
        ‚Ä¢ LSTM: MSE=0.1512
        ‚Ä¢ XGBoost: MSE=5.0624
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4888
        ‚Ä¢ Random Forest: MSE=5.5081
   ‚úÖ EWO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EWO (TargetReturn): TCN with MSE=0.1075
üêõ DEBUG: EWO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWO.
üêõ DEBUG: EWO - Moving model to CPU before return...
üêõ DEBUG [23:29:33.332]: EWO - Returning result metadata...
üêõ DEBUG: train_worker started for OCUL
üêõ DEBUG [23:29:33.333]: Main received result for EWO
  ‚öôÔ∏è Training models for OCUL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - OCUL: Initiating feature extraction for training.
  [DIAGNOSTIC] OCUL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OCUL: rows after features available: 126
üéØ OCUL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OCUL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OCUL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OCUL: Training LSTM (50 epochs)...
      ‚è≥ OCUL LSTM: Epoch 10/50 (20%)
       ‚úÖ BKCH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=77.8669 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BKCH XGBoost: Starting GridSearchCV fit...
      ‚è≥ OCUL LSTM: Epoch 20/50 (40%)
      ‚è≥ OCUL LSTM: Epoch 30/50 (60%)
      ‚è≥ OCUL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.248194
         RMSE: 0.498190
         R¬≤ Score: -0.6472 (Poor - 64.7% variance explained)
      üîπ OCUL: Training TCN (50 epochs)...
      ‚è≥ OCUL TCN: Epoch 10/50 (20%)
      ‚è≥ OCUL TCN: Epoch 20/50 (40%)
      ‚è≥ OCUL TCN: Epoch 30/50 (60%)
      ‚è≥ OCUL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.283660
         RMSE: 0.532597
         R¬≤ Score: -0.8826
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OCUL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OCUL Random Forest: Starting GridSearchCV fit...
       ‚úÖ OCUL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=46.6075 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OCUL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAAS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=21.5061 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.2483
    - TCN: MSE=0.1810
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1810
        ‚Ä¢ LSTM: MSE=0.2483
        ‚Ä¢ XGBoost: MSE=21.5061
        ‚Ä¢ Random Forest: MSE=24.0791
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.6976
   ‚úÖ PAAS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAAS (TargetReturn): TCN with MSE=0.1810
üêõ DEBUG: PAAS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAAS.
üêõ DEBUG: PAAS - Moving model to CPU before return...
üêõ DEBUG [23:29:39.117]: PAAS - Returning result metadata...
üêõ DEBUG [23:29:39.117]: Main received result for PAAS
üêõ DEBUG: train_worker started for ETHW
  ‚öôÔ∏è Training models for ETHW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - ETHW: Initiating feature extraction for training.
  [DIAGNOSTIC] ETHW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETHW: rows after features available: 126
üéØ ETHW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETHW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETHW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETHW: Training LSTM (50 epochs)...
      ‚è≥ ETHW LSTM: Epoch 10/50 (20%)
       ‚úÖ OCUL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=61.0003 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OCUL XGBoost: Starting GridSearchCV fit...
      ‚è≥ ETHW LSTM: Epoch 20/50 (40%)
      ‚è≥ ETHW LSTM: Epoch 30/50 (60%)
      ‚è≥ ETHW LSTM: Epoch 40/50 (80%)
       ‚úÖ JBTM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.4520 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.5679
    - TCN: MSE=0.5381
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5381
        ‚Ä¢ LSTM: MSE=0.5679
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.8467
        ‚Ä¢ Random Forest: MSE=20.1883
        ‚Ä¢ XGBoost: MSE=27.4520
   ‚úÖ JBTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JBTM (TargetReturn): TCN with MSE=0.5381
üêõ DEBUG: JBTM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JBTM.
üêõ DEBUG: JBTM - Moving model to CPU before return...
üêõ DEBUG [23:29:41.420]: JBTM - Returning result metadata...
üêõ DEBUG: train_worker started for ETHV
üêõ DEBUG [23:29:41.422]: Main received result for JBTM
üêõ DEBUG: Training progress: 496/959 done
  ‚öôÔ∏è Training models for ETHV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - ETHV: Initiating feature extraction for training.
  [DIAGNOSTIC] ETHV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETHV: rows after features available: 126
üéØ ETHV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETHV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETHV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETHV: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.528548
         RMSE: 0.727013
         R¬≤ Score: -0.8390 (Poor - 83.9% variance explained)
      üîπ ETHW: Training TCN (50 epochs)...
      ‚è≥ ETHW TCN: Epoch 10/50 (20%)
      ‚è≥ ETHW TCN: Epoch 20/50 (40%)
      ‚è≥ ETHW TCN: Epoch 30/50 (60%)
      ‚è≥ ETHW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.481237
         RMSE: 0.693713
         R¬≤ Score: -0.6744
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETHW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETHW Random Forest: Starting GridSearchCV fit...
      ‚è≥ ETHV LSTM: Epoch 10/50 (20%)
      ‚è≥ ETHV LSTM: Epoch 20/50 (40%)
      ‚è≥ ETHV LSTM: Epoch 30/50 (60%)
      ‚è≥ ETHV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.582753
         RMSE: 0.763382
         R¬≤ Score: -1.0287 (Poor - 102.9% variance explained)
      üîπ ETHV: Training TCN (50 epochs)...
      ‚è≥ ETHV TCN: Epoch 10/50 (20%)
      ‚è≥ ETHV TCN: Epoch 20/50 (40%)
      ‚è≥ ETHV TCN: Epoch 30/50 (60%)
      ‚è≥ ETHV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.464579
         RMSE: 0.681601
         R¬≤ Score: -0.6173
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETHV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETHV Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETHW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.6128 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETHW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ETHW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=77.6564 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETHW XGBoost: Starting GridSearchCV fit...
       ‚úÖ ETHV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.0800 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETHV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ETHV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=72.9389 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETHV XGBoost: Starting GridSearchCV fit...
       ‚úÖ SLVP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.0478 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.1104
    - TCN: MSE=0.0700
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0700
        ‚Ä¢ LSTM: MSE=0.1104
        ‚Ä¢ Random Forest: MSE=23.8906
        ‚Ä¢ XGBoost: MSE=24.0478
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.5259
   ‚úÖ SLVP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLVP (TargetReturn): TCN with MSE=0.0700
üêõ DEBUG: SLVP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLVP.
üêõ DEBUG: SLVP - Moving model to CPU before return...
üêõ DEBUG [23:29:58.824]: SLVP - Returning result metadata...
üêõ DEBUG: train_worker started for POET
üêõ DEBUG [23:29:58.826]: Main received result for SLVP
  ‚öôÔ∏è Training models for POET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - POET: Initiating feature extraction for training.
  [DIAGNOSTIC] POET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ POET: rows after features available: 126
üéØ POET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] POET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö POET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ POET: Training LSTM (50 epochs)...
      ‚è≥ POET LSTM: Epoch 10/50 (20%)
      ‚è≥ POET LSTM: Epoch 20/50 (40%)
      ‚è≥ POET LSTM: Epoch 30/50 (60%)
      ‚è≥ POET LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.335874
         RMSE: 0.579547
         R¬≤ Score: -0.5772 (Poor - 57.7% variance explained)
      üîπ POET: Training TCN (50 epochs)...
      ‚è≥ POET TCN: Epoch 10/50 (20%)
      ‚è≥ POET TCN: Epoch 20/50 (40%)
      ‚è≥ POET TCN: Epoch 30/50 (60%)
      ‚è≥ POET TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.323326
         RMSE: 0.568617
         R¬≤ Score: -0.5183
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä POET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ POET Random Forest: Starting GridSearchCV fit...
       ‚úÖ NPK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.6952 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.6750
    - TCN: MSE=0.7310
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6750
        ‚Ä¢ TCN: MSE=0.7310
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.8973
        ‚Ä¢ Random Forest: MSE=8.9836
        ‚Ä¢ XGBoost: MSE=10.6952
   ‚úÖ NPK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NPK (TargetReturn): LSTM with MSE=0.6750
üêõ DEBUG: NPK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NPK.
üêõ DEBUG: NPK - Moving model to CPU before return...
üêõ DEBUG [23:30:01.755]: NPK - Returning result metadata...
üêõ DEBUG: train_worker started for EZET
üêõ DEBUG [23:30:01.756]: Main received result for NPK
  ‚öôÔ∏è Training models for EZET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - EZET: Initiating feature extraction for training.
  [DIAGNOSTIC] EZET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EZET: rows after features available: 126
üéØ EZET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EZET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EZET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EZET: Training LSTM (50 epochs)...
      ‚è≥ EZET LSTM: Epoch 10/50 (20%)
      ‚è≥ EZET LSTM: Epoch 20/50 (40%)
      ‚è≥ EZET LSTM: Epoch 30/50 (60%)
      ‚è≥ EZET LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.565072
         RMSE: 0.751713
         R¬≤ Score: -0.9635 (Poor - 96.4% variance explained)
      üîπ EZET: Training TCN (50 epochs)...
      ‚è≥ EZET TCN: Epoch 10/50 (20%)
      ‚è≥ EZET TCN: Epoch 20/50 (40%)
       ‚úÖ POET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=138.8325 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ POET LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EZET TCN: Epoch 30/50 (60%)
      ‚è≥ EZET TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.463087
         RMSE: 0.680505
         R¬≤ Score: -0.6092
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EZET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EZET Random Forest: Starting GridSearchCV fit...
       ‚úÖ POET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=88.2166 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ POET XGBoost: Starting GridSearchCV fit...
       ‚úÖ EZET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.1506 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EZET LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EZET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=83.7489 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EZET XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRAB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.8423 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.1806
    - TCN: MSE=0.1344
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1344
        ‚Ä¢ LSTM: MSE=0.1806
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.4481
        ‚Ä¢ XGBoost: MSE=26.8423
        ‚Ä¢ Random Forest: MSE=29.9934
   ‚úÖ GRAB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRAB (TargetReturn): TCN with MSE=0.1344
üêõ DEBUG: GRAB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRAB.
üêõ DEBUG: GRAB - Moving model to CPU before return...
üêõ DEBUG [23:30:14.468]: GRAB - Returning result metadata...
üêõ DEBUG: train_worker started for RSI
  ‚öôÔ∏è Training models for RSI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - RSI: Initiating feature extraction for training.
  [DIAGNOSTIC] RSI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RSI: rows after features available: 126
üéØ RSI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RSI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RSI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RSI: Training LSTM (50 epochs)...
      ‚è≥ RSI LSTM: Epoch 10/50 (20%)
      ‚è≥ RSI LSTM: Epoch 20/50 (40%)
       ‚úÖ CENX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=49.6067 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.4685
    - TCN: MSE=0.5330
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4685
        ‚Ä¢ TCN: MSE=0.5330
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.4760
        ‚Ä¢ Random Forest: MSE=39.1125
        ‚Ä¢ XGBoost: MSE=49.6067
   ‚úÖ CENX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CENX (TargetReturn): LSTM with MSE=0.4685
üêõ DEBUG: CENX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CENX.
üêõ DEBUG: CENX - Moving model to CPU before return...
üêõ DEBUG [23:30:15.633]: CENX - Returning result metadata...
üêõ DEBUG: train_worker started for FETH
üêõ DEBUG [23:30:15.644]: Main received result for CENX
üêõ DEBUG [23:30:15.644]: Main received result for GRAB
üêõ DEBUG: Training progress: 500/959 done
  ‚öôÔ∏è Training models for FETH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FETH: Initiating feature extraction for training.
  [DIAGNOSTIC] FETH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FETH: rows after features available: 126
üéØ FETH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FETH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FETH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FETH: Training LSTM (50 epochs)...
      ‚è≥ RSI LSTM: Epoch 30/50 (60%)
      ‚è≥ FETH LSTM: Epoch 10/50 (20%)
      ‚è≥ RSI LSTM: Epoch 40/50 (80%)
      ‚è≥ FETH LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.538303
         RMSE: 0.733691
         R¬≤ Score: -0.7205 (Poor - 72.0% variance explained)
      üîπ RSI: Training TCN (50 epochs)...
      ‚è≥ FETH LSTM: Epoch 30/50 (60%)
      ‚è≥ RSI TCN: Epoch 10/50 (20%)
      ‚è≥ RSI TCN: Epoch 20/50 (40%)
      ‚è≥ RSI TCN: Epoch 30/50 (60%)
      ‚è≥ RSI TCN: Epoch 40/50 (80%)
      ‚è≥ FETH LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.596566
         RMSE: 0.772377
         R¬≤ Score: -0.9067
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RSI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RSI Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.373417
         RMSE: 0.611079
         R¬≤ Score: -0.2971 (Poor - 29.7% variance explained)
      üîπ FETH: Training TCN (50 epochs)...
      ‚è≥ FETH TCN: Epoch 10/50 (20%)
      ‚è≥ FETH TCN: Epoch 20/50 (40%)
      ‚è≥ FETH TCN: Epoch 30/50 (60%)
      ‚è≥ FETH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.588897
         RMSE: 0.767396
         R¬≤ Score: -1.0456
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FETH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FETH Random Forest: Starting GridSearchCV fit...
       ‚úÖ CASY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.1644 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.2009
    - TCN: MSE=0.1279
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1279
        ‚Ä¢ LSTM: MSE=0.2009
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0881
        ‚Ä¢ Random Forest: MSE=7.4004
        ‚Ä¢ XGBoost: MSE=9.1644
   ‚úÖ CASY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CASY (TargetReturn): TCN with MSE=0.1279
üêõ DEBUG: CASY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CASY.
üêõ DEBUG: CASY - Moving model to CPU before return...
üêõ DEBUG [23:30:20.821]: CASY - Returning result metadata...
üêõ DEBUG: train_worker started for PERI
       ‚úÖ RSI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.5634 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RSI LightGBM Regressor (CPU): Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for PERI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - PERI: Initiating feature extraction for training.
  [DIAGNOSTIC] PERI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PERI: rows after features available: 126
üéØ PERI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PERI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PERI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PERI: Training LSTM (50 epochs)...
       ‚úÖ MOS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=57.5001 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.5s
    - LSTM: MSE=0.2700
    - TCN: MSE=0.1365
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1365
        ‚Ä¢ LSTM: MSE=0.2700
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.7537
        ‚Ä¢ Random Forest: MSE=38.2358
        ‚Ä¢ XGBoost: MSE=57.5001
   ‚úÖ MOS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MOS (TargetReturn): TCN with MSE=0.1365
üêõ DEBUG: MOS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MOS.
üêõ DEBUG: MOS - Moving model to CPU before return...
üêõ DEBUG [23:30:20.972]: MOS - Returning result metadata...
üêõ DEBUG: train_worker started for TETH
üêõ DEBUG [23:30:20.973]: Main received result for MOS
üêõ DEBUG [23:30:20.973]: Main received result for CASY
  ‚öôÔ∏è Training models for TETH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - TETH: Initiating feature extraction for training.
  [DIAGNOSTIC] TETH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TETH: rows after features available: 126
üéØ TETH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TETH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TETH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TETH: Training LSTM (50 epochs)...
      ‚è≥ PERI LSTM: Epoch 10/50 (20%)
      ‚è≥ TETH LSTM: Epoch 10/50 (20%)
       ‚úÖ RSI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.6963 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RSI XGBoost: Starting GridSearchCV fit...
      ‚è≥ PERI LSTM: Epoch 20/50 (40%)
      ‚è≥ TETH LSTM: Epoch 20/50 (40%)
       ‚úÖ FETH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.3994 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FETH LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PERI LSTM: Epoch 30/50 (60%)
      ‚è≥ TETH LSTM: Epoch 30/50 (60%)
       ‚úÖ FETH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=104.4702 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FETH XGBoost: Starting GridSearchCV fit...
      ‚è≥ TETH LSTM: Epoch 40/50 (80%)
      ‚è≥ PERI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.442709
         RMSE: 0.665364
         R¬≤ Score: -0.5407 (Poor - 54.1% variance explained)
      üîπ TETH: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.245255
         RMSE: 0.495232
         R¬≤ Score: -0.5590 (Poor - 55.9% variance explained)
      üîπ PERI: Training TCN (50 epochs)...
      ‚è≥ TETH TCN: Epoch 10/50 (20%)
      ‚è≥ PERI TCN: Epoch 10/50 (20%)
      ‚è≥ TETH TCN: Epoch 20/50 (40%)
      ‚è≥ PERI TCN: Epoch 20/50 (40%)
      ‚è≥ TETH TCN: Epoch 30/50 (60%)
      ‚è≥ PERI TCN: Epoch 30/50 (60%)
      ‚è≥ TETH TCN: Epoch 40/50 (80%)
      ‚è≥ PERI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.591270
         RMSE: 0.768941
         R¬≤ Score: -1.0577
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TETH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TETH Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.195285
         RMSE: 0.441911
         R¬≤ Score: -0.2413
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PERI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PERI Random Forest: Starting GridSearchCV fit...
       ‚úÖ TETH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.3391 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TETH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PERI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.3789 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PERI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TETH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=107.2361 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TETH XGBoost: Starting GridSearchCV fit...
       ‚úÖ PERI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.3863 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PERI XGBoost: Starting GridSearchCV fit...
       ‚úÖ XAR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.0770 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.3510
    - TCN: MSE=0.2545
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2545
        ‚Ä¢ LSTM: MSE=0.3510
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.2264
        ‚Ä¢ Random Forest: MSE=13.5944
        ‚Ä¢ XGBoost: MSE=18.0770
   ‚úÖ XAR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XAR (TargetReturn): TCN with MSE=0.2545
üêõ DEBUG: XAR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XAR.
üêõ DEBUG: XAR - Moving model to CPU before return...
üêõ DEBUG [23:30:36.583]: XAR - Returning result metadata...
üêõ DEBUG [23:30:36.584]: Main received result for XARüêõ DEBUG: train_worker started for ETHA

  ‚öôÔ∏è Training models for ETHA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ETHA: Initiating feature extraction for training.
  [DIAGNOSTIC] ETHA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETHA: rows after features available: 126
üéØ ETHA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETHA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETHA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETHA: Training LSTM (50 epochs)...
      ‚è≥ ETHA LSTM: Epoch 10/50 (20%)
      ‚è≥ ETHA LSTM: Epoch 20/50 (40%)
      ‚è≥ ETHA LSTM: Epoch 30/50 (60%)
      ‚è≥ ETHA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.592946
         RMSE: 0.770030
         R¬≤ Score: -1.0591 (Poor - 105.9% variance explained)
      üîπ ETHA: Training TCN (50 epochs)...
      ‚è≥ ETHA TCN: Epoch 10/50 (20%)
      ‚è≥ ETHA TCN: Epoch 20/50 (40%)
      ‚è≥ ETHA TCN: Epoch 30/50 (60%)
      ‚è≥ ETHA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.485214
         RMSE: 0.696573
         R¬≤ Score: -0.6850
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETHA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETHA Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETHA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.9967 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETHA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ETHA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=95.3743 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETHA XGBoost: Starting GridSearchCV fit...
       ‚úÖ QXO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.5922 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 114.1s
    - LSTM: MSE=0.4261
    - TCN: MSE=0.3488
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3488
        ‚Ä¢ LSTM: MSE=0.4261
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.2426
        ‚Ä¢ XGBoost: MSE=38.5922
        ‚Ä¢ Random Forest: MSE=39.7067
   ‚úÖ QXO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QXO (TargetReturn): TCN with MSE=0.3488
üêõ DEBUG: QXO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QXO.
üêõ DEBUG: QXO - Moving model to CPU before return...
üêõ DEBUG [23:31:09.267]: QXO - Returning result metadata...
üêõ DEBUG: train_worker started for PTON
  ‚öôÔ∏è Training models for PTON (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - PTON: Initiating feature extraction for training.
  [DIAGNOSTIC] PTON: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PTON: rows after features available: 126
üéØ PTON: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PTON: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PTON: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PTON: Training LSTM (50 epochs)...
      ‚è≥ PTON LSTM: Epoch 10/50 (20%)
      ‚è≥ PTON LSTM: Epoch 20/50 (40%)
      ‚è≥ PTON LSTM: Epoch 30/50 (60%)
       ‚úÖ NIC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.1978 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.4975
    - TCN: MSE=0.2937
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2937
        ‚Ä¢ LSTM: MSE=0.4975
        ‚Ä¢ Random Forest: MSE=7.5579
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.1860
        ‚Ä¢ XGBoost: MSE=13.1978
   ‚úÖ NIC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NIC (TargetReturn): TCN with MSE=0.2937
üêõ DEBUG: NIC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NIC.
üêõ DEBUG: NIC - Moving model to CPU before return...
üêõ DEBUG [23:31:10.999]: NIC - Returning result metadata...
üêõ DEBUG [23:31:11.000]: Main received result for NIC
üêõ DEBUG: Training progress: 504/959 done
üêõ DEBUG [23:31:11.000]: Main received result for QXO
üêõ DEBUG: train_worker started for LAUR
  ‚öôÔ∏è Training models for LAUR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - LAUR: Initiating feature extraction for training.
  [DIAGNOSTIC] LAUR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LAUR: rows after features available: 126
üéØ LAUR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LAUR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LAUR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LAUR: Training LSTM (50 epochs)...
      ‚è≥ PTON LSTM: Epoch 40/50 (80%)
      ‚è≥ LAUR LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.299084
         RMSE: 0.546886
         R¬≤ Score: -0.3160 (Poor - 31.6% variance explained)
      üîπ PTON: Training TCN (50 epochs)...
      ‚è≥ PTON TCN: Epoch 10/50 (20%)
      ‚è≥ PTON TCN: Epoch 20/50 (40%)
      ‚è≥ PTON TCN: Epoch 30/50 (60%)
      ‚è≥ PTON TCN: Epoch 40/50 (80%)
      ‚è≥ LAUR LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.254093
         RMSE: 0.504076
         R¬≤ Score: -0.1180
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PTON: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PTON Random Forest: Starting GridSearchCV fit...
      ‚è≥ LAUR LSTM: Epoch 30/50 (60%)
      ‚è≥ LAUR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.163728
         RMSE: 0.404633
         R¬≤ Score: -0.3132 (Poor - 31.3% variance explained)
      üîπ LAUR: Training TCN (50 epochs)...
      ‚è≥ LAUR TCN: Epoch 10/50 (20%)
      ‚è≥ LAUR TCN: Epoch 20/50 (40%)
      ‚è≥ LAUR TCN: Epoch 30/50 (60%)
      ‚è≥ LAUR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.164925
         RMSE: 0.406109
         R¬≤ Score: -0.3228
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LAUR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LAUR Random Forest: Starting GridSearchCV fit...
       ‚úÖ PTON Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=46.4701 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PTON LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PTON LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=44.6537 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PTON XGBoost: Starting GridSearchCV fit...
       ‚úÖ LAUR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.7421 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LAUR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LAUR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.2798 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LAUR XGBoost: Starting GridSearchCV fit...
       ‚úÖ STX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=30.2743 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.5494
    - TCN: MSE=0.3241
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3241
        ‚Ä¢ LSTM: MSE=0.5494
        ‚Ä¢ XGBoost: MSE=30.2743
        ‚Ä¢ Random Forest: MSE=32.5514
        ‚Ä¢ LightGBM Regressor (CPU): MSE=53.8527
   ‚úÖ STX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STX (TargetReturn): TCN with MSE=0.3241
üêõ DEBUG: STX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STX.
üêõ DEBUG: STX - Moving model to CPU before return...
üêõ DEBUG [23:31:18.359]: STX - Returning result metadata...
üêõ DEBUG: train_worker started for WGMI
üêõ DEBUG [23:31:18.360]: Main received result for STX
  ‚öôÔ∏è Training models for WGMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - WGMI: Initiating feature extraction for training.
  [DIAGNOSTIC] WGMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WGMI: rows after features available: 126
üéØ WGMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WGMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WGMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WGMI: Training LSTM (50 epochs)...
       ‚úÖ ETH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=79.5040 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.4s
    - LSTM: MSE=0.5157
    - TCN: MSE=0.4701
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4701
        ‚Ä¢ LSTM: MSE=0.5157
        ‚Ä¢ Random Forest: MSE=47.3470
        ‚Ä¢ XGBoost: MSE=79.5040
        ‚Ä¢ LightGBM Regressor (CPU): MSE=105.6378
   ‚úÖ ETH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETH (TargetReturn): TCN with MSE=0.4701
üêõ DEBUG: ETH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETH.
üêõ DEBUG: ETH - Moving model to CPU before return...
üêõ DEBUG [23:31:18.876]: ETH - Returning result metadata...
üêõ DEBUG [23:31:18.877]: Main received result for ETH
üêõ DEBUG: train_worker started for QETH
  ‚öôÔ∏è Training models for QETH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - QETH: Initiating feature extraction for training.
  [DIAGNOSTIC] QETH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QETH: rows after features available: 126
üéØ QETH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QETH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QETH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QETH: Training LSTM (50 epochs)...
      ‚è≥ WGMI LSTM: Epoch 10/50 (20%)
      ‚è≥ QETH LSTM: Epoch 10/50 (20%)
      ‚è≥ WGMI LSTM: Epoch 20/50 (40%)
      ‚è≥ QETH LSTM: Epoch 20/50 (40%)
      ‚è≥ WGMI LSTM: Epoch 30/50 (60%)
      ‚è≥ QETH LSTM: Epoch 30/50 (60%)
      ‚è≥ WGMI LSTM: Epoch 40/50 (80%)
      ‚è≥ QETH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.562135
         RMSE: 0.749757
         R¬≤ Score: -0.6563 (Poor - 65.6% variance explained)
      üîπ WGMI: Training TCN (50 epochs)...
      ‚è≥ WGMI TCN: Epoch 10/50 (20%)
      ‚è≥ WGMI TCN: Epoch 20/50 (40%)
      ‚è≥ WGMI TCN: Epoch 30/50 (60%)
      ‚è≥ WGMI TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.422395
         RMSE: 0.649919
         R¬≤ Score: -0.4789 (Poor - 47.9% variance explained)
      üîπ QETH: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.631528
         RMSE: 0.794687
         R¬≤ Score: -0.8607
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WGMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WGMI Random Forest: Starting GridSearchCV fit...
      ‚è≥ QETH TCN: Epoch 10/50 (20%)
      ‚è≥ QETH TCN: Epoch 20/50 (40%)
      ‚è≥ QETH TCN: Epoch 30/50 (60%)
      ‚è≥ QETH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.458437
         RMSE: 0.677080
         R¬≤ Score: -0.6051
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QETH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QETH Random Forest: Starting GridSearchCV fit...
       ‚úÖ WGMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=47.3818 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WGMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QETH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.5741 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QETH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WGMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=112.7722 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WGMI XGBoost: Starting GridSearchCV fit...
       ‚úÖ QETH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=88.6282 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QETH XGBoost: Starting GridSearchCV fit...
       ‚úÖ BKCH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=64.1394 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.4996
    - TCN: MSE=0.5803
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4996
        ‚Ä¢ TCN: MSE=0.5803
        ‚Ä¢ Random Forest: MSE=45.3297
        ‚Ä¢ XGBoost: MSE=64.1394
        ‚Ä¢ LightGBM Regressor (CPU): MSE=77.8669
   ‚úÖ BKCH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BKCH (TargetReturn): LSTM with MSE=0.4996
üêõ DEBUG: BKCH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BKCH.
üêõ DEBUG: BKCH - Moving model to CPU before return...
üêõ DEBUG [23:31:33.561]: BKCH - Returning result metadata...
üêõ DEBUG [23:31:33.561]: Main received result for BKCH
üêõ DEBUG: Training progress: 508/959 done
üêõ DEBUG: train_worker started for PLMR
  ‚öôÔ∏è Training models for PLMR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - PLMR: Initiating feature extraction for training.
  [DIAGNOSTIC] PLMR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PLMR: rows after features available: 126
üéØ PLMR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PLMR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PLMR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PLMR: Training LSTM (50 epochs)...
      ‚è≥ PLMR LSTM: Epoch 10/50 (20%)
      ‚è≥ PLMR LSTM: Epoch 20/50 (40%)
      ‚è≥ PLMR LSTM: Epoch 30/50 (60%)
      ‚è≥ PLMR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.606201
         RMSE: 0.778589
         R¬≤ Score: -0.7616 (Poor - 76.2% variance explained)
      üîπ PLMR: Training TCN (50 epochs)...
      ‚è≥ PLMR TCN: Epoch 10/50 (20%)
      ‚è≥ PLMR TCN: Epoch 20/50 (40%)
      ‚è≥ PLMR TCN: Epoch 30/50 (60%)
      ‚è≥ PLMR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.419235
         RMSE: 0.647484
         R¬≤ Score: -0.2183
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PLMR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PLMR Random Forest: Starting GridSearchCV fit...
       ‚úÖ OCUL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=65.4560 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.0s
    - LSTM: MSE=0.2482
    - TCN: MSE=0.2837
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2482
        ‚Ä¢ TCN: MSE=0.2837
        ‚Ä¢ Random Forest: MSE=46.6075
        ‚Ä¢ LightGBM Regressor (CPU): MSE=61.0003
        ‚Ä¢ XGBoost: MSE=65.4560
   ‚úÖ OCUL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OCUL (TargetReturn): LSTM with MSE=0.2482
üêõ DEBUG: OCUL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OCUL.
üêõ DEBUG: OCUL - Moving model to CPU before return...
üêõ DEBUG [23:31:37.706]: OCUL - Returning result metadata...
üêõ DEBUG [23:31:37.707]: Main received result for OCULüêõ DEBUG: train_worker started for BTSG

  ‚öôÔ∏è Training models for BTSG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - BTSG: Initiating feature extraction for training.
  [DIAGNOSTIC] BTSG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BTSG: rows after features available: 126
üéØ BTSG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BTSG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BTSG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BTSG: Training LSTM (50 epochs)...
      ‚è≥ BTSG LSTM: Epoch 10/50 (20%)
      ‚è≥ BTSG LSTM: Epoch 20/50 (40%)
       ‚úÖ PLMR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.0329 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PLMR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BTSG LSTM: Epoch 30/50 (60%)
      ‚è≥ BTSG LSTM: Epoch 40/50 (80%)
       ‚úÖ PLMR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=23.1492 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PLMR XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.282329
         RMSE: 0.531346
         R¬≤ Score: -0.6898 (Poor - 69.0% variance explained)
      üîπ BTSG: Training TCN (50 epochs)...
      ‚è≥ BTSG TCN: Epoch 10/50 (20%)
      ‚è≥ BTSG TCN: Epoch 20/50 (40%)
      ‚è≥ BTSG TCN: Epoch 30/50 (60%)
      ‚è≥ BTSG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.194981
         RMSE: 0.441566
         R¬≤ Score: -0.1670
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BTSG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BTSG Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETHW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=71.7648 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.4s
    - LSTM: MSE=0.5285
    - TCN: MSE=0.4812
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4812
        ‚Ä¢ LSTM: MSE=0.5285
        ‚Ä¢ Random Forest: MSE=42.6128
        ‚Ä¢ XGBoost: MSE=71.7648
        ‚Ä¢ LightGBM Regressor (CPU): MSE=77.6564
   ‚úÖ ETHW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETHW (TargetReturn): TCN with MSE=0.4812
üêõ DEBUG: ETHW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETHW.
üêõ DEBUG: ETHW - Moving model to CPU before return...
üêõ DEBUG [23:31:42.530]: ETHW - Returning result metadata...
üêõ DEBUG [23:31:42.531]: Main received result for ETHW
üêõ DEBUG: train_worker started for RING
  ‚öôÔ∏è Training models for RING (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - RING: Initiating feature extraction for training.
  [DIAGNOSTIC] RING: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RING: rows after features available: 126
üéØ RING: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RING: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RING: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RING: Training LSTM (50 epochs)...
      ‚è≥ RING LSTM: Epoch 10/50 (20%)
       ‚úÖ BTSG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=58.1867 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BTSG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ RING LSTM: Epoch 20/50 (40%)
      ‚è≥ RING LSTM: Epoch 30/50 (60%)
       ‚úÖ BTSG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=57.5468 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BTSG XGBoost: Starting GridSearchCV fit...
      ‚è≥ RING LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.188302
         RMSE: 0.433938
         R¬≤ Score: -0.4460 (Poor - 44.6% variance explained)
      üîπ RING: Training TCN (50 epochs)...
      ‚è≥ RING TCN: Epoch 10/50 (20%)
      ‚è≥ RING TCN: Epoch 20/50 (40%)
      ‚è≥ RING TCN: Epoch 30/50 (60%)
      ‚è≥ RING TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.135494
         RMSE: 0.368096
         R¬≤ Score: -0.0405
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RING: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RING Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETHV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=79.7390 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.5828
    - TCN: MSE=0.4646
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4646
        ‚Ä¢ LSTM: MSE=0.5828
        ‚Ä¢ Random Forest: MSE=44.0800
        ‚Ä¢ LightGBM Regressor (CPU): MSE=72.9389
        ‚Ä¢ XGBoost: MSE=79.7390
   ‚úÖ ETHV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETHV (TargetReturn): TCN with MSE=0.4646
üêõ DEBUG: ETHV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETHV.
üêõ DEBUG: ETHV - Moving model to CPU before return...
üêõ DEBUG [23:31:45.384]: ETHV - Returning result metadata...
üêõ DEBUG [23:31:45.385]: Main received result for ETHV
üêõ DEBUG: train_worker started for LYV
  ‚öôÔ∏è Training models for LYV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - LYV: Initiating feature extraction for training.
  [DIAGNOSTIC] LYV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LYV: rows after features available: 126
üéØ LYV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LYV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LYV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LYV: Training LSTM (50 epochs)...
      ‚è≥ LYV LSTM: Epoch 10/50 (20%)
      ‚è≥ LYV LSTM: Epoch 20/50 (40%)
      ‚è≥ LYV LSTM: Epoch 30/50 (60%)
      ‚è≥ LYV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.439509
         RMSE: 0.662955
         R¬≤ Score: -0.3713 (Poor - 37.1% variance explained)
      üîπ LYV: Training TCN (50 epochs)...
      ‚è≥ LYV TCN: Epoch 10/50 (20%)
      ‚è≥ LYV TCN: Epoch 20/50 (40%)
      ‚è≥ LYV TCN: Epoch 30/50 (60%)
       ‚úÖ RING Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.2423 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RING LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ LYV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.500779
         RMSE: 0.707657
         R¬≤ Score: -0.5624
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LYV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LYV Random Forest: Starting GridSearchCV fit...
       ‚úÖ RING LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.2033 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RING XGBoost: Starting GridSearchCV fit...
       ‚úÖ LYV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.6798 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LYV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LYV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.6411 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LYV XGBoost: Starting GridSearchCV fit...
       ‚úÖ EZET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=77.5396 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.5651
    - TCN: MSE=0.4631
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4631
        ‚Ä¢ LSTM: MSE=0.5651
        ‚Ä¢ Random Forest: MSE=40.1506
        ‚Ä¢ XGBoost: MSE=77.5396
        ‚Ä¢ LightGBM Regressor (CPU): MSE=83.7489
   ‚úÖ EZET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EZET (TargetReturn): TCN with MSE=0.4631
üêõ DEBUG: EZET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EZET.
üêõ DEBUG: EZET - Moving model to CPU before return...
üêõ DEBUG [23:32:05.735]: EZET - Returning result metadata...
üêõ DEBUG: train_worker started for HMY
  ‚öôÔ∏è Training models for HMY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - HMY: Initiating feature extraction for training.
  [DIAGNOSTIC] HMY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HMY: rows after features available: 126
üéØ HMY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HMY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HMY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HMY: Training LSTM (50 epochs)...
      ‚è≥ HMY LSTM: Epoch 10/50 (20%)
      ‚è≥ HMY LSTM: Epoch 20/50 (40%)
      ‚è≥ HMY LSTM: Epoch 30/50 (60%)
      ‚è≥ HMY LSTM: Epoch 40/50 (80%)
       ‚úÖ POET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=140.6727 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.0s
    - LSTM: MSE=0.3359
    - TCN: MSE=0.3233
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3233
        ‚Ä¢ LSTM: MSE=0.3359
        ‚Ä¢ LightGBM Regressor (CPU): MSE=88.2166
        ‚Ä¢ Random Forest: MSE=138.8325
        ‚Ä¢ XGBoost: MSE=140.6727
   ‚úÖ POET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for POET (TargetReturn): TCN with MSE=0.3233
üêõ DEBUG: POET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for POET.
üêõ DEBUG: POET - Moving model to CPU before return...
üêõ DEBUG [23:32:08.017]: POET - Returning result metadata...
üêõ DEBUG [23:32:08.018]: Main received result for POETüêõ DEBUG: train_worker started for FTI

üêõ DEBUG: Training progress: 512/959 done
üêõ DEBUG [23:32:08.018]: Main received result for EZET
  ‚öôÔ∏è Training models for FTI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - FTI: Initiating feature extraction for training.
  [DIAGNOSTIC] FTI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FTI: rows after features available: 126
üéØ FTI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FTI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FTI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FTI: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.295855
         RMSE: 0.543926
         R¬≤ Score: -0.6852 (Poor - 68.5% variance explained)
      üîπ HMY: Training TCN (50 epochs)...
      ‚è≥ HMY TCN: Epoch 10/50 (20%)
      ‚è≥ HMY TCN: Epoch 20/50 (40%)
      ‚è≥ HMY TCN: Epoch 30/50 (60%)
      ‚è≥ HMY TCN: Epoch 40/50 (80%)
      ‚è≥ FTI LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.250961
         RMSE: 0.500960
         R¬≤ Score: -0.4295
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HMY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HMY Random Forest: Starting GridSearchCV fit...
      ‚è≥ FTI LSTM: Epoch 20/50 (40%)
      ‚è≥ FTI LSTM: Epoch 30/50 (60%)
      ‚è≥ FTI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.281687
         RMSE: 0.530742
         R¬≤ Score: -0.5870 (Poor - 58.7% variance explained)
      üîπ FTI: Training TCN (50 epochs)...
      ‚è≥ FTI TCN: Epoch 10/50 (20%)
      ‚è≥ FTI TCN: Epoch 20/50 (40%)
      ‚è≥ FTI TCN: Epoch 30/50 (60%)
      ‚è≥ FTI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.292889
         RMSE: 0.541192
         R¬≤ Score: -0.6501
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FTI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FTI Random Forest: Starting GridSearchCV fit...
       ‚úÖ HMY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=65.7041 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HMY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HMY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=65.9776 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HMY XGBoost: Starting GridSearchCV fit...
       ‚úÖ FTI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.0308 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FTI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FTI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=18.5487 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FTI XGBoost: Starting GridSearchCV fit...
       ‚úÖ RSI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.0096 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.5s
    - LSTM: MSE=0.5383
    - TCN: MSE=0.5966
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5383
        ‚Ä¢ TCN: MSE=0.5966
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6963
        ‚Ä¢ Random Forest: MSE=20.5634
        ‚Ä¢ XGBoost: MSE=36.0096
   ‚úÖ RSI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RSI (TargetReturn): LSTM with MSE=0.5383
üêõ DEBUG: RSI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RSI.
üêõ DEBUG: RSI - Moving model to CPU before return...
üêõ DEBUG [23:32:18.239]: RSI - Returning result metadata...
üêõ DEBUG [23:32:18.239]: Main received result for RSI
üêõ DEBUG: train_worker started for USD
  ‚öôÔ∏è Training models for USD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - USD: Initiating feature extraction for training.
  [DIAGNOSTIC] USD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ USD: rows after features available: 126
üéØ USD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] USD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö USD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ USD: Training LSTM (50 epochs)...
      ‚è≥ USD LSTM: Epoch 10/50 (20%)
      ‚è≥ USD LSTM: Epoch 20/50 (40%)
      ‚è≥ USD LSTM: Epoch 30/50 (60%)
      ‚è≥ USD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.416534
         RMSE: 0.645395
         R¬≤ Score: -0.4661 (Poor - 46.6% variance explained)
      üîπ USD: Training TCN (50 epochs)...
      ‚è≥ USD TCN: Epoch 10/50 (20%)
      ‚è≥ USD TCN: Epoch 20/50 (40%)
      ‚è≥ USD TCN: Epoch 30/50 (60%)
      ‚è≥ USD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.430374
         RMSE: 0.656029
         R¬≤ Score: -0.5148
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä USD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ USD Random Forest: Starting GridSearchCV fit...
       ‚úÖ FETH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=79.2195 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.3734
    - TCN: MSE=0.5889
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3734
        ‚Ä¢ TCN: MSE=0.5889
        ‚Ä¢ Random Forest: MSE=43.3994
        ‚Ä¢ XGBoost: MSE=79.2195
        ‚Ä¢ LightGBM Regressor (CPU): MSE=104.4702
   ‚úÖ FETH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FETH (TargetReturn): LSTM with MSE=0.3734
üêõ DEBUG: FETH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FETH.
üêõ DEBUG: FETH - Moving model to CPU before return...
üêõ DEBUG [23:32:23.582]: FETH - Returning result metadata...
üêõ DEBUG [23:32:23.582]: Main received result for FETH
üêõ DEBUG: train_worker started for TEL
  ‚öôÔ∏è Training models for TEL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TEL: Initiating feature extraction for training.
  [DIAGNOSTIC] TEL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TEL: rows after features available: 126
üéØ TEL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TEL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TEL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TEL: Training LSTM (50 epochs)...
      ‚è≥ TEL LSTM: Epoch 10/50 (20%)
       ‚úÖ USD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=78.9981 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ USD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TEL LSTM: Epoch 20/50 (40%)
       ‚úÖ USD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=274.6083 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ USD XGBoost: Starting GridSearchCV fit...
      ‚è≥ TEL LSTM: Epoch 30/50 (60%)
      ‚è≥ TEL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.463796
         RMSE: 0.681026
         R¬≤ Score: -0.9845 (Poor - 98.4% variance explained)
      üîπ TEL: Training TCN (50 epochs)...
      ‚è≥ TEL TCN: Epoch 10/50 (20%)
      ‚è≥ TEL TCN: Epoch 20/50 (40%)
      ‚è≥ TEL TCN: Epoch 30/50 (60%)
      ‚è≥ TEL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.291539
         RMSE: 0.539944
         R¬≤ Score: -0.2474
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TEL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TEL Random Forest: Starting GridSearchCV fit...
       ‚úÖ PERI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.6464 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.2453
    - TCN: MSE=0.1953
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1953
        ‚Ä¢ LSTM: MSE=0.2453
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3863
        ‚Ä¢ XGBoost: MSE=24.6464
        ‚Ä¢ Random Forest: MSE=26.3789
   ‚úÖ PERI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PERI (TargetReturn): TCN with MSE=0.1953
üêõ DEBUG: PERI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PERI.
üêõ DEBUG: PERI - Moving model to CPU before return...
üêõ DEBUG [23:32:26.551]: PERI - Returning result metadata...
üêõ DEBUG [23:32:26.552]: Main received result for PERI
üêõ DEBUG: Training progress: 516/959 done
üêõ DEBUG: train_worker started for RJF
  ‚öôÔ∏è Training models for RJF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - RJF: Initiating feature extraction for training.
  [DIAGNOSTIC] RJF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RJF: rows after features available: 126
üéØ RJF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RJF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RJF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RJF: Training LSTM (50 epochs)...
      ‚è≥ RJF LSTM: Epoch 10/50 (20%)
      ‚è≥ RJF LSTM: Epoch 20/50 (40%)
      ‚è≥ RJF LSTM: Epoch 30/50 (60%)
      ‚è≥ RJF LSTM: Epoch 40/50 (80%)
       ‚úÖ TETH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.9925 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.4427
    - TCN: MSE=0.5913
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4427
        ‚Ä¢ TCN: MSE=0.5913
        ‚Ä¢ Random Forest: MSE=44.3391
        ‚Ä¢ XGBoost: MSE=54.9925
        ‚Ä¢ LightGBM Regressor (CPU): MSE=107.2361
   ‚úÖ TETH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TETH (TargetReturn): LSTM with MSE=0.4427
üêõ DEBUG: TETH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TETH.
üêõ DEBUG: TETH - Moving model to CPU before return...
üêõ DEBUG [23:32:28.858]: TETH - Returning result metadata...
üêõ DEBUG [23:32:28.859]: Main received result for TETH
üêõ DEBUG: train_worker started for MAIN
  ‚öôÔ∏è Training models for MAIN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - MAIN: Initiating feature extraction for training.
  [DIAGNOSTIC] MAIN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MAIN: rows after features available: 126
üéØ MAIN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MAIN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MAIN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MAIN: Training LSTM (50 epochs)...
       ‚úÖ TEL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.9897 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TEL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.543164
         RMSE: 0.736996
         R¬≤ Score: -1.0606 (Poor - 106.1% variance explained)
      üîπ RJF: Training TCN (50 epochs)...
      ‚è≥ RJF TCN: Epoch 10/50 (20%)
      ‚è≥ RJF TCN: Epoch 20/50 (40%)
      ‚è≥ MAIN LSTM: Epoch 10/50 (20%)
      ‚è≥ RJF TCN: Epoch 30/50 (60%)
      ‚è≥ RJF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.471899
         RMSE: 0.686949
         R¬≤ Score: -0.7903
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RJF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RJF Random Forest: Starting GridSearchCV fit...
       ‚úÖ TEL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.6615 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TEL XGBoost: Starting GridSearchCV fit...
      ‚è≥ MAIN LSTM: Epoch 20/50 (40%)
      ‚è≥ MAIN LSTM: Epoch 30/50 (60%)
      ‚è≥ MAIN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.669202
         RMSE: 0.818047
         R¬≤ Score: -0.6602 (Poor - 66.0% variance explained)
      üîπ MAIN: Training TCN (50 epochs)...
      ‚è≥ MAIN TCN: Epoch 10/50 (20%)
      ‚è≥ MAIN TCN: Epoch 20/50 (40%)
      ‚è≥ MAIN TCN: Epoch 30/50 (60%)
      ‚è≥ MAIN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.828094
         RMSE: 0.909996
         R¬≤ Score: -1.0543
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MAIN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MAIN Random Forest: Starting GridSearchCV fit...
       ‚úÖ RJF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7826 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RJF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RJF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.3858 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RJF XGBoost: Starting GridSearchCV fit...
       ‚úÖ MAIN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.5227 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MAIN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MAIN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=10.8237 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MAIN XGBoost: Starting GridSearchCV fit...
       ‚úÖ ETHA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.6186 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.5929
    - TCN: MSE=0.4852
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4852
        ‚Ä¢ LSTM: MSE=0.5929
        ‚Ä¢ Random Forest: MSE=42.9967
        ‚Ä¢ XGBoost: MSE=54.6186
        ‚Ä¢ LightGBM Regressor (CPU): MSE=95.3743
   ‚úÖ ETHA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETHA (TargetReturn): TCN with MSE=0.4852
üêõ DEBUG: ETHA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETHA.
üêõ DEBUG: ETHA - Moving model to CPU before return...
üêõ DEBUG [23:32:41.199]: ETHA - Returning result metadata...
üêõ DEBUG [23:32:41.200]: Main received result for ETHA
üêõ DEBUG: train_worker started for DOCS
  ‚öôÔ∏è Training models for DOCS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - DOCS: Initiating feature extraction for training.
  [DIAGNOSTIC] DOCS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DOCS: rows after features available: 126
üéØ DOCS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DOCS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DOCS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DOCS: Training LSTM (50 epochs)...
      ‚è≥ DOCS LSTM: Epoch 10/50 (20%)
      ‚è≥ DOCS LSTM: Epoch 20/50 (40%)
      ‚è≥ DOCS LSTM: Epoch 30/50 (60%)
      ‚è≥ DOCS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.208446
         RMSE: 0.456559
         R¬≤ Score: -0.6300 (Poor - 63.0% variance explained)
      üîπ DOCS: Training TCN (50 epochs)...
      ‚è≥ DOCS TCN: Epoch 10/50 (20%)
      ‚è≥ DOCS TCN: Epoch 20/50 (40%)
      ‚è≥ DOCS TCN: Epoch 30/50 (60%)
      ‚è≥ DOCS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.132267
         RMSE: 0.363685
         R¬≤ Score: -0.0343
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DOCS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DOCS Random Forest: Starting GridSearchCV fit...
       ‚úÖ DOCS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.2166 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DOCS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DOCS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.2529 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DOCS XGBoost: Starting GridSearchCV fit...
       ‚úÖ PTON XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=40.6160 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.2991
    - TCN: MSE=0.2541
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2541
        ‚Ä¢ LSTM: MSE=0.2991
        ‚Ä¢ XGBoost: MSE=40.6160
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.6537
        ‚Ä¢ Random Forest: MSE=46.4701
   ‚úÖ PTON: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PTON (TargetReturn): TCN with MSE=0.2541
üêõ DEBUG: PTON - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PTON.
üêõ DEBUG: PTON - Moving model to CPU before return...
üêõ DEBUG [23:33:12.518]: PTON - Returning result metadata...
üêõ DEBUG [23:33:12.519]: Main received result for PTON
üêõ DEBUG: train_worker started for ATAT
  ‚öôÔ∏è Training models for ATAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - ATAT: Initiating feature extraction for training.
  [DIAGNOSTIC] ATAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATAT: rows after features available: 126
üéØ ATAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATAT: Training LSTM (50 epochs)...
      ‚è≥ ATAT LSTM: Epoch 10/50 (20%)
      ‚è≥ ATAT LSTM: Epoch 20/50 (40%)
      ‚è≥ ATAT LSTM: Epoch 30/50 (60%)
      ‚è≥ ATAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.592999
         RMSE: 0.770064
         R¬≤ Score: -0.7372 (Poor - 73.7% variance explained)
      üîπ ATAT: Training TCN (50 epochs)...
      ‚è≥ ATAT TCN: Epoch 10/50 (20%)
      ‚è≥ ATAT TCN: Epoch 20/50 (40%)
      ‚è≥ ATAT TCN: Epoch 30/50 (60%)
      ‚è≥ ATAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.538326
         RMSE: 0.733707
         R¬≤ Score: -0.5770
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ LAUR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.4379 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.1637
    - TCN: MSE=0.1649
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1637
        ‚Ä¢ TCN: MSE=0.1649
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.2798
        ‚Ä¢ Random Forest: MSE=8.7421
        ‚Ä¢ XGBoost: MSE=9.4379
   ‚úÖ LAUR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LAUR (TargetReturn): LSTM with MSE=0.1637
üêõ DEBUG: LAUR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LAUR.
üêõ DEBUG: LAUR - Moving model to CPU before return...
üêõ DEBUG [23:33:16.089]: LAUR - Returning result metadata...
üêõ DEBUG: train_worker started for BYRN
üêõ DEBUG [23:33:16.090]: Main received result for LAUR
üêõ DEBUG: Training progress: 520/959 done
  ‚öôÔ∏è Training models for BYRN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - BYRN: Initiating feature extraction for training.
  [DIAGNOSTIC] BYRN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BYRN: rows after features available: 126
üéØ BYRN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BYRN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BYRN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BYRN: Training LSTM (50 epochs)...
      ‚è≥ BYRN LSTM: Epoch 10/50 (20%)
      ‚è≥ BYRN LSTM: Epoch 20/50 (40%)
      ‚è≥ BYRN LSTM: Epoch 30/50 (60%)
      ‚è≥ BYRN LSTM: Epoch 40/50 (80%)
       ‚úÖ ATAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.0327 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.593160
         RMSE: 0.770169
         R¬≤ Score: -1.1556 (Poor - 115.6% variance explained)
      üîπ BYRN: Training TCN (50 epochs)...
      ‚è≥ BYRN TCN: Epoch 10/50 (20%)
      ‚è≥ BYRN TCN: Epoch 20/50 (40%)
      ‚è≥ BYRN TCN: Epoch 30/50 (60%)
      ‚è≥ BYRN TCN: Epoch 40/50 (80%)
       ‚úÖ ATAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=33.7041 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATAT XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.293122
         RMSE: 0.541407
         R¬≤ Score: -0.0652
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BYRN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BYRN Random Forest: Starting GridSearchCV fit...
       ‚úÖ BYRN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=193.8714 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BYRN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BYRN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=141.4198 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BYRN XGBoost: Starting GridSearchCV fit...
       ‚úÖ WGMI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=59.4342 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 117.6s
    - LSTM: MSE=0.5621
    - TCN: MSE=0.6315
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5621
        ‚Ä¢ TCN: MSE=0.6315
        ‚Ä¢ Random Forest: MSE=47.3818
        ‚Ä¢ XGBoost: MSE=59.4342
        ‚Ä¢ LightGBM Regressor (CPU): MSE=112.7722
   ‚úÖ WGMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WGMI (TargetReturn): LSTM with MSE=0.5621
üêõ DEBUG: WGMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WGMI.
üêõ DEBUG: WGMI - Moving model to CPU before return...
üêõ DEBUG [23:33:23.319]: WGMI - Returning result metadata...
üêõ DEBUG [23:33:23.319]: Main received result for WGMI
üêõ DEBUG: train_worker started for CX
  ‚öôÔ∏è Training models for CX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CX: Initiating feature extraction for training.
  [DIAGNOSTIC] CX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CX: rows after features available: 126
üéØ CX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CX: Training LSTM (50 epochs)...
       ‚úÖ QETH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=55.4908 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.7s
    - LSTM: MSE=0.4224
    - TCN: MSE=0.4584
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4224
        ‚Ä¢ TCN: MSE=0.4584
        ‚Ä¢ Random Forest: MSE=45.5741
        ‚Ä¢ XGBoost: MSE=55.4908
        ‚Ä¢ LightGBM Regressor (CPU): MSE=88.6282
   ‚úÖ QETH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QETH (TargetReturn): LSTM with MSE=0.4224
üêõ DEBUG: QETH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QETH.
üêõ DEBUG: QETH - Moving model to CPU before return...
üêõ DEBUG [23:33:23.736]: QETH - Returning result metadata...
üêõ DEBUG [23:33:23.737]: Main received result for QETH
üêõ DEBUG: train_worker started for UNM
  ‚öôÔ∏è Training models for UNM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - UNM: Initiating feature extraction for training.
  [DIAGNOSTIC] UNM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UNM: rows after features available: 126
üéØ UNM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UNM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UNM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UNM: Training LSTM (50 epochs)...
      ‚è≥ CX LSTM: Epoch 10/50 (20%)
      ‚è≥ CX LSTM: Epoch 20/50 (40%)
      ‚è≥ UNM LSTM: Epoch 10/50 (20%)
      ‚è≥ CX LSTM: Epoch 30/50 (60%)
      ‚è≥ UNM LSTM: Epoch 20/50 (40%)
      ‚è≥ CX LSTM: Epoch 40/50 (80%)
      ‚è≥ UNM LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.540600
         RMSE: 0.735255
         R¬≤ Score: -0.7610 (Poor - 76.1% variance explained)
      üîπ CX: Training TCN (50 epochs)...
      ‚è≥ CX TCN: Epoch 10/50 (20%)
      ‚è≥ CX TCN: Epoch 20/50 (40%)
      ‚è≥ CX TCN: Epoch 30/50 (60%)
      ‚è≥ UNM LSTM: Epoch 40/50 (80%)
      ‚è≥ CX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.514846
         RMSE: 0.717528
         R¬≤ Score: -0.6771
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CX Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.196296
         RMSE: 0.443054
         R¬≤ Score: -0.8710 (Poor - 87.1% variance explained)
      üîπ UNM: Training TCN (50 epochs)...
      ‚è≥ UNM TCN: Epoch 10/50 (20%)
      ‚è≥ UNM TCN: Epoch 20/50 (40%)
      ‚è≥ UNM TCN: Epoch 30/50 (60%)
      ‚è≥ UNM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.124965
         RMSE: 0.353504
         R¬≤ Score: -0.1911
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UNM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UNM Random Forest: Starting GridSearchCV fit...
       ‚úÖ CX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.8136 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.5832 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CX XGBoost: Starting GridSearchCV fit...
       ‚úÖ UNM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.4459 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UNM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UNM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.7427 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UNM XGBoost: Starting GridSearchCV fit...
       ‚úÖ PLMR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.6263 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.6062
    - TCN: MSE=0.4192
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4192
        ‚Ä¢ LSTM: MSE=0.6062
        ‚Ä¢ XGBoost: MSE=17.6263
        ‚Ä¢ Random Forest: MSE=20.0329
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.1492
   ‚úÖ PLMR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PLMR (TargetReturn): TCN with MSE=0.4192
üêõ DEBUG: PLMR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PLMR.
üêõ DEBUG: PLMR - Moving model to CPU before return...
üêõ DEBUG [23:33:41.550]: PLMR - Returning result metadata...
üêõ DEBUG [23:33:41.551]: Main received result for PLMR
üêõ DEBUG: train_worker started for EUFN
  ‚öôÔ∏è Training models for EUFN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - EUFN: Initiating feature extraction for training.
  [DIAGNOSTIC] EUFN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EUFN: rows after features available: 126
üéØ EUFN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EUFN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EUFN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EUFN: Training LSTM (50 epochs)...
      ‚è≥ EUFN LSTM: Epoch 10/50 (20%)
      ‚è≥ EUFN LSTM: Epoch 20/50 (40%)
      ‚è≥ EUFN LSTM: Epoch 30/50 (60%)
      ‚è≥ EUFN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.269876
         RMSE: 0.519496
         R¬≤ Score: -0.9907 (Poor - 99.1% variance explained)
      üîπ EUFN: Training TCN (50 epochs)...
      ‚è≥ EUFN TCN: Epoch 10/50 (20%)
      ‚è≥ EUFN TCN: Epoch 20/50 (40%)
      ‚è≥ EUFN TCN: Epoch 30/50 (60%)
      ‚è≥ EUFN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.134437
         RMSE: 0.366657
         R¬≤ Score: 0.0083
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EUFN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EUFN Random Forest: Starting GridSearchCV fit...
       ‚úÖ RING XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.3577 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.1883
    - TCN: MSE=0.1355
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1355
        ‚Ä¢ LSTM: MSE=0.1883
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.2033
        ‚Ä¢ XGBoost: MSE=24.3577
        ‚Ä¢ Random Forest: MSE=28.2423
   ‚úÖ RING: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RING (TargetReturn): TCN with MSE=0.1355
üêõ DEBUG: RING - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RING.
üêõ DEBUG: RING - Moving model to CPU before return...
üêõ DEBUG [23:33:46.167]: RING - Returning result metadata...
üêõ DEBUG: train_worker started for PLNT
  ‚öôÔ∏è Training models for PLNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - PLNT: Initiating feature extraction for training.
  [DIAGNOSTIC] PLNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PLNT: rows after features available: 126
üéØ PLNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PLNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PLNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PLNT: Training LSTM (50 epochs)...
      ‚è≥ PLNT LSTM: Epoch 10/50 (20%)
      ‚è≥ PLNT LSTM: Epoch 20/50 (40%)
       ‚úÖ BTSG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=81.1813 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.0s
    - LSTM: MSE=0.2823
    - TCN: MSE=0.1950
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1950
        ‚Ä¢ LSTM: MSE=0.2823
        ‚Ä¢ LightGBM Regressor (CPU): MSE=57.5468
        ‚Ä¢ Random Forest: MSE=58.1867
        ‚Ä¢ XGBoost: MSE=81.1813
   ‚úÖ BTSG: Phase 3/3 - Model selection complete!
       ‚úÖ EUFN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.0677 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EUFN LightGBM Regressor (CPU): Starting GridSearchCV fit...
  üèÜ WINNER for BTSG (TargetReturn): TCN with MSE=0.1950
üêõ DEBUG: BTSG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BTSG.
üêõ DEBUG: BTSG - Moving model to CPU before return...
üêõ DEBUG [23:33:47.169]: BTSG - Returning result metadata...
üêõ DEBUG: train_worker started for CSV
üêõ DEBUG [23:33:47.171]: Main received result for BTSG
üêõ DEBUG: Training progress: 524/959 done
üêõ DEBUG [23:33:47.171]: Main received result for RING
  ‚öôÔ∏è Training models for CSV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - CSV: Initiating feature extraction for training.
  [DIAGNOSTIC] CSV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CSV: rows after features available: 126
üéØ CSV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CSV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CSV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CSV: Training LSTM (50 epochs)...
      ‚è≥ PLNT LSTM: Epoch 30/50 (60%)
       ‚úÖ LYV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.9416 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.5s
    - LSTM: MSE=0.4395
    - TCN: MSE=0.5008
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4395
        ‚Ä¢ TCN: MSE=0.5008
        ‚Ä¢ Random Forest: MSE=6.6798
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.6411
        ‚Ä¢ XGBoost: MSE=7.9416
   ‚úÖ LYV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LYV (TargetReturn): LSTM with MSE=0.4395
üêõ DEBUG: LYV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LYV.
üêõ DEBUG: LYV - Moving model to CPU before return...
üêõ DEBUG [23:33:47.679]: LYV - Returning result metadata...
üêõ DEBUG [23:33:47.679]: Main received result for LYV
üêõ DEBUG: train_worker started for TMV
      ‚è≥ CSV LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for TMV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TMV: Initiating feature extraction for training.
  [DIAGNOSTIC] TMV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TMV: rows after features available: 126
üéØ TMV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TMV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TMV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TMV: Training LSTM (50 epochs)...
       ‚úÖ EUFN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.9063 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EUFN XGBoost: Starting GridSearchCV fit...
      ‚è≥ PLNT LSTM: Epoch 40/50 (80%)
      ‚è≥ TMV LSTM: Epoch 10/50 (20%)
      ‚è≥ CSV LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.216037
         RMSE: 0.464798
         R¬≤ Score: -0.5114 (Poor - 51.1% variance explained)
      üîπ PLNT: Training TCN (50 epochs)...
      ‚è≥ PLNT TCN: Epoch 10/50 (20%)
      ‚è≥ TMV LSTM: Epoch 20/50 (40%)
      ‚è≥ PLNT TCN: Epoch 20/50 (40%)
      ‚è≥ CSV LSTM: Epoch 30/50 (60%)
      ‚è≥ PLNT TCN: Epoch 30/50 (60%)
      ‚è≥ PLNT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.145791
         RMSE: 0.381826
         R¬≤ Score: -0.0200
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PLNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PLNT Random Forest: Starting GridSearchCV fit...
      ‚è≥ TMV LSTM: Epoch 30/50 (60%)
      ‚è≥ CSV LSTM: Epoch 40/50 (80%)
      ‚è≥ TMV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.521160
         RMSE: 0.721914
         R¬≤ Score: -0.9020 (Poor - 90.2% variance explained)
      üîπ CSV: Training TCN (50 epochs)...
      ‚è≥ CSV TCN: Epoch 10/50 (20%)
      ‚è≥ CSV TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.197018
         RMSE: 0.443868
         R¬≤ Score: -0.8117 (Poor - 81.2% variance explained)
      üîπ TMV: Training TCN (50 epochs)...
      ‚è≥ CSV TCN: Epoch 30/50 (60%)
      ‚è≥ TMV TCN: Epoch 10/50 (20%)
      ‚è≥ CSV TCN: Epoch 40/50 (80%)
      ‚è≥ TMV TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.403445
         RMSE: 0.635174
         R¬≤ Score: -0.4724
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CSV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CSV Random Forest: Starting GridSearchCV fit...
      ‚è≥ TMV TCN: Epoch 30/50 (60%)
      ‚è≥ TMV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.119802
         RMSE: 0.346125
         R¬≤ Score: -0.1017
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TMV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TMV Random Forest: Starting GridSearchCV fit...
       ‚úÖ PLNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.6586 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PLNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CSV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.6999 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CSV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PLNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.0372 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PLNT XGBoost: Starting GridSearchCV fit...
       ‚úÖ TMV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.9553 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TMV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CSV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.2574 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CSV XGBoost: Starting GridSearchCV fit...
       ‚úÖ TMV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.1359 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TMV XGBoost: Starting GridSearchCV fit...
       ‚úÖ HMY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=73.1008 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.2959
    - TCN: MSE=0.2510
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2510
        ‚Ä¢ LSTM: MSE=0.2959
        ‚Ä¢ Random Forest: MSE=65.7041
        ‚Ä¢ LightGBM Regressor (CPU): MSE=65.9776
        ‚Ä¢ XGBoost: MSE=73.1008
   ‚úÖ HMY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HMY (TargetReturn): TCN with MSE=0.2510
üêõ DEBUG: HMY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HMY.
üêõ DEBUG: HMY - Moving model to CPU before return...
üêõ DEBUG [23:34:11.549]: HMY - Returning result metadata...
üêõ DEBUG [23:34:11.550]: Main received result for HMY
üêõ DEBUG: train_worker started for MCB
  ‚öôÔ∏è Training models for MCB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - MCB: Initiating feature extraction for training.
  [DIAGNOSTIC] MCB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MCB: rows after features available: 126
üéØ MCB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MCB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MCB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MCB: Training LSTM (50 epochs)...
      ‚è≥ MCB LSTM: Epoch 10/50 (20%)
      ‚è≥ MCB LSTM: Epoch 20/50 (40%)
      ‚è≥ MCB LSTM: Epoch 30/50 (60%)
      ‚è≥ MCB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.342352
         RMSE: 0.585109
         R¬≤ Score: -0.7356 (Poor - 73.6% variance explained)
      üîπ MCB: Training TCN (50 epochs)...
      ‚è≥ MCB TCN: Epoch 10/50 (20%)
      ‚è≥ MCB TCN: Epoch 20/50 (40%)
      ‚è≥ MCB TCN: Epoch 30/50 (60%)
      ‚è≥ MCB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.308869
         RMSE: 0.555760
         R¬≤ Score: -0.5659
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MCB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MCB Random Forest: Starting GridSearchCV fit...
       ‚úÖ MCB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.7150 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MCB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FTI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.4983 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.1s
    - LSTM: MSE=0.2817
    - TCN: MSE=0.2929
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2817
        ‚Ä¢ TCN: MSE=0.2929
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.5487
        ‚Ä¢ Random Forest: MSE=30.0308
        ‚Ä¢ XGBoost: MSE=35.4983
   ‚úÖ FTI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FTI (TargetReturn): LSTM with MSE=0.2817
üêõ DEBUG: FTI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FTI.
üêõ DEBUG: FTI - Moving model to CPU before return...
üêõ DEBUG [23:34:17.364]: FTI - Returning result metadata...
üêõ DEBUG [23:34:17.366]: Main received result for FTI
üêõ DEBUG: Training progress: 528/959 done
üêõ DEBUG: train_worker started for UAE
  ‚öôÔ∏è Training models for UAE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - UAE: Initiating feature extraction for training.
  [DIAGNOSTIC] UAE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UAE: rows after features available: 126
üéØ UAE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UAE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UAE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UAE: Training LSTM (50 epochs)...
       ‚úÖ MCB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.7477 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MCB XGBoost: Starting GridSearchCV fit...
      ‚è≥ UAE LSTM: Epoch 10/50 (20%)
      ‚è≥ UAE LSTM: Epoch 20/50 (40%)
      ‚è≥ UAE LSTM: Epoch 30/50 (60%)
      ‚è≥ UAE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.288594
         RMSE: 0.537210
         R¬≤ Score: -0.5125 (Poor - 51.3% variance explained)
      üîπ UAE: Training TCN (50 epochs)...
      ‚è≥ UAE TCN: Epoch 10/50 (20%)
      ‚è≥ UAE TCN: Epoch 20/50 (40%)
      ‚è≥ UAE TCN: Epoch 30/50 (60%)
      ‚è≥ UAE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.328184
         RMSE: 0.572874
         R¬≤ Score: -0.7200
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UAE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UAE Random Forest: Starting GridSearchCV fit...
       ‚úÖ USD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=75.2226 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.6s
    - LSTM: MSE=0.4165
    - TCN: MSE=0.4304
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4165
        ‚Ä¢ TCN: MSE=0.4304
        ‚Ä¢ XGBoost: MSE=75.2226
        ‚Ä¢ Random Forest: MSE=78.9981
        ‚Ä¢ LightGBM Regressor (CPU): MSE=274.6083
   ‚úÖ USD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for USD (TargetReturn): LSTM with MSE=0.4165
üêõ DEBUG: USD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for USD.
üêõ DEBUG: USD - Moving model to CPU before return...
üêõ DEBUG [23:34:20.633]: USD - Returning result metadata...
üêõ DEBUG [23:34:20.634]: Main received result for USD
üêõ DEBUG: train_worker started for PJT
  ‚öôÔ∏è Training models for PJT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - PJT: Initiating feature extraction for training.
  [DIAGNOSTIC] PJT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PJT: rows after features available: 126
üéØ PJT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PJT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PJT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PJT: Training LSTM (50 epochs)...
      ‚è≥ PJT LSTM: Epoch 10/50 (20%)
      ‚è≥ PJT LSTM: Epoch 20/50 (40%)
      ‚è≥ PJT LSTM: Epoch 30/50 (60%)
      ‚è≥ PJT LSTM: Epoch 40/50 (80%)
       ‚úÖ UAE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.2979 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UAE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.676002
         RMSE: 0.822193
         R¬≤ Score: -0.7737 (Poor - 77.4% variance explained)
      üîπ PJT: Training TCN (50 epochs)...
      ‚è≥ PJT TCN: Epoch 10/50 (20%)
      ‚è≥ PJT TCN: Epoch 20/50 (40%)
      ‚è≥ PJT TCN: Epoch 30/50 (60%)
      ‚è≥ PJT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.716200
         RMSE: 0.846286
         R¬≤ Score: -0.8792
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PJT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PJT Random Forest: Starting GridSearchCV fit...
       ‚úÖ UAE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.7541 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UAE XGBoost: Starting GridSearchCV fit...
       ‚úÖ PJT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.3390 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PJT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PJT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8712 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PJT XGBoost: Starting GridSearchCV fit...
       ‚úÖ TEL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.0005 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.4638
    - TCN: MSE=0.2915
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2915
        ‚Ä¢ LSTM: MSE=0.4638
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.6615
        ‚Ä¢ XGBoost: MSE=23.0005
        ‚Ä¢ Random Forest: MSE=23.9897
   ‚úÖ TEL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TEL (TargetReturn): TCN with MSE=0.2915
üêõ DEBUG: TEL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TEL.
üêõ DEBUG: TEL - Moving model to CPU before return...
üêõ DEBUG [23:34:30.801]: TEL - Returning result metadata...
üêõ DEBUG: train_worker started for HNST
üêõ DEBUG [23:34:30.802]: Main received result for TEL
  ‚öôÔ∏è Training models for HNST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - HNST: Initiating feature extraction for training.
  [DIAGNOSTIC] HNST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HNST: rows after features available: 126
üéØ HNST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HNST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HNST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HNST: Training LSTM (50 epochs)...
       ‚úÖ RJF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.9255 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.5432
    - TCN: MSE=0.4719
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4719
        ‚Ä¢ LSTM: MSE=0.5432
        ‚Ä¢ Random Forest: MSE=7.7826
        ‚Ä¢ XGBoost: MSE=7.9255
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.3858
   ‚úÖ RJF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RJF (TargetReturn): TCN with MSE=0.4719
üêõ DEBUG: RJF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RJF.
üêõ DEBUG: RJF - Moving model to CPU before return...
üêõ DEBUG [23:34:31.145]: RJF - Returning result metadata...
üêõ DEBUG: train_worker started for MBI
üêõ DEBUG [23:34:31.146]: Main received result for RJF
  ‚öôÔ∏è Training models for MBI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - MBI: Initiating feature extraction for training.
  [DIAGNOSTIC] MBI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MBI: rows after features available: 126
üéØ MBI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MBI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MBI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MBI: Training LSTM (50 epochs)...
      ‚è≥ HNST LSTM: Epoch 10/50 (20%)
      ‚è≥ MBI LSTM: Epoch 10/50 (20%)
      ‚è≥ HNST LSTM: Epoch 20/50 (40%)
      ‚è≥ MBI LSTM: Epoch 20/50 (40%)
      ‚è≥ HNST LSTM: Epoch 30/50 (60%)
      ‚è≥ MBI LSTM: Epoch 30/50 (60%)
      ‚è≥ HNST LSTM: Epoch 40/50 (80%)
      ‚è≥ MBI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.076743
         RMSE: 0.277025
         R¬≤ Score: -0.8698 (Poor - 87.0% variance explained)
      üîπ HNST: Training TCN (50 epochs)...
      ‚è≥ HNST TCN: Epoch 10/50 (20%)
      ‚è≥ HNST TCN: Epoch 20/50 (40%)
      ‚è≥ HNST TCN: Epoch 30/50 (60%)
      ‚è≥ HNST TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.108814
         RMSE: 0.329870
         R¬≤ Score: -0.7574 (Poor - 75.7% variance explained)
      üîπ MBI: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.046183
         RMSE: 0.214902
         R¬≤ Score: -0.1252
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HNST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HNST Random Forest: Starting GridSearchCV fit...
      ‚è≥ MBI TCN: Epoch 10/50 (20%)
      ‚è≥ MBI TCN: Epoch 20/50 (40%)
      ‚è≥ MBI TCN: Epoch 30/50 (60%)
      ‚è≥ MBI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.066569
         RMSE: 0.258010
         R¬≤ Score: -0.0751
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MBI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MBI Random Forest: Starting GridSearchCV fit...
       ‚úÖ HNST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.7915 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HNST LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MBI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.1154 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MBI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HNST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.4519 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HNST XGBoost: Starting GridSearchCV fit...
       ‚úÖ MBI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.0519 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MBI XGBoost: Starting GridSearchCV fit...
       ‚úÖ MAIN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.7893 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.8s
    - LSTM: MSE=0.6692
    - TCN: MSE=0.8281
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6692
        ‚Ä¢ TCN: MSE=0.8281
        ‚Ä¢ Random Forest: MSE=7.5227
        ‚Ä¢ XGBoost: MSE=7.7893
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8237
   ‚úÖ MAIN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MAIN (TargetReturn): LSTM with MSE=0.6692
üêõ DEBUG: MAIN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MAIN.
üêõ DEBUG: MAIN - Moving model to CPU before return...
üêõ DEBUG [23:34:37.771]: MAIN - Returning result metadata...
üêõ DEBUG [23:34:37.772]: Main received result for MAIN
üêõ DEBUG: Training progress: 532/959 done
üêõ DEBUG: train_worker started for EMR
  ‚öôÔ∏è Training models for EMR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - EMR: Initiating feature extraction for training.
  [DIAGNOSTIC] EMR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EMR: rows after features available: 126
üéØ EMR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EMR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EMR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EMR: Training LSTM (50 epochs)...
      ‚è≥ EMR LSTM: Epoch 10/50 (20%)
      ‚è≥ EMR LSTM: Epoch 20/50 (40%)
      ‚è≥ EMR LSTM: Epoch 30/50 (60%)
      ‚è≥ EMR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.544588
         RMSE: 0.737962
         R¬≤ Score: -0.7254 (Poor - 72.5% variance explained)
      üîπ EMR: Training TCN (50 epochs)...
      ‚è≥ EMR TCN: Epoch 10/50 (20%)
      ‚è≥ EMR TCN: Epoch 20/50 (40%)
      ‚è≥ EMR TCN: Epoch 30/50 (60%)
      ‚è≥ EMR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.429086
         RMSE: 0.655046
         R¬≤ Score: -0.3595
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EMR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EMR Random Forest: Starting GridSearchCV fit...
       ‚úÖ EMR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.2836 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EMR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EMR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.1662 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EMR XGBoost: Starting GridSearchCV fit...
       ‚úÖ DOCS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.1862 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.8s
    - LSTM: MSE=0.2084
    - TCN: MSE=0.1323
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1323
        ‚Ä¢ LSTM: MSE=0.2084
        ‚Ä¢ Random Forest: MSE=17.2166
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.2529
        ‚Ä¢ XGBoost: MSE=22.1862
   ‚úÖ DOCS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DOCS (TargetReturn): TCN with MSE=0.1323
üêõ DEBUG: DOCS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DOCS.
üêõ DEBUG: DOCS - Moving model to CPU before return...
üêõ DEBUG [23:34:43.985]: DOCS - Returning result metadata...
üêõ DEBUG: train_worker started for ING
üêõ DEBUG [23:34:43.986]: Main received result for DOCS
  ‚öôÔ∏è Training models for ING (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ING: Initiating feature extraction for training.
  [DIAGNOSTIC] ING: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ING: rows after features available: 126
üéØ ING: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ING: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ING: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ING: Training LSTM (50 epochs)...
      ‚è≥ ING LSTM: Epoch 10/50 (20%)
      ‚è≥ ING LSTM: Epoch 20/50 (40%)
      ‚è≥ ING LSTM: Epoch 30/50 (60%)
      ‚è≥ ING LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.296695
         RMSE: 0.544697
         R¬≤ Score: -0.8520 (Poor - 85.2% variance explained)
      üîπ ING: Training TCN (50 epochs)...
      ‚è≥ ING TCN: Epoch 10/50 (20%)
      ‚è≥ ING TCN: Epoch 20/50 (40%)
      ‚è≥ ING TCN: Epoch 30/50 (60%)
      ‚è≥ ING TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.173880
         RMSE: 0.416990
         R¬≤ Score: -0.0854
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ING: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ING Random Forest: Starting GridSearchCV fit...
       ‚úÖ ING Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.6323 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ING LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ING LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=26.9492 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ING XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATAT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=26.5954 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.4s
    - LSTM: MSE=0.5930
    - TCN: MSE=0.5383
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5383
        ‚Ä¢ LSTM: MSE=0.5930
        ‚Ä¢ XGBoost: MSE=26.5954
        ‚Ä¢ Random Forest: MSE=29.0327
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.7041
   ‚úÖ ATAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATAT (TargetReturn): TCN with MSE=0.5383
üêõ DEBUG: ATAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATAT.
üêõ DEBUG: ATAT - Moving model to CPU before return...
üêõ DEBUG [23:35:15.952]: ATAT - Returning result metadata...
üêõ DEBUG: train_worker started for TMAT
üêõ DEBUG [23:35:15.955]: Main received result for ATAT
  ‚öôÔ∏è Training models for TMAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - TMAT: Initiating feature extraction for training.
  [DIAGNOSTIC] TMAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TMAT: rows after features available: 126
üéØ TMAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TMAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TMAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TMAT: Training LSTM (50 epochs)...
      ‚è≥ TMAT LSTM: Epoch 10/50 (20%)
      ‚è≥ TMAT LSTM: Epoch 20/50 (40%)
      ‚è≥ TMAT LSTM: Epoch 30/50 (60%)
      ‚è≥ TMAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.559952
         RMSE: 0.748299
         R¬≤ Score: -0.8934 (Poor - 89.3% variance explained)
      üîπ TMAT: Training TCN (50 epochs)...
      ‚è≥ TMAT TCN: Epoch 10/50 (20%)
      ‚è≥ TMAT TCN: Epoch 20/50 (40%)
      ‚è≥ TMAT TCN: Epoch 30/50 (60%)
      ‚è≥ TMAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.539909
         RMSE: 0.734785
         R¬≤ Score: -0.8256
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TMAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TMAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ BYRN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=286.3533 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 118.3s
    - LSTM: MSE=0.5932
    - TCN: MSE=0.2931
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2931
        ‚Ä¢ LSTM: MSE=0.5932
        ‚Ä¢ LightGBM Regressor (CPU): MSE=141.4198
        ‚Ä¢ Random Forest: MSE=193.8714
        ‚Ä¢ XGBoost: MSE=286.3533
   ‚úÖ BYRN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BYRN (TargetReturn): TCN with MSE=0.2931
üêõ DEBUG: BYRN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BYRN.
üêõ DEBUG: BYRN - Moving model to CPU before return...
üêõ DEBUG [23:35:21.075]: BYRN - Returning result metadata...
üêõ DEBUG: train_worker started for VEEV
üêõ DEBUG [23:35:21.076]: Main received result for BYRN
  ‚öôÔ∏è Training models for VEEV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - VEEV: Initiating feature extraction for training.
  [DIAGNOSTIC] VEEV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VEEV: rows after features available: 126
üéØ VEEV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VEEV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VEEV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VEEV: Training LSTM (50 epochs)...
      ‚è≥ VEEV LSTM: Epoch 10/50 (20%)
       ‚úÖ TMAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.4771 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TMAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ VEEV LSTM: Epoch 20/50 (40%)
       ‚úÖ TMAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.6827 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TMAT XGBoost: Starting GridSearchCV fit...
      ‚è≥ VEEV LSTM: Epoch 30/50 (60%)
      ‚è≥ VEEV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.483942
         RMSE: 0.695659
         R¬≤ Score: -0.4947 (Poor - 49.5% variance explained)
      üîπ VEEV: Training TCN (50 epochs)...
      ‚è≥ VEEV TCN: Epoch 10/50 (20%)
      ‚è≥ VEEV TCN: Epoch 20/50 (40%)
      ‚è≥ VEEV TCN: Epoch 30/50 (60%)
      ‚è≥ VEEV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.656756
         RMSE: 0.810405
         R¬≤ Score: -1.0285
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VEEV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VEEV Random Forest: Starting GridSearchCV fit...
       ‚úÖ VEEV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.7986 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VEEV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VEEV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.5129 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VEEV XGBoost: Starting GridSearchCV fit...
       ‚úÖ UNM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.9741 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.1963
    - TCN: MSE=0.1250
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1250
        ‚Ä¢ LSTM: MSE=0.1963
        ‚Ä¢ Random Forest: MSE=6.4459
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.7427
        ‚Ä¢ XGBoost: MSE=8.9741
   ‚úÖ UNM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UNM (TargetReturn): TCN with MSE=0.1250
üêõ DEBUG: UNM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UNM.
üêõ DEBUG: UNM - Moving model to CPU before return...
üêõ DEBUG [23:35:28.538]: UNM - Returning result metadata...
üêõ DEBUG: train_worker started for GTLS
  ‚öôÔ∏è Training models for GTLS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - GTLS: Initiating feature extraction for training.
  [DIAGNOSTIC] GTLS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GTLS: rows after features available: 126
üéØ GTLS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GTLS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GTLS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GTLS: Training LSTM (50 epochs)...
      ‚è≥ GTLS LSTM: Epoch 10/50 (20%)
       ‚úÖ CX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.4809 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.5406
    - TCN: MSE=0.5148
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5148
        ‚Ä¢ LSTM: MSE=0.5406
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.5832
        ‚Ä¢ Random Forest: MSE=23.8136
        ‚Ä¢ XGBoost: MSE=24.4809
   ‚úÖ CX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CX (TargetReturn): TCN with MSE=0.5148
üêõ DEBUG: CX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CX.
üêõ DEBUG: CX - Moving model to CPU before return...
üêõ DEBUG [23:35:29.334]: CX - Returning result metadata...
üêõ DEBUG [23:35:29.334]: Main received result for CX
üêõ DEBUG: Training progress: 536/959 done
üêõ DEBUG [23:35:29.334]: Main received result for UNM
üêõ DEBUG: train_worker started for VRT
  ‚öôÔ∏è Training models for VRT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - VRT: Initiating feature extraction for training.
  [DIAGNOSTIC] VRT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VRT: rows after features available: 126
üéØ VRT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VRT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VRT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VRT: Training LSTM (50 epochs)...
      ‚è≥ GTLS LSTM: Epoch 20/50 (40%)
      ‚è≥ VRT LSTM: Epoch 10/50 (20%)
      ‚è≥ GTLS LSTM: Epoch 30/50 (60%)
      ‚è≥ VRT LSTM: Epoch 20/50 (40%)
      ‚è≥ GTLS LSTM: Epoch 40/50 (80%)
      ‚è≥ VRT LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.284389
         RMSE: 0.533281
         R¬≤ Score: -0.8146 (Poor - 81.5% variance explained)
      üîπ GTLS: Training TCN (50 epochs)...
      ‚è≥ GTLS TCN: Epoch 10/50 (20%)
      ‚è≥ GTLS TCN: Epoch 20/50 (40%)
      ‚è≥ VRT LSTM: Epoch 40/50 (80%)
      ‚è≥ GTLS TCN: Epoch 30/50 (60%)
      ‚è≥ GTLS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.253685
         RMSE: 0.503671
         R¬≤ Score: -0.6187
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GTLS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GTLS Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.434249
         RMSE: 0.658976
         R¬≤ Score: -0.9664 (Poor - 96.6% variance explained)
      üîπ VRT: Training TCN (50 epochs)...
      ‚è≥ VRT TCN: Epoch 10/50 (20%)
      ‚è≥ VRT TCN: Epoch 20/50 (40%)
      ‚è≥ VRT TCN: Epoch 30/50 (60%)
      ‚è≥ VRT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.398798
         RMSE: 0.631505
         R¬≤ Score: -0.8059
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VRT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VRT Random Forest: Starting GridSearchCV fit...
       ‚úÖ GTLS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.4690 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GTLS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VRT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=61.4176 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VRT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GTLS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.7182 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GTLS XGBoost: Starting GridSearchCV fit...
       ‚úÖ VRT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=82.2137 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VRT XGBoost: Starting GridSearchCV fit...
       ‚úÖ PLNT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.1040 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.1s
    - LSTM: MSE=0.2160
    - TCN: MSE=0.1458
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1458
        ‚Ä¢ LSTM: MSE=0.2160
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0372
        ‚Ä¢ Random Forest: MSE=8.6586
        ‚Ä¢ XGBoost: MSE=11.1040
   ‚úÖ PLNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PLNT (TargetReturn): TCN with MSE=0.1458
üêõ DEBUG: PLNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PLNT.
üêõ DEBUG: PLNT - Moving model to CPU before return...
üêõ DEBUG [23:35:47.706]: PLNT - Returning result metadata...
üêõ DEBUG: train_worker started for TKO
  ‚öôÔ∏è Training models for TKO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - TKO: Initiating feature extraction for training.
  [DIAGNOSTIC] TKO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TKO: rows after features available: 126
üéØ TKO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TKO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TKO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TKO: Training LSTM (50 epochs)...
      ‚è≥ TKO LSTM: Epoch 10/50 (20%)
      ‚è≥ TKO LSTM: Epoch 20/50 (40%)
       ‚úÖ EUFN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.7911 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.2699
    - TCN: MSE=0.1344
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1344
        ‚Ä¢ LSTM: MSE=0.2699
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.9063
        ‚Ä¢ Random Forest: MSE=8.0677
        ‚Ä¢ XGBoost: MSE=8.7911
   ‚úÖ EUFN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EUFN (TargetReturn): TCN with MSE=0.1344
üêõ DEBUG: EUFN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EUFN.
üêõ DEBUG: EUFN - Moving model to CPU before return...
üêõ DEBUG [23:35:49.010]: EUFN - Returning result metadata...
üêõ DEBUG: train_worker started for PUK
üêõ DEBUG [23:35:49.011]: Main received result for EUFN
üêõ DEBUG [23:35:49.011]: Main received result for PLNT
  ‚öôÔ∏è Training models for PUK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - PUK: Initiating feature extraction for training.
  [DIAGNOSTIC] PUK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PUK: rows after features available: 126
üéØ PUK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PUK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PUK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PUK: Training LSTM (50 epochs)...
      ‚è≥ TKO LSTM: Epoch 30/50 (60%)
      ‚è≥ PUK LSTM: Epoch 10/50 (20%)
      ‚è≥ TKO LSTM: Epoch 40/50 (80%)
      ‚è≥ PUK LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.136905
         RMSE: 0.370006
         R¬≤ Score: -0.7221 (Poor - 72.2% variance explained)
      üîπ TKO: Training TCN (50 epochs)...
      ‚è≥ TKO TCN: Epoch 10/50 (20%)
      ‚è≥ TKO TCN: Epoch 20/50 (40%)
      ‚è≥ TKO TCN: Epoch 30/50 (60%)
      ‚è≥ PUK LSTM: Epoch 30/50 (60%)
      ‚è≥ TKO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.084876
         RMSE: 0.291334
         R¬≤ Score: -0.0676
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TKO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TKO Random Forest: Starting GridSearchCV fit...
      ‚è≥ PUK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.116039
         RMSE: 0.340645
         R¬≤ Score: -0.7498 (Poor - 75.0% variance explained)
      üîπ PUK: Training TCN (50 epochs)...
      ‚è≥ PUK TCN: Epoch 10/50 (20%)
      ‚è≥ PUK TCN: Epoch 20/50 (40%)
      ‚è≥ PUK TCN: Epoch 30/50 (60%)
      ‚è≥ PUK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.084570
         RMSE: 0.290810
         R¬≤ Score: -0.2752
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PUK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PUK Random Forest: Starting GridSearchCV fit...
       ‚úÖ TMV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.8962 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.1970
    - TCN: MSE=0.1198
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1198
        ‚Ä¢ LSTM: MSE=0.1970
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.1359
        ‚Ä¢ XGBoost: MSE=13.8962
        ‚Ä¢ Random Forest: MSE=13.9553
   ‚úÖ TMV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TMV (TargetReturn): TCN with MSE=0.1198
üêõ DEBUG: TMV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TMV.
üêõ DEBUG: TMV - Moving model to CPU before return...
üêõ DEBUG [23:35:52.743]: TMV - Returning result metadata...
üêõ DEBUG: train_worker started for SPNT
  ‚öôÔ∏è Training models for SPNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SPNT: Initiating feature extraction for training.
  [DIAGNOSTIC] SPNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPNT: rows after features available: 126
üéØ SPNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPNT: Training LSTM (50 epochs)...
       ‚úÖ CSV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.0675 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.5212
    - TCN: MSE=0.4034
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4034
        ‚Ä¢ LSTM: MSE=0.5212
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.2574
        ‚Ä¢ Random Forest: MSE=9.6999
        ‚Ä¢ XGBoost: MSE=11.0675
   ‚úÖ CSV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CSV (TargetReturn): TCN with MSE=0.4034
üêõ DEBUG: CSV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CSV.
üêõ DEBUG: CSV - Moving model to CPU before return...
üêõ DEBUG [23:35:53.021]: CSV - Returning result metadata...
üêõ DEBUG: train_worker started for WT
üêõ DEBUG [23:35:53.025]: Main received result for CSV
üêõ DEBUG: Training progress: 540/959 done
üêõ DEBUG [23:35:53.025]: Main received result for TMV
  ‚öôÔ∏è Training models for WT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - WT: Initiating feature extraction for training.
  [DIAGNOSTIC] WT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WT: rows after features available: 126
üéØ WT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WT: Training LSTM (50 epochs)...
      ‚è≥ SPNT LSTM: Epoch 10/50 (20%)
      ‚è≥ WT LSTM: Epoch 10/50 (20%)
       ‚úÖ TKO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.1805 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TKO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SPNT LSTM: Epoch 20/50 (40%)
      ‚è≥ WT LSTM: Epoch 20/50 (40%)
      ‚è≥ SPNT LSTM: Epoch 30/50 (60%)
       ‚úÖ TKO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.9792 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TKO XGBoost: Starting GridSearchCV fit...
      ‚è≥ WT LSTM: Epoch 30/50 (60%)
       ‚úÖ PUK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.1010 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PUK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SPNT LSTM: Epoch 40/50 (80%)
      ‚è≥ WT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.256529
         RMSE: 0.506487
         R¬≤ Score: -1.0125 (Poor - 101.2% variance explained)
      üîπ SPNT: Training TCN (50 epochs)...
      ‚è≥ SPNT TCN: Epoch 10/50 (20%)
       ‚úÖ PUK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.5507 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PUK XGBoost: Starting GridSearchCV fit...
      ‚è≥ SPNT TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.506165
         RMSE: 0.711453
         R¬≤ Score: -0.7808 (Poor - 78.1% variance explained)
      üîπ WT: Training TCN (50 epochs)...
      ‚è≥ WT TCN: Epoch 10/50 (20%)
      ‚è≥ SPNT TCN: Epoch 30/50 (60%)
      ‚è≥ WT TCN: Epoch 20/50 (40%)
      ‚è≥ SPNT TCN: Epoch 40/50 (80%)
      ‚è≥ WT TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.131623
         RMSE: 0.362799
         R¬≤ Score: -0.0326
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPNT Random Forest: Starting GridSearchCV fit...
      ‚è≥ WT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.518133
         RMSE: 0.719815
         R¬≤ Score: -0.8229
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WT Random Forest: Starting GridSearchCV fit...
       ‚úÖ SPNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.8598 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.8222 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SPNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.9207 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPNT XGBoost: Starting GridSearchCV fit...
       ‚úÖ WT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.3161 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WT XGBoost: Starting GridSearchCV fit...
       ‚úÖ MCB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.7719 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.3424
    - TCN: MSE=0.3089
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3089
        ‚Ä¢ LSTM: MSE=0.3424
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.7477
        ‚Ä¢ Random Forest: MSE=26.7150
        ‚Ä¢ XGBoost: MSE=28.7719
   ‚úÖ MCB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MCB (TargetReturn): TCN with MSE=0.3089
üêõ DEBUG: MCB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MCB.
üêõ DEBUG: MCB - Moving model to CPU before return...
üêõ DEBUG [23:36:15.798]: MCB - Returning result metadata...
üêõ DEBUG [23:36:15.799]: Main received result for MCB
üêõ DEBUG: train_worker started for DDS
  ‚öôÔ∏è Training models for DDS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - DDS: Initiating feature extraction for training.
  [DIAGNOSTIC] DDS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DDS: rows after features available: 126
üéØ DDS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DDS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DDS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DDS: Training LSTM (50 epochs)...
      ‚è≥ DDS LSTM: Epoch 10/50 (20%)
      ‚è≥ DDS LSTM: Epoch 20/50 (40%)
      ‚è≥ DDS LSTM: Epoch 30/50 (60%)
      ‚è≥ DDS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.670738
         RMSE: 0.818986
         R¬≤ Score: -0.9480 (Poor - 94.8% variance explained)
      üîπ DDS: Training TCN (50 epochs)...
      ‚è≥ DDS TCN: Epoch 10/50 (20%)
      ‚è≥ DDS TCN: Epoch 20/50 (40%)
      ‚è≥ DDS TCN: Epoch 30/50 (60%)
      ‚è≥ DDS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.601697
         RMSE: 0.775691
         R¬≤ Score: -0.7475
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DDS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DDS Random Forest: Starting GridSearchCV fit...
       ‚úÖ DDS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.7362 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DDS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DDS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.5165 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DDS XGBoost: Starting GridSearchCV fit...
       ‚úÖ UAE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.4350 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.2886
    - TCN: MSE=0.3282
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2886
        ‚Ä¢ TCN: MSE=0.3282
        ‚Ä¢ XGBoost: MSE=4.4350
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.7541
        ‚Ä¢ Random Forest: MSE=5.2979
   ‚úÖ UAE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UAE (TargetReturn): LSTM with MSE=0.2886
üêõ DEBUG: UAE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UAE.
üêõ DEBUG: UAE - Moving model to CPU before return...
üêõ DEBUG [23:36:24.835]: UAE - Returning result metadata...
üêõ DEBUG [23:36:24.836]: Main received result for UAE
üêõ DEBUG: train_worker started for IAI
  ‚öôÔ∏è Training models for IAI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - IAI: Initiating feature extraction for training.
  [DIAGNOSTIC] IAI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IAI: rows after features available: 126
üéØ IAI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IAI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IAI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IAI: Training LSTM (50 epochs)...
       ‚úÖ PJT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.3960 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.6760
    - TCN: MSE=0.7162
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6760
        ‚Ä¢ TCN: MSE=0.7162
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8712
        ‚Ä¢ Random Forest: MSE=12.3390
        ‚Ä¢ XGBoost: MSE=12.3960
   ‚úÖ PJT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PJT (TargetReturn): LSTM with MSE=0.6760
üêõ DEBUG: PJT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PJT.
üêõ DEBUG: PJT - Moving model to CPU before return...
üêõ DEBUG [23:36:25.283]: PJT - Returning result metadata...
üêõ DEBUG [23:36:25.284]: Main received result for PJT
üêõ DEBUG: Training progress: 544/959 done
üêõ DEBUG: train_worker started for PGNY
      ‚è≥ IAI LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for PGNY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - PGNY: Initiating feature extraction for training.
  [DIAGNOSTIC] PGNY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PGNY: rows after features available: 126
üéØ PGNY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PGNY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PGNY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PGNY: Training LSTM (50 epochs)...
      ‚è≥ PGNY LSTM: Epoch 10/50 (20%)
      ‚è≥ IAI LSTM: Epoch 20/50 (40%)
      ‚è≥ PGNY LSTM: Epoch 20/50 (40%)
      ‚è≥ IAI LSTM: Epoch 30/50 (60%)
      ‚è≥ PGNY LSTM: Epoch 30/50 (60%)
      ‚è≥ IAI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.563129
         RMSE: 0.750419
         R¬≤ Score: -1.0442 (Poor - 104.4% variance explained)
      üîπ IAI: Training TCN (50 epochs)...
      ‚è≥ PGNY LSTM: Epoch 40/50 (80%)
      ‚è≥ IAI TCN: Epoch 10/50 (20%)
      ‚è≥ IAI TCN: Epoch 20/50 (40%)
      ‚è≥ IAI TCN: Epoch 30/50 (60%)
      ‚è≥ IAI TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.026990
         RMSE: 0.164287
         R¬≤ Score: -0.6926 (Poor - 69.3% variance explained)
      üîπ PGNY: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.537618
         RMSE: 0.733224
         R¬≤ Score: -0.9516
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IAI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IAI Random Forest: Starting GridSearchCV fit...
      ‚è≥ PGNY TCN: Epoch 10/50 (20%)
      ‚è≥ PGNY TCN: Epoch 20/50 (40%)
      ‚è≥ PGNY TCN: Epoch 30/50 (60%)
      ‚è≥ PGNY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.020576
         RMSE: 0.143442
         R¬≤ Score: -0.2904
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PGNY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PGNY Random Forest: Starting GridSearchCV fit...
       ‚úÖ IAI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.3640 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IAI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PGNY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.8977 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PGNY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IAI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.4612 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IAI XGBoost: Starting GridSearchCV fit...
       ‚úÖ PGNY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.0807 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PGNY XGBoost: Starting GridSearchCV fit...
       ‚úÖ MBI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=43.9696 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.1088
    - TCN: MSE=0.0666
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0666
        ‚Ä¢ LSTM: MSE=0.1088
        ‚Ä¢ Random Forest: MSE=28.1154
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.0519
        ‚Ä¢ XGBoost: MSE=43.9696
   ‚úÖ MBI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MBI (TargetReturn): TCN with MSE=0.0666
üêõ DEBUG: MBI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MBI.
üêõ DEBUG: MBI - Moving model to CPU before return...
üêõ DEBUG [23:36:36.142]: MBI - Returning result metadata...
üêõ DEBUG: train_worker started for SGI
  ‚öôÔ∏è Training models for SGI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - SGI: Initiating feature extraction for training.
  [DIAGNOSTIC] SGI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SGI: rows after features available: 126
üéØ SGI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SGI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SGI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SGI: Training LSTM (50 epochs)...
      ‚è≥ SGI LSTM: Epoch 10/50 (20%)
      ‚è≥ SGI LSTM: Epoch 20/50 (40%)
      ‚è≥ SGI LSTM: Epoch 30/50 (60%)
      ‚è≥ SGI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.507239
         RMSE: 0.712207
         R¬≤ Score: -1.3477 (Poor - 134.8% variance explained)
      üîπ SGI: Training TCN (50 epochs)...
      ‚è≥ SGI TCN: Epoch 10/50 (20%)
      ‚è≥ SGI TCN: Epoch 20/50 (40%)
       ‚úÖ HNST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.4272 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.0767
    - TCN: MSE=0.0462
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0462
        ‚Ä¢ LSTM: MSE=0.0767
        ‚Ä¢ Random Forest: MSE=21.7915
        ‚Ä¢ XGBoost: MSE=46.4272
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.4519
   ‚úÖ HNST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HNST (TargetReturn): TCN with MSE=0.0462
üêõ DEBUG: HNST - train_and_evaluate_models completed
      ‚è≥ SGI TCN: Epoch 30/50 (60%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HNST.
üêõ DEBUG: HNST - Moving model to CPU before return...
üêõ DEBUG [23:36:38.904]: HNST - Returning result metadata...
üêõ DEBUG [23:36:38.904]: Main received result for HNSTüêõ DEBUG: train_worker started for STN

üêõ DEBUG [23:36:38.904]: Main received result for MBI
  ‚öôÔ∏è Training models for STN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - STN: Initiating feature extraction for training.
  [DIAGNOSTIC] STN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STN: rows after features available: 126
üéØ STN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STN: Training LSTM (50 epochs)...
      ‚è≥ SGI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.393999
         RMSE: 0.627693
         R¬≤ Score: -0.8236
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SGI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SGI Random Forest: Starting GridSearchCV fit...
      ‚è≥ STN LSTM: Epoch 10/50 (20%)
      ‚è≥ STN LSTM: Epoch 20/50 (40%)
      ‚è≥ STN LSTM: Epoch 30/50 (60%)
      ‚è≥ STN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.175953
         RMSE: 0.419467
         R¬≤ Score: -1.4017 (Poor - 140.2% variance explained)
      üîπ STN: Training TCN (50 epochs)...
      ‚è≥ STN TCN: Epoch 10/50 (20%)
      ‚è≥ STN TCN: Epoch 20/50 (40%)
      ‚è≥ STN TCN: Epoch 30/50 (60%)
      ‚è≥ STN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.077991
         RMSE: 0.279269
         R¬≤ Score: -0.0646
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STN Random Forest: Starting GridSearchCV fit...
       ‚úÖ SGI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.7680 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SGI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SGI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.9338 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SGI XGBoost: Starting GridSearchCV fit...
       ‚úÖ EMR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.7021 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.5446
    - TCN: MSE=0.4291
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4291
        ‚Ä¢ LSTM: MSE=0.5446
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.1662
        ‚Ä¢ XGBoost: MSE=17.7021
        ‚Ä¢ Random Forest: MSE=18.2836
   ‚úÖ EMR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EMR (TargetReturn): TCN with MSE=0.4291
üêõ DEBUG: EMR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EMR.
üêõ DEBUG: EMR - Moving model to CPU before return...
üêõ DEBUG [23:36:44.449]: EMR - Returning result metadata...
üêõ DEBUG: train_worker started for ETHE
üêõ DEBUG [23:36:44.454]: Main received result for EMR
  ‚öôÔ∏è Training models for ETHE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ETHE: Initiating feature extraction for training.
  [DIAGNOSTIC] ETHE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETHE: rows after features available: 126
üéØ ETHE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETHE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETHE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETHE: Training LSTM (50 epochs)...
       ‚úÖ STN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=41.5006 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ETHE LSTM: Epoch 10/50 (20%)
       ‚úÖ STN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.0018 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STN XGBoost: Starting GridSearchCV fit...
      ‚è≥ ETHE LSTM: Epoch 20/50 (40%)
      ‚è≥ ETHE LSTM: Epoch 30/50 (60%)
      ‚è≥ ETHE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.436928
         RMSE: 0.661005
         R¬≤ Score: -0.5177 (Poor - 51.8% variance explained)
      üîπ ETHE: Training TCN (50 epochs)...
      ‚è≥ ETHE TCN: Epoch 10/50 (20%)
      ‚è≥ ETHE TCN: Epoch 20/50 (40%)
      ‚è≥ ETHE TCN: Epoch 30/50 (60%)
      ‚è≥ ETHE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.479305
         RMSE: 0.692319
         R¬≤ Score: -0.6649
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETHE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETHE Random Forest: Starting GridSearchCV fit...
       ‚úÖ ING XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.2344 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.2967
    - TCN: MSE=0.1739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1739
        ‚Ä¢ LSTM: MSE=0.2967
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.9492
        ‚Ä¢ Random Forest: MSE=37.6323
        ‚Ä¢ XGBoost: MSE=52.2344
   ‚úÖ ING: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ING (TargetReturn): TCN with MSE=0.1739
üêõ DEBUG: ING - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ING.
üêõ DEBUG: ING - Moving model to CPU before return...
üêõ DEBUG [23:36:48.136]: ING - Returning result metadata...
üêõ DEBUG: train_worker started for FAS
üêõ DEBUG [23:36:48.140]: Main received result for ING
üêõ DEBUG: Training progress: 548/959 done
  ‚öôÔ∏è Training models for FAS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - FAS: Initiating feature extraction for training.
  [DIAGNOSTIC] FAS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FAS: rows after features available: 126
üéØ FAS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FAS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FAS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FAS: Training LSTM (50 epochs)...
      ‚è≥ FAS LSTM: Epoch 10/50 (20%)
      ‚è≥ FAS LSTM: Epoch 20/50 (40%)
      ‚è≥ FAS LSTM: Epoch 30/50 (60%)
      ‚è≥ FAS LSTM: Epoch 40/50 (80%)
       ‚úÖ ETHE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=42.9043 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETHE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.326249
         RMSE: 0.571182
         R¬≤ Score: -0.4214 (Poor - 42.1% variance explained)
      üîπ FAS: Training TCN (50 epochs)...
      ‚è≥ FAS TCN: Epoch 10/50 (20%)
      ‚è≥ FAS TCN: Epoch 20/50 (40%)
      ‚è≥ FAS TCN: Epoch 30/50 (60%)
      ‚è≥ FAS TCN: Epoch 40/50 (80%)
       ‚úÖ ETHE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=112.9701 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETHE XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.262864
         RMSE: 0.512703
         R¬≤ Score: -0.1452
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FAS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FAS Random Forest: Starting GridSearchCV fit...
       ‚úÖ FAS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.2205 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FAS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FAS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=52.6576 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FAS XGBoost: Starting GridSearchCV fit...
       ‚úÖ TMAT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.1028 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.1s
    - LSTM: MSE=0.5600
    - TCN: MSE=0.5399
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5399
        ‚Ä¢ LSTM: MSE=0.5600
        ‚Ä¢ Random Forest: MSE=12.4771
        ‚Ä¢ XGBoost: MSE=13.1028
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.6827
   ‚úÖ TMAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TMAT (TargetReturn): TCN with MSE=0.5399
üêõ DEBUG: TMAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TMAT.
üêõ DEBUG: TMAT - Moving model to CPU before return...
üêõ DEBUG [23:37:17.354]: TMAT - Returning result metadata...
üêõ DEBUG [23:37:17.354]: Main received result for TMATüêõ DEBUG: train_worker started for BAP

  ‚öôÔ∏è Training models for BAP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - BAP: Initiating feature extraction for training.
  [DIAGNOSTIC] BAP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BAP: rows after features available: 126
üéØ BAP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BAP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BAP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BAP: Training LSTM (50 epochs)...
      ‚è≥ BAP LSTM: Epoch 10/50 (20%)
      ‚è≥ BAP LSTM: Epoch 20/50 (40%)
      ‚è≥ BAP LSTM: Epoch 30/50 (60%)
      ‚è≥ BAP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.367933
         RMSE: 0.606575
         R¬≤ Score: -2.0572 (Poor - 205.7% variance explained)
      üîπ BAP: Training TCN (50 epochs)...
      ‚è≥ BAP TCN: Epoch 10/50 (20%)
      ‚è≥ BAP TCN: Epoch 20/50 (40%)
      ‚è≥ BAP TCN: Epoch 30/50 (60%)
      ‚è≥ BAP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.125651
         RMSE: 0.354473
         R¬≤ Score: -0.0441
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BAP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BAP Random Forest: Starting GridSearchCV fit...
       ‚úÖ BAP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.3429 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BAP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BAP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=33.3240 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BAP XGBoost: Starting GridSearchCV fit...
       ‚úÖ VEEV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8440 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.4839
    - TCN: MSE=0.6568
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4839
        ‚Ä¢ TCN: MSE=0.6568
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.5129
        ‚Ä¢ XGBoost: MSE=16.8440
        ‚Ä¢ Random Forest: MSE=17.7986
   ‚úÖ VEEV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VEEV (TargetReturn): LSTM with MSE=0.4839
üêõ DEBUG: VEEV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VEEV.
üêõ DEBUG: VEEV - Moving model to CPU before return...
üêõ DEBUG [23:37:26.448]: VEEV - Returning result metadata...
üêõ DEBUG: train_worker started for OLLI
üêõ DEBUG [23:37:26.449]: Main received result for VEEV
  ‚öôÔ∏è Training models for OLLI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - OLLI: Initiating feature extraction for training.
  [DIAGNOSTIC] OLLI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OLLI: rows after features available: 126
üéØ OLLI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OLLI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OLLI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OLLI: Training LSTM (50 epochs)...
      ‚è≥ OLLI LSTM: Epoch 10/50 (20%)
      ‚è≥ OLLI LSTM: Epoch 20/50 (40%)
      ‚è≥ OLLI LSTM: Epoch 30/50 (60%)
      ‚è≥ OLLI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.299374
         RMSE: 0.547151
         R¬≤ Score: -0.9919 (Poor - 99.2% variance explained)
      üîπ OLLI: Training TCN (50 epochs)...
      ‚è≥ OLLI TCN: Epoch 10/50 (20%)
      ‚è≥ OLLI TCN: Epoch 20/50 (40%)
      ‚è≥ OLLI TCN: Epoch 30/50 (60%)
      ‚è≥ OLLI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.203695
         RMSE: 0.451325
         R¬≤ Score: -0.3553
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OLLI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OLLI Random Forest: Starting GridSearchCV fit...
       ‚úÖ GTLS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=32.9795 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.3s
    - LSTM: MSE=0.2844
    - TCN: MSE=0.2537
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2537
        ‚Ä¢ LSTM: MSE=0.2844
        ‚Ä¢ XGBoost: MSE=32.9795
        ‚Ä¢ Random Forest: MSE=40.4690
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.7182
   ‚úÖ GTLS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GTLS (TargetReturn): TCN with MSE=0.2537
üêõ DEBUG: GTLS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GTLS.
üêõ DEBUG: GTLS - Moving model to CPU before return...
üêõ DEBUG [23:37:30.544]: GTLS - Returning result metadata...
üêõ DEBUG [23:37:30.544]: Main received result for GTLS
üêõ DEBUG: train_worker started for NRGV
  ‚öôÔ∏è Training models for NRGV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NRGV: Initiating feature extraction for training.
  [DIAGNOSTIC] NRGV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NRGV: rows after features available: 126
üéØ NRGV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NRGV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NRGV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NRGV: Training LSTM (50 epochs)...
      ‚è≥ NRGV LSTM: Epoch 10/50 (20%)
      ‚è≥ NRGV LSTM: Epoch 20/50 (40%)
       ‚úÖ OLLI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.0340 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OLLI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NRGV LSTM: Epoch 30/50 (60%)
      ‚è≥ NRGV LSTM: Epoch 40/50 (80%)
       ‚úÖ OLLI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.4328 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OLLI XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.473408
         RMSE: 0.688047
         R¬≤ Score: -1.3298 (Poor - 133.0% variance explained)
      üîπ NRGV: Training TCN (50 epochs)...
      ‚è≥ NRGV TCN: Epoch 10/50 (20%)
      ‚è≥ NRGV TCN: Epoch 20/50 (40%)
      ‚è≥ NRGV TCN: Epoch 30/50 (60%)
      ‚è≥ NRGV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.397242
         RMSE: 0.630272
         R¬≤ Score: -0.9550
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NRGV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NRGV Random Forest: Starting GridSearchCV fit...
       ‚úÖ VRT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=65.6105 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.4342
    - TCN: MSE=0.3988
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3988
        ‚Ä¢ LSTM: MSE=0.4342
        ‚Ä¢ Random Forest: MSE=61.4176
        ‚Ä¢ XGBoost: MSE=65.6105
        ‚Ä¢ LightGBM Regressor (CPU): MSE=82.2137
   ‚úÖ VRT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VRT (TargetReturn): TCN with MSE=0.3988
üêõ DEBUG: VRT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VRT.
üêõ DEBUG: VRT - Moving model to CPU before return...
üêõ DEBUG [23:37:34.917]: VRT - Returning result metadata...
üêõ DEBUG: train_worker started for KLTR
üêõ DEBUG [23:37:34.918]: Main received result for VRT
üêõ DEBUG: Training progress: 552/959 done
  ‚öôÔ∏è Training models for KLTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - KLTR: Initiating feature extraction for training.
  [DIAGNOSTIC] KLTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KLTR: rows after features available: 126
üéØ KLTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KLTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KLTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KLTR: Training LSTM (50 epochs)...
      ‚è≥ KLTR LSTM: Epoch 10/50 (20%)
      ‚è≥ KLTR LSTM: Epoch 20/50 (40%)
       ‚úÖ NRGV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=86.5523 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NRGV LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ KLTR LSTM: Epoch 30/50 (60%)
      ‚è≥ KLTR LSTM: Epoch 40/50 (80%)
       ‚úÖ NRGV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=84.6976 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NRGV XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.064145
         RMSE: 0.253269
         R¬≤ Score: -0.7712 (Poor - 77.1% variance explained)
      üîπ KLTR: Training TCN (50 epochs)...
      ‚è≥ KLTR TCN: Epoch 10/50 (20%)
      ‚è≥ KLTR TCN: Epoch 20/50 (40%)
      ‚è≥ KLTR TCN: Epoch 30/50 (60%)
      ‚è≥ KLTR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.040535
         RMSE: 0.201334
         R¬≤ Score: -0.1193
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KLTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KLTR Random Forest: Starting GridSearchCV fit...
       ‚úÖ KLTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.0177 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KLTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KLTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.9705 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KLTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ TKO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.0470 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.1369
    - TCN: MSE=0.0849
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0849
        ‚Ä¢ LSTM: MSE=0.1369
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.9792
        ‚Ä¢ XGBoost: MSE=15.0470
        ‚Ä¢ Random Forest: MSE=15.1805
   ‚úÖ TKO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TKO (TargetReturn): TCN with MSE=0.0849
üêõ DEBUG: TKO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TKO.
üêõ DEBUG: TKO - Moving model to CPU before return...
üêõ DEBUG [23:37:51.482]: TKO - Returning result metadata...
üêõ DEBUG: train_worker started for MTW
üêõ DEBUG [23:37:51.486]: Main received result for TKO
  ‚öôÔ∏è Training models for MTW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - MTW: Initiating feature extraction for training.
  [DIAGNOSTIC] MTW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MTW: rows after features available: 126
üéØ MTW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MTW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MTW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MTW: Training LSTM (50 epochs)...
      ‚è≥ MTW LSTM: Epoch 10/50 (20%)
      ‚è≥ MTW LSTM: Epoch 20/50 (40%)
      ‚è≥ MTW LSTM: Epoch 30/50 (60%)
      ‚è≥ MTW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.704839
         RMSE: 0.839547
         R¬≤ Score: -0.9269 (Poor - 92.7% variance explained)
      üîπ MTW: Training TCN (50 epochs)...
      ‚è≥ MTW TCN: Epoch 10/50 (20%)
      ‚è≥ MTW TCN: Epoch 20/50 (40%)
      ‚è≥ MTW TCN: Epoch 30/50 (60%)
      ‚è≥ MTW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.526472
         RMSE: 0.725584
         R¬≤ Score: -0.4393
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MTW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MTW Random Forest: Starting GridSearchCV fit...
       ‚úÖ PUK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.2546 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.1160
    - TCN: MSE=0.0846
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0846
        ‚Ä¢ LSTM: MSE=0.1160
        ‚Ä¢ Random Forest: MSE=16.1010
        ‚Ä¢ XGBoost: MSE=17.2546
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.5507
   ‚úÖ PUK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PUK (TargetReturn): TCN with MSE=0.0846
üêõ DEBUG: PUK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PUK.
üêõ DEBUG: PUK - Moving model to CPU before return...
üêõ DEBUG [23:37:55.401]: PUK - Returning result metadata...
üêõ DEBUG [23:37:55.401]: Main received result for PUK
üêõ DEBUG: train_worker started for DCO
  ‚öôÔ∏è Training models for DCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - DCO: Initiating feature extraction for training.
  [DIAGNOSTIC] DCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DCO: rows after features available: 126
üéØ DCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DCO: Training LSTM (50 epochs)...
      ‚è≥ DCO LSTM: Epoch 10/50 (20%)
      ‚è≥ DCO LSTM: Epoch 20/50 (40%)
       ‚úÖ SPNT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.8403 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.2565
    - TCN: MSE=0.1316
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1316
        ‚Ä¢ LSTM: MSE=0.2565
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.9207
        ‚Ä¢ Random Forest: MSE=17.8598
        ‚Ä¢ XGBoost: MSE=29.8403
   ‚úÖ SPNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPNT (TargetReturn): TCN with MSE=0.1316
üêõ DEBUG: SPNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPNT.
üêõ DEBUG: SPNT - Moving model to CPU before return...
üêõ DEBUG [23:37:57.119]: SPNT - Returning result metadata...
üêõ DEBUG [23:37:57.120]: Main received result for SPNT
üêõ DEBUG: train_worker started for TRMB
      ‚è≥ DCO LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for TRMB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TRMB: Initiating feature extraction for training.
  [DIAGNOSTIC] TRMB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TRMB: rows after features available: 126
üéØ TRMB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TRMB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TRMB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TRMB: Training LSTM (50 epochs)...
      ‚è≥ DCO LSTM: Epoch 40/50 (80%)
      ‚è≥ TRMB LSTM: Epoch 10/50 (20%)
       ‚úÖ MTW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=66.3063 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MTW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.831287
         RMSE: 0.911749
         R¬≤ Score: -1.4245 (Poor - 142.5% variance explained)
      üîπ DCO: Training TCN (50 epochs)...
      ‚è≥ TRMB LSTM: Epoch 20/50 (40%)
      ‚è≥ DCO TCN: Epoch 10/50 (20%)
      ‚è≥ DCO TCN: Epoch 20/50 (40%)
      ‚è≥ DCO TCN: Epoch 30/50 (60%)
      ‚è≥ DCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.633064
         RMSE: 0.795653
         R¬≤ Score: -0.8464
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DCO Random Forest: Starting GridSearchCV fit...
      ‚è≥ TRMB LSTM: Epoch 30/50 (60%)
       ‚úÖ MTW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=79.7250 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MTW XGBoost: Starting GridSearchCV fit...
      ‚è≥ TRMB LSTM: Epoch 40/50 (80%)
       ‚úÖ WT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.1520 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.5062
    - TCN: MSE=0.5181
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5062
        ‚Ä¢ TCN: MSE=0.5181
        ‚Ä¢ Random Forest: MSE=13.8222
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3161
        ‚Ä¢ XGBoost: MSE=35.1520
   ‚úÖ WT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WT (TargetReturn): LSTM with MSE=0.5062
üêõ DEBUG: WT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WT.
üêõ DEBUG: WT - Moving model to CPU before return...
üêõ DEBUG [23:37:59.376]: WT - Returning result metadata...
üêõ DEBUG: train_worker started for KT
üêõ DEBUG [23:37:59.377]: Main received result for WT
üêõ DEBUG: Training progress: 556/959 done
  ‚öôÔ∏è Training models for KT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - KT: Initiating feature extraction for training.
  [DIAGNOSTIC] KT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KT: rows after features available: 126
üéØ KT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KT: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.721283
         RMSE: 0.849284
         R¬≤ Score: -1.5277 (Poor - 152.8% variance explained)
      üîπ TRMB: Training TCN (50 epochs)...
      ‚è≥ KT LSTM: Epoch 10/50 (20%)
      ‚è≥ TRMB TCN: Epoch 10/50 (20%)
      ‚è≥ TRMB TCN: Epoch 20/50 (40%)
      ‚è≥ TRMB TCN: Epoch 30/50 (60%)
      ‚è≥ TRMB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.516111
         RMSE: 0.718409
         R¬≤ Score: -0.8087
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TRMB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TRMB Random Forest: Starting GridSearchCV fit...
      ‚è≥ KT LSTM: Epoch 20/50 (40%)
      ‚è≥ KT LSTM: Epoch 30/50 (60%)
      ‚è≥ KT LSTM: Epoch 40/50 (80%)
       ‚úÖ DCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.9548 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.177511
         RMSE: 0.421320
         R¬≤ Score: -0.9165 (Poor - 91.7% variance explained)
      üîπ KT: Training TCN (50 epochs)...
      ‚è≥ KT TCN: Epoch 10/50 (20%)
      ‚è≥ KT TCN: Epoch 20/50 (40%)
      ‚è≥ KT TCN: Epoch 30/50 (60%)
       ‚úÖ DCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.2068 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DCO XGBoost: Starting GridSearchCV fit...
      ‚è≥ KT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.095844
         RMSE: 0.309587
         R¬≤ Score: -0.0348
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KT Random Forest: Starting GridSearchCV fit...
       ‚úÖ TRMB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.0330 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TRMB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TRMB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.7103 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TRMB XGBoost: Starting GridSearchCV fit...
       ‚úÖ KT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5458 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4111 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KT XGBoost: Starting GridSearchCV fit...
       ‚úÖ DDS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=43.6347 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.8s
    - LSTM: MSE=0.6707
    - TCN: MSE=0.6017
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6017
        ‚Ä¢ LSTM: MSE=0.6707
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.5165
        ‚Ä¢ Random Forest: MSE=40.7362
        ‚Ä¢ XGBoost: MSE=43.6347
   ‚úÖ DDS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DDS (TargetReturn): TCN with MSE=0.6017
üêõ DEBUG: DDS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DDS.
üêõ DEBUG: DDS - Moving model to CPU before return...
üêõ DEBUG [23:38:18.036]: DDS - Returning result metadata...
üêõ DEBUG: train_worker started for FHI
üêõ DEBUG [23:38:18.037]: Main received result for DDS
  ‚öôÔ∏è Training models for FHI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FHI: Initiating feature extraction for training.
  [DIAGNOSTIC] FHI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FHI: rows after features available: 126
üéØ FHI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FHI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FHI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FHI: Training LSTM (50 epochs)...
      ‚è≥ FHI LSTM: Epoch 10/50 (20%)
      ‚è≥ FHI LSTM: Epoch 20/50 (40%)
      ‚è≥ FHI LSTM: Epoch 30/50 (60%)
      ‚è≥ FHI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.329308
         RMSE: 0.573854
         R¬≤ Score: -1.1824 (Poor - 118.2% variance explained)
      üîπ FHI: Training TCN (50 epochs)...
      ‚è≥ FHI TCN: Epoch 10/50 (20%)
      ‚è≥ FHI TCN: Epoch 20/50 (40%)
      ‚è≥ FHI TCN: Epoch 30/50 (60%)
      ‚è≥ FHI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.228680
         RMSE: 0.478204
         R¬≤ Score: -0.5155
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FHI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FHI Random Forest: Starting GridSearchCV fit...
       ‚úÖ FHI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.5961 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FHI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FHI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.9727 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FHI XGBoost: Starting GridSearchCV fit...
       ‚úÖ PGNY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.3630 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.5s
    - LSTM: MSE=0.0270
    - TCN: MSE=0.0206
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0206
        ‚Ä¢ LSTM: MSE=0.0270
        ‚Ä¢ Random Forest: MSE=14.8977
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.0807
        ‚Ä¢ XGBoost: MSE=17.3630
   ‚úÖ PGNY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PGNY (TargetReturn): TCN with MSE=0.0206
üêõ DEBUG: PGNY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PGNY.
üêõ DEBUG: PGNY - Moving model to CPU before return...
üêõ DEBUG [23:38:28.022]: PGNY - Returning result metadata...
üêõ DEBUG: train_worker started for GDX
  ‚öôÔ∏è Training models for GDX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - GDX: Initiating feature extraction for training.
  [DIAGNOSTIC] GDX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GDX: rows after features available: 126
üéØ GDX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GDX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GDX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GDX: Training LSTM (50 epochs)...
      ‚è≥ GDX LSTM: Epoch 10/50 (20%)
      ‚è≥ GDX LSTM: Epoch 20/50 (40%)
      ‚è≥ GDX LSTM: Epoch 30/50 (60%)
      ‚è≥ GDX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.259045
         RMSE: 0.508964
         R¬≤ Score: -1.0554 (Poor - 105.5% variance explained)
      üîπ GDX: Training TCN (50 epochs)...
      ‚è≥ GDX TCN: Epoch 10/50 (20%)
      ‚è≥ GDX TCN: Epoch 20/50 (40%)
      ‚è≥ GDX TCN: Epoch 30/50 (60%)
      ‚è≥ GDX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.152492
         RMSE: 0.390502
         R¬≤ Score: -0.2099
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GDX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GDX Random Forest: Starting GridSearchCV fit...
       ‚úÖ IAI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8166 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.5631
    - TCN: MSE=0.5376
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5376
        ‚Ä¢ LSTM: MSE=0.5631
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.4612
        ‚Ä¢ XGBoost: MSE=16.8166
        ‚Ä¢ Random Forest: MSE=18.3640
   ‚úÖ IAI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IAI (TargetReturn): TCN with MSE=0.5376
üêõ DEBUG: IAI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IAI.
üêõ DEBUG: IAI - Moving model to CPU before return...
üêõ DEBUG [23:38:31.977]: IAI - Returning result metadata...
üêõ DEBUG [23:38:31.977]: Main received result for IAI
üêõ DEBUG [23:38:31.977]: Main received result for PGNY
üêõ DEBUG: train_worker started for BWXT
  ‚öôÔ∏è Training models for BWXT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BWXT: Initiating feature extraction for training.
  [DIAGNOSTIC] BWXT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BWXT: rows after features available: 126
üéØ BWXT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BWXT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BWXT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BWXT: Training LSTM (50 epochs)...
      ‚è≥ BWXT LSTM: Epoch 10/50 (20%)
      ‚è≥ BWXT LSTM: Epoch 20/50 (40%)
      ‚è≥ BWXT LSTM: Epoch 30/50 (60%)
       ‚úÖ GDX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.9604 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GDX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BWXT LSTM: Epoch 40/50 (80%)
       ‚úÖ GDX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.0284 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GDX XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.483700
         RMSE: 0.695486
         R¬≤ Score: -1.2711 (Poor - 127.1% variance explained)
      üîπ BWXT: Training TCN (50 epochs)...
      ‚è≥ BWXT TCN: Epoch 10/50 (20%)
      ‚è≥ BWXT TCN: Epoch 20/50 (40%)
      ‚è≥ BWXT TCN: Epoch 30/50 (60%)
      ‚è≥ BWXT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.389414
         RMSE: 0.624030
         R¬≤ Score: -0.8284
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BWXT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BWXT Random Forest: Starting GridSearchCV fit...
       ‚úÖ BWXT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.8007 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BWXT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BWXT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.6472 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BWXT XGBoost: Starting GridSearchCV fit...
       ‚úÖ SGI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.1903 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.5072
    - TCN: MSE=0.3940
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3940
        ‚Ä¢ LSTM: MSE=0.5072
        ‚Ä¢ Random Forest: MSE=14.7680
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.9338
        ‚Ä¢ XGBoost: MSE=16.1903
   ‚úÖ SGI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SGI (TargetReturn): TCN with MSE=0.3940
üêõ DEBUG: SGI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SGI.
üêõ DEBUG: SGI - Moving model to CPU before return...
üêõ DEBUG [23:38:40.751]: SGI - Returning result metadata...
üêõ DEBUG [23:38:40.752]: Main received result for SGI
üêõ DEBUG: Training progress: 560/959 done
üêõ DEBUG: train_worker started for ETR
  ‚öôÔ∏è Training models for ETR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - ETR: Initiating feature extraction for training.
  [DIAGNOSTIC] ETR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ETR: rows after features available: 126
üéØ ETR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ETR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ETR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ETR: Training LSTM (50 epochs)...
      ‚è≥ ETR LSTM: Epoch 10/50 (20%)
      ‚è≥ ETR LSTM: Epoch 20/50 (40%)
      ‚è≥ ETR LSTM: Epoch 30/50 (60%)
      ‚è≥ ETR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.115030
         RMSE: 0.339161
         R¬≤ Score: -0.9793 (Poor - 97.9% variance explained)
      üîπ ETR: Training TCN (50 epochs)...
      ‚è≥ ETR TCN: Epoch 10/50 (20%)
      ‚è≥ ETR TCN: Epoch 20/50 (40%)
      ‚è≥ ETR TCN: Epoch 30/50 (60%)
      ‚è≥ ETR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.058899
         RMSE: 0.242692
         R¬≤ Score: -0.0135
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ETR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ETR Random Forest: Starting GridSearchCV fit...
       ‚úÖ STN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=63.5499 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.1760
    - TCN: MSE=0.0780
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0780
        ‚Ä¢ LSTM: MSE=0.1760
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.0018
        ‚Ä¢ Random Forest: MSE=41.5006
        ‚Ä¢ XGBoost: MSE=63.5499
   ‚úÖ STN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STN (TargetReturn): TCN with MSE=0.0780
üêõ DEBUG: STN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STN.
üêõ DEBUG: STN - Moving model to CPU before return...
üêõ DEBUG [23:38:45.304]: STN - Returning result metadata...
üêõ DEBUG [23:38:45.305]: Main received result for STNüêõ DEBUG: train_worker started for CGNT

  ‚öôÔ∏è Training models for CGNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - CGNT: Initiating feature extraction for training.
  [DIAGNOSTIC] CGNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CGNT: rows after features available: 126
üéØ CGNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CGNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CGNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CGNT: Training LSTM (50 epochs)...
      ‚è≥ CGNT LSTM: Epoch 10/50 (20%)
      ‚è≥ CGNT LSTM: Epoch 20/50 (40%)
       ‚úÖ ETR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.2178 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ETR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CGNT LSTM: Epoch 30/50 (60%)
       ‚úÖ ETR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.8459 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ETR XGBoost: Starting GridSearchCV fit...
      ‚è≥ CGNT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.306662
         RMSE: 0.553771
         R¬≤ Score: -0.7728 (Poor - 77.3% variance explained)
      üîπ CGNT: Training TCN (50 epochs)...
      ‚è≥ CGNT TCN: Epoch 10/50 (20%)
      ‚è≥ CGNT TCN: Epoch 20/50 (40%)
      ‚è≥ CGNT TCN: Epoch 30/50 (60%)
      ‚è≥ CGNT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180367
         RMSE: 0.424696
         R¬≤ Score: -0.0427
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CGNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CGNT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETHE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.2184 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.4369
    - TCN: MSE=0.4793
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4369
        ‚Ä¢ TCN: MSE=0.4793
        ‚Ä¢ Random Forest: MSE=42.9043
        ‚Ä¢ XGBoost: MSE=53.2184
        ‚Ä¢ LightGBM Regressor (CPU): MSE=112.9701
   ‚úÖ ETHE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETHE (TargetReturn): LSTM with MSE=0.4369
üêõ DEBUG: ETHE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETHE.
üêõ DEBUG: ETHE - Moving model to CPU before return...
üêõ DEBUG [23:38:50.858]: ETHE - Returning result metadata...
üêõ DEBUG [23:38:50.858]: Main received result for ETHE
üêõ DEBUG: train_worker started for SAH
  ‚öôÔ∏è Training models for SAH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - SAH: Initiating feature extraction for training.
  [DIAGNOSTIC] SAH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SAH: rows after features available: 126
üéØ SAH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SAH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SAH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SAH: Training LSTM (50 epochs)...
       ‚úÖ CGNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.9892 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CGNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SAH LSTM: Epoch 10/50 (20%)
       ‚úÖ CGNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.4638 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CGNT XGBoost: Starting GridSearchCV fit...
      ‚è≥ SAH LSTM: Epoch 20/50 (40%)
      ‚è≥ SAH LSTM: Epoch 30/50 (60%)
      ‚è≥ SAH LSTM: Epoch 40/50 (80%)
       ‚úÖ FAS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=42.3400 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.3262
    - TCN: MSE=0.2629
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2629
        ‚Ä¢ LSTM: MSE=0.3262
        ‚Ä¢ XGBoost: MSE=42.3400
        ‚Ä¢ Random Forest: MSE=44.2205
        ‚Ä¢ LightGBM Regressor (CPU): MSE=52.6576
   ‚úÖ FAS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FAS (TargetReturn): TCN with MSE=0.2629
üêõ DEBUG: FAS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FAS.
üêõ DEBUG: FAS - Moving model to CPU before return...
üêõ DEBUG [23:38:53.019]: FAS - Returning result metadata...
üêõ DEBUG [23:38:53.019]: Main received result for FAS
üêõ DEBUG: train_worker started for FPX
  ‚öôÔ∏è Training models for FPX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - FPX: Initiating feature extraction for training.
  [DIAGNOSTIC] FPX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FPX: rows after features available: 126
üéØ FPX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FPX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FPX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FPX: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.837010
         RMSE: 0.914883
         R¬≤ Score: -1.0289 (Poor - 102.9% variance explained)
      üîπ SAH: Training TCN (50 epochs)...
      ‚è≥ SAH TCN: Epoch 10/50 (20%)
      ‚è≥ FPX LSTM: Epoch 10/50 (20%)
      ‚è≥ SAH TCN: Epoch 20/50 (40%)
      ‚è≥ SAH TCN: Epoch 30/50 (60%)
      ‚è≥ SAH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.682179
         RMSE: 0.825941
         R¬≤ Score: -0.6536
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SAH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SAH Random Forest: Starting GridSearchCV fit...
      ‚è≥ FPX LSTM: Epoch 20/50 (40%)
      ‚è≥ FPX LSTM: Epoch 30/50 (60%)
      ‚è≥ FPX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.537154
         RMSE: 0.732908
         R¬≤ Score: -0.9431 (Poor - 94.3% variance explained)
      üîπ FPX: Training TCN (50 epochs)...
      ‚è≥ FPX TCN: Epoch 10/50 (20%)
      ‚è≥ FPX TCN: Epoch 20/50 (40%)
      ‚è≥ FPX TCN: Epoch 30/50 (60%)
      ‚è≥ FPX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.662521
         RMSE: 0.813954
         R¬≤ Score: -1.3966
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FPX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FPX Random Forest: Starting GridSearchCV fit...
       ‚úÖ SAH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.3898 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SAH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SAH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.8776 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SAH XGBoost: Starting GridSearchCV fit...
       ‚úÖ FPX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.8289 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FPX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FPX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.5638 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FPX XGBoost: Starting GridSearchCV fit...
       ‚úÖ BAP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.8202 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.0s
    - LSTM: MSE=0.3679
    - TCN: MSE=0.1257
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1257
        ‚Ä¢ LSTM: MSE=0.3679
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.3240
        ‚Ä¢ XGBoost: MSE=35.8202
        ‚Ä¢ Random Forest: MSE=37.3429
   ‚úÖ BAP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BAP (TargetReturn): TCN with MSE=0.1257
üêõ DEBUG: BAP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BAP.
üêõ DEBUG: BAP - Moving model to CPU before return...
üêõ DEBUG [23:39:19.149]: BAP - Returning result metadata...
üêõ DEBUG: train_worker started for TBPH
üêõ DEBUG [23:39:19.150]: Main received result for BAP
üêõ DEBUG: Training progress: 564/959 done
  ‚öôÔ∏è Training models for TBPH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - TBPH: Initiating feature extraction for training.
  [DIAGNOSTIC] TBPH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TBPH: rows after features available: 126
üéØ TBPH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TBPH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TBPH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TBPH: Training LSTM (50 epochs)...
      ‚è≥ TBPH LSTM: Epoch 10/50 (20%)
      ‚è≥ TBPH LSTM: Epoch 20/50 (40%)
      ‚è≥ TBPH LSTM: Epoch 30/50 (60%)
      ‚è≥ TBPH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.493850
         RMSE: 0.702745
         R¬≤ Score: -0.8655 (Poor - 86.6% variance explained)
      üîπ TBPH: Training TCN (50 epochs)...
      ‚è≥ TBPH TCN: Epoch 10/50 (20%)
      ‚è≥ TBPH TCN: Epoch 20/50 (40%)
      ‚è≥ TBPH TCN: Epoch 30/50 (60%)
      ‚è≥ TBPH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.441594
         RMSE: 0.664525
         R¬≤ Score: -0.6681
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TBPH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TBPH Random Forest: Starting GridSearchCV fit...
       ‚úÖ TBPH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.5357 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TBPH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TBPH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.6232 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TBPH XGBoost: Starting GridSearchCV fit...
       ‚úÖ OLLI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.8808 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.2994
    - TCN: MSE=0.2037
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2037
        ‚Ä¢ LSTM: MSE=0.2994
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.4328
        ‚Ä¢ Random Forest: MSE=25.0340
        ‚Ä¢ XGBoost: MSE=27.8808
   ‚úÖ OLLI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OLLI (TargetReturn): TCN with MSE=0.2037
üêõ DEBUG: OLLI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OLLI.
üêõ DEBUG: OLLI - Moving model to CPU before return...
üêõ DEBUG [23:39:31.587]: OLLI - Returning result metadata...
üêõ DEBUG: train_worker started for RBA
üêõ DEBUG [23:39:31.588]: Main received result for OLLI
  ‚öôÔ∏è Training models for RBA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - RBA: Initiating feature extraction for training.
  [DIAGNOSTIC] RBA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RBA: rows after features available: 126
üéØ RBA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RBA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RBA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RBA: Training LSTM (50 epochs)...
      ‚è≥ RBA LSTM: Epoch 10/50 (20%)
      ‚è≥ RBA LSTM: Epoch 20/50 (40%)
      ‚è≥ RBA LSTM: Epoch 30/50 (60%)
      ‚è≥ RBA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.184565
         RMSE: 0.429610
         R¬≤ Score: -0.2563 (Poor - 25.6% variance explained)
      üîπ RBA: Training TCN (50 epochs)...
      ‚è≥ RBA TCN: Epoch 10/50 (20%)
      ‚è≥ RBA TCN: Epoch 20/50 (40%)
      ‚è≥ RBA TCN: Epoch 30/50 (60%)
      ‚è≥ RBA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.147824
         RMSE: 0.384479
         R¬≤ Score: -0.0062
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RBA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RBA Random Forest: Starting GridSearchCV fit...
       ‚úÖ NRGV XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=64.7827 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.4734
    - TCN: MSE=0.3972
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3972
        ‚Ä¢ LSTM: MSE=0.4734
        ‚Ä¢ XGBoost: MSE=64.7827
        ‚Ä¢ LightGBM Regressor (CPU): MSE=84.6976
        ‚Ä¢ Random Forest: MSE=86.5523
   ‚úÖ NRGV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NRGV (TargetReturn): TCN with MSE=0.3972
üêõ DEBUG: NRGV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NRGV.
üêõ DEBUG: NRGV - Moving model to CPU before return...
üêõ DEBUG [23:39:35.705]: NRGV - Returning result metadata...
üêõ DEBUG: train_worker started for TCBX
üêõ DEBUG [23:39:35.706]: Main received result for NRGV
  ‚öôÔ∏è Training models for TCBX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - TCBX: Initiating feature extraction for training.
  [DIAGNOSTIC] TCBX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TCBX: rows after features available: 126
üéØ TCBX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TCBX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TCBX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TCBX: Training LSTM (50 epochs)...
      ‚è≥ TCBX LSTM: Epoch 10/50 (20%)
      ‚è≥ TCBX LSTM: Epoch 20/50 (40%)
      ‚è≥ TCBX LSTM: Epoch 30/50 (60%)
      ‚è≥ TCBX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.608582
         RMSE: 0.780117
         R¬≤ Score: -0.3992 (Poor - 39.9% variance explained)
      üîπ TCBX: Training TCN (50 epochs)...
      ‚è≥ TCBX TCN: Epoch 10/50 (20%)
      ‚è≥ TCBX TCN: Epoch 20/50 (40%)
      ‚è≥ TCBX TCN: Epoch 30/50 (60%)
       ‚úÖ RBA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.7765 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RBA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TCBX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.578974
         RMSE: 0.760904
         R¬≤ Score: -0.3312
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TCBX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TCBX Random Forest: Starting GridSearchCV fit...
       ‚úÖ RBA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.6912 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RBA XGBoost: Starting GridSearchCV fit...
       ‚úÖ KLTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.7075 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.0641
    - TCN: MSE=0.0405
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0405
        ‚Ä¢ LSTM: MSE=0.0641
        ‚Ä¢ Random Forest: MSE=27.0177
        ‚Ä¢ XGBoost: MSE=35.7075
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.9705
   ‚úÖ KLTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KLTR (TargetReturn): TCN with MSE=0.0405
üêõ DEBUG: KLTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KLTR.
üêõ DEBUG: KLTR - Moving model to CPU before return...
üêõ DEBUG [23:39:40.060]: KLTR - Returning result metadata...
üêõ DEBUG [23:39:40.060]: Main received result for KLTR
üêõ DEBUG: train_worker started for CHAT
  ‚öôÔ∏è Training models for CHAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CHAT: Initiating feature extraction for training.
  [DIAGNOSTIC] CHAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CHAT: rows after features available: 126
üéØ CHAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CHAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CHAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CHAT: Training LSTM (50 epochs)...
      ‚è≥ CHAT LSTM: Epoch 10/50 (20%)
      ‚è≥ CHAT LSTM: Epoch 20/50 (40%)
       ‚úÖ TCBX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.8478 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TCBX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CHAT LSTM: Epoch 30/50 (60%)
       ‚úÖ TCBX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.9134 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TCBX XGBoost: Starting GridSearchCV fit...
      ‚è≥ CHAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.619650
         RMSE: 0.787178
         R¬≤ Score: -1.0639 (Poor - 106.4% variance explained)
      üîπ CHAT: Training TCN (50 epochs)...
      ‚è≥ CHAT TCN: Epoch 10/50 (20%)
      ‚è≥ CHAT TCN: Epoch 20/50 (40%)
      ‚è≥ CHAT TCN: Epoch 30/50 (60%)
      ‚è≥ CHAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.666253
         RMSE: 0.816243
         R¬≤ Score: -1.2191
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CHAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CHAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ CHAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.4265 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CHAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CHAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.2092 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CHAT XGBoost: Starting GridSearchCV fit...
       ‚úÖ MTW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=90.5615 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.7048
    - TCN: MSE=0.5265
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5265
        ‚Ä¢ LSTM: MSE=0.7048
        ‚Ä¢ Random Forest: MSE=66.3063
        ‚Ä¢ LightGBM Regressor (CPU): MSE=79.7250
        ‚Ä¢ XGBoost: MSE=90.5615
   ‚úÖ MTW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MTW (TargetReturn): TCN with MSE=0.5265
üêõ DEBUG: MTW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MTW.
üêõ DEBUG: MTW - Moving model to CPU before return...
üêõ DEBUG [23:39:55.574]: MTW - Returning result metadata...
üêõ DEBUG [23:39:55.574]: Main received result for MTW
üêõ DEBUG: Training progress: 568/959 done
üêõ DEBUG: train_worker started for WWW
  ‚öôÔ∏è Training models for WWW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - WWW: Initiating feature extraction for training.
  [DIAGNOSTIC] WWW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WWW: rows after features available: 126
üéØ WWW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WWW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WWW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WWW: Training LSTM (50 epochs)...
      ‚è≥ WWW LSTM: Epoch 10/50 (20%)
      ‚è≥ WWW LSTM: Epoch 20/50 (40%)
      ‚è≥ WWW LSTM: Epoch 30/50 (60%)
      ‚è≥ WWW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.508499
         RMSE: 0.713091
         R¬≤ Score: -0.7378 (Poor - 73.8% variance explained)
      üîπ WWW: Training TCN (50 epochs)...
      ‚è≥ WWW TCN: Epoch 10/50 (20%)
      ‚è≥ WWW TCN: Epoch 20/50 (40%)
      ‚è≥ WWW TCN: Epoch 30/50 (60%)
      ‚è≥ WWW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.603648
         RMSE: 0.776948
         R¬≤ Score: -1.0630
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WWW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WWW Random Forest: Starting GridSearchCV fit...
       ‚úÖ WWW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.1639 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WWW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.3316 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.8313
    - TCN: MSE=0.6331
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6331
        ‚Ä¢ LSTM: MSE=0.8313
        ‚Ä¢ Random Forest: MSE=11.9548
        ‚Ä¢ XGBoost: MSE=13.3316
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.2068
   ‚úÖ DCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DCO (TargetReturn): TCN with MSE=0.6331
üêõ DEBUG: DCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DCO.
üêõ DEBUG: DCO - Moving model to CPU before return...
üêõ DEBUG [23:40:01.606]: DCO - Returning result metadata...
üêõ DEBUG [23:40:01.606]: Main received result for DCO
üêõ DEBUG: train_worker started for METV
  ‚öôÔ∏è Training models for METV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - METV: Initiating feature extraction for training.
  [DIAGNOSTIC] METV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ METV: rows after features available: 126
üéØ METV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] METV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö METV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ METV: Training LSTM (50 epochs)...
       ‚úÖ WWW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.9723 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WWW XGBoost: Starting GridSearchCV fit...
       ‚úÖ TRMB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.3451 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.7213
    - TCN: MSE=0.5161
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5161
        ‚Ä¢ LSTM: MSE=0.7213
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.7103
        ‚Ä¢ Random Forest: MSE=17.0330
        ‚Ä¢ XGBoost: MSE=20.3451
   ‚úÖ TRMB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TRMB (TargetReturn): TCN with MSE=0.5161
üêõ DEBUG: TRMB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TRMB.
üêõ DEBUG: TRMB - Moving model to CPU before return...
üêõ DEBUG [23:40:01.895]: TRMB - Returning result metadata...
üêõ DEBUG [23:40:01.895]: Main received result for TRMB
üêõ DEBUG: train_worker started for BAM
  ‚öôÔ∏è Training models for BAM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BAM: Initiating feature extraction for training.
  [DIAGNOSTIC] BAM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BAM: rows after features available: 126
üéØ BAM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BAM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BAM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BAM: Training LSTM (50 epochs)...
      ‚è≥ METV LSTM: Epoch 10/50 (20%)
      ‚è≥ BAM LSTM: Epoch 10/50 (20%)
      ‚è≥ METV LSTM: Epoch 20/50 (40%)
      ‚è≥ BAM LSTM: Epoch 20/50 (40%)
      ‚è≥ METV LSTM: Epoch 30/50 (60%)
      ‚è≥ BAM LSTM: Epoch 30/50 (60%)
      ‚è≥ METV LSTM: Epoch 40/50 (80%)
      ‚è≥ BAM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.478237
         RMSE: 0.691547
         R¬≤ Score: -0.7633 (Poor - 76.3% variance explained)
      üîπ METV: Training TCN (50 epochs)...
      ‚è≥ METV TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.358376
         RMSE: 0.598645
         R¬≤ Score: -0.7128 (Poor - 71.3% variance explained)
      üîπ BAM: Training TCN (50 epochs)...
      ‚è≥ METV TCN: Epoch 20/50 (40%)
      ‚è≥ BAM TCN: Epoch 10/50 (20%)
      ‚è≥ METV TCN: Epoch 30/50 (60%)
      ‚è≥ METV TCN: Epoch 40/50 (80%)
      ‚è≥ BAM TCN: Epoch 20/50 (40%)
      ‚è≥ BAM TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.597563
         RMSE: 0.773022
         R¬≤ Score: -1.2033
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä METV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ METV Random Forest: Starting GridSearchCV fit...
      ‚è≥ BAM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.394434
         RMSE: 0.628040
         R¬≤ Score: -0.8851
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BAM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BAM Random Forest: Starting GridSearchCV fit...
       ‚úÖ KT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3.6861 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.1775
    - TCN: MSE=0.0958
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0958
        ‚Ä¢ LSTM: MSE=0.1775
        ‚Ä¢ XGBoost: MSE=3.6861
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4111
        ‚Ä¢ Random Forest: MSE=5.5458
   ‚úÖ KT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KT (TargetReturn): TCN with MSE=0.0958
üêõ DEBUG: KT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KT.
üêõ DEBUG: KT - Moving model to CPU before return...
üêõ DEBUG [23:40:05.216]: KT - Returning result metadata...
üêõ DEBUG: train_worker started for PM
üêõ DEBUG [23:40:05.218]: Main received result for KT
  ‚öôÔ∏è Training models for PM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - PM: Initiating feature extraction for training.
  [DIAGNOSTIC] PM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PM: rows after features available: 126
üéØ PM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PM: Training LSTM (50 epochs)...
      ‚è≥ PM LSTM: Epoch 10/50 (20%)
      ‚è≥ PM LSTM: Epoch 20/50 (40%)
      ‚è≥ PM LSTM: Epoch 30/50 (60%)
      ‚è≥ PM LSTM: Epoch 40/50 (80%)
       ‚úÖ METV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.3332 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ METV LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.408611
         RMSE: 0.639227
         R¬≤ Score: -1.4711 (Poor - 147.1% variance explained)
      üîπ PM: Training TCN (50 epochs)...
       ‚úÖ BAM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.6248 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BAM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PM TCN: Epoch 10/50 (20%)
      ‚è≥ PM TCN: Epoch 20/50 (40%)
      ‚è≥ PM TCN: Epoch 30/50 (60%)
      ‚è≥ PM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.284488
         RMSE: 0.533375
         R¬≤ Score: -0.7204
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PM Random Forest: Starting GridSearchCV fit...
       ‚úÖ METV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.2187 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ METV XGBoost: Starting GridSearchCV fit...
       ‚úÖ BAM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.8889 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BAM XGBoost: Starting GridSearchCV fit...
       ‚úÖ PM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.6611 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.0642 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PM XGBoost: Starting GridSearchCV fit...
       ‚úÖ FHI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.5465 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.3293
    - TCN: MSE=0.2287
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2287
        ‚Ä¢ LSTM: MSE=0.3293
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.9727
        ‚Ä¢ Random Forest: MSE=8.5961
        ‚Ä¢ XGBoost: MSE=17.5465
   ‚úÖ FHI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FHI (TargetReturn): TCN with MSE=0.2287
üêõ DEBUG: FHI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FHI.
üêõ DEBUG: FHI - Moving model to CPU before return...
üêõ DEBUG [23:40:21.154]: FHI - Returning result metadata...
üêõ DEBUG [23:40:21.154]: Main received result for FHI
üêõ DEBUG: Training progress: 572/959 done
üêõ DEBUG: train_worker started for AVAV
  ‚öôÔ∏è Training models for AVAV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - AVAV: Initiating feature extraction for training.
  [DIAGNOSTIC] AVAV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AVAV: rows after features available: 126
üéØ AVAV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AVAV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AVAV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AVAV: Training LSTM (50 epochs)...
      ‚è≥ AVAV LSTM: Epoch 10/50 (20%)
      ‚è≥ AVAV LSTM: Epoch 20/50 (40%)
      ‚è≥ AVAV LSTM: Epoch 30/50 (60%)
      ‚è≥ AVAV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.463572
         RMSE: 0.680861
         R¬≤ Score: -0.5916 (Poor - 59.2% variance explained)
      üîπ AVAV: Training TCN (50 epochs)...
      ‚è≥ AVAV TCN: Epoch 10/50 (20%)
      ‚è≥ AVAV TCN: Epoch 20/50 (40%)
      ‚è≥ AVAV TCN: Epoch 30/50 (60%)
      ‚è≥ AVAV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.554640
         RMSE: 0.744742
         R¬≤ Score: -0.9043
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AVAV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AVAV Random Forest: Starting GridSearchCV fit...
       ‚úÖ AVAV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=78.9940 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AVAV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AVAV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=123.3035 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AVAV XGBoost: Starting GridSearchCV fit...
       ‚úÖ GDX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=34.3967 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.2590
    - TCN: MSE=0.1525
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1525
        ‚Ä¢ LSTM: MSE=0.2590
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.0284
        ‚Ä¢ Random Forest: MSE=31.9604
        ‚Ä¢ XGBoost: MSE=34.3967
   ‚úÖ GDX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GDX (TargetReturn): TCN with MSE=0.1525
üêõ DEBUG: GDX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GDX.
üêõ DEBUG: GDX - Moving model to CPU before return...
üêõ DEBUG [23:40:32.963]: GDX - Returning result metadata...
üêõ DEBUG [23:40:32.963]: Main received result for GDX
üêõ DEBUG: train_worker started for OKTA
  ‚öôÔ∏è Training models for OKTA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - OKTA: Initiating feature extraction for training.
  [DIAGNOSTIC] OKTA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OKTA: rows after features available: 126
üéØ OKTA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OKTA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OKTA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OKTA: Training LSTM (50 epochs)...
      ‚è≥ OKTA LSTM: Epoch 10/50 (20%)
      ‚è≥ OKTA LSTM: Epoch 20/50 (40%)
      ‚è≥ OKTA LSTM: Epoch 30/50 (60%)
      ‚è≥ OKTA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.756946
         RMSE: 0.870027
         R¬≤ Score: -1.1000 (Poor - 110.0% variance explained)
      üîπ OKTA: Training TCN (50 epochs)...
      ‚è≥ OKTA TCN: Epoch 10/50 (20%)
      ‚è≥ OKTA TCN: Epoch 20/50 (40%)
      ‚è≥ OKTA TCN: Epoch 30/50 (60%)
      ‚è≥ OKTA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.445996
         RMSE: 0.667830
         R¬≤ Score: -0.2373
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OKTA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OKTA Random Forest: Starting GridSearchCV fit...
       ‚úÖ BWXT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.7319 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.4837
    - TCN: MSE=0.3894
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3894
        ‚Ä¢ LSTM: MSE=0.4837
        ‚Ä¢ XGBoost: MSE=15.7319
        ‚Ä¢ Random Forest: MSE=17.8007
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.6472
   ‚úÖ BWXT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BWXT (TargetReturn): TCN with MSE=0.3894
üêõ DEBUG: BWXT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BWXT.
üêõ DEBUG: BWXT - Moving model to CPU before return...
üêõ DEBUG [23:40:38.636]: BWXT - Returning result metadata...
üêõ DEBUG [23:40:38.636]: Main received result for BWXT
üêõ DEBUG: train_worker started for EFXT
  ‚öôÔ∏è Training models for EFXT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - EFXT: Initiating feature extraction for training.
  [DIAGNOSTIC] EFXT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EFXT: rows after features available: 126
üéØ EFXT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EFXT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EFXT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EFXT: Training LSTM (50 epochs)...
       ‚úÖ OKTA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=33.8279 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OKTA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EFXT LSTM: Epoch 10/50 (20%)
       ‚úÖ OKTA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.5762 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OKTA XGBoost: Starting GridSearchCV fit...
      ‚è≥ EFXT LSTM: Epoch 20/50 (40%)
      ‚è≥ EFXT LSTM: Epoch 30/50 (60%)
      ‚è≥ EFXT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.368347
         RMSE: 0.606916
         R¬≤ Score: -1.4324 (Poor - 143.2% variance explained)
      üîπ EFXT: Training TCN (50 epochs)...
      ‚è≥ EFXT TCN: Epoch 10/50 (20%)
      ‚è≥ EFXT TCN: Epoch 20/50 (40%)
      ‚è≥ EFXT TCN: Epoch 30/50 (60%)
      ‚è≥ EFXT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.368868
         RMSE: 0.607345
         R¬≤ Score: -1.4358
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EFXT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EFXT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ETR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.7004 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.5s
    - LSTM: MSE=0.1150
    - TCN: MSE=0.0589
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0589
        ‚Ä¢ LSTM: MSE=0.1150
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.8459
        ‚Ä¢ Random Forest: MSE=6.2178
        ‚Ä¢ XGBoost: MSE=6.7004
   ‚úÖ ETR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ETR (TargetReturn): TCN with MSE=0.0589
üêõ DEBUG: ETR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ETR.
üêõ DEBUG: ETR - Moving model to CPU before return...
üêõ DEBUG [23:40:42.583]: ETR - Returning result metadata...
üêõ DEBUG [23:40:42.583]: Main received result for ETR
üêõ DEBUG: train_worker started for FFIV
  ‚öôÔ∏è Training models for FFIV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - FFIV: Initiating feature extraction for training.
  [DIAGNOSTIC] FFIV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FFIV: rows after features available: 126
üéØ FFIV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FFIV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FFIV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FFIV: Training LSTM (50 epochs)...
      ‚è≥ FFIV LSTM: Epoch 10/50 (20%)
      ‚è≥ FFIV LSTM: Epoch 20/50 (40%)
      ‚è≥ FFIV LSTM: Epoch 30/50 (60%)
       ‚úÖ EFXT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2611 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EFXT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FFIV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.418758
         RMSE: 0.647115
         R¬≤ Score: -0.7489 (Poor - 74.9% variance explained)
      üîπ FFIV: Training TCN (50 epochs)...
      ‚è≥ FFIV TCN: Epoch 10/50 (20%)
      ‚è≥ FFIV TCN: Epoch 20/50 (40%)
       ‚úÖ EFXT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.2425 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EFXT XGBoost: Starting GridSearchCV fit...
      ‚è≥ FFIV TCN: Epoch 30/50 (60%)
      ‚è≥ FFIV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.453858
         RMSE: 0.673690
         R¬≤ Score: -0.8955
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FFIV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FFIV Random Forest: Starting GridSearchCV fit...
       ‚úÖ FFIV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.2512 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FFIV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FFIV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8398 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FFIV XGBoost: Starting GridSearchCV fit...
       ‚úÖ CGNT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.9779 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.3067
    - TCN: MSE=0.1804
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1804
        ‚Ä¢ LSTM: MSE=0.3067
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.4638
        ‚Ä¢ Random Forest: MSE=25.9892
        ‚Ä¢ XGBoost: MSE=27.9779
   ‚úÖ CGNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CGNT (TargetReturn): TCN with MSE=0.1804
üêõ DEBUG: CGNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CGNT.
üêõ DEBUG: CGNT - Moving model to CPU before return...
üêõ DEBUG [23:40:52.261]: CGNT - Returning result metadata...
üêõ DEBUG [23:40:52.261]: Main received result for CGNT
üêõ DEBUG: Training progress: 576/959 done
üêõ DEBUG: train_worker started for FAST
  ‚öôÔ∏è Training models for FAST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FAST: Initiating feature extraction for training.
  [DIAGNOSTIC] FAST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FAST: rows after features available: 126
üéØ FAST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FAST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FAST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FAST: Training LSTM (50 epochs)...
      ‚è≥ FAST LSTM: Epoch 10/50 (20%)
      ‚è≥ FAST LSTM: Epoch 20/50 (40%)
      ‚è≥ FAST LSTM: Epoch 30/50 (60%)
      ‚è≥ FAST LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.110163
         RMSE: 0.331909
         R¬≤ Score: -1.1171 (Poor - 111.7% variance explained)
      üîπ FAST: Training TCN (50 epochs)...
      ‚è≥ FAST TCN: Epoch 10/50 (20%)
      ‚è≥ FAST TCN: Epoch 20/50 (40%)
      ‚è≥ FAST TCN: Epoch 30/50 (60%)
      ‚è≥ FAST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.052318
         RMSE: 0.228731
         R¬≤ Score: -0.0055
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FAST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FAST Random Forest: Starting GridSearchCV fit...
       ‚úÖ FAST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.4886 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FAST LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FAST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.1515 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FAST XGBoost: Starting GridSearchCV fit...
       ‚úÖ SAH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.2810 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.8370
    - TCN: MSE=0.6822
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6822
        ‚Ä¢ LSTM: MSE=0.8370
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.8776
        ‚Ä¢ Random Forest: MSE=30.3898
        ‚Ä¢ XGBoost: MSE=44.2810
   ‚úÖ SAH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SAH (TargetReturn): TCN with MSE=0.6822
üêõ DEBUG: SAH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SAH.
üêõ DEBUG: SAH - Moving model to CPU before return...
üêõ DEBUG [23:40:58.521]: SAH - Returning result metadata...
üêõ DEBUG [23:40:58.522]: Main received result for SAH
üêõ DEBUG: train_worker started for FBIO
  ‚öôÔ∏è Training models for FBIO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - FBIO: Initiating feature extraction for training.
  [DIAGNOSTIC] FBIO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FBIO: rows after features available: 126
üéØ FBIO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FBIO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FBIO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FBIO: Training LSTM (50 epochs)...
      ‚è≥ FBIO LSTM: Epoch 10/50 (20%)
      ‚è≥ FBIO LSTM: Epoch 20/50 (40%)
       ‚úÖ FPX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.0013 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.5372
    - TCN: MSE=0.6625
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5372
        ‚Ä¢ TCN: MSE=0.6625
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.5638
        ‚Ä¢ XGBoost: MSE=17.0013
        ‚Ä¢ Random Forest: MSE=17.8289
   ‚úÖ FPX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FPX (TargetReturn): LSTM with MSE=0.5372
üêõ DEBUG: FPX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FPX.
üêõ DEBUG: FPX - Moving model to CPU before return...
üêõ DEBUG [23:40:59.539]: FPX - Returning result metadata...
üêõ DEBUG [23:40:59.539]: Main received result for FPX
üêõ DEBUG: train_worker started for GENI
  ‚öôÔ∏è Training models for GENI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - GENI: Initiating feature extraction for training.
  [DIAGNOSTIC] GENI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GENI: rows after features available: 126
üéØ GENI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GENI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GENI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GENI: Training LSTM (50 epochs)...
      ‚è≥ FBIO LSTM: Epoch 30/50 (60%)
      ‚è≥ GENI LSTM: Epoch 10/50 (20%)
      ‚è≥ FBIO LSTM: Epoch 40/50 (80%)
      ‚è≥ GENI LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.189067
         RMSE: 0.434818
         R¬≤ Score: -0.5742 (Poor - 57.4% variance explained)
      üîπ FBIO: Training TCN (50 epochs)...
      ‚è≥ FBIO TCN: Epoch 10/50 (20%)
      ‚è≥ FBIO TCN: Epoch 20/50 (40%)
      ‚è≥ FBIO TCN: Epoch 30/50 (60%)
      ‚è≥ GENI LSTM: Epoch 30/50 (60%)
      ‚è≥ FBIO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.150317
         RMSE: 0.387707
         R¬≤ Score: -0.2515
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FBIO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FBIO Random Forest: Starting GridSearchCV fit...
      ‚è≥ GENI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.125522
         RMSE: 0.354291
         R¬≤ Score: -0.2306 (Poor - 23.1% variance explained)
      üîπ GENI: Training TCN (50 epochs)...
      ‚è≥ GENI TCN: Epoch 10/50 (20%)
      ‚è≥ GENI TCN: Epoch 20/50 (40%)
      ‚è≥ GENI TCN: Epoch 30/50 (60%)
      ‚è≥ GENI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.131222
         RMSE: 0.362245
         R¬≤ Score: -0.2864
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GENI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GENI Random Forest: Starting GridSearchCV fit...
       ‚úÖ FBIO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.0981 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FBIO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FBIO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.5200 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FBIO XGBoost: Starting GridSearchCV fit...
       ‚úÖ GENI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.4338 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GENI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GENI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.9855 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GENI XGBoost: Starting GridSearchCV fit...
       ‚úÖ TBPH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.2448 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.1s
    - LSTM: MSE=0.4938
    - TCN: MSE=0.4416
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4416
        ‚Ä¢ LSTM: MSE=0.4938
        ‚Ä¢ Random Forest: MSE=17.5357
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.6232
        ‚Ä¢ XGBoost: MSE=26.2448
   ‚úÖ TBPH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TBPH (TargetReturn): TCN with MSE=0.4416
üêõ DEBUG: TBPH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TBPH.
üêõ DEBUG: TBPH - Moving model to CPU before return...
üêõ DEBUG [23:41:19.404]: TBPH - Returning result metadata...
üêõ DEBUG [23:41:19.405]: Main received result for TBPH
üêõ DEBUG: train_worker started for LGND
  ‚öôÔ∏è Training models for LGND (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - LGND: Initiating feature extraction for training.
  [DIAGNOSTIC] LGND: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LGND: rows after features available: 126
üéØ LGND: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LGND: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LGND: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LGND: Training LSTM (50 epochs)...
      ‚è≥ LGND LSTM: Epoch 10/50 (20%)
      ‚è≥ LGND LSTM: Epoch 20/50 (40%)
      ‚è≥ LGND LSTM: Epoch 30/50 (60%)
      ‚è≥ LGND LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.488204
         RMSE: 0.698716
         R¬≤ Score: -0.6898 (Poor - 69.0% variance explained)
      üîπ LGND: Training TCN (50 epochs)...
      ‚è≥ LGND TCN: Epoch 10/50 (20%)
      ‚è≥ LGND TCN: Epoch 20/50 (40%)
      ‚è≥ LGND TCN: Epoch 30/50 (60%)
      ‚è≥ LGND TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.351792
         RMSE: 0.593120
         R¬≤ Score: -0.2176
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LGND: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LGND Random Forest: Starting GridSearchCV fit...
       ‚úÖ LGND Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.9914 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LGND LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LGND LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.1874 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LGND XGBoost: Starting GridSearchCV fit...
       ‚úÖ TCBX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.2631 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 113.6s
    - LSTM: MSE=0.6086
    - TCN: MSE=0.5790
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 116.9 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5790
        ‚Ä¢ LSTM: MSE=0.6086
        ‚Ä¢ Random Forest: MSE=11.8478
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.9134
        ‚Ä¢ XGBoost: MSE=19.2631
   ‚úÖ TCBX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TCBX (TargetReturn): TCN with MSE=0.5790
üêõ DEBUG: TCBX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TCBX.
üêõ DEBUG: TCBX - Moving model to CPU before return...
üêõ DEBUG [23:41:35.297]: TCBX - Returning result metadata...
üêõ DEBUG: train_worker started for IGIC
  ‚öôÔ∏è Training models for IGIC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - IGIC: Initiating feature extraction for training.
  [DIAGNOSTIC] IGIC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IGIC: rows after features available: 126
üéØ IGIC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IGIC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IGIC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IGIC: Training LSTM (50 epochs)...
      ‚è≥ IGIC LSTM: Epoch 10/50 (20%)
      ‚è≥ IGIC LSTM: Epoch 20/50 (40%)
      ‚è≥ IGIC LSTM: Epoch 30/50 (60%)
      ‚è≥ IGIC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.263260
         RMSE: 0.513088
         R¬≤ Score: -1.0296 (Poor - 103.0% variance explained)
      üîπ IGIC: Training TCN (50 epochs)...
      ‚è≥ IGIC TCN: Epoch 10/50 (20%)
      ‚è≥ IGIC TCN: Epoch 20/50 (40%)
      ‚è≥ IGIC TCN: Epoch 30/50 (60%)
      ‚è≥ IGIC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.130682
         RMSE: 0.361500
         R¬≤ Score: -0.0075
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IGIC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IGIC Random Forest: Starting GridSearchCV fit...
       ‚úÖ RBA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.3484 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.1846
    - TCN: MSE=0.1478
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1478
        ‚Ä¢ LSTM: MSE=0.1846
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.6912
        ‚Ä¢ Random Forest: MSE=5.7765
        ‚Ä¢ XGBoost: MSE=6.3484
   ‚úÖ RBA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RBA (TargetReturn): TCN with MSE=0.1478
üêõ DEBUG: RBA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RBA.
üêõ DEBUG: RBA - Moving model to CPU before return...
üêõ DEBUG [23:41:38.238]: RBA - Returning result metadata...
üêõ DEBUG: train_worker started for NGVC
üêõ DEBUG [23:41:38.242]: Main received result for RBA
üêõ DEBUG: Training progress: 580/959 done
üêõ DEBUG [23:41:38.242]: Main received result for TCBX
  ‚öôÔ∏è Training models for NGVC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NGVC: Initiating feature extraction for training.
  [DIAGNOSTIC] NGVC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NGVC: rows after features available: 126
üéØ NGVC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NGVC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NGVC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NGVC: Training LSTM (50 epochs)...
      ‚è≥ NGVC LSTM: Epoch 10/50 (20%)
      ‚è≥ NGVC LSTM: Epoch 20/50 (40%)
      ‚è≥ NGVC LSTM: Epoch 30/50 (60%)
      ‚è≥ NGVC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.184152
         RMSE: 0.429129
         R¬≤ Score: -0.5425 (Poor - 54.3% variance explained)
      üîπ NGVC: Training TCN (50 epochs)...
       ‚úÖ IGIC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0597 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IGIC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NGVC TCN: Epoch 10/50 (20%)
      ‚è≥ NGVC TCN: Epoch 20/50 (40%)
      ‚è≥ NGVC TCN: Epoch 30/50 (60%)
      ‚è≥ NGVC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.118377
         RMSE: 0.344059
         R¬≤ Score: 0.0084
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NGVC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NGVC Random Forest: Starting GridSearchCV fit...
       ‚úÖ IGIC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.1657 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IGIC XGBoost: Starting GridSearchCV fit...
       ‚úÖ NGVC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=33.1736 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NGVC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NGVC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=46.5373 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NGVC XGBoost: Starting GridSearchCV fit...
       ‚úÖ CHAT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.1357 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.6196
    - TCN: MSE=0.6663
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6196
        ‚Ä¢ TCN: MSE=0.6663
        ‚Ä¢ XGBoost: MSE=14.1357
        ‚Ä¢ Random Forest: MSE=18.4265
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.2092
   ‚úÖ CHAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CHAT (TargetReturn): LSTM with MSE=0.6196
üêõ DEBUG: CHAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CHAT.
üêõ DEBUG: CHAT - Moving model to CPU before return...
üêõ DEBUG [23:41:45.772]: CHAT - Returning result metadata...
üêõ DEBUG: train_worker started for HLF
üêõ DEBUG [23:41:45.774]: Main received result for CHAT
  ‚öôÔ∏è Training models for HLF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - HLF: Initiating feature extraction for training.
  [DIAGNOSTIC] HLF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HLF: rows after features available: 126
üéØ HLF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HLF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HLF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HLF: Training LSTM (50 epochs)...
      ‚è≥ HLF LSTM: Epoch 10/50 (20%)
      ‚è≥ HLF LSTM: Epoch 20/50 (40%)
      ‚è≥ HLF LSTM: Epoch 30/50 (60%)
      ‚è≥ HLF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.262550
         RMSE: 0.512397
         R¬≤ Score: -0.3328 (Poor - 33.3% variance explained)
      üîπ HLF: Training TCN (50 epochs)...
      ‚è≥ HLF TCN: Epoch 10/50 (20%)
      ‚è≥ HLF TCN: Epoch 20/50 (40%)
      ‚è≥ HLF TCN: Epoch 30/50 (60%)
      ‚è≥ HLF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.193090
         RMSE: 0.439420
         R¬≤ Score: 0.0198
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HLF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HLF Random Forest: Starting GridSearchCV fit...
       ‚úÖ HLF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=72.0454 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HLF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HLF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=68.4734 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HLF XGBoost: Starting GridSearchCV fit...
       ‚úÖ WWW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=42.2288 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 113.6s
    - LSTM: MSE=0.5085
    - TCN: MSE=0.6036
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.0 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5085
        ‚Ä¢ TCN: MSE=0.6036
        ‚Ä¢ Random Forest: MSE=30.1639
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.9723
        ‚Ä¢ XGBoost: MSE=42.2288
   ‚úÖ WWW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WWW (TargetReturn): LSTM with MSE=0.5085
üêõ DEBUG: WWW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WWW.
üêõ DEBUG: WWW - Moving model to CPU before return...
üêõ DEBUG [23:41:55.458]: WWW - Returning result metadata...
üêõ DEBUG [23:41:55.459]: Main received result for WWW
üêõ DEBUG: train_worker started for MTA
  ‚öôÔ∏è Training models for MTA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - MTA: Initiating feature extraction for training.
  [DIAGNOSTIC] MTA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MTA: rows after features available: 126
üéØ MTA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MTA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MTA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MTA: Training LSTM (50 epochs)...
      ‚è≥ MTA LSTM: Epoch 10/50 (20%)
      ‚è≥ MTA LSTM: Epoch 20/50 (40%)
      ‚è≥ MTA LSTM: Epoch 30/50 (60%)
      ‚è≥ MTA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.392746
         RMSE: 0.626695
         R¬≤ Score: -1.5952 (Poor - 159.5% variance explained)
      üîπ MTA: Training TCN (50 epochs)...
      ‚è≥ MTA TCN: Epoch 10/50 (20%)
      ‚è≥ MTA TCN: Epoch 20/50 (40%)
      ‚è≥ MTA TCN: Epoch 30/50 (60%)
      ‚è≥ MTA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.258680
         RMSE: 0.508606
         R¬≤ Score: -0.7093
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MTA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MTA Random Forest: Starting GridSearchCV fit...
       ‚úÖ MTA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.3573 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MTA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MTA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.6006 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MTA XGBoost: Starting GridSearchCV fit...
       ‚úÖ BAM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8605 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.3584
    - TCN: MSE=0.3944
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3584
        ‚Ä¢ TCN: MSE=0.3944
        ‚Ä¢ Random Forest: MSE=15.6248
        ‚Ä¢ XGBoost: MSE=16.8605
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.8889
   ‚úÖ BAM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BAM (TargetReturn): LSTM with MSE=0.3584
üêõ DEBUG: BAM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BAM.
üêõ DEBUG: BAM - Moving model to CPU before return...
üêõ DEBUG [23:42:07.783]: BAM - Returning result metadata...
üêõ DEBUG: train_worker started for GRMN
  ‚öôÔ∏è Training models for GRMN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - GRMN: Initiating feature extraction for training.
  [DIAGNOSTIC] GRMN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GRMN: rows after features available: 126
üéØ GRMN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GRMN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GRMN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GRMN: Training LSTM (50 epochs)...
      ‚è≥ GRMN LSTM: Epoch 10/50 (20%)
      ‚è≥ GRMN LSTM: Epoch 20/50 (40%)
      ‚è≥ GRMN LSTM: Epoch 30/50 (60%)
       ‚úÖ METV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.6370 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.4782
    - TCN: MSE=0.5976
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4782
        ‚Ä¢ TCN: MSE=0.5976
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.2187
        ‚Ä¢ XGBoost: MSE=14.6370
        ‚Ä¢ Random Forest: MSE=17.3332
   ‚úÖ METV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for METV (TargetReturn): LSTM with MSE=0.4782
üêõ DEBUG: METV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for METV.
üêõ DEBUG: METV - Moving model to CPU before return...
üêõ DEBUG [23:42:09.554]: METV - Returning result metadata...
üêõ DEBUG [23:42:09.555]: Main received result for METV
üêõ DEBUG: Training progress: 584/959 done
üêõ DEBUG [23:42:09.555]: Main received result for BAM
üêõ DEBUG: train_worker started for OSW
  ‚öôÔ∏è Training models for OSW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - OSW: Initiating feature extraction for training.
  [DIAGNOSTIC] OSW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OSW: rows after features available: 126
üéØ OSW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OSW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OSW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OSW: Training LSTM (50 epochs)...
      ‚è≥ GRMN LSTM: Epoch 40/50 (80%)
      ‚è≥ OSW LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.452200
         RMSE: 0.672458
         R¬≤ Score: -1.1233 (Poor - 112.3% variance explained)
      üîπ GRMN: Training TCN (50 epochs)...
      ‚è≥ GRMN TCN: Epoch 10/50 (20%)
      ‚è≥ GRMN TCN: Epoch 20/50 (40%)
      ‚è≥ OSW LSTM: Epoch 20/50 (40%)
      ‚è≥ GRMN TCN: Epoch 30/50 (60%)
      ‚è≥ GRMN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.288134
         RMSE: 0.536781
         R¬≤ Score: -0.3529
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GRMN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GRMN Random Forest: Starting GridSearchCV fit...
      ‚è≥ OSW LSTM: Epoch 30/50 (60%)
      ‚è≥ OSW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.518202
         RMSE: 0.719863
         R¬≤ Score: -0.6221 (Poor - 62.2% variance explained)
      üîπ OSW: Training TCN (50 epochs)...
      ‚è≥ OSW TCN: Epoch 10/50 (20%)
      ‚è≥ OSW TCN: Epoch 20/50 (40%)
      ‚è≥ OSW TCN: Epoch 30/50 (60%)
      ‚è≥ OSW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.644226
         RMSE: 0.802637
         R¬≤ Score: -1.0166
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OSW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OSW Random Forest: Starting GridSearchCV fit...
       ‚úÖ PM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=60.6005 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.4086
    - TCN: MSE=0.2845
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2845
        ‚Ä¢ LSTM: MSE=0.4086
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.0642
        ‚Ä¢ Random Forest: MSE=28.6611
        ‚Ä¢ XGBoost: MSE=60.6005
   ‚úÖ PM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PM (TargetReturn): TCN with MSE=0.2845
üêõ DEBUG: PM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PM.
üêõ DEBUG: PM - Moving model to CPU before return...
üêõ DEBUG [23:42:13.060]: PM - Returning result metadata...
üêõ DEBUG [23:42:13.060]: Main received result for PM
üêõ DEBUG: train_worker started for EMBJ
  ‚öôÔ∏è Training models for EMBJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - EMBJ: Initiating feature extraction for training.
  [DIAGNOSTIC] EMBJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EMBJ: rows after features available: 126
üéØ EMBJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EMBJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EMBJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EMBJ: Training LSTM (50 epochs)...
      ‚è≥ EMBJ LSTM: Epoch 10/50 (20%)
       ‚úÖ GRMN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.6596 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GRMN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EMBJ LSTM: Epoch 20/50 (40%)
       ‚úÖ GRMN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.9370 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GRMN XGBoost: Starting GridSearchCV fit...
      ‚è≥ EMBJ LSTM: Epoch 30/50 (60%)
       ‚úÖ OSW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.9611 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OSW LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EMBJ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.303509
         RMSE: 0.550916
         R¬≤ Score: -1.0578 (Poor - 105.8% variance explained)
      üîπ EMBJ: Training TCN (50 epochs)...
      ‚è≥ EMBJ TCN: Epoch 10/50 (20%)
       ‚úÖ OSW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8058 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OSW XGBoost: Starting GridSearchCV fit...
      ‚è≥ EMBJ TCN: Epoch 20/50 (40%)
      ‚è≥ EMBJ TCN: Epoch 30/50 (60%)
      ‚è≥ EMBJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.172324
         RMSE: 0.415119
         R¬≤ Score: -0.1684
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EMBJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EMBJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ EMBJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.2975 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EMBJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EMBJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=27.1807 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EMBJ XGBoost: Starting GridSearchCV fit...
       ‚úÖ AVAV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=111.8746 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.4636
    - TCN: MSE=0.5546
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4636
        ‚Ä¢ TCN: MSE=0.5546
        ‚Ä¢ Random Forest: MSE=78.9940
        ‚Ä¢ XGBoost: MSE=111.8746
        ‚Ä¢ LightGBM Regressor (CPU): MSE=123.3035
   ‚úÖ AVAV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AVAV (TargetReturn): LSTM with MSE=0.4636
üêõ DEBUG: AVAV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AVAV.
üêõ DEBUG: AVAV - Moving model to CPU before return...
üêõ DEBUG [23:42:25.377]: AVAV - Returning result metadata...
üêõ DEBUG: train_worker started for ROK
üêõ DEBUG [23:42:25.377]: Main received result for AVAV
  ‚öôÔ∏è Training models for ROK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - ROK: Initiating feature extraction for training.
  [DIAGNOSTIC] ROK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ROK: rows after features available: 126
üéØ ROK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ROK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ROK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ROK: Training LSTM (50 epochs)...
      ‚è≥ ROK LSTM: Epoch 10/50 (20%)
      ‚è≥ ROK LSTM: Epoch 20/50 (40%)
      ‚è≥ ROK LSTM: Epoch 30/50 (60%)
      ‚è≥ ROK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.535053
         RMSE: 0.731474
         R¬≤ Score: -0.8408 (Poor - 84.1% variance explained)
      üîπ ROK: Training TCN (50 epochs)...
      ‚è≥ ROK TCN: Epoch 10/50 (20%)
      ‚è≥ ROK TCN: Epoch 20/50 (40%)
      ‚è≥ ROK TCN: Epoch 30/50 (60%)
      ‚è≥ ROK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.502267
         RMSE: 0.708708
         R¬≤ Score: -0.7280
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ROK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ROK Random Forest: Starting GridSearchCV fit...
       ‚úÖ ROK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.1277 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ROK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ROK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.4577 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ROK XGBoost: Starting GridSearchCV fit...
       ‚úÖ OKTA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=55.7733 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.7569
    - TCN: MSE=0.4460
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4460
        ‚Ä¢ LSTM: MSE=0.7569
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.5762
        ‚Ä¢ Random Forest: MSE=33.8279
        ‚Ä¢ XGBoost: MSE=55.7733
   ‚úÖ OKTA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OKTA (TargetReturn): TCN with MSE=0.4460
üêõ DEBUG: OKTA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OKTA.
üêõ DEBUG: OKTA - Moving model to CPU before return...
üêõ DEBUG [23:42:38.005]: OKTA - Returning result metadata...
üêõ DEBUG [23:42:38.006]: Main received result for OKTA
üêõ DEBUG: Training progress: 588/959 done
üêõ DEBUG: train_worker started for ISRA
  ‚öôÔ∏è Training models for ISRA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - ISRA: Initiating feature extraction for training.
  [DIAGNOSTIC] ISRA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ISRA: rows after features available: 126
üéØ ISRA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ISRA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ISRA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ISRA: Training LSTM (50 epochs)...
      ‚è≥ ISRA LSTM: Epoch 10/50 (20%)
      ‚è≥ ISRA LSTM: Epoch 20/50 (40%)
      ‚è≥ ISRA LSTM: Epoch 30/50 (60%)
      ‚è≥ ISRA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.415589
         RMSE: 0.644662
         R¬≤ Score: -0.7171 (Poor - 71.7% variance explained)
      üîπ ISRA: Training TCN (50 epochs)...
      ‚è≥ ISRA TCN: Epoch 10/50 (20%)
      ‚è≥ ISRA TCN: Epoch 20/50 (40%)
      ‚è≥ ISRA TCN: Epoch 30/50 (60%)
      ‚è≥ ISRA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.471403
         RMSE: 0.686588
         R¬≤ Score: -0.9477
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ISRA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ISRA Random Forest: Starting GridSearchCV fit...
       ‚úÖ ISRA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.0012 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ISRA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ISRA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.5715 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ISRA XGBoost: Starting GridSearchCV fit...
       ‚úÖ FFIV XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.0300 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.4188
    - TCN: MSE=0.4539
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4188
        ‚Ä¢ TCN: MSE=0.4539
        ‚Ä¢ XGBoost: MSE=10.0300
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8398
        ‚Ä¢ Random Forest: MSE=11.2512
   ‚úÖ FFIV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FFIV (TargetReturn): LSTM with MSE=0.4188
üêõ DEBUG: FFIV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FFIV.
üêõ DEBUG: FFIV - Moving model to CPU before return...
üêõ DEBUG [23:42:45.818]: FFIV - Returning result metadata...
üêõ DEBUG: train_worker started for FTNT
  ‚öôÔ∏è Training models for FTNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - FTNT: Initiating feature extraction for training.
  [DIAGNOSTIC] FTNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FTNT: rows after features available: 126
üéØ FTNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FTNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FTNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FTNT: Training LSTM (50 epochs)...
      ‚è≥ FTNT LSTM: Epoch 10/50 (20%)
       ‚úÖ EFXT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.2777 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.3683
    - TCN: MSE=0.3689
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3683
        ‚Ä¢ TCN: MSE=0.3689
        ‚Ä¢ Random Forest: MSE=14.2611
        ‚Ä¢ XGBoost: MSE=14.2777
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.2425
   ‚úÖ EFXT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EFXT (TargetReturn): LSTM with MSE=0.3683
üêõ DEBUG: EFXT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EFXT.
üêõ DEBUG: EFXT - Moving model to CPU before return...
üêõ DEBUG [23:42:46.730]: EFXT - Returning result metadata...
üêõ DEBUG [23:42:46.730]: Main received result for EFXT
üêõ DEBUG [23:42:46.730]: Main received result for FFIV
üêõ DEBUG: train_worker started for MEXX
  ‚öôÔ∏è Training models for MEXX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - MEXX: Initiating feature extraction for training.
  [DIAGNOSTIC] MEXX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MEXX: rows after features available: 126
üéØ MEXX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MEXX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MEXX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MEXX: Training LSTM (50 epochs)...
      ‚è≥ FTNT LSTM: Epoch 20/50 (40%)
      ‚è≥ MEXX LSTM: Epoch 10/50 (20%)
      ‚è≥ FTNT LSTM: Epoch 30/50 (60%)
      ‚è≥ MEXX LSTM: Epoch 20/50 (40%)
      ‚è≥ FTNT LSTM: Epoch 40/50 (80%)
      ‚è≥ MEXX LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.183279
         RMSE: 0.428111
         R¬≤ Score: -0.8100 (Poor - 81.0% variance explained)
      üîπ FTNT: Training TCN (50 epochs)...
      ‚è≥ FTNT TCN: Epoch 10/50 (20%)
      ‚è≥ FTNT TCN: Epoch 20/50 (40%)
      ‚è≥ MEXX LSTM: Epoch 40/50 (80%)
      ‚è≥ FTNT TCN: Epoch 30/50 (60%)
      ‚è≥ FTNT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.112680
         RMSE: 0.335678
         R¬≤ Score: -0.1128
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FTNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FTNT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.188414
         RMSE: 0.434067
         R¬≤ Score: -1.0668 (Poor - 106.7% variance explained)
      üîπ MEXX: Training TCN (50 epochs)...
      ‚è≥ MEXX TCN: Epoch 10/50 (20%)
      ‚è≥ MEXX TCN: Epoch 20/50 (40%)
      ‚è≥ MEXX TCN: Epoch 30/50 (60%)
      ‚è≥ MEXX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.097359
         RMSE: 0.312024
         R¬≤ Score: -0.0680
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MEXX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MEXX Random Forest: Starting GridSearchCV fit...
       ‚úÖ FTNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.1203 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FTNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MEXX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=65.7421 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MEXX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FTNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.9842 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FTNT XGBoost: Starting GridSearchCV fit...
       ‚úÖ MEXX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=64.8817 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MEXX XGBoost: Starting GridSearchCV fit...
       ‚úÖ FAST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.2715 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.1102
    - TCN: MSE=0.0523
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0523
        ‚Ä¢ LSTM: MSE=0.1102
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.1515
        ‚Ä¢ XGBoost: MSE=13.2715
        ‚Ä¢ Random Forest: MSE=13.4886
   ‚úÖ FAST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FAST (TargetReturn): TCN with MSE=0.0523
üêõ DEBUG: FAST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FAST.
üêõ DEBUG: FAST - Moving model to CPU before return...
üêõ DEBUG [23:42:58.434]: FAST - Returning result metadata...
üêõ DEBUG [23:42:58.435]: Main received result for FAST
üêõ DEBUG: train_worker started for FOX
  ‚öôÔ∏è Training models for FOX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FOX: Initiating feature extraction for training.
  [DIAGNOSTIC] FOX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FOX: rows after features available: 126
üéØ FOX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FOX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FOX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FOX: Training LSTM (50 epochs)...
      ‚è≥ FOX LSTM: Epoch 10/50 (20%)
      ‚è≥ FOX LSTM: Epoch 20/50 (40%)
      ‚è≥ FOX LSTM: Epoch 30/50 (60%)
      ‚è≥ FOX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.275156
         RMSE: 0.524553
         R¬≤ Score: -0.6730 (Poor - 67.3% variance explained)
      üîπ FOX: Training TCN (50 epochs)...
      ‚è≥ FOX TCN: Epoch 10/50 (20%)
      ‚è≥ FOX TCN: Epoch 20/50 (40%)
      ‚è≥ FOX TCN: Epoch 30/50 (60%)
      ‚è≥ FOX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.260146
         RMSE: 0.510045
         R¬≤ Score: -0.5817
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FOX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FOX Random Forest: Starting GridSearchCV fit...
       ‚úÖ FOX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.8690 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FOX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FOX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.6977 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FOX XGBoost: Starting GridSearchCV fit...
       ‚úÖ GENI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.0785 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.1255
    - TCN: MSE=0.1312
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1255
        ‚Ä¢ TCN: MSE=0.1312
        ‚Ä¢ XGBoost: MSE=17.0785
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.9855
        ‚Ä¢ Random Forest: MSE=24.4338
   ‚úÖ GENI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GENI (TargetReturn): LSTM with MSE=0.1255
üêõ DEBUG: GENI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GENI.
üêõ DEBUG: GENI - Moving model to CPU before return...
üêõ DEBUG [23:43:06.808]: GENI - Returning result metadata...
üêõ DEBUG: train_worker started for TOYO
  ‚öôÔ∏è Training models for TOYO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - TOYO: Initiating feature extraction for training.
  [DIAGNOSTIC] TOYO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TOYO: rows after features available: 126
üéØ TOYO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TOYO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TOYO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TOYO: Training LSTM (50 epochs)...
      ‚è≥ TOYO LSTM: Epoch 10/50 (20%)
      ‚è≥ TOYO LSTM: Epoch 20/50 (40%)
       ‚úÖ FBIO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=27.2341 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.5s
    - LSTM: MSE=0.1891
    - TCN: MSE=0.1503
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1503
        ‚Ä¢ LSTM: MSE=0.1891
        ‚Ä¢ XGBoost: MSE=27.2341
        ‚Ä¢ Random Forest: MSE=29.0981
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.5200
   ‚úÖ FBIO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FBIO (TargetReturn): TCN with MSE=0.1503
üêõ DEBUG: FBIO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FBIO.
üêõ DEBUG: FBIO - Moving model to CPU before return...
üêõ DEBUG [23:43:08.145]: FBIO - Returning result metadata...
üêõ DEBUG [23:43:08.146]: Main received result for FBIO
üêõ DEBUG: Training progress: 592/959 done
üêõ DEBUG [23:43:08.146]: Main received result for GENI
üêõ DEBUG: train_worker started for BBCP
      ‚è≥ TOYO LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for BBCP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - BBCP: Initiating feature extraction for training.
  [DIAGNOSTIC] BBCP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBCP: rows after features available: 126
üéØ BBCP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BBCP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBCP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBCP: Training LSTM (50 epochs)...
      ‚è≥ TOYO LSTM: Epoch 40/50 (80%)
      ‚è≥ BBCP LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.220696
         RMSE: 0.469783
         R¬≤ Score: -1.0036 (Poor - 100.4% variance explained)
      üîπ TOYO: Training TCN (50 epochs)...
      ‚è≥ BBCP LSTM: Epoch 20/50 (40%)
      ‚è≥ TOYO TCN: Epoch 10/50 (20%)
      ‚è≥ TOYO TCN: Epoch 20/50 (40%)
      ‚è≥ TOYO TCN: Epoch 30/50 (60%)
      ‚è≥ TOYO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.246216
         RMSE: 0.496202
         R¬≤ Score: -1.2352
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TOYO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TOYO Random Forest: Starting GridSearchCV fit...
      ‚è≥ BBCP LSTM: Epoch 30/50 (60%)
      ‚è≥ BBCP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.149615
         RMSE: 0.386801
         R¬≤ Score: -0.6415 (Poor - 64.2% variance explained)
      üîπ BBCP: Training TCN (50 epochs)...
      ‚è≥ BBCP TCN: Epoch 10/50 (20%)
      ‚è≥ BBCP TCN: Epoch 20/50 (40%)
      ‚è≥ BBCP TCN: Epoch 30/50 (60%)
      ‚è≥ BBCP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.134078
         RMSE: 0.366167
         R¬≤ Score: -0.4710
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBCP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBCP Random Forest: Starting GridSearchCV fit...
       ‚úÖ TOYO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.3678 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TOYO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TOYO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=30.4195 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TOYO XGBoost: Starting GridSearchCV fit...
       ‚úÖ BBCP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.6618 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBCP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BBCP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.3305 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBCP XGBoost: Starting GridSearchCV fit...
       ‚úÖ LGND XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.8915 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.5s
    - LSTM: MSE=0.4882
    - TCN: MSE=0.3518
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3518
        ‚Ä¢ LSTM: MSE=0.4882
        ‚Ä¢ Random Forest: MSE=14.9914
        ‚Ä¢ XGBoost: MSE=17.8915
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.1874
   ‚úÖ LGND: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LGND (TargetReturn): TCN with MSE=0.3518
üêõ DEBUG: LGND - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LGND.
üêõ DEBUG: LGND - Moving model to CPU before return...
üêõ DEBUG [23:43:21.966]: LGND - Returning result metadata...
üêõ DEBUG [23:43:21.966]: Main received result for LGND
üêõ DEBUG: train_worker started for CDE
  ‚öôÔ∏è Training models for CDE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - CDE: Initiating feature extraction for training.
  [DIAGNOSTIC] CDE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CDE: rows after features available: 126
üéØ CDE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CDE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CDE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CDE: Training LSTM (50 epochs)...
      ‚è≥ CDE LSTM: Epoch 10/50 (20%)
      ‚è≥ CDE LSTM: Epoch 20/50 (40%)
      ‚è≥ CDE LSTM: Epoch 30/50 (60%)
      ‚è≥ CDE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.623961
         RMSE: 0.789912
         R¬≤ Score: -1.5841 (Poor - 158.4% variance explained)
      üîπ CDE: Training TCN (50 epochs)...
      ‚è≥ CDE TCN: Epoch 10/50 (20%)
      ‚è≥ CDE TCN: Epoch 20/50 (40%)
      ‚è≥ CDE TCN: Epoch 30/50 (60%)
      ‚è≥ CDE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.349255
         RMSE: 0.590978
         R¬≤ Score: -0.4464
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CDE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CDE Random Forest: Starting GridSearchCV fit...
       ‚úÖ CDE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.1545 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CDE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CDE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=51.8662 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CDE XGBoost: Starting GridSearchCV fit...
       ‚úÖ IGIC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=8.5765 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.8s
    - LSTM: MSE=0.2633
    - TCN: MSE=0.1307
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1307
        ‚Ä¢ LSTM: MSE=0.2633
        ‚Ä¢ XGBoost: MSE=8.5765
        ‚Ä¢ Random Forest: MSE=9.0597
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.1657
   ‚úÖ IGIC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IGIC (TargetReturn): TCN with MSE=0.1307
üêõ DEBUG: IGIC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IGIC.
üêõ DEBUG: IGIC - Moving model to CPU before return...
üêõ DEBUG [23:43:36.085]: IGIC - Returning result metadata...
üêõ DEBUG [23:43:36.085]: Main received result for IGICüêõ DEBUG: train_worker started for WRLD

  ‚öôÔ∏è Training models for WRLD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - WRLD: Initiating feature extraction for training.
  [DIAGNOSTIC] WRLD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WRLD: rows after features available: 126
üéØ WRLD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WRLD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WRLD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WRLD: Training LSTM (50 epochs)...
      ‚è≥ WRLD LSTM: Epoch 10/50 (20%)
      ‚è≥ WRLD LSTM: Epoch 20/50 (40%)
      ‚è≥ WRLD LSTM: Epoch 30/50 (60%)
      ‚è≥ WRLD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.556378
         RMSE: 0.745907
         R¬≤ Score: -0.8737 (Poor - 87.4% variance explained)
      üîπ WRLD: Training TCN (50 epochs)...
      ‚è≥ WRLD TCN: Epoch 10/50 (20%)
      ‚è≥ WRLD TCN: Epoch 20/50 (40%)
      ‚è≥ WRLD TCN: Epoch 30/50 (60%)
      ‚è≥ WRLD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.545783
         RMSE: 0.738771
         R¬≤ Score: -0.8380
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WRLD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WRLD Random Forest: Starting GridSearchCV fit...
       ‚úÖ WRLD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.4644 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WRLD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WRLD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.4283 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WRLD XGBoost: Starting GridSearchCV fit...
       ‚úÖ NGVC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.7617 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.1842
    - TCN: MSE=0.1184
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1184
        ‚Ä¢ LSTM: MSE=0.1842
        ‚Ä¢ Random Forest: MSE=33.1736
        ‚Ä¢ XGBoost: MSE=38.7617
        ‚Ä¢ LightGBM Regressor (CPU): MSE=46.5373
   ‚úÖ NGVC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NGVC (TargetReturn): TCN with MSE=0.1184
üêõ DEBUG: NGVC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NGVC.
üêõ DEBUG: NGVC - Moving model to CPU before return...
üêõ DEBUG [23:43:44.863]: NGVC - Returning result metadata...
üêõ DEBUG [23:43:44.864]: Main received result for NGVC
üêõ DEBUG: Training progress: 596/959 done
üêõ DEBUG: train_worker started for AX
  ‚öôÔ∏è Training models for AX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - AX: Initiating feature extraction for training.
  [DIAGNOSTIC] AX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AX: rows after features available: 126
üéØ AX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AX: Training LSTM (50 epochs)...
      ‚è≥ AX LSTM: Epoch 10/50 (20%)
      ‚è≥ AX LSTM: Epoch 20/50 (40%)
      ‚è≥ AX LSTM: Epoch 30/50 (60%)
      ‚è≥ AX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.448537
         RMSE: 0.669729
         R¬≤ Score: -0.7626 (Poor - 76.3% variance explained)
      üîπ AX: Training TCN (50 epochs)...
      ‚è≥ AX TCN: Epoch 10/50 (20%)
      ‚è≥ AX TCN: Epoch 20/50 (40%)
      ‚è≥ AX TCN: Epoch 30/50 (60%)
      ‚è≥ AX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.523086
         RMSE: 0.723247
         R¬≤ Score: -1.0556
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AX Random Forest: Starting GridSearchCV fit...
       ‚úÖ AX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.4810 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=16.1911 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AX XGBoost: Starting GridSearchCV fit...
       ‚úÖ HLF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=86.9789 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.2626
    - TCN: MSE=0.1931
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1931
        ‚Ä¢ LSTM: MSE=0.2626
        ‚Ä¢ LightGBM Regressor (CPU): MSE=68.4734
        ‚Ä¢ Random Forest: MSE=72.0454
        ‚Ä¢ XGBoost: MSE=86.9789
   ‚úÖ HLF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HLF (TargetReturn): TCN with MSE=0.1931
üêõ DEBUG: HLF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HLF.
üêõ DEBUG: HLF - Moving model to CPU before return...
üêõ DEBUG [23:43:53.034]: HLF - Returning result metadata...
üêõ DEBUG [23:43:53.038]: Main received result for HLF
üêõ DEBUG: train_worker started for IIIV
  ‚öôÔ∏è Training models for IIIV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - IIIV: Initiating feature extraction for training.
  [DIAGNOSTIC] IIIV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IIIV: rows after features available: 126
üéØ IIIV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IIIV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IIIV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IIIV: Training LSTM (50 epochs)...
      ‚è≥ IIIV LSTM: Epoch 10/50 (20%)
      ‚è≥ IIIV LSTM: Epoch 20/50 (40%)
      ‚è≥ IIIV LSTM: Epoch 30/50 (60%)
      ‚è≥ IIIV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.463726
         RMSE: 0.680974
         R¬≤ Score: -1.1958 (Poor - 119.6% variance explained)
      üîπ IIIV: Training TCN (50 epochs)...
      ‚è≥ IIIV TCN: Epoch 10/50 (20%)
      ‚è≥ IIIV TCN: Epoch 20/50 (40%)
      ‚è≥ IIIV TCN: Epoch 30/50 (60%)
      ‚è≥ IIIV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.246856
         RMSE: 0.496846
         R¬≤ Score: -0.1689
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IIIV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IIIV Random Forest: Starting GridSearchCV fit...
       ‚úÖ IIIV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.0027 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IIIV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MTA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.0058 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.3927
    - TCN: MSE=0.2587
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2587
        ‚Ä¢ LSTM: MSE=0.3927
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.6006
        ‚Ä¢ Random Forest: MSE=28.3573
        ‚Ä¢ XGBoost: MSE=32.0058
   ‚úÖ MTA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MTA (TargetReturn): TCN with MSE=0.2587
üêõ DEBUG: MTA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MTA.
üêõ DEBUG: MTA - Moving model to CPU before return...
üêõ DEBUG [23:43:59.096]: MTA - Returning result metadata...
üêõ DEBUG: train_worker started for TR
üêõ DEBUG [23:43:59.097]: Main received result for MTA
  ‚öôÔ∏è Training models for TR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - TR: Initiating feature extraction for training.
  [DIAGNOSTIC] TR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TR: rows after features available: 126
üéØ TR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TR: Training LSTM (50 epochs)...
       ‚úÖ IIIV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.7420 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IIIV XGBoost: Starting GridSearchCV fit...
      ‚è≥ TR LSTM: Epoch 10/50 (20%)
      ‚è≥ TR LSTM: Epoch 20/50 (40%)
      ‚è≥ TR LSTM: Epoch 30/50 (60%)
      ‚è≥ TR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.194760
         RMSE: 0.441316
         R¬≤ Score: -1.0855 (Poor - 108.6% variance explained)
      üîπ TR: Training TCN (50 epochs)...
      ‚è≥ TR TCN: Epoch 10/50 (20%)
      ‚è≥ TR TCN: Epoch 20/50 (40%)
      ‚è≥ TR TCN: Epoch 30/50 (60%)
      ‚è≥ TR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.093689
         RMSE: 0.306086
         R¬≤ Score: -0.0032
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TR Random Forest: Starting GridSearchCV fit...
       ‚úÖ TR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9424 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.0377 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TR XGBoost: Starting GridSearchCV fit...
       ‚úÖ GRMN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.3860 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.4522
    - TCN: MSE=0.2881
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2881
        ‚Ä¢ LSTM: MSE=0.4522
        ‚Ä¢ Random Forest: MSE=10.6596
        ‚Ä¢ XGBoost: MSE=11.3860
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.9370
   ‚úÖ GRMN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GRMN (TargetReturn): TCN with MSE=0.2881
üêõ DEBUG: GRMN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GRMN.
üêõ DEBUG: GRMN - Moving model to CPU before return...
üêõ DEBUG [23:44:13.409]: GRMN - Returning result metadata...
üêõ DEBUG: train_worker started for BLX
üêõ DEBUG [23:44:13.410]: Main received result for GRMN
  ‚öôÔ∏è Training models for BLX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BLX: Initiating feature extraction for training.
  [DIAGNOSTIC] BLX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BLX: rows after features available: 126
üéØ BLX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BLX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BLX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BLX: Training LSTM (50 epochs)...
      ‚è≥ BLX LSTM: Epoch 10/50 (20%)
      ‚è≥ BLX LSTM: Epoch 20/50 (40%)
      ‚è≥ BLX LSTM: Epoch 30/50 (60%)
      ‚è≥ BLX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.146828
         RMSE: 0.383182
         R¬≤ Score: -0.5866 (Poor - 58.7% variance explained)
      üîπ BLX: Training TCN (50 epochs)...
      ‚è≥ BLX TCN: Epoch 10/50 (20%)
      ‚è≥ BLX TCN: Epoch 20/50 (40%)
      ‚è≥ BLX TCN: Epoch 30/50 (60%)
      ‚è≥ BLX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.175673
         RMSE: 0.419133
         R¬≤ Score: -0.8982
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BLX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BLX Random Forest: Starting GridSearchCV fit...
       ‚úÖ OSW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.7831 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.5182
    - TCN: MSE=0.6442
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5182
        ‚Ä¢ TCN: MSE=0.6442
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8058
        ‚Ä¢ Random Forest: MSE=12.9611
        ‚Ä¢ XGBoost: MSE=15.7831
   ‚úÖ OSW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OSW (TargetReturn): LSTM with MSE=0.5182
üêõ DEBUG: OSW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OSW.
üêõ DEBUG: OSW - Moving model to CPU before return...
üêõ DEBUG [23:44:16.489]: OSW - Returning result metadata...
üêõ DEBUG [23:44:16.489]: Main received result for OSW
üêõ DEBUG: Training progress: 600/959 done
üêõ DEBUG: train_worker started for CBNK
  ‚öôÔ∏è Training models for CBNK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - CBNK: Initiating feature extraction for training.
  [DIAGNOSTIC] CBNK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CBNK: rows after features available: 126
üéØ CBNK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CBNK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CBNK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CBNK: Training LSTM (50 epochs)...
      ‚è≥ CBNK LSTM: Epoch 10/50 (20%)
      ‚è≥ CBNK LSTM: Epoch 20/50 (40%)
      ‚è≥ CBNK LSTM: Epoch 30/50 (60%)
      ‚è≥ CBNK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.399852
         RMSE: 0.632339
         R¬≤ Score: -0.5984 (Poor - 59.8% variance explained)
      üîπ CBNK: Training TCN (50 epochs)...
      ‚è≥ CBNK TCN: Epoch 10/50 (20%)
      ‚è≥ CBNK TCN: Epoch 20/50 (40%)
       ‚úÖ BLX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.2423 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BLX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CBNK TCN: Epoch 30/50 (60%)
      ‚è≥ CBNK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.478748
         RMSE: 0.691916
         R¬≤ Score: -0.9138
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CBNK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CBNK Random Forest: Starting GridSearchCV fit...
       ‚úÖ BLX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4063 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BLX XGBoost: Starting GridSearchCV fit...
       ‚úÖ EMBJ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=26.8811 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.3035
    - TCN: MSE=0.1723
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1723
        ‚Ä¢ LSTM: MSE=0.3035
        ‚Ä¢ XGBoost: MSE=26.8811
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.1807
        ‚Ä¢ Random Forest: MSE=29.2975
   ‚úÖ EMBJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EMBJ (TargetReturn): TCN with MSE=0.1723
üêõ DEBUG: EMBJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EMBJ.
üêõ DEBUG: EMBJ - Moving model to CPU before return...
üêõ DEBUG [23:44:21.250]: EMBJ - Returning result metadata...
üêõ DEBUG: train_worker started for FOXA
üêõ DEBUG [23:44:21.251]: Main received result for EMBJ
  ‚öôÔ∏è Training models for FOXA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - FOXA: Initiating feature extraction for training.
  [DIAGNOSTIC] FOXA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FOXA: rows after features available: 126
üéØ FOXA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FOXA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FOXA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FOXA: Training LSTM (50 epochs)...
      ‚è≥ FOXA LSTM: Epoch 10/50 (20%)
      ‚è≥ FOXA LSTM: Epoch 20/50 (40%)
       ‚úÖ CBNK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.8520 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CBNK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FOXA LSTM: Epoch 30/50 (60%)
       ‚úÖ CBNK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.4537 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CBNK XGBoost: Starting GridSearchCV fit...
      ‚è≥ FOXA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.309085
         RMSE: 0.555954
         R¬≤ Score: -0.7357 (Poor - 73.6% variance explained)
      üîπ FOXA: Training TCN (50 epochs)...
      ‚è≥ FOXA TCN: Epoch 10/50 (20%)
      ‚è≥ FOXA TCN: Epoch 20/50 (40%)
      ‚è≥ FOXA TCN: Epoch 30/50 (60%)
      ‚è≥ FOXA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.334576
         RMSE: 0.578426
         R¬≤ Score: -0.8789
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FOXA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FOXA Random Forest: Starting GridSearchCV fit...
       ‚úÖ FOXA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.5691 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FOXA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FOXA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.5025 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FOXA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ROK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.7322 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.5351
    - TCN: MSE=0.5023
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5023
        ‚Ä¢ LSTM: MSE=0.5351
        ‚Ä¢ XGBoost: MSE=14.7322
        ‚Ä¢ Random Forest: MSE=16.1277
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.4577
   ‚úÖ ROK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ROK (TargetReturn): TCN with MSE=0.5023
üêõ DEBUG: ROK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ROK.
üêõ DEBUG: ROK - Moving model to CPU before return...
üêõ DEBUG [23:44:29.688]: ROK - Returning result metadata...
üêõ DEBUG [23:44:29.688]: Main received result for ROK
üêõ DEBUG: train_worker started for LANV
  ‚öôÔ∏è Training models for LANV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - LANV: Initiating feature extraction for training.
  [DIAGNOSTIC] LANV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LANV: rows after features available: 126
üéØ LANV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LANV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LANV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LANV: Training LSTM (50 epochs)...
      ‚è≥ LANV LSTM: Epoch 10/50 (20%)
      ‚è≥ LANV LSTM: Epoch 20/50 (40%)
      ‚è≥ LANV LSTM: Epoch 30/50 (60%)
      ‚è≥ LANV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.329188
         RMSE: 0.573749
         R¬≤ Score: -1.0193 (Poor - 101.9% variance explained)
      üîπ LANV: Training TCN (50 epochs)...
      ‚è≥ LANV TCN: Epoch 10/50 (20%)
      ‚è≥ LANV TCN: Epoch 20/50 (40%)
      ‚è≥ LANV TCN: Epoch 30/50 (60%)
      ‚è≥ LANV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.170960
         RMSE: 0.413473
         R¬≤ Score: -0.0487
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LANV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LANV Random Forest: Starting GridSearchCV fit...
       ‚úÖ LANV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.3789 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LANV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LANV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=38.5630 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LANV XGBoost: Starting GridSearchCV fit...
       ‚úÖ ISRA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.8732 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.4156
    - TCN: MSE=0.4714
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4156
        ‚Ä¢ TCN: MSE=0.4714
        ‚Ä¢ Random Forest: MSE=5.0012
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.5715
        ‚Ä¢ XGBoost: MSE=5.8732
   ‚úÖ ISRA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ISRA (TargetReturn): LSTM with MSE=0.4156
üêõ DEBUG: ISRA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ISRA.
üêõ DEBUG: ISRA - Moving model to CPU before return...
üêõ DEBUG [23:44:42.829]: ISRA - Returning result metadata...
üêõ DEBUG: train_worker started for BFH
üêõ DEBUG [23:44:42.830]: Main received result for ISRA
  ‚öôÔ∏è Training models for BFH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - BFH: Initiating feature extraction for training.
  [DIAGNOSTIC] BFH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BFH: rows after features available: 126
üéØ BFH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BFH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BFH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BFH: Training LSTM (50 epochs)...
      ‚è≥ BFH LSTM: Epoch 10/50 (20%)
      ‚è≥ BFH LSTM: Epoch 20/50 (40%)
      ‚è≥ BFH LSTM: Epoch 30/50 (60%)
      ‚è≥ BFH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.576721
         RMSE: 0.759422
         R¬≤ Score: -0.8162 (Poor - 81.6% variance explained)
      üîπ BFH: Training TCN (50 epochs)...
      ‚è≥ BFH TCN: Epoch 10/50 (20%)
      ‚è≥ BFH TCN: Epoch 20/50 (40%)
      ‚è≥ BFH TCN: Epoch 30/50 (60%)
      ‚è≥ BFH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.591899
         RMSE: 0.769350
         R¬≤ Score: -0.8640
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BFH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BFH Random Forest: Starting GridSearchCV fit...
       ‚úÖ BFH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.5193 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BFH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BFH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.2681 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BFH XGBoost: Starting GridSearchCV fit...
       ‚úÖ FTNT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.6137 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.1833
    - TCN: MSE=0.1127
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1127
        ‚Ä¢ LSTM: MSE=0.1833
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.9842
        ‚Ä¢ Random Forest: MSE=12.1203
        ‚Ä¢ XGBoost: MSE=13.6137
   ‚úÖ FTNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FTNT (TargetReturn): TCN with MSE=0.1127
üêõ DEBUG: FTNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FTNT.
üêõ DEBUG: FTNT - Moving model to CPU before return...
üêõ DEBUG [23:44:50.969]: FTNT - Returning result metadata...
üêõ DEBUG [23:44:50.969]: Main received result for FTNT
üêõ DEBUG: Training progress: 604/959 done
üêõ DEBUG: train_worker started for BKNG
  ‚öôÔ∏è Training models for BKNG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - BKNG: Initiating feature extraction for training.
  [DIAGNOSTIC] BKNG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BKNG: rows after features available: 126
üéØ BKNG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BKNG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BKNG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BKNG: Training LSTM (50 epochs)...
      ‚è≥ BKNG LSTM: Epoch 10/50 (20%)
      ‚è≥ BKNG LSTM: Epoch 20/50 (40%)
      ‚è≥ BKNG LSTM: Epoch 30/50 (60%)
      ‚è≥ BKNG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.235385
         RMSE: 0.485165
         R¬≤ Score: -1.0943 (Poor - 109.4% variance explained)
      üîπ BKNG: Training TCN (50 epochs)...
      ‚è≥ BKNG TCN: Epoch 10/50 (20%)
      ‚è≥ BKNG TCN: Epoch 20/50 (40%)
      ‚è≥ BKNG TCN: Epoch 30/50 (60%)
      ‚è≥ BKNG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.187328
         RMSE: 0.432814
         R¬≤ Score: -0.6667
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BKNG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BKNG Random Forest: Starting GridSearchCV fit...
       ‚úÖ MEXX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=96.2410 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.1884
    - TCN: MSE=0.0974
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0974
        ‚Ä¢ LSTM: MSE=0.1884
        ‚Ä¢ LightGBM Regressor (CPU): MSE=64.8817
        ‚Ä¢ Random Forest: MSE=65.7421
        ‚Ä¢ XGBoost: MSE=96.2410
   ‚úÖ MEXX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MEXX (TargetReturn): TCN with MSE=0.0974
üêõ DEBUG: MEXX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MEXX.
üêõ DEBUG: MEXX - Moving model to CPU before return...
üêõ DEBUG [23:44:55.391]: MEXX - Returning result metadata...
üêõ DEBUG [23:44:55.391]: Main received result for MEXX
üêõ DEBUG: train_worker started for IAG
  ‚öôÔ∏è Training models for IAG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - IAG: Initiating feature extraction for training.
  [DIAGNOSTIC] IAG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IAG: rows after features available: 126
üéØ IAG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IAG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IAG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IAG: Training LSTM (50 epochs)...
      ‚è≥ IAG LSTM: Epoch 10/50 (20%)
      ‚è≥ IAG LSTM: Epoch 20/50 (40%)
      ‚è≥ IAG LSTM: Epoch 30/50 (60%)
       ‚úÖ BKNG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.7973 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BKNG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IAG LSTM: Epoch 40/50 (80%)
       ‚úÖ BKNG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.6417 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BKNG XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.404810
         RMSE: 0.636247
         R¬≤ Score: -0.9671 (Poor - 96.7% variance explained)
      üîπ IAG: Training TCN (50 epochs)...
      ‚è≥ IAG TCN: Epoch 10/50 (20%)
      ‚è≥ IAG TCN: Epoch 20/50 (40%)
      ‚è≥ IAG TCN: Epoch 30/50 (60%)
      ‚è≥ IAG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.226051
         RMSE: 0.475448
         R¬≤ Score: -0.0984
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IAG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IAG Random Forest: Starting GridSearchCV fit...
       ‚úÖ IAG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=62.5406 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IAG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IAG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=44.7682 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IAG XGBoost: Starting GridSearchCV fit...
       ‚úÖ FOX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.1223 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.2752
    - TCN: MSE=0.2601
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2601
        ‚Ä¢ LSTM: MSE=0.2752
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.6977
        ‚Ä¢ Random Forest: MSE=7.8690
        ‚Ä¢ XGBoost: MSE=8.1223
   ‚úÖ FOX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FOX (TargetReturn): TCN with MSE=0.2601
üêõ DEBUG: FOX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FOX.
üêõ DEBUG: FOX - Moving model to CPU before return...
üêõ DEBUG [23:45:04.493]: FOX - Returning result metadata...
üêõ DEBUG [23:45:04.493]: Main received result for FOXüêõ DEBUG: train_worker started for ELPC

  ‚öôÔ∏è Training models for ELPC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - ELPC: Initiating feature extraction for training.
  [DIAGNOSTIC] ELPC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ELPC: rows after features available: 126
üéØ ELPC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ELPC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ELPC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ELPC: Training LSTM (50 epochs)...
      ‚è≥ ELPC LSTM: Epoch 10/50 (20%)
      ‚è≥ ELPC LSTM: Epoch 20/50 (40%)
      ‚è≥ ELPC LSTM: Epoch 30/50 (60%)
      ‚è≥ ELPC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.191105
         RMSE: 0.437156
         R¬≤ Score: -1.0463 (Poor - 104.6% variance explained)
      üîπ ELPC: Training TCN (50 epochs)...
      ‚è≥ ELPC TCN: Epoch 10/50 (20%)
      ‚è≥ ELPC TCN: Epoch 20/50 (40%)
      ‚è≥ ELPC TCN: Epoch 30/50 (60%)
      ‚è≥ ELPC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.120242
         RMSE: 0.346760
         R¬≤ Score: -0.2875
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ELPC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ELPC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ELPC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.8851 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ELPC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ELPC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=28.9017 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ELPC XGBoost: Starting GridSearchCV fit...
       ‚úÖ TOYO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.5514 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.2207
    - TCN: MSE=0.2462
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2207
        ‚Ä¢ TCN: MSE=0.2462
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.4195
        ‚Ä¢ XGBoost: MSE=32.5514
        ‚Ä¢ Random Forest: MSE=38.3678
   ‚úÖ TOYO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TOYO (TargetReturn): LSTM with MSE=0.2207
üêõ DEBUG: TOYO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TOYO.
üêõ DEBUG: TOYO - Moving model to CPU before return...
üêõ DEBUG [23:45:12.913]: TOYO - Returning result metadata...
üêõ DEBUG: train_worker started for LOUP
üêõ DEBUG [23:45:12.914]: Main received result for TOYO
  ‚öôÔ∏è Training models for LOUP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - LOUP: Initiating feature extraction for training.
  [DIAGNOSTIC] LOUP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LOUP: rows after features available: 126
üéØ LOUP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LOUP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LOUP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LOUP: Training LSTM (50 epochs)...
      ‚è≥ LOUP LSTM: Epoch 10/50 (20%)
      ‚è≥ LOUP LSTM: Epoch 20/50 (40%)
      ‚è≥ LOUP LSTM: Epoch 30/50 (60%)
      ‚è≥ LOUP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.529461
         RMSE: 0.727641
         R¬≤ Score: -0.7972 (Poor - 79.7% variance explained)
      üîπ LOUP: Training TCN (50 epochs)...
      ‚è≥ LOUP TCN: Epoch 10/50 (20%)
      ‚è≥ LOUP TCN: Epoch 20/50 (40%)
      ‚è≥ LOUP TCN: Epoch 30/50 (60%)
      ‚è≥ LOUP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.640392
         RMSE: 0.800245
         R¬≤ Score: -1.1737
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LOUP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LOUP Random Forest: Starting GridSearchCV fit...
       ‚úÖ BBCP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.4382 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.8s
    - LSTM: MSE=0.1496
    - TCN: MSE=0.1341
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1341
        ‚Ä¢ LSTM: MSE=0.1496
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.3305
        ‚Ä¢ Random Forest: MSE=26.6618
        ‚Ä¢ XGBoost: MSE=30.4382
   ‚úÖ BBCP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBCP (TargetReturn): TCN with MSE=0.1341
üêõ DEBUG: BBCP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBCP.
üêõ DEBUG: BBCP - Moving model to CPU before return...
üêõ DEBUG [23:45:17.184]: BBCP - Returning result metadata...
üêõ DEBUG [23:45:17.184]: Main received result for BBCP
üêõ DEBUG: Training progress: 608/959 done
üêõ DEBUG: train_worker started for CFLT
  ‚öôÔ∏è Training models for CFLT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - CFLT: Initiating feature extraction for training.
  [DIAGNOSTIC] CFLT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CFLT: rows after features available: 126
üéØ CFLT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CFLT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CFLT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CFLT: Training LSTM (50 epochs)...
      ‚è≥ CFLT LSTM: Epoch 10/50 (20%)
      ‚è≥ CFLT LSTM: Epoch 20/50 (40%)
       ‚úÖ LOUP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.1686 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LOUP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CFLT LSTM: Epoch 30/50 (60%)
      ‚è≥ CFLT LSTM: Epoch 40/50 (80%)
       ‚úÖ LOUP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.9077 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LOUP XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.499134
         RMSE: 0.706494
         R¬≤ Score: -0.9940 (Poor - 99.4% variance explained)
      üîπ CFLT: Training TCN (50 epochs)...
      ‚è≥ CFLT TCN: Epoch 10/50 (20%)
      ‚è≥ CFLT TCN: Epoch 20/50 (40%)
      ‚è≥ CFLT TCN: Epoch 30/50 (60%)
      ‚è≥ CFLT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.388955
         RMSE: 0.623663
         R¬≤ Score: -0.5539
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CFLT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CFLT Random Forest: Starting GridSearchCV fit...
       ‚úÖ CFLT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=38.5733 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CFLT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CDE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=48.1114 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.0s
    - LSTM: MSE=0.6240
    - TCN: MSE=0.3493
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3493
        ‚Ä¢ LSTM: MSE=0.6240
        ‚Ä¢ Random Forest: MSE=38.1545
        ‚Ä¢ XGBoost: MSE=48.1114
        ‚Ä¢ LightGBM Regressor (CPU): MSE=51.8662
   ‚úÖ CDE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CDE (TargetReturn): TCN with MSE=0.3493
üêõ DEBUG: CDE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CDE.
üêõ DEBUG: CDE - Moving model to CPU before return...
üêõ DEBUG [23:45:22.829]: CDE - Returning result metadata...
üêõ DEBUG: train_worker started for FDD
üêõ DEBUG [23:45:22.829]: Main received result for CDE
  ‚öôÔ∏è Training models for FDD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FDD: Initiating feature extraction for training.
  [DIAGNOSTIC] FDD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDD: rows after features available: 126
üéØ FDD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDD: Training LSTM (50 epochs)...
       ‚úÖ CFLT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=36.9974 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CFLT XGBoost: Starting GridSearchCV fit...
      ‚è≥ FDD LSTM: Epoch 10/50 (20%)
      ‚è≥ FDD LSTM: Epoch 20/50 (40%)
      ‚è≥ FDD LSTM: Epoch 30/50 (60%)
      ‚è≥ FDD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.129293
         RMSE: 0.359574
         R¬≤ Score: -0.5697 (Poor - 57.0% variance explained)
      üîπ FDD: Training TCN (50 epochs)...
      ‚è≥ FDD TCN: Epoch 10/50 (20%)
      ‚è≥ FDD TCN: Epoch 20/50 (40%)
      ‚è≥ FDD TCN: Epoch 30/50 (60%)
      ‚è≥ FDD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.088704
         RMSE: 0.297833
         R¬≤ Score: -0.0769
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDD Random Forest: Starting GridSearchCV fit...
       ‚úÖ FDD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7568 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.8568 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDD XGBoost: Starting GridSearchCV fit...
       ‚úÖ WRLD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.6978 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.5564
    - TCN: MSE=0.5458
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5458
        ‚Ä¢ LSTM: MSE=0.5564
        ‚Ä¢ Random Forest: MSE=22.4644
        ‚Ä¢ XGBoost: MSE=22.6978
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.4283
   ‚úÖ WRLD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WRLD (TargetReturn): TCN with MSE=0.5458
üêõ DEBUG: WRLD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WRLD.
üêõ DEBUG: WRLD - Moving model to CPU before return...
üêõ DEBUG [23:45:40.443]: WRLD - Returning result metadata...
üêõ DEBUG [23:45:40.444]: Main received result for WRLD
üêõ DEBUG: train_worker started for MEDP
  ‚öôÔ∏è Training models for MEDP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - MEDP: Initiating feature extraction for training.
  [DIAGNOSTIC] MEDP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MEDP: rows after features available: 126
üéØ MEDP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MEDP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MEDP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MEDP: Training LSTM (50 epochs)...
      ‚è≥ MEDP LSTM: Epoch 10/50 (20%)
      ‚è≥ MEDP LSTM: Epoch 20/50 (40%)
      ‚è≥ MEDP LSTM: Epoch 30/50 (60%)
      ‚è≥ MEDP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.275722
         RMSE: 0.525092
         R¬≤ Score: -0.6760 (Poor - 67.6% variance explained)
      üîπ MEDP: Training TCN (50 epochs)...
      ‚è≥ MEDP TCN: Epoch 10/50 (20%)
      ‚è≥ MEDP TCN: Epoch 20/50 (40%)
      ‚è≥ MEDP TCN: Epoch 30/50 (60%)
      ‚è≥ MEDP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.172137
         RMSE: 0.414894
         R¬≤ Score: -0.0464
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MEDP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MEDP Random Forest: Starting GridSearchCV fit...
       ‚úÖ MEDP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.0619 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MEDP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MEDP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.4337 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MEDP XGBoost: Starting GridSearchCV fit...
       ‚úÖ AX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=9.5486 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.4485
    - TCN: MSE=0.5231
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4485
        ‚Ä¢ TCN: MSE=0.5231
        ‚Ä¢ XGBoost: MSE=9.5486
        ‚Ä¢ Random Forest: MSE=10.4810
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.1911
   ‚úÖ AX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AX (TargetReturn): LSTM with MSE=0.4485
üêõ DEBUG: AX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AX.
üêõ DEBUG: AX - Moving model to CPU before return...
üêõ DEBUG [23:45:50.427]: AX - Returning result metadata...
üêõ DEBUG: train_worker started for ARLO
üêõ DEBUG [23:45:50.428]: Main received result for AX
  ‚öôÔ∏è Training models for ARLO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - ARLO: Initiating feature extraction for training.
  [DIAGNOSTIC] ARLO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARLO: rows after features available: 126
üéØ ARLO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARLO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARLO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARLO: Training LSTM (50 epochs)...
      ‚è≥ ARLO LSTM: Epoch 10/50 (20%)
      ‚è≥ ARLO LSTM: Epoch 20/50 (40%)
      ‚è≥ ARLO LSTM: Epoch 30/50 (60%)
      ‚è≥ ARLO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.490073
         RMSE: 0.700052
         R¬≤ Score: -0.4999 (Poor - 50.0% variance explained)
      üîπ ARLO: Training TCN (50 epochs)...
      ‚è≥ ARLO TCN: Epoch 10/50 (20%)
      ‚è≥ ARLO TCN: Epoch 20/50 (40%)
      ‚è≥ ARLO TCN: Epoch 30/50 (60%)
      ‚è≥ ARLO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.588784
         RMSE: 0.767322
         R¬≤ Score: -0.8020
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARLO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARLO Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARLO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=43.0185 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARLO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARLO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=55.3029 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARLO XGBoost: Starting GridSearchCV fit...
       ‚úÖ IIIV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.2181 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=0.4637
    - TCN: MSE=0.2469
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2469
        ‚Ä¢ LSTM: MSE=0.4637
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.7420
        ‚Ä¢ Random Forest: MSE=13.0027
        ‚Ä¢ XGBoost: MSE=14.2181
   ‚úÖ IIIV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IIIV (TargetReturn): TCN with MSE=0.2469
üêõ DEBUG: IIIV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IIIV.
üêõ DEBUG: IIIV - Moving model to CPU before return...
üêõ DEBUG [23:45:59.631]: IIIV - Returning result metadata...
üêõ DEBUG: train_worker started for COCO
üêõ DEBUG [23:45:59.632]: Main received result for IIIV
üêõ DEBUG: Training progress: 612/959 done
  ‚öôÔ∏è Training models for COCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - COCO: Initiating feature extraction for training.
  [DIAGNOSTIC] COCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COCO: rows after features available: 126
üéØ COCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COCO: Training LSTM (50 epochs)...
      ‚è≥ COCO LSTM: Epoch 10/50 (20%)
      ‚è≥ COCO LSTM: Epoch 20/50 (40%)
      ‚è≥ COCO LSTM: Epoch 30/50 (60%)
      ‚è≥ COCO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.612150
         RMSE: 0.782400
         R¬≤ Score: -1.0250 (Poor - 102.5% variance explained)
      üîπ COCO: Training TCN (50 epochs)...
      ‚è≥ COCO TCN: Epoch 10/50 (20%)
      ‚è≥ COCO TCN: Epoch 20/50 (40%)
      ‚è≥ COCO TCN: Epoch 30/50 (60%)
      ‚è≥ COCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.513927
         RMSE: 0.716887
         R¬≤ Score: -0.7001
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COCO Random Forest: Starting GridSearchCV fit...
       ‚úÖ TR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.8980 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.3s
    - LSTM: MSE=0.1948
    - TCN: MSE=0.0937
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0937
        ‚Ä¢ LSTM: MSE=0.1948
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.0377
        ‚Ä¢ XGBoost: MSE=9.8980
        ‚Ä¢ Random Forest: MSE=10.9424
   ‚úÖ TR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TR (TargetReturn): TCN with MSE=0.0937
üêõ DEBUG: TR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TR.
üêõ DEBUG: TR - Moving model to CPU before return...
üêõ DEBUG [23:46:02.946]: TR - Returning result metadata...
üêõ DEBUG [23:46:02.947]: Main received result for TR
üêõ DEBUG: train_worker started for BYD
  ‚öôÔ∏è Training models for BYD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - BYD: Initiating feature extraction for training.
  [DIAGNOSTIC] BYD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BYD: rows after features available: 126
üéØ BYD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BYD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BYD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BYD: Training LSTM (50 epochs)...
      ‚è≥ BYD LSTM: Epoch 10/50 (20%)
      ‚è≥ BYD LSTM: Epoch 20/50 (40%)
      ‚è≥ BYD LSTM: Epoch 30/50 (60%)
      ‚è≥ BYD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.636742
         RMSE: 0.797961
         R¬≤ Score: -0.8690 (Poor - 86.9% variance explained)
      üîπ BYD: Training TCN (50 epochs)...
       ‚úÖ COCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.4323 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BYD TCN: Epoch 10/50 (20%)
      ‚è≥ BYD TCN: Epoch 20/50 (40%)
      ‚è≥ BYD TCN: Epoch 30/50 (60%)
      ‚è≥ BYD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.764169
         RMSE: 0.874168
         R¬≤ Score: -1.2430
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BYD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BYD Random Forest: Starting GridSearchCV fit...
       ‚úÖ COCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.2286 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COCO XGBoost: Starting GridSearchCV fit...
       ‚úÖ BYD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.3563 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BYD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BYD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.5521 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BYD XGBoost: Starting GridSearchCV fit...
       ‚úÖ BLX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.9027 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.7s
    - LSTM: MSE=0.1468
    - TCN: MSE=0.1757
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1468
        ‚Ä¢ TCN: MSE=0.1757
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4063
        ‚Ä¢ Random Forest: MSE=7.2423
        ‚Ä¢ XGBoost: MSE=7.9027
   ‚úÖ BLX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BLX (TargetReturn): LSTM with MSE=0.1468
üêõ DEBUG: BLX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BLX.
üêõ DEBUG: BLX - Moving model to CPU before return...
üêõ DEBUG [23:46:17.744]: BLX - Returning result metadata...
üêõ DEBUG: train_worker started for AEIS
üêõ DEBUG [23:46:17.745]: Main received result for BLX
  ‚öôÔ∏è Training models for AEIS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - AEIS: Initiating feature extraction for training.
  [DIAGNOSTIC] AEIS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AEIS: rows after features available: 126
üéØ AEIS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AEIS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AEIS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AEIS: Training LSTM (50 epochs)...
      ‚è≥ AEIS LSTM: Epoch 10/50 (20%)
      ‚è≥ AEIS LSTM: Epoch 20/50 (40%)
      ‚è≥ AEIS LSTM: Epoch 30/50 (60%)
      ‚è≥ AEIS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.476927
         RMSE: 0.690599
         R¬≤ Score: -0.9186 (Poor - 91.9% variance explained)
      üîπ AEIS: Training TCN (50 epochs)...
      ‚è≥ AEIS TCN: Epoch 10/50 (20%)
      ‚è≥ AEIS TCN: Epoch 20/50 (40%)
      ‚è≥ AEIS TCN: Epoch 30/50 (60%)
      ‚è≥ AEIS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.476481
         RMSE: 0.690276
         R¬≤ Score: -0.9168
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AEIS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AEIS Random Forest: Starting GridSearchCV fit...
       ‚úÖ AEIS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.0212 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AEIS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AEIS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.5519 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AEIS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CBNK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.2825 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.3999
    - TCN: MSE=0.4787
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3999
        ‚Ä¢ TCN: MSE=0.4787
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.4537
        ‚Ä¢ XGBoost: MSE=10.2825
        ‚Ä¢ Random Forest: MSE=12.8520
   ‚úÖ CBNK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CBNK (TargetReturn): LSTM with MSE=0.3999
üêõ DEBUG: CBNK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CBNK.
üêõ DEBUG: CBNK - Moving model to CPU before return...
üêõ DEBUG [23:46:24.988]: CBNK - Returning result metadata...
üêõ DEBUG: train_worker started for AZZ
üêõ DEBUG [23:46:24.989]: Main received result for CBNK
  ‚öôÔ∏è Training models for AZZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - AZZ: Initiating feature extraction for training.
  [DIAGNOSTIC] AZZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AZZ: rows after features available: 126
üéØ AZZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AZZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AZZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AZZ: Training LSTM (50 epochs)...
      ‚è≥ AZZ LSTM: Epoch 10/50 (20%)
      ‚è≥ AZZ LSTM: Epoch 20/50 (40%)
      ‚è≥ AZZ LSTM: Epoch 30/50 (60%)
      ‚è≥ AZZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.481525
         RMSE: 0.693920
         R¬≤ Score: -0.8270 (Poor - 82.7% variance explained)
      üîπ AZZ: Training TCN (50 epochs)...
      ‚è≥ AZZ TCN: Epoch 10/50 (20%)
      ‚è≥ AZZ TCN: Epoch 20/50 (40%)
      ‚è≥ AZZ TCN: Epoch 30/50 (60%)
      ‚è≥ AZZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.293824
         RMSE: 0.542056
         R¬≤ Score: -0.1148
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AZZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AZZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ FOXA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.0045 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.3091
    - TCN: MSE=0.3346
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3091
        ‚Ä¢ TCN: MSE=0.3346
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.5025
        ‚Ä¢ Random Forest: MSE=7.5691
        ‚Ä¢ XGBoost: MSE=8.0045
   ‚úÖ FOXA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FOXA (TargetReturn): LSTM with MSE=0.3091
üêõ DEBUG: FOXA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FOXA.
üêõ DEBUG: FOXA - Moving model to CPU before return...
üêõ DEBUG [23:46:28.030]: FOXA - Returning result metadata...
üêõ DEBUG: train_worker started for ECNS
üêõ DEBUG [23:46:28.032]: Main received result for FOXA
üêõ DEBUG: Training progress: 616/959 done
  ‚öôÔ∏è Training models for ECNS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ECNS: Initiating feature extraction for training.
  [DIAGNOSTIC] ECNS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ECNS: rows after features available: 126
üéØ ECNS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ECNS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ECNS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ECNS: Training LSTM (50 epochs)...
      ‚è≥ ECNS LSTM: Epoch 10/50 (20%)
      ‚è≥ ECNS LSTM: Epoch 20/50 (40%)
      ‚è≥ ECNS LSTM: Epoch 30/50 (60%)
      ‚è≥ ECNS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.529853
         RMSE: 0.727910
         R¬≤ Score: -0.6233 (Poor - 62.3% variance explained)
      üîπ ECNS: Training TCN (50 epochs)...
      ‚è≥ ECNS TCN: Epoch 10/50 (20%)
      ‚è≥ ECNS TCN: Epoch 20/50 (40%)
      ‚è≥ ECNS TCN: Epoch 30/50 (60%)
       ‚úÖ AZZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.1540 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AZZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ECNS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.642139
         RMSE: 0.801336
         R¬≤ Score: -0.9673
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ECNS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ECNS Random Forest: Starting GridSearchCV fit...
       ‚úÖ AZZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.7188 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AZZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ ECNS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.4384 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ECNS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LANV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.5382 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.3292
    - TCN: MSE=0.1710
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1710
        ‚Ä¢ LSTM: MSE=0.3292
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.5630
        ‚Ä¢ Random Forest: MSE=39.3789
        ‚Ä¢ XGBoost: MSE=40.5382
   ‚úÖ LANV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LANV (TargetReturn): TCN with MSE=0.1710
üêõ DEBUG: LANV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LANV.
üêõ DEBUG: LANV - Moving model to CPU before return...
üêõ DEBUG [23:46:33.912]: LANV - Returning result metadata...
üêõ DEBUG [23:46:33.912]: Main received result for LANV
üêõ DEBUG: train_worker started for USFD
  ‚öôÔ∏è Training models for USFD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - USFD: Initiating feature extraction for training.
  [DIAGNOSTIC] USFD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ USFD: rows after features available: 126
üéØ USFD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] USFD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö USFD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ USFD: Training LSTM (50 epochs)...
      ‚è≥ USFD LSTM: Epoch 10/50 (20%)
       ‚úÖ ECNS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.0330 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ECNS XGBoost: Starting GridSearchCV fit...
      ‚è≥ USFD LSTM: Epoch 20/50 (40%)
      ‚è≥ USFD LSTM: Epoch 30/50 (60%)
      ‚è≥ USFD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.482813
         RMSE: 0.694847
         R¬≤ Score: -0.7274 (Poor - 72.7% variance explained)
      üîπ USFD: Training TCN (50 epochs)...
      ‚è≥ USFD TCN: Epoch 10/50 (20%)
      ‚è≥ USFD TCN: Epoch 20/50 (40%)
      ‚è≥ USFD TCN: Epoch 30/50 (60%)
      ‚è≥ USFD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.644830
         RMSE: 0.803013
         R¬≤ Score: -1.3071
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä USFD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ USFD Random Forest: Starting GridSearchCV fit...
       ‚úÖ USFD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.0914 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ USFD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ USFD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.1160 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ USFD XGBoost: Starting GridSearchCV fit...
       ‚úÖ BFH XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.6362 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.7s
    - LSTM: MSE=0.5767
    - TCN: MSE=0.5919
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5767
        ‚Ä¢ TCN: MSE=0.5919
        ‚Ä¢ XGBoost: MSE=16.6362
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.2681
        ‚Ä¢ Random Forest: MSE=19.5193
   ‚úÖ BFH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BFH (TargetReturn): LSTM with MSE=0.5767
üêõ DEBUG: BFH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BFH.
üêõ DEBUG: BFH - Moving model to CPU before return...
üêõ DEBUG [23:46:44.419]: BFH - Returning result metadata...
üêõ DEBUG [23:46:44.420]: Main received result for BFH
üêõ DEBUG: train_worker started for OMF
  ‚öôÔ∏è Training models for OMF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - OMF: Initiating feature extraction for training.
  [DIAGNOSTIC] OMF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OMF: rows after features available: 126
üéØ OMF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OMF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OMF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OMF: Training LSTM (50 epochs)...
      ‚è≥ OMF LSTM: Epoch 10/50 (20%)
      ‚è≥ OMF LSTM: Epoch 20/50 (40%)
      ‚è≥ OMF LSTM: Epoch 30/50 (60%)
      ‚è≥ OMF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.437422
         RMSE: 0.661379
         R¬≤ Score: -0.8024 (Poor - 80.2% variance explained)
      üîπ OMF: Training TCN (50 epochs)...
      ‚è≥ OMF TCN: Epoch 10/50 (20%)
      ‚è≥ OMF TCN: Epoch 20/50 (40%)
      ‚è≥ OMF TCN: Epoch 30/50 (60%)
      ‚è≥ OMF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.385549
         RMSE: 0.620926
         R¬≤ Score: -0.5886
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OMF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OMF Random Forest: Starting GridSearchCV fit...
       ‚úÖ OMF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.6374 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OMF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OMF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.8226 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OMF XGBoost: Starting GridSearchCV fit...
       ‚úÖ BKNG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.6922 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.7s
    - LSTM: MSE=0.2354
    - TCN: MSE=0.1873
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1873
        ‚Ä¢ LSTM: MSE=0.2354
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6417
        ‚Ä¢ Random Forest: MSE=13.7973
        ‚Ä¢ XGBoost: MSE=14.6922
   ‚úÖ BKNG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BKNG (TargetReturn): TCN with MSE=0.1873
üêõ DEBUG: BKNG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BKNG.
üêõ DEBUG: BKNG - Moving model to CPU before return...
üêõ DEBUG [23:46:54.408]: BKNG - Returning result metadata...
üêõ DEBUG [23:46:54.409]: Main received result for BKNG
üêõ DEBUG: train_worker started for MRUS
  ‚öôÔ∏è Training models for MRUS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - MRUS: Initiating feature extraction for training.
  [DIAGNOSTIC] MRUS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MRUS: rows after features available: 126
üéØ MRUS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MRUS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MRUS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MRUS: Training LSTM (50 epochs)...
      ‚è≥ MRUS LSTM: Epoch 10/50 (20%)
      ‚è≥ MRUS LSTM: Epoch 20/50 (40%)
      ‚è≥ MRUS LSTM: Epoch 30/50 (60%)
      ‚è≥ MRUS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.417254
         RMSE: 0.645952
         R¬≤ Score: -1.1114 (Poor - 111.1% variance explained)
      üîπ MRUS: Training TCN (50 epochs)...
      ‚è≥ MRUS TCN: Epoch 10/50 (20%)
      ‚è≥ MRUS TCN: Epoch 20/50 (40%)
      ‚è≥ MRUS TCN: Epoch 30/50 (60%)
      ‚è≥ MRUS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.275225
         RMSE: 0.524619
         R¬≤ Score: -0.3927
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MRUS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MRUS Random Forest: Starting GridSearchCV fit...
       ‚úÖ MRUS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.0499 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MRUS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MRUS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=56.2661 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MRUS XGBoost: Starting GridSearchCV fit...
       ‚úÖ IAG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=61.8723 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.9s
    - LSTM: MSE=0.4048
    - TCN: MSE=0.2261
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2261
        ‚Ä¢ LSTM: MSE=0.4048
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.7682
        ‚Ä¢ XGBoost: MSE=61.8723
        ‚Ä¢ Random Forest: MSE=62.5406
   ‚úÖ IAG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IAG (TargetReturn): TCN with MSE=0.2261
üêõ DEBUG: IAG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IAG.
üêõ DEBUG: IAG - Moving model to CPU before return...
üêõ DEBUG [23:47:04.633]: IAG - Returning result metadata...
üêõ DEBUG [23:47:04.633]: Main received result for IAG
üêõ DEBUG: Training progress: 620/959 done
üêõ DEBUG: train_worker started for TXNM
  ‚öôÔ∏è Training models for TXNM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - TXNM: Initiating feature extraction for training.
  [DIAGNOSTIC] TXNM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TXNM: rows after features available: 126
üéØ TXNM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TXNM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TXNM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TXNM: Training LSTM (50 epochs)...
      ‚è≥ TXNM LSTM: Epoch 10/50 (20%)
      ‚è≥ TXNM LSTM: Epoch 20/50 (40%)
      ‚è≥ TXNM LSTM: Epoch 30/50 (60%)
      ‚è≥ TXNM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.146740
         RMSE: 0.383067
         R¬≤ Score: -0.3636 (Poor - 36.4% variance explained)
      üîπ TXNM: Training TCN (50 epochs)...
      ‚è≥ TXNM TCN: Epoch 10/50 (20%)
      ‚è≥ TXNM TCN: Epoch 20/50 (40%)
      ‚è≥ TXNM TCN: Epoch 30/50 (60%)
      ‚è≥ TXNM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.107092
         RMSE: 0.327250
         R¬≤ Score: 0.0049
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TXNM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TXNM Random Forest: Starting GridSearchCV fit...
       ‚úÖ TXNM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.2338 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TXNM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TXNM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.2180 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TXNM XGBoost: Starting GridSearchCV fit...
       ‚úÖ ELPC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=23.8622 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.1911
    - TCN: MSE=0.1202
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1202
        ‚Ä¢ LSTM: MSE=0.1911
        ‚Ä¢ XGBoost: MSE=23.8622
        ‚Ä¢ Random Forest: MSE=26.8851
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.9017
   ‚úÖ ELPC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ELPC (TargetReturn): TCN with MSE=0.1202
üêõ DEBUG: ELPC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ELPC.
üêõ DEBUG: ELPC - Moving model to CPU before return...
üêõ DEBUG [23:47:12.561]: ELPC - Returning result metadata...
üêõ DEBUG [23:47:12.561]: Main received result for ELPCüêõ DEBUG: train_worker started for XPP

  ‚öôÔ∏è Training models for XPP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - XPP: Initiating feature extraction for training.
  [DIAGNOSTIC] XPP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XPP: rows after features available: 126
üéØ XPP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XPP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XPP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XPP: Training LSTM (50 epochs)...
      ‚è≥ XPP LSTM: Epoch 10/50 (20%)
      ‚è≥ XPP LSTM: Epoch 20/50 (40%)
      ‚è≥ XPP LSTM: Epoch 30/50 (60%)
      ‚è≥ XPP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.313620
         RMSE: 0.560017
         R¬≤ Score: -0.6666 (Poor - 66.7% variance explained)
      üîπ XPP: Training TCN (50 epochs)...
      ‚è≥ XPP TCN: Epoch 10/50 (20%)
      ‚è≥ XPP TCN: Epoch 20/50 (40%)
      ‚è≥ XPP TCN: Epoch 30/50 (60%)
      ‚è≥ XPP TCN: Epoch 40/50 (80%)
       ‚úÖ LOUP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.3359 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.7s
    - LSTM: MSE=0.5295
    - TCN: MSE=0.6404
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5295
        ‚Ä¢ TCN: MSE=0.6404
        ‚Ä¢ Random Forest: MSE=18.1686
        ‚Ä¢ XGBoost: MSE=18.3359
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.9077
   ‚úÖ LOUP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LOUP (TargetReturn): LSTM with MSE=0.5295
üêõ DEBUG: LOUP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LOUP.
üêõ DEBUG: LOUP - Moving model to CPU before return...
üêõ DEBUG [23:47:14.935]: LOUP - Returning result metadata...
üêõ DEBUG [23:47:14.936]: Main received result for LOUP
üêõ DEBUG: train_worker started for EWP
      üìä TCN Regression Metrics:
         MSE: 0.219024
         RMSE: 0.468000
         R¬≤ Score: -0.1639
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XPP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XPP Random Forest: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for EWP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - EWP: Initiating feature extraction for training.
  [DIAGNOSTIC] EWP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWP: rows after features available: 126
üéØ EWP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWP: Training LSTM (50 epochs)...
      ‚è≥ EWP LSTM: Epoch 10/50 (20%)
      ‚è≥ EWP LSTM: Epoch 20/50 (40%)
      ‚è≥ EWP LSTM: Epoch 30/50 (60%)
      ‚è≥ EWP LSTM: Epoch 40/50 (80%)
       ‚úÖ XPP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=46.7069 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XPP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.177426
         RMSE: 0.421219
         R¬≤ Score: -1.4290 (Poor - 142.9% variance explained)
      üîπ EWP: Training TCN (50 epochs)...
      ‚è≥ EWP TCN: Epoch 10/50 (20%)
      ‚è≥ EWP TCN: Epoch 20/50 (40%)
      ‚è≥ EWP TCN: Epoch 30/50 (60%)
      ‚è≥ EWP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.078756
         RMSE: 0.280635
         R¬≤ Score: -0.0782
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWP Random Forest: Starting GridSearchCV fit...
       ‚úÖ XPP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=44.7952 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XPP XGBoost: Starting GridSearchCV fit...
       ‚úÖ EWP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.7593 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.0027 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWP XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.0287 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.7s
    - LSTM: MSE=0.1293
    - TCN: MSE=0.0887
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0887
        ‚Ä¢ LSTM: MSE=0.1293
        ‚Ä¢ XGBoost: MSE=6.0287
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.8568
        ‚Ä¢ Random Forest: MSE=7.7568
   ‚úÖ FDD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDD (TargetReturn): TCN with MSE=0.0887
üêõ DEBUG: FDD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDD.
üêõ DEBUG: FDD - Moving model to CPU before return...
üêõ DEBUG [23:47:24.743]: FDD - Returning result metadata...
üêõ DEBUG: train_worker started for WDC
  ‚öôÔ∏è Training models for WDC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - WDC: Initiating feature extraction for training.
  [DIAGNOSTIC] WDC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WDC: rows after features available: 126
üéØ WDC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WDC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WDC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WDC: Training LSTM (50 epochs)...
      ‚è≥ WDC LSTM: Epoch 10/50 (20%)
       ‚úÖ CFLT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.6685 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.3s
    - LSTM: MSE=0.4991
    - TCN: MSE=0.3890
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3890
        ‚Ä¢ LSTM: MSE=0.4991
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.9974
        ‚Ä¢ Random Forest: MSE=38.5733
        ‚Ä¢ XGBoost: MSE=44.6685
   ‚úÖ CFLT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CFLT (TargetReturn): TCN with MSE=0.3890
üêõ DEBUG: CFLT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CFLT.
üêõ DEBUG: CFLT - Moving model to CPU before return...
üêõ DEBUG [23:47:25.672]: CFLT - Returning result metadata...
üêõ DEBUG [23:47:25.672]: Main received result for CFLT
üêõ DEBUG [23:47:25.672]: Main received result for FDD
üêõ DEBUG: Training progress: 624/959 done
üêõ DEBUG: train_worker started for PAC
  ‚öôÔ∏è Training models for PAC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - PAC: Initiating feature extraction for training.
  [DIAGNOSTIC] PAC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAC: rows after features available: 126
üéØ PAC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAC: Training LSTM (50 epochs)...
      ‚è≥ WDC LSTM: Epoch 20/50 (40%)
      ‚è≥ PAC LSTM: Epoch 10/50 (20%)
      ‚è≥ WDC LSTM: Epoch 30/50 (60%)
      ‚è≥ PAC LSTM: Epoch 20/50 (40%)
      ‚è≥ WDC LSTM: Epoch 40/50 (80%)
      ‚è≥ PAC LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.493439
         RMSE: 0.702452
         R¬≤ Score: -0.8482 (Poor - 84.8% variance explained)
      üîπ WDC: Training TCN (50 epochs)...
      ‚è≥ WDC TCN: Epoch 10/50 (20%)
      ‚è≥ PAC LSTM: Epoch 40/50 (80%)
      ‚è≥ WDC TCN: Epoch 20/50 (40%)
      ‚è≥ WDC TCN: Epoch 30/50 (60%)
      ‚è≥ WDC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.428236
         RMSE: 0.654398
         R¬≤ Score: -0.6040
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WDC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WDC Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.336143
         RMSE: 0.579778
         R¬≤ Score: -1.2256 (Poor - 122.6% variance explained)
      üîπ PAC: Training TCN (50 epochs)...
      ‚è≥ PAC TCN: Epoch 10/50 (20%)
      ‚è≥ PAC TCN: Epoch 20/50 (40%)
      ‚è≥ PAC TCN: Epoch 30/50 (60%)
      ‚è≥ PAC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.214336
         RMSE: 0.462964
         R¬≤ Score: -0.4191
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAC Random Forest: Starting GridSearchCV fit...
       ‚úÖ WDC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.6872 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WDC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.5394 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WDC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=66.5716 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WDC XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=10.6517 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAC XGBoost: Starting GridSearchCV fit...
       ‚úÖ MEDP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=53.7355 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.6s
    - LSTM: MSE=0.2757
    - TCN: MSE=0.1721
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1721
        ‚Ä¢ LSTM: MSE=0.2757
        ‚Ä¢ Random Forest: MSE=25.0619
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.4337
        ‚Ä¢ XGBoost: MSE=53.7355
   ‚úÖ MEDP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MEDP (TargetReturn): TCN with MSE=0.1721
üêõ DEBUG: MEDP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MEDP.
üêõ DEBUG: MEDP - Moving model to CPU before return...
üêõ DEBUG [23:47:41.304]: MEDP - Returning result metadata...
üêõ DEBUG: train_worker started for T
üêõ DEBUG [23:47:41.304]: Main received result for MEDP
  ‚öôÔ∏è Training models for T (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - T: Initiating feature extraction for training.
  [DIAGNOSTIC] T: fetch_training_data - Initial data rows: 205
   ‚Ü≥ T: rows after features available: 126
üéØ T: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] T: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö T: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ T: Training LSTM (50 epochs)...
      ‚è≥ T LSTM: Epoch 10/50 (20%)
      ‚è≥ T LSTM: Epoch 20/50 (40%)
      ‚è≥ T LSTM: Epoch 30/50 (60%)
      ‚è≥ T LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.194639
         RMSE: 0.441179
         R¬≤ Score: -0.6088 (Poor - 60.9% variance explained)
      üîπ T: Training TCN (50 epochs)...
      ‚è≥ T TCN: Epoch 10/50 (20%)
      ‚è≥ T TCN: Epoch 20/50 (40%)
      ‚è≥ T TCN: Epoch 30/50 (60%)
      ‚è≥ T TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.167620
         RMSE: 0.409414
         R¬≤ Score: -0.3855
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä T: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ T Random Forest: Starting GridSearchCV fit...
       ‚úÖ T Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.6377 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ T LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ T LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=3.6023 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ T XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARLO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=83.0461 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.4901
    - TCN: MSE=0.5888
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4901
        ‚Ä¢ TCN: MSE=0.5888
        ‚Ä¢ Random Forest: MSE=43.0185
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.3029
        ‚Ä¢ XGBoost: MSE=83.0461
   ‚úÖ ARLO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARLO (TargetReturn): LSTM with MSE=0.4901
üêõ DEBUG: ARLO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARLO.
üêõ DEBUG: ARLO - Moving model to CPU before return...
üêõ DEBUG [23:47:57.610]: ARLO - Returning result metadata...
üêõ DEBUG [23:47:57.610]: Main received result for ARLO
üêõ DEBUG: train_worker started for ITA
  ‚öôÔ∏è Training models for ITA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - ITA: Initiating feature extraction for training.
  [DIAGNOSTIC] ITA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ITA: rows after features available: 126
üéØ ITA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ITA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ITA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ITA: Training LSTM (50 epochs)...
      ‚è≥ ITA LSTM: Epoch 10/50 (20%)
      ‚è≥ ITA LSTM: Epoch 20/50 (40%)
      ‚è≥ ITA LSTM: Epoch 30/50 (60%)
      ‚è≥ ITA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.464894
         RMSE: 0.681831
         R¬≤ Score: -1.0489 (Poor - 104.9% variance explained)
      üîπ ITA: Training TCN (50 epochs)...
      ‚è≥ ITA TCN: Epoch 10/50 (20%)
      ‚è≥ ITA TCN: Epoch 20/50 (40%)
      ‚è≥ ITA TCN: Epoch 30/50 (60%)
      ‚è≥ ITA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.259626
         RMSE: 0.509535
         R¬≤ Score: -0.1443
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ITA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ITA Random Forest: Starting GridSearchCV fit...
       ‚úÖ ITA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.3043 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ITA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ITA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.4614 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ITA XGBoost: Starting GridSearchCV fit...
       ‚úÖ BYD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=9.3384 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.6s
    - LSTM: MSE=0.6367
    - TCN: MSE=0.7642
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6367
        ‚Ä¢ TCN: MSE=0.7642
        ‚Ä¢ XGBoost: MSE=9.3384
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5521
        ‚Ä¢ Random Forest: MSE=10.3563
   ‚úÖ BYD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BYD (TargetReturn): LSTM with MSE=0.6367
üêõ DEBUG: BYD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BYD.
üêõ DEBUG: BYD - Moving model to CPU before return...
üêõ DEBUG [23:48:04.451]: BYD - Returning result metadata...
üêõ DEBUG: train_worker started for QUAD
  ‚öôÔ∏è Training models for QUAD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - QUAD: Initiating feature extraction for training.
  [DIAGNOSTIC] QUAD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QUAD: rows after features available: 126
üéØ QUAD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QUAD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QUAD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QUAD: Training LSTM (50 epochs)...
      ‚è≥ QUAD LSTM: Epoch 10/50 (20%)
      ‚è≥ QUAD LSTM: Epoch 20/50 (40%)
       ‚úÖ COCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.3832 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.6122
    - TCN: MSE=0.5139
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5139
        ‚Ä¢ LSTM: MSE=0.6122
        ‚Ä¢ Random Forest: MSE=15.4323
        ‚Ä¢ XGBoost: MSE=17.3832
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.2286
   ‚úÖ COCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COCO (TargetReturn): TCN with MSE=0.5139
üêõ DEBUG: COCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COCO.
üêõ DEBUG: COCO - Moving model to CPU before return...
üêõ DEBUG [23:48:05.900]: COCO - Returning result metadata...
üêõ DEBUG [23:48:05.900]: Main received result for COCOüêõ DEBUG: train_worker started for LENZ

üêõ DEBUG [23:48:05.901]: Main received result for BYD
üêõ DEBUG: Training progress: 628/959 done
  ‚öôÔ∏è Training models for LENZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - LENZ: Initiating feature extraction for training.
  [DIAGNOSTIC] LENZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LENZ: rows after features available: 126
üéØ LENZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LENZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LENZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LENZ: Training LSTM (50 epochs)...
      ‚è≥ QUAD LSTM: Epoch 30/50 (60%)
      ‚è≥ QUAD LSTM: Epoch 40/50 (80%)
      ‚è≥ LENZ LSTM: Epoch 10/50 (20%)
      ‚è≥ LENZ LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.348279
         RMSE: 0.590151
         R¬≤ Score: -1.3383 (Poor - 133.8% variance explained)
      üîπ QUAD: Training TCN (50 epochs)...
      ‚è≥ QUAD TCN: Epoch 10/50 (20%)
      ‚è≥ QUAD TCN: Epoch 20/50 (40%)
      ‚è≥ QUAD TCN: Epoch 30/50 (60%)
      ‚è≥ QUAD TCN: Epoch 40/50 (80%)
      ‚è≥ LENZ LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.271188
         RMSE: 0.520757
         R¬≤ Score: -0.8207
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QUAD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QUAD Random Forest: Starting GridSearchCV fit...
      ‚è≥ LENZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.188754
         RMSE: 0.434458
         R¬≤ Score: -1.5255 (Poor - 152.5% variance explained)
      üîπ LENZ: Training TCN (50 epochs)...
      ‚è≥ LENZ TCN: Epoch 10/50 (20%)
      ‚è≥ LENZ TCN: Epoch 20/50 (40%)
      ‚è≥ LENZ TCN: Epoch 30/50 (60%)
      ‚è≥ LENZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.102472
         RMSE: 0.320112
         R¬≤ Score: -0.3710
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LENZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LENZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ QUAD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.1718 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QUAD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QUAD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.0731 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QUAD XGBoost: Starting GridSearchCV fit...
       ‚úÖ LENZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.2433 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LENZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LENZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.4705 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LENZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ AEIS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.9031 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.4769
    - TCN: MSE=0.4765
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4765
        ‚Ä¢ LSTM: MSE=0.4769
        ‚Ä¢ Random Forest: MSE=24.0212
        ‚Ä¢ XGBoost: MSE=25.9031
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.5519
   ‚úÖ AEIS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AEIS (TargetReturn): TCN with MSE=0.4765
üêõ DEBUG: AEIS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AEIS.
üêõ DEBUG: AEIS - Moving model to CPU before return...
üêõ DEBUG [23:48:22.120]: AEIS - Returning result metadata...
üêõ DEBUG [23:48:22.120]: Main received result for AEIS
üêõ DEBUG: train_worker started for PRLB
  ‚öôÔ∏è Training models for PRLB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - PRLB: Initiating feature extraction for training.
  [DIAGNOSTIC] PRLB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRLB: rows after features available: 126
üéØ PRLB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRLB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRLB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRLB: Training LSTM (50 epochs)...
      ‚è≥ PRLB LSTM: Epoch 10/50 (20%)
      ‚è≥ PRLB LSTM: Epoch 20/50 (40%)
      ‚è≥ PRLB LSTM: Epoch 30/50 (60%)
      ‚è≥ PRLB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.199745
         RMSE: 0.446928
         R¬≤ Score: -0.6759 (Poor - 67.6% variance explained)
      üîπ PRLB: Training TCN (50 epochs)...
      ‚è≥ PRLB TCN: Epoch 10/50 (20%)
      ‚è≥ PRLB TCN: Epoch 20/50 (40%)
      ‚è≥ PRLB TCN: Epoch 30/50 (60%)
      ‚è≥ PRLB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.166308
         RMSE: 0.407808
         R¬≤ Score: -0.3953
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRLB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRLB Random Forest: Starting GridSearchCV fit...
       ‚úÖ PRLB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.3652 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRLB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PRLB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=23.6838 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRLB XGBoost: Starting GridSearchCV fit...
       ‚úÖ AZZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.8647 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.4815
    - TCN: MSE=0.2938
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2938
        ‚Ä¢ LSTM: MSE=0.4815
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.7188
        ‚Ä¢ Random Forest: MSE=20.1540
        ‚Ä¢ XGBoost: MSE=20.8647
   ‚úÖ AZZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AZZ (TargetReturn): TCN with MSE=0.2938
üêõ DEBUG: AZZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AZZ.
üêõ DEBUG: AZZ - Moving model to CPU before return...
üêõ DEBUG [23:48:31.880]: AZZ - Returning result metadata...
üêõ DEBUG [23:48:31.880]: Main received result for AZZ
üêõ DEBUG: train_worker started for SHAK
  ‚öôÔ∏è Training models for SHAK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - SHAK: Initiating feature extraction for training.
  [DIAGNOSTIC] SHAK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SHAK: rows after features available: 126
üéØ SHAK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SHAK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SHAK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SHAK: Training LSTM (50 epochs)...
      ‚è≥ SHAK LSTM: Epoch 10/50 (20%)
      ‚è≥ SHAK LSTM: Epoch 20/50 (40%)
      ‚è≥ SHAK LSTM: Epoch 30/50 (60%)
      ‚è≥ SHAK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.583609
         RMSE: 0.763943
         R¬≤ Score: -1.1802 (Poor - 118.0% variance explained)
      üîπ SHAK: Training TCN (50 epochs)...
      ‚è≥ SHAK TCN: Epoch 10/50 (20%)
      ‚è≥ SHAK TCN: Epoch 20/50 (40%)
      ‚è≥ SHAK TCN: Epoch 30/50 (60%)
      ‚è≥ SHAK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.688458
         RMSE: 0.829734
         R¬≤ Score: -1.5719
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SHAK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SHAK Random Forest: Starting GridSearchCV fit...
       ‚úÖ ECNS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.0495 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.5299
    - TCN: MSE=0.6421
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5299
        ‚Ä¢ TCN: MSE=0.6421
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.0330
        ‚Ä¢ XGBoost: MSE=11.0495
        ‚Ä¢ Random Forest: MSE=11.4384
   ‚úÖ ECNS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ECNS (TargetReturn): LSTM with MSE=0.5299
üêõ DEBUG: ECNS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ECNS.
üêõ DEBUG: ECNS - Moving model to CPU before return...
üêõ DEBUG [23:48:36.298]: ECNS - Returning result metadata...
üêõ DEBUG [23:48:36.298]: Main received result for ECNS
üêõ DEBUG: train_worker started for FNGS
  ‚öôÔ∏è Training models for FNGS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - FNGS: Initiating feature extraction for training.
  [DIAGNOSTIC] FNGS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FNGS: rows after features available: 126
üéØ FNGS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FNGS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FNGS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FNGS: Training LSTM (50 epochs)...
      ‚è≥ FNGS LSTM: Epoch 10/50 (20%)
      ‚è≥ FNGS LSTM: Epoch 20/50 (40%)
       ‚úÖ SHAK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.0216 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SHAK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FNGS LSTM: Epoch 30/50 (60%)
      ‚è≥ FNGS LSTM: Epoch 40/50 (80%)
       ‚úÖ SHAK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=28.7264 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SHAK XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.505182
         RMSE: 0.710762
         R¬≤ Score: -1.0500 (Poor - 105.0% variance explained)
      üîπ FNGS: Training TCN (50 epochs)...
      ‚è≥ FNGS TCN: Epoch 10/50 (20%)
       ‚úÖ USFD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.3944 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.4828
    - TCN: MSE=0.6448
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4828
        ‚Ä¢ TCN: MSE=0.6448
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.1160
        ‚Ä¢ Random Forest: MSE=11.0914
        ‚Ä¢ XGBoost: MSE=18.3944
   ‚úÖ USFD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for USFD (TargetReturn): LSTM with MSE=0.4828
üêõ DEBUG: USFD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for USFD.
üêõ DEBUG: USFD - Moving model to CPU before return...
üêõ DEBUG [23:48:38.675]: USFD - Returning result metadata...
üêõ DEBUG [23:48:38.675]: Main received result for USFD
üêõ DEBUG: Training progress: 632/959 done
üêõ DEBUG: train_worker started for MTRX
  ‚öôÔ∏è Training models for MTRX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - MTRX: Initiating feature extraction for training.
  [DIAGNOSTIC] MTRX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MTRX: rows after features available: 126
üéØ MTRX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MTRX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MTRX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MTRX: Training LSTM (50 epochs)...
      ‚è≥ FNGS TCN: Epoch 20/50 (40%)
      ‚è≥ FNGS TCN: Epoch 30/50 (60%)
      ‚è≥ FNGS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.554386
         RMSE: 0.744571
         R¬≤ Score: -1.2497
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FNGS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FNGS Random Forest: Starting GridSearchCV fit...
      ‚è≥ MTRX LSTM: Epoch 10/50 (20%)
      ‚è≥ MTRX LSTM: Epoch 20/50 (40%)
      ‚è≥ MTRX LSTM: Epoch 30/50 (60%)
      ‚è≥ MTRX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.438752
         RMSE: 0.662384
         R¬≤ Score: -0.6383 (Poor - 63.8% variance explained)
      üîπ MTRX: Training TCN (50 epochs)...
      ‚è≥ MTRX TCN: Epoch 10/50 (20%)
      ‚è≥ MTRX TCN: Epoch 20/50 (40%)
      ‚è≥ MTRX TCN: Epoch 30/50 (60%)
      ‚è≥ MTRX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.460137
         RMSE: 0.678334
         R¬≤ Score: -0.7182
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MTRX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MTRX Random Forest: Starting GridSearchCV fit...
       ‚úÖ FNGS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6255 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FNGS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FNGS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.2752 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FNGS XGBoost: Starting GridSearchCV fit...
       ‚úÖ MTRX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.9288 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MTRX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MTRX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.3115 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MTRX XGBoost: Starting GridSearchCV fit...
       ‚úÖ OMF XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.2313 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 116.5s
    - LSTM: MSE=0.4374
    - TCN: MSE=0.3855
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3855
        ‚Ä¢ LSTM: MSE=0.4374
        ‚Ä¢ XGBoost: MSE=16.2313
        ‚Ä¢ Random Forest: MSE=16.6374
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.8226
   ‚úÖ OMF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OMF (TargetReturn): TCN with MSE=0.3855
üêõ DEBUG: OMF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OMF.
üêõ DEBUG: OMF - Moving model to CPU before return...
üêõ DEBUG [23:48:47.349]: OMF - Returning result metadata...
üêõ DEBUG: train_worker started for MCRI
üêõ DEBUG [23:48:47.351]: Main received result for OMF
  ‚öôÔ∏è Training models for MCRI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - MCRI: Initiating feature extraction for training.
  [DIAGNOSTIC] MCRI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MCRI: rows after features available: 126
üéØ MCRI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MCRI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MCRI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MCRI: Training LSTM (50 epochs)...
      ‚è≥ MCRI LSTM: Epoch 10/50 (20%)
      ‚è≥ MCRI LSTM: Epoch 20/50 (40%)
      ‚è≥ MCRI LSTM: Epoch 30/50 (60%)
      ‚è≥ MCRI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.645314
         RMSE: 0.803314
         R¬≤ Score: -1.0711 (Poor - 107.1% variance explained)
      üîπ MCRI: Training TCN (50 epochs)...
      ‚è≥ MCRI TCN: Epoch 10/50 (20%)
      ‚è≥ MCRI TCN: Epoch 20/50 (40%)
      ‚è≥ MCRI TCN: Epoch 30/50 (60%)
      ‚è≥ MCRI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.476103
         RMSE: 0.690002
         R¬≤ Score: -0.5280
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MCRI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MCRI Random Forest: Starting GridSearchCV fit...
       ‚úÖ MCRI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.0300 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MCRI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MCRI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.4246 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MCRI XGBoost: Starting GridSearchCV fit...
       ‚úÖ MRUS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=38.7173 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.4173
    - TCN: MSE=0.2752
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2752
        ‚Ä¢ LSTM: MSE=0.4173
        ‚Ä¢ Random Forest: MSE=34.0499
        ‚Ä¢ XGBoost: MSE=38.7173
        ‚Ä¢ LightGBM Regressor (CPU): MSE=56.2661
   ‚úÖ MRUS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MRUS (TargetReturn): TCN with MSE=0.2752
üêõ DEBUG: MRUS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MRUS.
üêõ DEBUG: MRUS - Moving model to CPU before return...
üêõ DEBUG [23:48:59.134]: MRUS - Returning result metadata...
üêõ DEBUG [23:48:59.134]: Main received result for MRUS
üêõ DEBUG: train_worker started for STVN
  ‚öôÔ∏è Training models for STVN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - STVN: Initiating feature extraction for training.
  [DIAGNOSTIC] STVN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STVN: rows after features available: 126
üéØ STVN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STVN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STVN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STVN: Training LSTM (50 epochs)...
      ‚è≥ STVN LSTM: Epoch 10/50 (20%)
      ‚è≥ STVN LSTM: Epoch 20/50 (40%)
      ‚è≥ STVN LSTM: Epoch 30/50 (60%)
      ‚è≥ STVN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.192573
         RMSE: 0.438831
         R¬≤ Score: -0.4182 (Poor - 41.8% variance explained)
      üîπ STVN: Training TCN (50 epochs)...
      ‚è≥ STVN TCN: Epoch 10/50 (20%)
      ‚è≥ STVN TCN: Epoch 20/50 (40%)
      ‚è≥ STVN TCN: Epoch 30/50 (60%)
      ‚è≥ STVN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.147848
         RMSE: 0.384509
         R¬≤ Score: -0.0888
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STVN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STVN Random Forest: Starting GridSearchCV fit...
       ‚úÖ STVN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.7335 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STVN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ STVN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.6490 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STVN XGBoost: Starting GridSearchCV fit...
       ‚úÖ TXNM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3.2029 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.1467
    - TCN: MSE=0.1071
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1071
        ‚Ä¢ LSTM: MSE=0.1467
        ‚Ä¢ XGBoost: MSE=3.2029
        ‚Ä¢ Random Forest: MSE=3.2338
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.2180
   ‚úÖ TXNM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TXNM (TargetReturn): TCN with MSE=0.1071
üêõ DEBUG: TXNM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TXNM.
üêõ DEBUG: TXNM - Moving model to CPU before return...
üêõ DEBUG [23:49:11.301]: TXNM - Returning result metadata...
üêõ DEBUG [23:49:11.301]: Main received result for TXNM
üêõ DEBUG: train_worker started for CNP
  ‚öôÔ∏è Training models for CNP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - CNP: Initiating feature extraction for training.
  [DIAGNOSTIC] CNP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CNP: rows after features available: 126
üéØ CNP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CNP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CNP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CNP: Training LSTM (50 epochs)...
      ‚è≥ CNP LSTM: Epoch 10/50 (20%)
      ‚è≥ CNP LSTM: Epoch 20/50 (40%)
      ‚è≥ CNP LSTM: Epoch 30/50 (60%)
      ‚è≥ CNP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.807942
         RMSE: 0.898856
         R¬≤ Score: -1.0947 (Poor - 109.5% variance explained)
      üîπ CNP: Training TCN (50 epochs)...
      ‚è≥ CNP TCN: Epoch 10/50 (20%)
      ‚è≥ CNP TCN: Epoch 20/50 (40%)
      ‚è≥ CNP TCN: Epoch 30/50 (60%)
      ‚è≥ CNP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.854507
         RMSE: 0.924395
         R¬≤ Score: -1.2154
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CNP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CNP Random Forest: Starting GridSearchCV fit...
       ‚úÖ CNP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2.3906 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CNP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CNP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=2.6275 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.0s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CNP XGBoost: Starting GridSearchCV fit...
       ‚úÖ XPP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=49.1606 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.3136
    - TCN: MSE=0.2190
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2190
        ‚Ä¢ LSTM: MSE=0.3136
        ‚Ä¢ LightGBM Regressor (CPU): MSE=44.7952
        ‚Ä¢ Random Forest: MSE=46.7069
        ‚Ä¢ XGBoost: MSE=49.1606
   ‚úÖ XPP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XPP (TargetReturn): TCN with MSE=0.2190
üêõ DEBUG: XPP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XPP.
üêõ DEBUG: XPP - Moving model to CPU before return...
üêõ DEBUG [23:49:20.110]: XPP - Returning result metadata...
üêõ DEBUG: train_worker started for MIRM
üêõ DEBUG [23:49:20.111]: Main received result for XPP
üêõ DEBUG: Training progress: 636/959 done
  ‚öôÔ∏è Training models for MIRM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - MIRM: Initiating feature extraction for training.
  [DIAGNOSTIC] MIRM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MIRM: rows after features available: 126
üéØ MIRM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MIRM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MIRM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MIRM: Training LSTM (50 epochs)...
      ‚è≥ MIRM LSTM: Epoch 10/50 (20%)
      ‚è≥ MIRM LSTM: Epoch 20/50 (40%)
       ‚úÖ EWP XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.2131 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.1774
    - TCN: MSE=0.0788
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0788
        ‚Ä¢ LSTM: MSE=0.1774
        ‚Ä¢ XGBoost: MSE=5.2131
        ‚Ä¢ Random Forest: MSE=5.7593
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.0027
   ‚úÖ EWP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EWP (TargetReturn): TCN with MSE=0.0788
üêõ DEBUG: EWP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWP.
üêõ DEBUG: EWP - Moving model to CPU before return...
üêõ DEBUG [23:49:21.106]: EWP - Returning result metadata...
üêõ DEBUG [23:49:21.106]: Main received result for EWP
üêõ DEBUG: train_worker started for CUBI
  ‚öôÔ∏è Training models for CUBI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CUBI: Initiating feature extraction for training.
  [DIAGNOSTIC] CUBI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CUBI: rows after features available: 126
üéØ CUBI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CUBI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CUBI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CUBI: Training LSTM (50 epochs)...
      ‚è≥ MIRM LSTM: Epoch 30/50 (60%)
      ‚è≥ CUBI LSTM: Epoch 10/50 (20%)
      ‚è≥ MIRM LSTM: Epoch 40/50 (80%)
      ‚è≥ CUBI LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.644338
         RMSE: 0.802706
         R¬≤ Score: -0.7968 (Poor - 79.7% variance explained)
      üîπ MIRM: Training TCN (50 epochs)...
      ‚è≥ MIRM TCN: Epoch 10/50 (20%)
      ‚è≥ MIRM TCN: Epoch 20/50 (40%)
      ‚è≥ CUBI LSTM: Epoch 30/50 (60%)
      ‚è≥ MIRM TCN: Epoch 30/50 (60%)
      ‚è≥ MIRM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.800463
         RMSE: 0.894686
         R¬≤ Score: -1.2322
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MIRM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MIRM Random Forest: Starting GridSearchCV fit...
      ‚è≥ CUBI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.555453
         RMSE: 0.745287
         R¬≤ Score: -0.7524 (Poor - 75.2% variance explained)
      üîπ CUBI: Training TCN (50 epochs)...
      ‚è≥ CUBI TCN: Epoch 10/50 (20%)
      ‚è≥ CUBI TCN: Epoch 20/50 (40%)
      ‚è≥ CUBI TCN: Epoch 30/50 (60%)
      ‚è≥ CUBI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.371164
         RMSE: 0.609232
         R¬≤ Score: -0.1710
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CUBI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CUBI Random Forest: Starting GridSearchCV fit...
       ‚úÖ MIRM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.8662 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MIRM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MIRM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.2476 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MIRM XGBoost: Starting GridSearchCV fit...
       ‚úÖ CUBI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=51.8758 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CUBI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CUBI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.0765 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CUBI XGBoost: Starting GridSearchCV fit...
       ‚úÖ WDC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.4340 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.4934
    - TCN: MSE=0.4282
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4282
        ‚Ä¢ LSTM: MSE=0.4934
        ‚Ä¢ Random Forest: MSE=32.6872
        ‚Ä¢ XGBoost: MSE=46.4340
        ‚Ä¢ LightGBM Regressor (CPU): MSE=66.5716
   ‚úÖ WDC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WDC (TargetReturn): TCN with MSE=0.4282
üêõ DEBUG: WDC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WDC.
üêõ DEBUG: WDC - Moving model to CPU before return...
üêõ DEBUG [23:49:28.496]: WDC - Returning result metadata...
üêõ DEBUG: train_worker started for DXCM
üêõ DEBUG [23:49:28.497]: Main received result for WDC
  ‚öôÔ∏è Training models for DXCM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DXCM: Initiating feature extraction for training.
  [DIAGNOSTIC] DXCM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DXCM: rows after features available: 126
üéØ DXCM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DXCM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DXCM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DXCM: Training LSTM (50 epochs)...
      ‚è≥ DXCM LSTM: Epoch 10/50 (20%)
      ‚è≥ DXCM LSTM: Epoch 20/50 (40%)
      ‚è≥ DXCM LSTM: Epoch 30/50 (60%)
      ‚è≥ DXCM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.578121
         RMSE: 0.760342
         R¬≤ Score: -1.1251 (Poor - 112.5% variance explained)
      üîπ DXCM: Training TCN (50 epochs)...
      ‚è≥ DXCM TCN: Epoch 10/50 (20%)
      ‚è≥ DXCM TCN: Epoch 20/50 (40%)
      ‚è≥ DXCM TCN: Epoch 30/50 (60%)
      ‚è≥ DXCM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.322222
         RMSE: 0.567646
         R¬≤ Score: -0.1844
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DXCM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DXCM Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.5727 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.3361
    - TCN: MSE=0.2143
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2143
        ‚Ä¢ LSTM: MSE=0.3361
        ‚Ä¢ Random Forest: MSE=10.5394
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.6517
        ‚Ä¢ XGBoost: MSE=15.5727
   ‚úÖ PAC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAC (TargetReturn): TCN with MSE=0.2143
üêõ DEBUG: PAC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAC.
üêõ DEBUG: PAC - Moving model to CPU before return...
üêõ DEBUG [23:49:33.966]: PAC - Returning result metadata...
üêõ DEBUG [23:49:33.967]: Main received result for PAC
üêõ DEBUG: train_worker started for RAMP
  ‚öôÔ∏è Training models for RAMP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - RAMP: Initiating feature extraction for training.
  [DIAGNOSTIC] RAMP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RAMP: rows after features available: 126
üéØ RAMP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RAMP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RAMP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RAMP: Training LSTM (50 epochs)...
       ‚úÖ DXCM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.7998 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DXCM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ RAMP LSTM: Epoch 10/50 (20%)
      ‚è≥ RAMP LSTM: Epoch 20/50 (40%)
       ‚úÖ DXCM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.4611 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DXCM XGBoost: Starting GridSearchCV fit...
      ‚è≥ RAMP LSTM: Epoch 30/50 (60%)
      ‚è≥ RAMP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.777178
         RMSE: 0.881577
         R¬≤ Score: -1.0269 (Poor - 102.7% variance explained)
      üîπ RAMP: Training TCN (50 epochs)...
      ‚è≥ RAMP TCN: Epoch 10/50 (20%)
      ‚è≥ RAMP TCN: Epoch 20/50 (40%)
      ‚è≥ RAMP TCN: Epoch 30/50 (60%)
      ‚è≥ RAMP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.791278
         RMSE: 0.889538
         R¬≤ Score: -1.0637
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RAMP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RAMP Random Forest: Starting GridSearchCV fit...
       ‚úÖ RAMP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.0513 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RAMP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RAMP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.5826 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RAMP XGBoost: Starting GridSearchCV fit...
       ‚úÖ T XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.2135 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.1946
    - TCN: MSE=0.1676
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1676
        ‚Ä¢ LSTM: MSE=0.1946
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.6023
        ‚Ä¢ Random Forest: MSE=3.6377
        ‚Ä¢ XGBoost: MSE=4.2135
   ‚úÖ T: Phase 3/3 - Model selection complete!
  üèÜ WINNER for T (TargetReturn): TCN with MSE=0.1676
üêõ DEBUG: T - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for T.
üêõ DEBUG: T - Moving model to CPU before return...
üêõ DEBUG [23:49:43.939]: T - Returning result metadata...
üêõ DEBUG [23:49:43.939]: Main received result for T
üêõ DEBUG: Training progress: 640/959 done
üêõ DEBUG: train_worker started for FWONK
  ‚öôÔ∏è Training models for FWONK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - FWONK: Initiating feature extraction for training.
  [DIAGNOSTIC] FWONK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FWONK: rows after features available: 126
üéØ FWONK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FWONK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FWONK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FWONK: Training LSTM (50 epochs)...
      ‚è≥ FWONK LSTM: Epoch 10/50 (20%)
      ‚è≥ FWONK LSTM: Epoch 20/50 (40%)
      ‚è≥ FWONK LSTM: Epoch 30/50 (60%)
      ‚è≥ FWONK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.510674
         RMSE: 0.714615
         R¬≤ Score: -1.0577 (Poor - 105.8% variance explained)
      üîπ FWONK: Training TCN (50 epochs)...
      ‚è≥ FWONK TCN: Epoch 10/50 (20%)
      ‚è≥ FWONK TCN: Epoch 20/50 (40%)
      ‚è≥ FWONK TCN: Epoch 30/50 (60%)
      ‚è≥ FWONK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.475023
         RMSE: 0.689219
         R¬≤ Score: -0.9140
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FWONK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FWONK Random Forest: Starting GridSearchCV fit...
       ‚úÖ FWONK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7972 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FWONK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FWONK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.1728 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FWONK XGBoost: Starting GridSearchCV fit...
       ‚úÖ ITA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.4279 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.4649
    - TCN: MSE=0.2596
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2596
        ‚Ä¢ LSTM: MSE=0.4649
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.4614
        ‚Ä¢ Random Forest: MSE=13.3043
        ‚Ä¢ XGBoost: MSE=13.4279
   ‚úÖ ITA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ITA (TargetReturn): TCN with MSE=0.2596
üêõ DEBUG: ITA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ITA.
üêõ DEBUG: ITA - Moving model to CPU before return...
üêõ DEBUG [23:50:03.909]: ITA - Returning result metadata...
üêõ DEBUG [23:50:03.909]: Main received result for ITA
üêõ DEBUG: train_worker started for CSCO
  ‚öôÔ∏è Training models for CSCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - CSCO: Initiating feature extraction for training.
  [DIAGNOSTIC] CSCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CSCO: rows after features available: 126
üéØ CSCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CSCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CSCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CSCO: Training LSTM (50 epochs)...
      ‚è≥ CSCO LSTM: Epoch 10/50 (20%)
      ‚è≥ CSCO LSTM: Epoch 20/50 (40%)
      ‚è≥ CSCO LSTM: Epoch 30/50 (60%)
      ‚è≥ CSCO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.501395
         RMSE: 0.708092
         R¬≤ Score: -0.9079 (Poor - 90.8% variance explained)
      üîπ CSCO: Training TCN (50 epochs)...
      ‚è≥ CSCO TCN: Epoch 10/50 (20%)
      ‚è≥ CSCO TCN: Epoch 20/50 (40%)
      ‚è≥ CSCO TCN: Epoch 30/50 (60%)
      ‚è≥ CSCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.512154
         RMSE: 0.715649
         R¬≤ Score: -0.9488
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CSCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CSCO Random Forest: Starting GridSearchCV fit...
       ‚úÖ QUAD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.3679 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.3483
    - TCN: MSE=0.2712
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2712
        ‚Ä¢ LSTM: MSE=0.3483
        ‚Ä¢ Random Forest: MSE=21.1718
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.0731
        ‚Ä¢ XGBoost: MSE=24.3679
   ‚úÖ QUAD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QUAD (TargetReturn): TCN with MSE=0.2712
üêõ DEBUG: QUAD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QUAD.
üêõ DEBUG: QUAD - Moving model to CPU before return...
üêõ DEBUG [23:50:08.497]: QUAD - Returning result metadata...
üêõ DEBUG: train_worker started for CTRI
üêõ DEBUG [23:50:08.498]: Main received result for QUAD
  ‚öôÔ∏è Training models for CTRI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - CTRI: Initiating feature extraction for training.
  [DIAGNOSTIC] CTRI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTRI: rows after features available: 126
üéØ CTRI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTRI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTRI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTRI: Training LSTM (50 epochs)...
      ‚è≥ CTRI LSTM: Epoch 10/50 (20%)
      ‚è≥ CTRI LSTM: Epoch 20/50 (40%)
       ‚úÖ LENZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=39.8398 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.6s
    - LSTM: MSE=0.1888
    - TCN: MSE=0.1025
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1025
        ‚Ä¢ LSTM: MSE=0.1888
        ‚Ä¢ Random Forest: MSE=32.2433
        ‚Ä¢ XGBoost: MSE=39.8398
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.4705
   ‚úÖ LENZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LENZ (TargetReturn): TCN with MSE=0.1025
üêõ DEBUG: LENZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LENZ.
üêõ DEBUG: LENZ - Moving model to CPU before return...
üêõ DEBUG [23:50:09.815]: LENZ - Returning result metadata...
üêõ DEBUG [23:50:09.815]: Main received result for LENZ
üêõ DEBUG: train_worker started for RPRX
  ‚öôÔ∏è Training models for RPRX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - RPRX: Initiating feature extraction for training.
  [DIAGNOSTIC] RPRX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RPRX: rows after features available: 126
üéØ RPRX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RPRX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RPRX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RPRX: Training LSTM (50 epochs)...
      ‚è≥ CTRI LSTM: Epoch 30/50 (60%)
       ‚úÖ CSCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.4311 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CSCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ RPRX LSTM: Epoch 10/50 (20%)
      ‚è≥ CTRI LSTM: Epoch 40/50 (80%)
       ‚úÖ CSCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.1896 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CSCO XGBoost: Starting GridSearchCV fit...
      ‚è≥ RPRX LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.530198
         RMSE: 0.728147
         R¬≤ Score: -1.0457 (Poor - 104.6% variance explained)
      üîπ CTRI: Training TCN (50 epochs)...
      ‚è≥ CTRI TCN: Epoch 10/50 (20%)
      ‚è≥ CTRI TCN: Epoch 20/50 (40%)
      ‚è≥ CTRI TCN: Epoch 30/50 (60%)
      ‚è≥ RPRX LSTM: Epoch 30/50 (60%)
      ‚è≥ CTRI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.273027
         RMSE: 0.522520
         R¬≤ Score: -0.0534
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTRI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTRI Random Forest: Starting GridSearchCV fit...
      ‚è≥ RPRX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.147762
         RMSE: 0.384399
         R¬≤ Score: -0.7959 (Poor - 79.6% variance explained)
      üîπ RPRX: Training TCN (50 epochs)...
      ‚è≥ RPRX TCN: Epoch 10/50 (20%)
      ‚è≥ RPRX TCN: Epoch 20/50 (40%)
      ‚è≥ RPRX TCN: Epoch 30/50 (60%)
      ‚è≥ RPRX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.089977
         RMSE: 0.299961
         R¬≤ Score: -0.0936
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RPRX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RPRX Random Forest: Starting GridSearchCV fit...
       ‚úÖ CTRI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.0710 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTRI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CTRI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.4715 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTRI XGBoost: Starting GridSearchCV fit...
       ‚úÖ RPRX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.1903 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RPRX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RPRX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.3815 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RPRX XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRLB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.9631 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.1997
    - TCN: MSE=0.1663
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1663
        ‚Ä¢ LSTM: MSE=0.1997
        ‚Ä¢ Random Forest: MSE=15.3652
        ‚Ä¢ XGBoost: MSE=19.9631
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.6838
   ‚úÖ PRLB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRLB (TargetReturn): TCN with MSE=0.1663
üêõ DEBUG: PRLB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRLB.
üêõ DEBUG: PRLB - Moving model to CPU before return...
üêõ DEBUG [23:50:26.069]: PRLB - Returning result metadata...
üêõ DEBUG [23:50:26.069]: Main received result for PRLB
üêõ DEBUG: Training progress: 644/959 done
üêõ DEBUG: train_worker started for COLO
  ‚öôÔ∏è Training models for COLO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - COLO: Initiating feature extraction for training.
  [DIAGNOSTIC] COLO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COLO: rows after features available: 126
üéØ COLO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COLO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COLO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COLO: Training LSTM (50 epochs)...
      ‚è≥ COLO LSTM: Epoch 10/50 (20%)
      ‚è≥ COLO LSTM: Epoch 20/50 (40%)
      ‚è≥ COLO LSTM: Epoch 30/50 (60%)
      ‚è≥ COLO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.268914
         RMSE: 0.518569
         R¬≤ Score: -0.6601 (Poor - 66.0% variance explained)
      üîπ COLO: Training TCN (50 epochs)...
      ‚è≥ COLO TCN: Epoch 10/50 (20%)
      ‚è≥ COLO TCN: Epoch 20/50 (40%)
      ‚è≥ COLO TCN: Epoch 30/50 (60%)
      ‚è≥ COLO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.213724
         RMSE: 0.462303
         R¬≤ Score: -0.3194
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COLO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COLO Random Forest: Starting GridSearchCV fit...
       ‚úÖ COLO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.2386 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COLO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ COLO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=3.8150 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COLO XGBoost: Starting GridSearchCV fit...
       ‚úÖ SHAK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.3575 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.8s
    - LSTM: MSE=0.5836
    - TCN: MSE=0.6885
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5836
        ‚Ä¢ TCN: MSE=0.6885
        ‚Ä¢ Random Forest: MSE=20.0216
        ‚Ä¢ XGBoost: MSE=22.3575
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.7264
   ‚úÖ SHAK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SHAK (TargetReturn): LSTM with MSE=0.5836
üêõ DEBUG: SHAK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SHAK.
üêõ DEBUG: SHAK - Moving model to CPU before return...
üêõ DEBUG [23:50:34.945]: SHAK - Returning result metadata...
üêõ DEBUG: train_worker started for ULTA
üêõ DEBUG [23:50:34.946]: Main received result for SHAK
  ‚öôÔ∏è Training models for ULTA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - ULTA: Initiating feature extraction for training.
  [DIAGNOSTIC] ULTA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ULTA: rows after features available: 126
üéØ ULTA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ULTA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ULTA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ULTA: Training LSTM (50 epochs)...
      ‚è≥ ULTA LSTM: Epoch 10/50 (20%)
      ‚è≥ ULTA LSTM: Epoch 20/50 (40%)
      ‚è≥ ULTA LSTM: Epoch 30/50 (60%)
      ‚è≥ ULTA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.331709
         RMSE: 0.575942
         R¬≤ Score: -0.9950 (Poor - 99.5% variance explained)
      üîπ ULTA: Training TCN (50 epochs)...
      ‚è≥ ULTA TCN: Epoch 10/50 (20%)
      ‚è≥ ULTA TCN: Epoch 20/50 (40%)
      ‚è≥ ULTA TCN: Epoch 30/50 (60%)
      ‚è≥ ULTA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.191472
         RMSE: 0.437575
         R¬≤ Score: -0.1516
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ULTA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ULTA Random Forest: Starting GridSearchCV fit...
       ‚úÖ ULTA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.4283 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ULTA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ULTA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.5134 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ULTA XGBoost: Starting GridSearchCV fit...
       ‚úÖ FNGS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.9367 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.5052
    - TCN: MSE=0.5544
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5052
        ‚Ä¢ TCN: MSE=0.5544
        ‚Ä¢ Random Forest: MSE=11.6255
        ‚Ä¢ XGBoost: MSE=11.9367
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.2752
   ‚úÖ FNGS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FNGS (TargetReturn): LSTM with MSE=0.5052
üêõ DEBUG: FNGS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FNGS.
üêõ DEBUG: FNGS - Moving model to CPU before return...
üêõ DEBUG [23:50:42.262]: FNGS - Returning result metadata...
üêõ DEBUG [23:50:42.262]: Main received result for FNGS
üêõ DEBUG: train_worker started for FNV
  ‚öôÔ∏è Training models for FNV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - FNV: Initiating feature extraction for training.
  [DIAGNOSTIC] FNV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FNV: rows after features available: 126
üéØ FNV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FNV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FNV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FNV: Training LSTM (50 epochs)...
      ‚è≥ FNV LSTM: Epoch 10/50 (20%)
      ‚è≥ FNV LSTM: Epoch 20/50 (40%)
      ‚è≥ FNV LSTM: Epoch 30/50 (60%)
      ‚è≥ FNV LSTM: Epoch 40/50 (80%)
       ‚úÖ MTRX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.2982 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.4388
    - TCN: MSE=0.4601
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4388
        ‚Ä¢ TCN: MSE=0.4601
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3115
        ‚Ä¢ Random Forest: MSE=31.9288
        ‚Ä¢ XGBoost: MSE=33.2982
   ‚úÖ MTRX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MTRX (TargetReturn): LSTM with MSE=0.4388
üêõ DEBUG: MTRX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MTRX.
üêõ DEBUG: MTRX - Moving model to CPU before return...
üêõ DEBUG [23:50:44.236]: MTRX - Returning result metadata...
üêõ DEBUG [23:50:44.237]: Main received result for MTRX
üêõ DEBUG: train_worker started for GSAT
  ‚öôÔ∏è Training models for GSAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - GSAT: Initiating feature extraction for training.
  [DIAGNOSTIC] GSAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GSAT: rows after features available: 126
üéØ GSAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GSAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GSAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GSAT: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.416365
         RMSE: 0.645264
         R¬≤ Score: -0.7968 (Poor - 79.7% variance explained)
      üîπ FNV: Training TCN (50 epochs)...
      ‚è≥ FNV TCN: Epoch 10/50 (20%)
      ‚è≥ FNV TCN: Epoch 20/50 (40%)
      ‚è≥ GSAT LSTM: Epoch 10/50 (20%)
      ‚è≥ FNV TCN: Epoch 30/50 (60%)
      ‚è≥ FNV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.404656
         RMSE: 0.636126
         R¬≤ Score: -0.7463
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FNV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FNV Random Forest: Starting GridSearchCV fit...
      ‚è≥ GSAT LSTM: Epoch 20/50 (40%)
      ‚è≥ GSAT LSTM: Epoch 30/50 (60%)
      ‚è≥ GSAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.270326
         RMSE: 0.519929
         R¬≤ Score: -1.3671 (Poor - 136.7% variance explained)
      üîπ GSAT: Training TCN (50 epochs)...
      ‚è≥ GSAT TCN: Epoch 10/50 (20%)
      ‚è≥ GSAT TCN: Epoch 20/50 (40%)
      ‚è≥ GSAT TCN: Epoch 30/50 (60%)
      ‚è≥ GSAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.224146
         RMSE: 0.473441
         R¬≤ Score: -0.9627
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GSAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GSAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ FNV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.6830 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FNV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FNV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.3064 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FNV XGBoost: Starting GridSearchCV fit...
       ‚úÖ GSAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.5048 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GSAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GSAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=48.2670 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GSAT XGBoost: Starting GridSearchCV fit...
       ‚úÖ MCRI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.4344 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.6453
    - TCN: MSE=0.4761
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4761
        ‚Ä¢ LSTM: MSE=0.6453
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.4246
        ‚Ä¢ Random Forest: MSE=19.0300
        ‚Ä¢ XGBoost: MSE=27.4344
   ‚úÖ MCRI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MCRI (TargetReturn): TCN with MSE=0.4761
üêõ DEBUG: MCRI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MCRI.
üêõ DEBUG: MCRI - Moving model to CPU before return...
üêõ DEBUG [23:50:52.148]: MCRI - Returning result metadata...
üêõ DEBUG: train_worker started for SKYW
üêõ DEBUG [23:50:52.148]: Main received result for MCRI
üêõ DEBUG: Training progress: 648/959 done
  ‚öôÔ∏è Training models for SKYW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - SKYW: Initiating feature extraction for training.
  [DIAGNOSTIC] SKYW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SKYW: rows after features available: 126
üéØ SKYW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SKYW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SKYW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SKYW: Training LSTM (50 epochs)...
      ‚è≥ SKYW LSTM: Epoch 10/50 (20%)
      ‚è≥ SKYW LSTM: Epoch 20/50 (40%)
      ‚è≥ SKYW LSTM: Epoch 30/50 (60%)
      ‚è≥ SKYW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.604808
         RMSE: 0.777694
         R¬≤ Score: -0.8800 (Poor - 88.0% variance explained)
      üîπ SKYW: Training TCN (50 epochs)...
      ‚è≥ SKYW TCN: Epoch 10/50 (20%)
      ‚è≥ SKYW TCN: Epoch 20/50 (40%)
      ‚è≥ SKYW TCN: Epoch 30/50 (60%)
      ‚è≥ SKYW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.400905
         RMSE: 0.633171
         R¬≤ Score: -0.2462
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SKYW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SKYW Random Forest: Starting GridSearchCV fit...
       ‚úÖ SKYW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.6479 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SKYW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SKYW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.3223 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SKYW XGBoost: Starting GridSearchCV fit...
       ‚úÖ STVN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.0502 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.1926
    - TCN: MSE=0.1478
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1478
        ‚Ä¢ LSTM: MSE=0.1926
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.6490
        ‚Ä¢ Random Forest: MSE=24.7335
        ‚Ä¢ XGBoost: MSE=28.0502
   ‚úÖ STVN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STVN (TargetReturn): TCN with MSE=0.1478
üêõ DEBUG: STVN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STVN.
üêõ DEBUG: STVN - Moving model to CPU before return...
üêõ DEBUG [23:51:04.647]: STVN - Returning result metadata...
üêõ DEBUG [23:51:04.648]: Main received result for STVN
üêõ DEBUG: train_worker started for TASK
  ‚öôÔ∏è Training models for TASK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - TASK: Initiating feature extraction for training.
  [DIAGNOSTIC] TASK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TASK: rows after features available: 126
üéØ TASK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TASK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TASK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TASK: Training LSTM (50 epochs)...
      ‚è≥ TASK LSTM: Epoch 10/50 (20%)
      ‚è≥ TASK LSTM: Epoch 20/50 (40%)
      ‚è≥ TASK LSTM: Epoch 30/50 (60%)
      ‚è≥ TASK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.501742
         RMSE: 0.708338
         R¬≤ Score: -1.0907 (Poor - 109.1% variance explained)
      üîπ TASK: Training TCN (50 epochs)...
      ‚è≥ TASK TCN: Epoch 10/50 (20%)
      ‚è≥ TASK TCN: Epoch 20/50 (40%)
      ‚è≥ TASK TCN: Epoch 30/50 (60%)
      ‚è≥ TASK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.379499
         RMSE: 0.616035
         R¬≤ Score: -0.5814
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TASK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TASK Random Forest: Starting GridSearchCV fit...
       ‚úÖ TASK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.4206 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TASK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TASK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.9833 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TASK XGBoost: Starting GridSearchCV fit...
       ‚úÖ CNP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=2.6809 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.8079
    - TCN: MSE=0.8545
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.8079
        ‚Ä¢ TCN: MSE=0.8545
        ‚Ä¢ Random Forest: MSE=2.3906
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2.6275
        ‚Ä¢ XGBoost: MSE=2.6809
   ‚úÖ CNP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CNP (TargetReturn): LSTM with MSE=0.8079
üêõ DEBUG: CNP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CNP.
üêõ DEBUG: CNP - Moving model to CPU before return...
üêõ DEBUG [23:51:17.396]: CNP - Returning result metadata...
üêõ DEBUG [23:51:17.397]: Main received result for CNP
üêõ DEBUG: train_worker started for DSP
  ‚öôÔ∏è Training models for DSP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - DSP: Initiating feature extraction for training.
  [DIAGNOSTIC] DSP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DSP: rows after features available: 126
üéØ DSP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DSP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DSP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DSP: Training LSTM (50 epochs)...
      ‚è≥ DSP LSTM: Epoch 10/50 (20%)
      ‚è≥ DSP LSTM: Epoch 20/50 (40%)
      ‚è≥ DSP LSTM: Epoch 30/50 (60%)
      ‚è≥ DSP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.138135
         RMSE: 0.371666
         R¬≤ Score: -0.6959 (Poor - 69.6% variance explained)
      üîπ DSP: Training TCN (50 epochs)...
      ‚è≥ DSP TCN: Epoch 10/50 (20%)
      ‚è≥ DSP TCN: Epoch 20/50 (40%)
      ‚è≥ DSP TCN: Epoch 30/50 (60%)
      ‚è≥ DSP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.122139
         RMSE: 0.349485
         R¬≤ Score: -0.4995
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DSP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DSP Random Forest: Starting GridSearchCV fit...
       ‚úÖ DSP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.4000 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DSP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DSP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=47.3258 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DSP XGBoost: Starting GridSearchCV fit...
       ‚úÖ MIRM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.8649 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.6443
    - TCN: MSE=0.8005
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6443
        ‚Ä¢ TCN: MSE=0.8005
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.2476
        ‚Ä¢ Random Forest: MSE=16.8662
        ‚Ä¢ XGBoost: MSE=17.8649
   ‚úÖ MIRM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MIRM (TargetReturn): LSTM with MSE=0.6443
üêõ DEBUG: MIRM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MIRM.
üêõ DEBUG: MIRM - Moving model to CPU before return...
üêõ DEBUG [23:51:27.044]: MIRM - Returning result metadata...
üêõ DEBUG: train_worker started for STT
üêõ DEBUG [23:51:27.044]: Main received result for MIRM
  ‚öôÔ∏è Training models for STT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - STT: Initiating feature extraction for training.
  [DIAGNOSTIC] STT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STT: rows after features available: 126
üéØ STT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STT: Training LSTM (50 epochs)...
       ‚úÖ CUBI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=50.4590 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.5555
    - TCN: MSE=0.3712
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3712
        ‚Ä¢ LSTM: MSE=0.5555
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.0765
        ‚Ä¢ XGBoost: MSE=50.4590
        ‚Ä¢ Random Forest: MSE=51.8758
   ‚úÖ CUBI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CUBI (TargetReturn): TCN with MSE=0.3712
üêõ DEBUG: CUBI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CUBI.
üêõ DEBUG: CUBI - Moving model to CPU before return...
üêõ DEBUG [23:51:27.245]: CUBI - Returning result metadata...
üêõ DEBUG [23:51:27.246]: Main received result for CUBI
üêõ DEBUG: Training progress: 652/959 done
üêõ DEBUG: train_worker started for LIND
  ‚öôÔ∏è Training models for LIND (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - LIND: Initiating feature extraction for training.
  [DIAGNOSTIC] LIND: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LIND: rows after features available: 126
üéØ LIND: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LIND: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LIND: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LIND: Training LSTM (50 epochs)...
      ‚è≥ STT LSTM: Epoch 10/50 (20%)
      ‚è≥ LIND LSTM: Epoch 10/50 (20%)
      ‚è≥ STT LSTM: Epoch 20/50 (40%)
      ‚è≥ LIND LSTM: Epoch 20/50 (40%)
      ‚è≥ STT LSTM: Epoch 30/50 (60%)
      ‚è≥ LIND LSTM: Epoch 30/50 (60%)
      ‚è≥ STT LSTM: Epoch 40/50 (80%)
      ‚è≥ LIND LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.450818
         RMSE: 0.671430
         R¬≤ Score: -0.8535 (Poor - 85.4% variance explained)
      üîπ STT: Training TCN (50 epochs)...
      ‚è≥ STT TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.714212
         RMSE: 0.845111
         R¬≤ Score: -0.9506 (Poor - 95.1% variance explained)
      üîπ LIND: Training TCN (50 epochs)...
      ‚è≥ STT TCN: Epoch 20/50 (40%)
      ‚è≥ LIND TCN: Epoch 10/50 (20%)
      ‚è≥ STT TCN: Epoch 30/50 (60%)
      ‚è≥ LIND TCN: Epoch 20/50 (40%)
      ‚è≥ STT TCN: Epoch 40/50 (80%)
      ‚è≥ LIND TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.434768
         RMSE: 0.659369
         R¬≤ Score: -0.7875
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STT Random Forest: Starting GridSearchCV fit...
      ‚è≥ LIND TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.449321
         RMSE: 0.670314
         R¬≤ Score: -0.2271
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LIND: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LIND Random Forest: Starting GridSearchCV fit...
       ‚úÖ DXCM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.5092 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.5781
    - TCN: MSE=0.3222
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3222
        ‚Ä¢ LSTM: MSE=0.5781
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.4611
        ‚Ä¢ XGBoost: MSE=18.5092
        ‚Ä¢ Random Forest: MSE=20.7998
   ‚úÖ DXCM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DXCM (TargetReturn): TCN with MSE=0.3222
üêõ DEBUG: DXCM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DXCM.
üêõ DEBUG: DXCM - Moving model to CPU before return...
üêõ DEBUG [23:51:31.834]: DXCM - Returning result metadata...
üêõ DEBUG: train_worker started for CME
üêõ DEBUG [23:51:31.844]: Main received result for DXCM
  ‚öôÔ∏è Training models for CME (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - CME: Initiating feature extraction for training.
  [DIAGNOSTIC] CME: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CME: rows after features available: 126
üéØ CME: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CME: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CME: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CME: Training LSTM (50 epochs)...
      ‚è≥ CME LSTM: Epoch 10/50 (20%)
      ‚è≥ CME LSTM: Epoch 20/50 (40%)
      ‚è≥ CME LSTM: Epoch 30/50 (60%)
       ‚úÖ STT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6721 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LIND Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.9748 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LIND LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CME LSTM: Epoch 40/50 (80%)
       ‚úÖ STT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.3521 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STT XGBoost: Starting GridSearchCV fit...
       ‚úÖ LIND LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=33.3411 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LIND XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.375891
         RMSE: 0.613099
         R¬≤ Score: -0.4042 (Poor - 40.4% variance explained)
      üîπ CME: Training TCN (50 epochs)...
      ‚è≥ CME TCN: Epoch 10/50 (20%)
      ‚è≥ CME TCN: Epoch 20/50 (40%)
      ‚è≥ CME TCN: Epoch 30/50 (60%)
      ‚è≥ CME TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.449350
         RMSE: 0.670336
         R¬≤ Score: -0.6786
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CME: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CME Random Forest: Starting GridSearchCV fit...
       ‚úÖ CME Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2.8756 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CME LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CME LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=2.5026 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CME XGBoost: Starting GridSearchCV fit...
       ‚úÖ RAMP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=42.6875 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.7772
    - TCN: MSE=0.7913
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7772
        ‚Ä¢ TCN: MSE=0.7913
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.5826
        ‚Ä¢ Random Forest: MSE=30.0513
        ‚Ä¢ XGBoost: MSE=42.6875
   ‚úÖ RAMP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RAMP (TargetReturn): LSTM with MSE=0.7772
üêõ DEBUG: RAMP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RAMP.
üêõ DEBUG: RAMP - Moving model to CPU before return...
üêõ DEBUG [23:51:41.081]: RAMP - Returning result metadata...
üêõ DEBUG [23:51:41.081]: Main received result for RAMPüêõ DEBUG: train_worker started for AB

  ‚öôÔ∏è Training models for AB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - AB: Initiating feature extraction for training.
  [DIAGNOSTIC] AB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AB: rows after features available: 126
üéØ AB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AB: Training LSTM (50 epochs)...
      ‚è≥ AB LSTM: Epoch 10/50 (20%)
      ‚è≥ AB LSTM: Epoch 20/50 (40%)
      ‚è≥ AB LSTM: Epoch 30/50 (60%)
      ‚è≥ AB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.232353
         RMSE: 0.482030
         R¬≤ Score: -0.8494 (Poor - 84.9% variance explained)
      üîπ AB: Training TCN (50 epochs)...
      ‚è≥ AB TCN: Epoch 10/50 (20%)
      ‚è≥ AB TCN: Epoch 20/50 (40%)
      ‚è≥ AB TCN: Epoch 30/50 (60%)
      ‚è≥ AB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.137182
         RMSE: 0.370382
         R¬≤ Score: -0.0919
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AB Random Forest: Starting GridSearchCV fit...
       ‚úÖ FWONK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.1880 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.5107
    - TCN: MSE=0.4750
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4750
        ‚Ä¢ LSTM: MSE=0.5107
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.1728
        ‚Ä¢ Random Forest: MSE=7.7972
        ‚Ä¢ XGBoost: MSE=8.1880
   ‚úÖ FWONK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FWONK (TargetReturn): TCN with MSE=0.4750
üêõ DEBUG: FWONK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FWONK.
üêõ DEBUG: FWONK - Moving model to CPU before return...
üêõ DEBUG [23:51:45.869]: FWONK - Returning result metadata...
üêõ DEBUG [23:51:45.869]: Main received result for FWONK
üêõ DEBUG: train_worker started for SEIC
  ‚öôÔ∏è Training models for SEIC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - SEIC: Initiating feature extraction for training.
  [DIAGNOSTIC] SEIC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SEIC: rows after features available: 126
üéØ SEIC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SEIC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SEIC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SEIC: Training LSTM (50 epochs)...
      ‚è≥ SEIC LSTM: Epoch 10/50 (20%)
       ‚úÖ AB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0402 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SEIC LSTM: Epoch 20/50 (40%)
       ‚úÖ AB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.9130 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AB XGBoost: Starting GridSearchCV fit...
      ‚è≥ SEIC LSTM: Epoch 30/50 (60%)
      ‚è≥ SEIC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.380361
         RMSE: 0.616734
         R¬≤ Score: -0.8104 (Poor - 81.0% variance explained)
      üîπ SEIC: Training TCN (50 epochs)...
      ‚è≥ SEIC TCN: Epoch 10/50 (20%)
      ‚è≥ SEIC TCN: Epoch 20/50 (40%)
      ‚è≥ SEIC TCN: Epoch 30/50 (60%)
      ‚è≥ SEIC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.277512
         RMSE: 0.526794
         R¬≤ Score: -0.3209
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SEIC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SEIC Random Forest: Starting GridSearchCV fit...
       ‚úÖ SEIC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.7668 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SEIC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SEIC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.6736 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SEIC XGBoost: Starting GridSearchCV fit...
       ‚úÖ CSCO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=7.7422 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.5014
    - TCN: MSE=0.5122
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5014
        ‚Ä¢ TCN: MSE=0.5122
        ‚Ä¢ XGBoost: MSE=7.7422
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.1896
        ‚Ä¢ Random Forest: MSE=10.4311
   ‚úÖ CSCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CSCO (TargetReturn): LSTM with MSE=0.5014
üêõ DEBUG: CSCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CSCO.
üêõ DEBUG: CSCO - Moving model to CPU before return...
üêõ DEBUG [23:52:10.528]: CSCO - Returning result metadata...
üêõ DEBUG [23:52:10.528]: Main received result for CSCO
üêõ DEBUG: Training progress: 656/959 done
üêõ DEBUG: train_worker started for WBS
  ‚öôÔ∏è Training models for WBS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - WBS: Initiating feature extraction for training.
  [DIAGNOSTIC] WBS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WBS: rows after features available: 126
üéØ WBS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WBS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WBS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WBS: Training LSTM (50 epochs)...
      ‚è≥ WBS LSTM: Epoch 10/50 (20%)
      ‚è≥ WBS LSTM: Epoch 20/50 (40%)
      ‚è≥ WBS LSTM: Epoch 30/50 (60%)
      ‚è≥ WBS LSTM: Epoch 40/50 (80%)
       ‚úÖ RPRX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.8734 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.4s
    - LSTM: MSE=0.1478
    - TCN: MSE=0.0900
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0900
        ‚Ä¢ LSTM: MSE=0.1478
        ‚Ä¢ Random Forest: MSE=4.1903
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.3815
        ‚Ä¢ XGBoost: MSE=4.8734
   ‚úÖ RPRX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RPRX (TargetReturn): TCN with MSE=0.0900
üêõ DEBUG: RPRX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RPRX.
üêõ DEBUG: RPRX - Moving model to CPU before return...
üêõ DEBUG [23:52:12.369]: RPRX - Returning result metadata...
üêõ DEBUG: train_worker started for PAHC
  ‚öôÔ∏è Training models for PAHC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - PAHC: Initiating feature extraction for training.
  [DIAGNOSTIC] PAHC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAHC: rows after features available: 126
üéØ PAHC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAHC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAHC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAHC: Training LSTM (50 epochs)...
      ‚è≥ PAHC LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.636183
         RMSE: 0.797611
         R¬≤ Score: -0.9156 (Poor - 91.6% variance explained)
      üîπ WBS: Training TCN (50 epochs)...
      ‚è≥ WBS TCN: Epoch 10/50 (20%)
      ‚è≥ WBS TCN: Epoch 20/50 (40%)
      ‚è≥ WBS TCN: Epoch 30/50 (60%)
      ‚è≥ WBS TCN: Epoch 40/50 (80%)
       ‚úÖ CTRI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.0998 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.5302
    - TCN: MSE=0.2730
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2730
        ‚Ä¢ LSTM: MSE=0.5302
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.4715
        ‚Ä¢ Random Forest: MSE=21.0710
        ‚Ä¢ XGBoost: MSE=22.0998
   ‚úÖ CTRI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTRI (TargetReturn): TCN with MSE=0.2730
üêõ DEBUG: CTRI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTRI.
üêõ DEBUG: CTRI - Moving model to CPU before return...
üêõ DEBUG [23:52:13.262]: CTRI - Returning result metadata...
üêõ DEBUG [23:52:13.264]: Main received result for CTRI
üêõ DEBUG [23:52:13.264]: Main received result for RPRX
üêõ DEBUG: train_worker started for DAX
  ‚öôÔ∏è Training models for DAX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - DAX: Initiating feature extraction for training.
  [DIAGNOSTIC] DAX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAX: rows after features available: 126
üéØ DAX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAX: Training LSTM (50 epochs)...
      ‚è≥ PAHC LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.579396
         RMSE: 0.761180
         R¬≤ Score: -0.7446
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WBS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WBS Random Forest: Starting GridSearchCV fit...
      ‚è≥ PAHC LSTM: Epoch 30/50 (60%)
      ‚è≥ DAX LSTM: Epoch 10/50 (20%)
      ‚è≥ PAHC LSTM: Epoch 40/50 (80%)
      ‚è≥ DAX LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.595399
         RMSE: 0.771621
         R¬≤ Score: -0.8665 (Poor - 86.7% variance explained)
      üîπ PAHC: Training TCN (50 epochs)...
      ‚è≥ PAHC TCN: Epoch 10/50 (20%)
      ‚è≥ DAX LSTM: Epoch 30/50 (60%)
      ‚è≥ PAHC TCN: Epoch 20/50 (40%)
      ‚è≥ PAHC TCN: Epoch 30/50 (60%)
      ‚è≥ PAHC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.457919
         RMSE: 0.676697
         R¬≤ Score: -0.4355
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAHC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAHC Random Forest: Starting GridSearchCV fit...
      ‚è≥ DAX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.248159
         RMSE: 0.498155
         R¬≤ Score: -0.9528 (Poor - 95.3% variance explained)
      üîπ DAX: Training TCN (50 epochs)...
      ‚è≥ DAX TCN: Epoch 10/50 (20%)
      ‚è≥ DAX TCN: Epoch 20/50 (40%)
      ‚è≥ DAX TCN: Epoch 30/50 (60%)
      ‚è≥ DAX TCN: Epoch 40/50 (80%)
       ‚úÖ WBS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.9756 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WBS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.124693
         RMSE: 0.353119
         R¬≤ Score: 0.0188
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAX Random Forest: Starting GridSearchCV fit...
       ‚úÖ WBS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.6606 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WBS XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAHC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.0420 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAHC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAHC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=40.5666 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAHC XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.3872 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DAX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.0026 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAX XGBoost: Starting GridSearchCV fit...
       ‚úÖ COLO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.0680 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.6s
    - LSTM: MSE=0.2689
    - TCN: MSE=0.2137
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2137
        ‚Ä¢ LSTM: MSE=0.2689
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.8150
        ‚Ä¢ Random Forest: MSE=5.2386
        ‚Ä¢ XGBoost: MSE=6.0680
   ‚úÖ COLO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COLO (TargetReturn): TCN with MSE=0.2137
üêõ DEBUG: COLO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COLO.
üêõ DEBUG: COLO - Moving model to CPU before return...
üêõ DEBUG [23:52:27.676]: COLO - Returning result metadata...
üêõ DEBUG [23:52:27.677]: Main received result for COLO
üêõ DEBUG: train_worker started for HERO
  ‚öôÔ∏è Training models for HERO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - HERO: Initiating feature extraction for training.
  [DIAGNOSTIC] HERO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HERO: rows after features available: 126
üéØ HERO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HERO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HERO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HERO: Training LSTM (50 epochs)...
      ‚è≥ HERO LSTM: Epoch 10/50 (20%)
      ‚è≥ HERO LSTM: Epoch 20/50 (40%)
      ‚è≥ HERO LSTM: Epoch 30/50 (60%)
      ‚è≥ HERO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.324535
         RMSE: 0.569680
         R¬≤ Score: -0.9690 (Poor - 96.9% variance explained)
      üîπ HERO: Training TCN (50 epochs)...
      ‚è≥ HERO TCN: Epoch 10/50 (20%)
      ‚è≥ HERO TCN: Epoch 20/50 (40%)
      ‚è≥ HERO TCN: Epoch 30/50 (60%)
      ‚è≥ HERO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.244674
         RMSE: 0.494645
         R¬≤ Score: -0.4845
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HERO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HERO Random Forest: Starting GridSearchCV fit...
       ‚úÖ HERO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.3569 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HERO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HERO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.3024 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HERO XGBoost: Starting GridSearchCV fit...
       ‚úÖ ULTA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.7874 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.3317
    - TCN: MSE=0.1915
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1915
        ‚Ä¢ LSTM: MSE=0.3317
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.5134
        ‚Ä¢ Random Forest: MSE=20.4283
        ‚Ä¢ XGBoost: MSE=22.7874
   ‚úÖ ULTA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ULTA (TargetReturn): TCN with MSE=0.1915
üêõ DEBUG: ULTA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ULTA.
üêõ DEBUG: ULTA - Moving model to CPU before return...
üêõ DEBUG [23:52:40.000]: ULTA - Returning result metadata...
üêõ DEBUG [23:52:40.000]: Main received result for ULTA
üêõ DEBUG: Training progress: 660/959 done
üêõ DEBUG: train_worker started for RTX
  ‚öôÔ∏è Training models for RTX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - RTX: Initiating feature extraction for training.
  [DIAGNOSTIC] RTX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RTX: rows after features available: 126
üéØ RTX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RTX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RTX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RTX: Training LSTM (50 epochs)...
      ‚è≥ RTX LSTM: Epoch 10/50 (20%)
      ‚è≥ RTX LSTM: Epoch 20/50 (40%)
      ‚è≥ RTX LSTM: Epoch 30/50 (60%)
      ‚è≥ RTX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.309451
         RMSE: 0.556283
         R¬≤ Score: -0.8894 (Poor - 88.9% variance explained)
      üîπ RTX: Training TCN (50 epochs)...
      ‚è≥ RTX TCN: Epoch 10/50 (20%)
      ‚è≥ RTX TCN: Epoch 20/50 (40%)
      ‚è≥ RTX TCN: Epoch 30/50 (60%)
      ‚è≥ RTX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.345848
         RMSE: 0.588088
         R¬≤ Score: -1.1116
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RTX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RTX Random Forest: Starting GridSearchCV fit...
       ‚úÖ RTX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.2755 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RTX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RTX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8733 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RTX XGBoost: Starting GridSearchCV fit...
       ‚úÖ FNV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.8603 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.4164
    - TCN: MSE=0.4047
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4047
        ‚Ä¢ LSTM: MSE=0.4164
        ‚Ä¢ Random Forest: MSE=14.6830
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.3064
        ‚Ä¢ XGBoost: MSE=19.8603
   ‚úÖ FNV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FNV (TargetReturn): TCN with MSE=0.4047
üêõ DEBUG: FNV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FNV.
üêõ DEBUG: FNV - Moving model to CPU before return...
üêõ DEBUG [23:52:49.066]: FNV - Returning result metadata...
üêõ DEBUG [23:52:49.067]: Main received result for FNV
üêõ DEBUG: train_worker started for EWI
  ‚öôÔ∏è Training models for EWI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - EWI: Initiating feature extraction for training.
  [DIAGNOSTIC] EWI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWI: rows after features available: 126
üéØ EWI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWI: Training LSTM (50 epochs)...
      ‚è≥ EWI LSTM: Epoch 10/50 (20%)
       ‚úÖ GSAT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=49.7759 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.2703
    - TCN: MSE=0.2241
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2241
        ‚Ä¢ LSTM: MSE=0.2703
        ‚Ä¢ Random Forest: MSE=39.5048
        ‚Ä¢ LightGBM Regressor (CPU): MSE=48.2670
        ‚Ä¢ XGBoost: MSE=49.7759
   ‚úÖ GSAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GSAT (TargetReturn): TCN with MSE=0.2241
üêõ DEBUG: GSAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GSAT.
üêõ DEBUG: GSAT - Moving model to CPU before return...
üêõ DEBUG [23:52:49.802]: GSAT - Returning result metadata...
üêõ DEBUG: train_worker started for BMO
üêõ DEBUG [23:52:49.803]: Main received result for GSAT
  ‚öôÔ∏è Training models for BMO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - BMO: Initiating feature extraction for training.
  [DIAGNOSTIC] BMO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BMO: rows after features available: 126
üéØ BMO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BMO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BMO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BMO: Training LSTM (50 epochs)...
      ‚è≥ EWI LSTM: Epoch 20/50 (40%)
      ‚è≥ BMO LSTM: Epoch 10/50 (20%)
      ‚è≥ EWI LSTM: Epoch 30/50 (60%)
      ‚è≥ BMO LSTM: Epoch 20/50 (40%)
      ‚è≥ EWI LSTM: Epoch 40/50 (80%)
      ‚è≥ BMO LSTM: Epoch 30/50 (60%)
      ‚è≥ BMO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.158686
         RMSE: 0.398354
         R¬≤ Score: -0.9691 (Poor - 96.9% variance explained)
      üîπ EWI: Training TCN (50 epochs)...
      ‚è≥ EWI TCN: Epoch 10/50 (20%)
      ‚è≥ EWI TCN: Epoch 20/50 (40%)
      ‚è≥ EWI TCN: Epoch 30/50 (60%)
      ‚è≥ EWI TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.394401
         RMSE: 0.628013
         R¬≤ Score: -0.5310 (Poor - 53.1% variance explained)
      üîπ BMO: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.083294
         RMSE: 0.288606
         R¬≤ Score: -0.0336
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWI Random Forest: Starting GridSearchCV fit...
      ‚è≥ BMO TCN: Epoch 10/50 (20%)
      ‚è≥ BMO TCN: Epoch 20/50 (40%)
      ‚è≥ BMO TCN: Epoch 30/50 (60%)
      ‚è≥ BMO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.415947
         RMSE: 0.644940
         R¬≤ Score: -0.6146
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BMO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BMO Random Forest: Starting GridSearchCV fit...
       ‚úÖ EWI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.8121 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BMO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.5685 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BMO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.5182 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWI XGBoost: Starting GridSearchCV fit...
       ‚úÖ BMO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.0777 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BMO XGBoost: Starting GridSearchCV fit...
       ‚úÖ SKYW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.8582 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.6048
    - TCN: MSE=0.4009
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4009
        ‚Ä¢ LSTM: MSE=0.6048
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.3223
        ‚Ä¢ XGBoost: MSE=29.8582
        ‚Ä¢ Random Forest: MSE=31.6479
   ‚úÖ SKYW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SKYW (TargetReturn): TCN with MSE=0.4009
üêõ DEBUG: SKYW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SKYW.
üêõ DEBUG: SKYW - Moving model to CPU before return...
üêõ DEBUG [23:52:58.706]: SKYW - Returning result metadata...
üêõ DEBUG [23:52:58.707]: Main received result for SKYW
üêõ DEBUG: train_worker started for ECVT
  ‚öôÔ∏è Training models for ECVT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - ECVT: Initiating feature extraction for training.
  [DIAGNOSTIC] ECVT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ECVT: rows after features available: 126
üéØ ECVT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ECVT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ECVT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ECVT: Training LSTM (50 epochs)...
      ‚è≥ ECVT LSTM: Epoch 10/50 (20%)
      ‚è≥ ECVT LSTM: Epoch 20/50 (40%)
      ‚è≥ ECVT LSTM: Epoch 30/50 (60%)
      ‚è≥ ECVT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.640611
         RMSE: 0.800382
         R¬≤ Score: -0.8443 (Poor - 84.4% variance explained)
      üîπ ECVT: Training TCN (50 epochs)...
      ‚è≥ ECVT TCN: Epoch 10/50 (20%)
      ‚è≥ ECVT TCN: Epoch 20/50 (40%)
      ‚è≥ ECVT TCN: Epoch 30/50 (60%)
      ‚è≥ ECVT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.528818
         RMSE: 0.727199
         R¬≤ Score: -0.5224
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ECVT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ECVT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ECVT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.1392 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ECVT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ECVT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.2732 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ECVT XGBoost: Starting GridSearchCV fit...
       ‚úÖ TASK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.8058 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.5017
    - TCN: MSE=0.3795
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3795
        ‚Ä¢ LSTM: MSE=0.5017
        ‚Ä¢ XGBoost: MSE=10.8058
        ‚Ä¢ Random Forest: MSE=12.4206
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.9833
   ‚úÖ TASK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TASK (TargetReturn): TCN with MSE=0.3795
üêõ DEBUG: TASK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TASK.
üêõ DEBUG: TASK - Moving model to CPU before return...
üêõ DEBUG [23:53:08.448]: TASK - Returning result metadata...
üêõ DEBUG [23:53:08.449]: Main received result for TASK
üêõ DEBUG: Training progress: 664/959 done
üêõ DEBUG: train_worker started for EETH
  ‚öôÔ∏è Training models for EETH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - EETH: Initiating feature extraction for training.
  [DIAGNOSTIC] EETH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EETH: rows after features available: 126
üéØ EETH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EETH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EETH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EETH: Training LSTM (50 epochs)...
      ‚è≥ EETH LSTM: Epoch 10/50 (20%)
      ‚è≥ EETH LSTM: Epoch 20/50 (40%)
      ‚è≥ EETH LSTM: Epoch 30/50 (60%)
      ‚è≥ EETH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.608266
         RMSE: 0.779914
         R¬≤ Score: -1.1223 (Poor - 112.2% variance explained)
      üîπ EETH: Training TCN (50 epochs)...
      ‚è≥ EETH TCN: Epoch 10/50 (20%)
      ‚è≥ EETH TCN: Epoch 20/50 (40%)
      ‚è≥ EETH TCN: Epoch 30/50 (60%)
      ‚è≥ EETH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.622725
         RMSE: 0.789129
         R¬≤ Score: -1.1727
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EETH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EETH Random Forest: Starting GridSearchCV fit...
       ‚úÖ EETH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.7505 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EETH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EETH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=74.5554 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EETH XGBoost: Starting GridSearchCV fit...
       ‚úÖ DSP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.3235 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.3s
    - LSTM: MSE=0.1381
    - TCN: MSE=0.1221
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1221
        ‚Ä¢ LSTM: MSE=0.1381
        ‚Ä¢ Random Forest: MSE=40.4000
        ‚Ä¢ XGBoost: MSE=46.3235
        ‚Ä¢ LightGBM Regressor (CPU): MSE=47.3258
   ‚úÖ DSP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DSP (TargetReturn): TCN with MSE=0.1221
üêõ DEBUG: DSP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DSP.
üêõ DEBUG: DSP - Moving model to CPU before return...
üêõ DEBUG [23:53:27.306]: DSP - Returning result metadata...
üêõ DEBUG [23:53:27.306]: Main received result for DSP
üêõ DEBUG: train_worker started for EYPT
  ‚öôÔ∏è Training models for EYPT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - EYPT: Initiating feature extraction for training.
  [DIAGNOSTIC] EYPT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EYPT: rows after features available: 126
üéØ EYPT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EYPT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EYPT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EYPT: Training LSTM (50 epochs)...
      ‚è≥ EYPT LSTM: Epoch 10/50 (20%)
      ‚è≥ EYPT LSTM: Epoch 20/50 (40%)
      ‚è≥ EYPT LSTM: Epoch 30/50 (60%)
      ‚è≥ EYPT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.401651
         RMSE: 0.633760
         R¬≤ Score: -0.8212 (Poor - 82.1% variance explained)
      üîπ EYPT: Training TCN (50 epochs)...
      ‚è≥ EYPT TCN: Epoch 10/50 (20%)
      ‚è≥ EYPT TCN: Epoch 20/50 (40%)
      ‚è≥ EYPT TCN: Epoch 30/50 (60%)
      ‚è≥ EYPT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.426109
         RMSE: 0.652771
         R¬≤ Score: -0.9321
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EYPT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EYPT Random Forest: Starting GridSearchCV fit...
       ‚úÖ LIND XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.6317 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.7142
    - TCN: MSE=0.4493
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4493
        ‚Ä¢ LSTM: MSE=0.7142
        ‚Ä¢ Random Forest: MSE=15.9748
        ‚Ä¢ XGBoost: MSE=24.6317
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.3411
   ‚úÖ LIND: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LIND (TargetReturn): TCN with MSE=0.4493
üêõ DEBUG: LIND - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LIND.
üêõ DEBUG: LIND - Moving model to CPU before return...
üêõ DEBUG [23:53:32.873]: LIND - Returning result metadata...
üêõ DEBUG: train_worker started for CMCM
  ‚öôÔ∏è Training models for CMCM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CMCM: Initiating feature extraction for training.
  [DIAGNOSTIC] CMCM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CMCM: rows after features available: 126
üéØ CMCM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CMCM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CMCM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CMCM: Training LSTM (50 epochs)...
       ‚úÖ EYPT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=96.1794 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EYPT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CME XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3.7618 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.0s
    - LSTM: MSE=0.3759
    - TCN: MSE=0.4494
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3759
        ‚Ä¢ TCN: MSE=0.4494
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2.5026
        ‚Ä¢ Random Forest: MSE=2.8756
        ‚Ä¢ XGBoost: MSE=3.7618
   ‚úÖ CME: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CME (TargetReturn): LSTM with MSE=0.3759
üêõ DEBUG: CME - train_and_evaluate_models completed
      ‚è≥ CMCM LSTM: Epoch 10/50 (20%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CME.
üêõ DEBUG: CME - Moving model to CPU before return...
üêõ DEBUG [23:53:33.451]: CME - Returning result metadata...
üêõ DEBUG: train_worker started for NDAQ
  ‚öôÔ∏è Training models for NDAQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - NDAQ: Initiating feature extraction for training.
  [DIAGNOSTIC] NDAQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NDAQ: rows after features available: 126
üéØ NDAQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NDAQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NDAQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NDAQ: Training LSTM (50 epochs)...
       ‚úÖ EYPT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=106.5488 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EYPT XGBoost: Starting GridSearchCV fit...
      ‚è≥ CMCM LSTM: Epoch 20/50 (40%)
      ‚è≥ NDAQ LSTM: Epoch 10/50 (20%)
       ‚úÖ STT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.7017 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.4508
    - TCN: MSE=0.4348
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4348
        ‚Ä¢ LSTM: MSE=0.4508
        ‚Ä¢ Random Forest: MSE=11.6721
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.3521
        ‚Ä¢ XGBoost: MSE=12.7017
   ‚úÖ STT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STT (TargetReturn): TCN with MSE=0.4348
üêõ DEBUG: STT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STT.
üêõ DEBUG: STT - Moving model to CPU before return...
üêõ DEBUG [23:53:34.180]: STT - Returning result metadata...
üêõ DEBUG: train_worker started for OGIG
üêõ DEBUG [23:53:34.180]: Main received result for STT
üêõ DEBUG [23:53:34.181]: Main received result for LIND
üêõ DEBUG [23:53:34.181]: Main received result for CME
üêõ DEBUG: Training progress: 668/959 done
  ‚öôÔ∏è Training models for OGIG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - OGIG: Initiating feature extraction for training.
  [DIAGNOSTIC] OGIG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OGIG: rows after features available: 126
üéØ OGIG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OGIG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OGIG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OGIG: Training LSTM (50 epochs)...
      ‚è≥ CMCM LSTM: Epoch 30/50 (60%)
      ‚è≥ NDAQ LSTM: Epoch 20/50 (40%)
      ‚è≥ OGIG LSTM: Epoch 10/50 (20%)
      ‚è≥ CMCM LSTM: Epoch 40/50 (80%)
      ‚è≥ NDAQ LSTM: Epoch 30/50 (60%)
      ‚è≥ OGIG LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.600728
         RMSE: 0.775067
         R¬≤ Score: -0.8345 (Poor - 83.5% variance explained)
      üîπ CMCM: Training TCN (50 epochs)...
      ‚è≥ CMCM TCN: Epoch 10/50 (20%)
      ‚è≥ CMCM TCN: Epoch 20/50 (40%)
      ‚è≥ OGIG LSTM: Epoch 30/50 (60%)
      ‚è≥ CMCM TCN: Epoch 30/50 (60%)
      ‚è≥ NDAQ LSTM: Epoch 40/50 (80%)
      ‚è≥ CMCM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.328384
         RMSE: 0.573048
         R¬≤ Score: -0.0028
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CMCM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CMCM Random Forest: Starting GridSearchCV fit...
      ‚è≥ OGIG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.568046
         RMSE: 0.753688
         R¬≤ Score: -1.3133 (Poor - 131.3% variance explained)
      üîπ NDAQ: Training TCN (50 epochs)...
      ‚è≥ NDAQ TCN: Epoch 10/50 (20%)
      ‚è≥ NDAQ TCN: Epoch 20/50 (40%)
      ‚è≥ NDAQ TCN: Epoch 30/50 (60%)
      ‚è≥ NDAQ TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.590661
         RMSE: 0.768544
         R¬≤ Score: -1.2049 (Poor - 120.5% variance explained)
      üîπ OGIG: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.457814
         RMSE: 0.676620
         R¬≤ Score: -0.8644
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NDAQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NDAQ Random Forest: Starting GridSearchCV fit...
      ‚è≥ OGIG TCN: Epoch 10/50 (20%)
      ‚è≥ OGIG TCN: Epoch 20/50 (40%)
      ‚è≥ OGIG TCN: Epoch 30/50 (60%)
      ‚è≥ OGIG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.358211
         RMSE: 0.598507
         R¬≤ Score: -0.3372
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OGIG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OGIG Random Forest: Starting GridSearchCV fit...
       ‚úÖ CMCM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.3115 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CMCM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NDAQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.4819 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NDAQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CMCM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=33.9458 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.4s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CMCM XGBoost: Starting GridSearchCV fit...
       ‚úÖ OGIG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.1683 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OGIG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NDAQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.3816 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NDAQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ OGIG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.5512 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OGIG XGBoost: Starting GridSearchCV fit...
       ‚úÖ SEIC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.4830 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.4s
    - LSTM: MSE=0.3804
    - TCN: MSE=0.2775
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2775
        ‚Ä¢ LSTM: MSE=0.3804
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.6736
        ‚Ä¢ XGBoost: MSE=17.4830
        ‚Ä¢ Random Forest: MSE=17.7668
   ‚úÖ SEIC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SEIC (TargetReturn): TCN with MSE=0.2775
üêõ DEBUG: SEIC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SEIC.
üêõ DEBUG: SEIC - Moving model to CPU before return...
üêõ DEBUG [23:53:46.378]: SEIC - Returning result metadata...
üêõ DEBUG: train_worker started for CEPU
  ‚öôÔ∏è Training models for CEPU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CEPU: Initiating feature extraction for training.
  [DIAGNOSTIC] CEPU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CEPU: rows after features available: 126
üéØ CEPU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CEPU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CEPU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CEPU: Training LSTM (50 epochs)...
      ‚è≥ CEPU LSTM: Epoch 10/50 (20%)
      ‚è≥ CEPU LSTM: Epoch 20/50 (40%)
      ‚è≥ CEPU LSTM: Epoch 30/50 (60%)
      ‚è≥ CEPU LSTM: Epoch 40/50 (80%)
       ‚úÖ AB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.9268 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.2324
    - TCN: MSE=0.1372
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1372
        ‚Ä¢ LSTM: MSE=0.2324
        ‚Ä¢ Random Forest: MSE=9.0402
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.9130
        ‚Ä¢ XGBoost: MSE=9.9268
   ‚úÖ AB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AB (TargetReturn): TCN with MSE=0.1372
üêõ DEBUG: AB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AB.
üêõ DEBUG: AB - Moving model to CPU before return...
üêõ DEBUG [23:53:48.678]: AB - Returning result metadata...
üêõ DEBUG [23:53:48.679]: Main received result for AB
üêõ DEBUG [23:53:48.679]: Main received result for SEIC
üêõ DEBUG: train_worker started for PAR
  ‚öôÔ∏è Training models for PAR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - PAR: Initiating feature extraction for training.
  [DIAGNOSTIC] PAR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAR: rows after features available: 126
üéØ PAR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAR: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.143855
         RMSE: 0.379282
         R¬≤ Score: -0.1565 (Poor - 15.6% variance explained)
      üîπ CEPU: Training TCN (50 epochs)...
      ‚è≥ CEPU TCN: Epoch 10/50 (20%)
      ‚è≥ CEPU TCN: Epoch 20/50 (40%)
      ‚è≥ CEPU TCN: Epoch 30/50 (60%)
      ‚è≥ CEPU TCN: Epoch 40/50 (80%)
      ‚è≥ PAR LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.175011
         RMSE: 0.418343
         R¬≤ Score: -0.4069
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CEPU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CEPU Random Forest: Starting GridSearchCV fit...
      ‚è≥ PAR LSTM: Epoch 20/50 (40%)
      ‚è≥ PAR LSTM: Epoch 30/50 (60%)
      ‚è≥ PAR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.348336
         RMSE: 0.590200
         R¬≤ Score: -0.6742 (Poor - 67.4% variance explained)
      üîπ PAR: Training TCN (50 epochs)...
      ‚è≥ PAR TCN: Epoch 10/50 (20%)
      ‚è≥ PAR TCN: Epoch 20/50 (40%)
      ‚è≥ PAR TCN: Epoch 30/50 (60%)
      ‚è≥ PAR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.293042
         RMSE: 0.541334
         R¬≤ Score: -0.4085
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAR Random Forest: Starting GridSearchCV fit...
       ‚úÖ CEPU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.5432 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CEPU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CEPU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.1854 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CEPU XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.9958 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.7854 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAR XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.0406 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.2482
    - TCN: MSE=0.1247
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1247
        ‚Ä¢ LSTM: MSE=0.2482
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0026
        ‚Ä¢ XGBoost: MSE=7.0406
        ‚Ä¢ Random Forest: MSE=7.3872
   ‚úÖ DAX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAX (TargetReturn): TCN with MSE=0.1247
üêõ DEBUG: DAX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAX.
üêõ DEBUG: DAX - Moving model to CPU before return...
üêõ DEBUG [23:54:17.537]: DAX - Returning result metadata...
üêõ DEBUG: train_worker started for LTL
  ‚öôÔ∏è Training models for LTL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - LTL: Initiating feature extraction for training.
  [DIAGNOSTIC] LTL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LTL: rows after features available: 126
üéØ LTL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LTL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LTL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LTL: Training LSTM (50 epochs)...
      ‚è≥ LTL LSTM: Epoch 10/50 (20%)
       ‚úÖ WBS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.7255 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.3s
    - LSTM: MSE=0.6362
    - TCN: MSE=0.5794
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5794
        ‚Ä¢ LSTM: MSE=0.6362
        ‚Ä¢ XGBoost: MSE=11.7255
        ‚Ä¢ Random Forest: MSE=13.9756
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.6606
   ‚úÖ WBS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WBS (TargetReturn): TCN with MSE=0.5794
üêõ DEBUG: WBS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WBS.
üêõ DEBUG: WBS - Moving model to CPU before return...
üêõ DEBUG [23:54:18.268]: WBS - Returning result metadata...
üêõ DEBUG: train_worker started for THFF
üêõ DEBUG [23:54:18.272]: Main received result for WBS
  ‚öôÔ∏è Training models for THFF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - THFF: Initiating feature extraction for training.
  [DIAGNOSTIC] THFF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ THFF: rows after features available: 126
üéØ THFF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] THFF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö THFF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ THFF: Training LSTM (50 epochs)...
      ‚è≥ LTL LSTM: Epoch 20/50 (40%)
      ‚è≥ THFF LSTM: Epoch 10/50 (20%)
      ‚è≥ LTL LSTM: Epoch 30/50 (60%)
       ‚úÖ PAHC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=65.1432 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.5954
    - TCN: MSE=0.4579
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4579
        ‚Ä¢ LSTM: MSE=0.5954
        ‚Ä¢ Random Forest: MSE=35.0420
        ‚Ä¢ LightGBM Regressor (CPU): MSE=40.5666
        ‚Ä¢ XGBoost: MSE=65.1432
   ‚úÖ PAHC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAHC (TargetReturn): TCN with MSE=0.4579
üêõ DEBUG: PAHC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAHC.
üêõ DEBUG: PAHC - Moving model to CPU before return...
üêõ DEBUG [23:54:19.213]: PAHC - Returning result metadata...
üêõ DEBUG [23:54:19.213]: Main received result for PAHCüêõ DEBUG: train_worker started for SMMT

üêõ DEBUG: Training progress: 672/959 done
üêõ DEBUG [23:54:19.214]: Main received result for DAX
  ‚öôÔ∏è Training models for SMMT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - SMMT: Initiating feature extraction for training.
  [DIAGNOSTIC] SMMT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMMT: rows after features available: 126
üéØ SMMT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SMMT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMMT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMMT: Training LSTM (50 epochs)...
      ‚è≥ THFF LSTM: Epoch 20/50 (40%)
      ‚è≥ LTL LSTM: Epoch 40/50 (80%)
      ‚è≥ SMMT LSTM: Epoch 10/50 (20%)
      ‚è≥ THFF LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.566104
         RMSE: 0.752399
         R¬≤ Score: -0.9650 (Poor - 96.5% variance explained)
      üîπ LTL: Training TCN (50 epochs)...
      ‚è≥ LTL TCN: Epoch 10/50 (20%)
      ‚è≥ LTL TCN: Epoch 20/50 (40%)
      ‚è≥ LTL TCN: Epoch 30/50 (60%)
      ‚è≥ SMMT LSTM: Epoch 20/50 (40%)
      ‚è≥ THFF LSTM: Epoch 40/50 (80%)
      ‚è≥ LTL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.488386
         RMSE: 0.698846
         R¬≤ Score: -0.6952
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LTL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LTL Random Forest: Starting GridSearchCV fit...
      ‚è≥ SMMT LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.544159
         RMSE: 0.737671
         R¬≤ Score: -0.9489 (Poor - 94.9% variance explained)
      üîπ THFF: Training TCN (50 epochs)...
      ‚è≥ THFF TCN: Epoch 10/50 (20%)
      ‚è≥ THFF TCN: Epoch 20/50 (40%)
      ‚è≥ THFF TCN: Epoch 30/50 (60%)
      ‚è≥ THFF TCN: Epoch 40/50 (80%)
      ‚è≥ SMMT LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.290782
         RMSE: 0.539242
         R¬≤ Score: -0.0414
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä THFF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ THFF Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.180766
         RMSE: 0.425166
         R¬≤ Score: -0.2869 (Poor - 28.7% variance explained)
      üîπ SMMT: Training TCN (50 epochs)...
      ‚è≥ SMMT TCN: Epoch 10/50 (20%)
      ‚è≥ SMMT TCN: Epoch 20/50 (40%)
      ‚è≥ SMMT TCN: Epoch 30/50 (60%)
      ‚è≥ SMMT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.149648
         RMSE: 0.386844
         R¬≤ Score: -0.0654
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMMT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMMT Random Forest: Starting GridSearchCV fit...
       ‚úÖ LTL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.1840 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LTL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ LTL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.3313 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LTL XGBoost: Starting GridSearchCV fit...
       ‚úÖ THFF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.0324 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ THFF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ THFF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.4970 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ THFF XGBoost: Starting GridSearchCV fit...
       ‚úÖ SMMT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=125.0505 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMMT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SMMT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=71.3193 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMMT XGBoost: Starting GridSearchCV fit...
       ‚úÖ HERO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.7740 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.3245
    - TCN: MSE=0.2447
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2447
        ‚Ä¢ LSTM: MSE=0.3245
        ‚Ä¢ XGBoost: MSE=5.7740
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.3024
        ‚Ä¢ Random Forest: MSE=6.3569
   ‚úÖ HERO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HERO (TargetReturn): TCN with MSE=0.2447
üêõ DEBUG: HERO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HERO.
üêõ DEBUG: HERO - Moving model to CPU before return...
üêõ DEBUG [23:54:30.024]: HERO - Returning result metadata...
üêõ DEBUG: train_worker started for IMAX
üêõ DEBUG [23:54:30.027]: Main received result for HERO
  ‚öôÔ∏è Training models for IMAX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - IMAX: Initiating feature extraction for training.
  [DIAGNOSTIC] IMAX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IMAX: rows after features available: 126
üéØ IMAX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IMAX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IMAX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IMAX: Training LSTM (50 epochs)...
      ‚è≥ IMAX LSTM: Epoch 10/50 (20%)
      ‚è≥ IMAX LSTM: Epoch 20/50 (40%)
      ‚è≥ IMAX LSTM: Epoch 30/50 (60%)
      ‚è≥ IMAX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.534008
         RMSE: 0.730759
         R¬≤ Score: -1.3986 (Poor - 139.9% variance explained)
      üîπ IMAX: Training TCN (50 epochs)...
      ‚è≥ IMAX TCN: Epoch 10/50 (20%)
      ‚è≥ IMAX TCN: Epoch 20/50 (40%)
      ‚è≥ IMAX TCN: Epoch 30/50 (60%)
      ‚è≥ IMAX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.242383
         RMSE: 0.492324
         R¬≤ Score: -0.0887
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IMAX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IMAX Random Forest: Starting GridSearchCV fit...
       ‚úÖ IMAX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0336 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IMAX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IMAX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.1049 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IMAX XGBoost: Starting GridSearchCV fit...
       ‚úÖ RTX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.6820 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.3095
    - TCN: MSE=0.3458
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3095
        ‚Ä¢ TCN: MSE=0.3458
        ‚Ä¢ XGBoost: MSE=10.6820
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8733
        ‚Ä¢ Random Forest: MSE=12.2755
   ‚úÖ RTX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RTX (TargetReturn): LSTM with MSE=0.3095
üêõ DEBUG: RTX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RTX.
üêõ DEBUG: RTX - Moving model to CPU before return...
üêõ DEBUG [23:54:44.054]: RTX - Returning result metadata...
üêõ DEBUG [23:54:44.054]: Main received result for RTXüêõ DEBUG: train_worker started for SMCI

  ‚öôÔ∏è Training models for SMCI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - SMCI: Initiating feature extraction for training.
  [DIAGNOSTIC] SMCI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMCI: rows after features available: 126
üéØ SMCI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SMCI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMCI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMCI: Training LSTM (50 epochs)...
      ‚è≥ SMCI LSTM: Epoch 10/50 (20%)
      ‚è≥ SMCI LSTM: Epoch 20/50 (40%)
      ‚è≥ SMCI LSTM: Epoch 30/50 (60%)
      ‚è≥ SMCI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.166619
         RMSE: 0.408190
         R¬≤ Score: -1.2776 (Poor - 127.8% variance explained)
      üîπ SMCI: Training TCN (50 epochs)...
      ‚è≥ SMCI TCN: Epoch 10/50 (20%)
      ‚è≥ SMCI TCN: Epoch 20/50 (40%)
      ‚è≥ SMCI TCN: Epoch 30/50 (60%)
      ‚è≥ SMCI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.075957
         RMSE: 0.275602
         R¬≤ Score: -0.0383
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMCI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMCI Random Forest: Starting GridSearchCV fit...
       ‚úÖ SMCI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=196.9603 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMCI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SMCI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=232.5650 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMCI XGBoost: Starting GridSearchCV fit...
       ‚úÖ BMO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.7715 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.3944
    - TCN: MSE=0.4159
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3944
        ‚Ä¢ TCN: MSE=0.4159
        ‚Ä¢ Random Forest: MSE=3.5685
        ‚Ä¢ XGBoost: MSE=4.7715
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.0777
   ‚úÖ BMO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BMO (TargetReturn): LSTM with MSE=0.3944
üêõ DEBUG: BMO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BMO.
üêõ DEBUG: BMO - Moving model to CPU before return...
üêõ DEBUG [23:54:52.999]: BMO - Returning result metadata...
üêõ DEBUG: train_worker started for BGM
  ‚öôÔ∏è Training models for BGM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - BGM: Initiating feature extraction for training.
  [DIAGNOSTIC] BGM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BGM: rows after features available: 126
üéØ BGM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BGM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BGM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BGM: Training LSTM (50 epochs)...
      ‚è≥ BGM LSTM: Epoch 10/50 (20%)
      ‚è≥ BGM LSTM: Epoch 20/50 (40%)
      ‚è≥ BGM LSTM: Epoch 30/50 (60%)
      ‚è≥ BGM LSTM: Epoch 40/50 (80%)
       ‚úÖ EWI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.8710 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.5s
    - LSTM: MSE=0.1587
    - TCN: MSE=0.0833
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0833
        ‚Ä¢ LSTM: MSE=0.1587
        ‚Ä¢ Random Forest: MSE=7.8121
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.5182
        ‚Ä¢ XGBoost: MSE=12.8710
   ‚úÖ EWI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EWI (TargetReturn): TCN with MSE=0.0833
üêõ DEBUG: EWI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWI.
üêõ DEBUG: EWI - Moving model to CPU before return...
üêõ DEBUG [23:54:55.130]: EWI - Returning result metadata...
üêõ DEBUG: train_worker started for NG
üêõ DEBUG [23:54:55.131]: Main received result for EWI
üêõ DEBUG: Training progress: 676/959 done
üêõ DEBUG [23:54:55.131]: Main received result for BMO
  ‚öôÔ∏è Training models for NG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - NG: Initiating feature extraction for training.
  [DIAGNOSTIC] NG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NG: rows after features available: 126
üéØ NG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NG: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.326724
         RMSE: 0.571597
         R¬≤ Score: -0.7727 (Poor - 77.3% variance explained)
      üîπ BGM: Training TCN (50 epochs)...
      ‚è≥ BGM TCN: Epoch 10/50 (20%)
      ‚è≥ BGM TCN: Epoch 20/50 (40%)
      ‚è≥ NG LSTM: Epoch 10/50 (20%)
      ‚è≥ BGM TCN: Epoch 30/50 (60%)
      ‚è≥ BGM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.202661
         RMSE: 0.450179
         R¬≤ Score: -0.0996
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BGM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BGM Random Forest: Starting GridSearchCV fit...
      ‚è≥ NG LSTM: Epoch 20/50 (40%)
      ‚è≥ NG LSTM: Epoch 30/50 (60%)
      ‚è≥ NG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.172159
         RMSE: 0.414921
         R¬≤ Score: -0.6888 (Poor - 68.9% variance explained)
      üîπ NG: Training TCN (50 epochs)...
      ‚è≥ NG TCN: Epoch 10/50 (20%)
      ‚è≥ NG TCN: Epoch 20/50 (40%)
      ‚è≥ NG TCN: Epoch 30/50 (60%)
      ‚è≥ NG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.108832
         RMSE: 0.329896
         R¬≤ Score: -0.0676
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NG Random Forest: Starting GridSearchCV fit...
       ‚úÖ BGM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=112.4556 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BGM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BGM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=92.8535 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BGM XGBoost: Starting GridSearchCV fit...
       ‚úÖ NG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=76.3796 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=71.6973 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NG XGBoost: Starting GridSearchCV fit...
       ‚úÖ ECVT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.0159 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.6406
    - TCN: MSE=0.5288
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5288
        ‚Ä¢ LSTM: MSE=0.6406
        ‚Ä¢ Random Forest: MSE=18.1392
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.2732
        ‚Ä¢ XGBoost: MSE=22.0159
   ‚úÖ ECVT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ECVT (TargetReturn): TCN with MSE=0.5288
üêõ DEBUG: ECVT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ECVT.
üêõ DEBUG: ECVT - Moving model to CPU before return...
üêõ DEBUG [23:55:01.972]: ECVT - Returning result metadata...
üêõ DEBUG [23:55:01.973]: Main received result for ECVT
üêõ DEBUG: train_worker started for FN
  ‚öôÔ∏è Training models for FN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - FN: Initiating feature extraction for training.
  [DIAGNOSTIC] FN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FN: rows after features available: 126
üéØ FN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FN: Training LSTM (50 epochs)...
      ‚è≥ FN LSTM: Epoch 10/50 (20%)
      ‚è≥ FN LSTM: Epoch 20/50 (40%)
      ‚è≥ FN LSTM: Epoch 30/50 (60%)
      ‚è≥ FN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.370261
         RMSE: 0.608491
         R¬≤ Score: -0.7328 (Poor - 73.3% variance explained)
      üîπ FN: Training TCN (50 epochs)...
      ‚è≥ FN TCN: Epoch 10/50 (20%)
      ‚è≥ FN TCN: Epoch 20/50 (40%)
      ‚è≥ FN TCN: Epoch 30/50 (60%)
      ‚è≥ FN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.432706
         RMSE: 0.657804
         R¬≤ Score: -1.0251
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FN Random Forest: Starting GridSearchCV fit...
       ‚úÖ FN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=60.0020 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=55.9334 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FN XGBoost: Starting GridSearchCV fit...
       ‚úÖ EETH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=46.0068 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.6083
    - TCN: MSE=0.6227
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6083
        ‚Ä¢ TCN: MSE=0.6227
        ‚Ä¢ Random Forest: MSE=39.7505
        ‚Ä¢ XGBoost: MSE=46.0068
        ‚Ä¢ LightGBM Regressor (CPU): MSE=74.5554
   ‚úÖ EETH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EETH (TargetReturn): LSTM with MSE=0.6083
üêõ DEBUG: EETH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EETH.
üêõ DEBUG: EETH - Moving model to CPU before return...
üêõ DEBUG [23:55:11.902]: EETH - Returning result metadata...
üêõ DEBUG [23:55:11.902]: Main received result for EETH
üêõ DEBUG: train_worker started for UFCS
  ‚öôÔ∏è Training models for UFCS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - UFCS: Initiating feature extraction for training.
  [DIAGNOSTIC] UFCS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UFCS: rows after features available: 126
üéØ UFCS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UFCS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UFCS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UFCS: Training LSTM (50 epochs)...
      ‚è≥ UFCS LSTM: Epoch 10/50 (20%)
      ‚è≥ UFCS LSTM: Epoch 20/50 (40%)
      ‚è≥ UFCS LSTM: Epoch 30/50 (60%)
      ‚è≥ UFCS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.126869
         RMSE: 0.356186
         R¬≤ Score: -1.0470 (Poor - 104.7% variance explained)
      üîπ UFCS: Training TCN (50 epochs)...
      ‚è≥ UFCS TCN: Epoch 10/50 (20%)
      ‚è≥ UFCS TCN: Epoch 20/50 (40%)
      ‚è≥ UFCS TCN: Epoch 30/50 (60%)
      ‚è≥ UFCS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.117674
         RMSE: 0.343036
         R¬≤ Score: -0.8986
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UFCS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UFCS Random Forest: Starting GridSearchCV fit...
       ‚úÖ UFCS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.1744 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UFCS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UFCS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.3029 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UFCS XGBoost: Starting GridSearchCV fit...
       ‚úÖ EYPT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=123.5063 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.4017
    - TCN: MSE=0.4261
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4017
        ‚Ä¢ TCN: MSE=0.4261
        ‚Ä¢ Random Forest: MSE=96.1794
        ‚Ä¢ LightGBM Regressor (CPU): MSE=106.5488
        ‚Ä¢ XGBoost: MSE=123.5063
   ‚úÖ EYPT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EYPT (TargetReturn): LSTM with MSE=0.4017
üêõ DEBUG: EYPT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EYPT.
üêõ DEBUG: EYPT - Moving model to CPU before return...
üêõ DEBUG [23:55:34.548]: EYPT - Returning result metadata...
üêõ DEBUG [23:55:34.549]: Main received result for EYPT
üêõ DEBUG: Training progress: 680/959 done
üêõ DEBUG: train_worker started for NOTV
  ‚öôÔ∏è Training models for NOTV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - NOTV: Initiating feature extraction for training.
  [DIAGNOSTIC] NOTV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NOTV: rows after features available: 126
üéØ NOTV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NOTV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NOTV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NOTV: Training LSTM (50 epochs)...
      ‚è≥ NOTV LSTM: Epoch 10/50 (20%)
      ‚è≥ NOTV LSTM: Epoch 20/50 (40%)
      ‚è≥ NOTV LSTM: Epoch 30/50 (60%)
      ‚è≥ NOTV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.125014
         RMSE: 0.353573
         R¬≤ Score: -0.9409 (Poor - 94.1% variance explained)
      üîπ NOTV: Training TCN (50 epochs)...
      ‚è≥ NOTV TCN: Epoch 10/50 (20%)
      ‚è≥ NOTV TCN: Epoch 20/50 (40%)
      ‚è≥ NOTV TCN: Epoch 30/50 (60%)
      ‚è≥ NOTV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.078861
         RMSE: 0.280822
         R¬≤ Score: -0.2243
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NOTV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NOTV Random Forest: Starting GridSearchCV fit...
       ‚úÖ NDAQ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.1129 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.5680
    - TCN: MSE=0.4578
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4578
        ‚Ä¢ LSTM: MSE=0.5680
        ‚Ä¢ XGBoost: MSE=6.1129
        ‚Ä¢ Random Forest: MSE=7.4819
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.3816
   ‚úÖ NDAQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NDAQ (TargetReturn): TCN with MSE=0.4578
üêõ DEBUG: NDAQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NDAQ.
üêõ DEBUG: NDAQ - Moving model to CPU before return...
üêõ DEBUG [23:55:37.166]: NDAQ - Returning result metadata...
üêõ DEBUG: train_worker started for DG
  ‚öôÔ∏è Training models for DG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DG: Initiating feature extraction for training.
  [DIAGNOSTIC] DG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DG: rows after features available: 126
üéØ DG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DG: Training LSTM (50 epochs)...
      ‚è≥ DG LSTM: Epoch 10/50 (20%)
      ‚è≥ DG LSTM: Epoch 20/50 (40%)
      ‚è≥ DG LSTM: Epoch 30/50 (60%)
      ‚è≥ DG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.124838
         RMSE: 0.353324
         R¬≤ Score: -1.1582 (Poor - 115.8% variance explained)
      üîπ DG: Training TCN (50 epochs)...
      ‚è≥ DG TCN: Epoch 10/50 (20%)
      ‚è≥ DG TCN: Epoch 20/50 (40%)
       ‚úÖ CMCM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.1482 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.6007
    - TCN: MSE=0.3284
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3284
        ‚Ä¢ LSTM: MSE=0.6007
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.9458
        ‚Ä¢ Random Forest: MSE=45.3115
        ‚Ä¢ XGBoost: MSE=54.1482
   ‚úÖ CMCM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CMCM (TargetReturn): TCN with MSE=0.3284
üêõ DEBUG: CMCM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CMCM.
üêõ DEBUG: CMCM - Moving model to CPU before return...
üêõ DEBUG [23:55:39.756]: CMCM - Returning result metadata...
üêõ DEBUG [23:55:39.757]: Main received result for CMCM
üêõ DEBUG [23:55:39.757]: Main received result for NDAQ
üêõ DEBUG: train_worker started for SMFG
  ‚öôÔ∏è Training models for SMFG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - SMFG: Initiating feature extraction for training.
  [DIAGNOSTIC] SMFG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMFG: rows after features available: 126
üéØ SMFG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ DG TCN: Epoch 30/50 (60%)
  [DIAGNOSTIC] SMFG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMFG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMFG: Training LSTM (50 epochs)...
      ‚è≥ DG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.065946
         RMSE: 0.256799
         R¬≤ Score: -0.1401
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DG Random Forest: Starting GridSearchCV fit...
       ‚úÖ NOTV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=79.2133 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NOTV LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SMFG LSTM: Epoch 10/50 (20%)
      ‚è≥ SMFG LSTM: Epoch 20/50 (40%)
       ‚úÖ NOTV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=206.4707 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NOTV XGBoost: Starting GridSearchCV fit...
       ‚úÖ OGIG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.9523 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.5907
    - TCN: MSE=0.3582
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3582
        ‚Ä¢ LSTM: MSE=0.5907
        ‚Ä¢ XGBoost: MSE=6.9523
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5512
        ‚Ä¢ Random Forest: MSE=10.1683
   ‚úÖ OGIG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OGIG (TargetReturn): TCN with MSE=0.3582
üêõ DEBUG: OGIG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OGIG.
üêõ DEBUG: OGIG - Moving model to CPU before return...
üêõ DEBUG [23:55:40.998]: OGIG - Returning result metadata...
üêõ DEBUG [23:55:40.998]: Main received result for OGIG
üêõ DEBUG: train_worker started for CCEP
  ‚öôÔ∏è Training models for CCEP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - CCEP: Initiating feature extraction for training.
  [DIAGNOSTIC] CCEP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CCEP: rows after features available: 126
üéØ CCEP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CCEP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CCEP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CCEP: Training LSTM (50 epochs)...
      ‚è≥ SMFG LSTM: Epoch 30/50 (60%)
      ‚è≥ CCEP LSTM: Epoch 10/50 (20%)
      ‚è≥ SMFG LSTM: Epoch 40/50 (80%)
      ‚è≥ CCEP LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.355873
         RMSE: 0.596551
         R¬≤ Score: -1.1557 (Poor - 115.6% variance explained)
      üîπ SMFG: Training TCN (50 epochs)...
      ‚è≥ SMFG TCN: Epoch 10/50 (20%)
      ‚è≥ SMFG TCN: Epoch 20/50 (40%)
      ‚è≥ SMFG TCN: Epoch 30/50 (60%)
      ‚è≥ CCEP LSTM: Epoch 30/50 (60%)
      ‚è≥ SMFG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.238621
         RMSE: 0.488488
         R¬≤ Score: -0.4455
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMFG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMFG Random Forest: Starting GridSearchCV fit...
       ‚úÖ DG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=31.4570 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ CCEP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.384406
         RMSE: 0.620005
         R¬≤ Score: -1.0061 (Poor - 100.6% variance explained)
      üîπ CCEP: Training TCN (50 epochs)...
       ‚úÖ DG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.2774 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DG XGBoost: Starting GridSearchCV fit...
      ‚è≥ CCEP TCN: Epoch 10/50 (20%)
      ‚è≥ CCEP TCN: Epoch 20/50 (40%)
      ‚è≥ CCEP TCN: Epoch 30/50 (60%)
      ‚è≥ CCEP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.195657
         RMSE: 0.442332
         R¬≤ Score: -0.0211
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CCEP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CCEP Random Forest: Starting GridSearchCV fit...
       ‚úÖ SMFG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.2087 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMFG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SMFG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.1543 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMFG XGBoost: Starting GridSearchCV fit...
       ‚úÖ CCEP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.4082 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CCEP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CCEP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.0064 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CCEP XGBoost: Starting GridSearchCV fit...
       ‚úÖ CEPU XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.8184 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.1439
    - TCN: MSE=0.1750
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1439
        ‚Ä¢ TCN: MSE=0.1750
        ‚Ä¢ XGBoost: MSE=13.8184
        ‚Ä¢ Random Forest: MSE=16.5432
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.1854
   ‚úÖ CEPU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CEPU (TargetReturn): LSTM with MSE=0.1439
üêõ DEBUG: CEPU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CEPU.
üêõ DEBUG: CEPU - Moving model to CPU before return...
üêõ DEBUG [23:55:49.403]: CEPU - Returning result metadata...
üêõ DEBUG [23:55:49.403]: Main received result for CEPU
üêõ DEBUG: Training progress: 684/959 done
üêõ DEBUG: train_worker started for MT
  ‚öôÔ∏è Training models for MT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - MT: Initiating feature extraction for training.
  [DIAGNOSTIC] MT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MT: rows after features available: 126
üéØ MT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MT: Training LSTM (50 epochs)...
      ‚è≥ MT LSTM: Epoch 10/50 (20%)
      ‚è≥ MT LSTM: Epoch 20/50 (40%)
      ‚è≥ MT LSTM: Epoch 30/50 (60%)
      ‚è≥ MT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.450425
         RMSE: 0.671137
         R¬≤ Score: -1.1914 (Poor - 119.1% variance explained)
      üîπ MT: Training TCN (50 epochs)...
      ‚è≥ MT TCN: Epoch 10/50 (20%)
       ‚úÖ PAR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.7789 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.6s
    - LSTM: MSE=0.3483
    - TCN: MSE=0.2930
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2930
        ‚Ä¢ LSTM: MSE=0.3483
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.7854
        ‚Ä¢ Random Forest: MSE=11.9958
        ‚Ä¢ XGBoost: MSE=15.7789
   ‚úÖ PAR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAR (TargetReturn): TCN with MSE=0.2930
üêõ DEBUG: PAR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAR.
üêõ DEBUG: PAR - Moving model to CPU before return...
üêõ DEBUG [23:55:52.632]: PAR - Returning result metadata...
üêõ DEBUG: train_worker started for V
üêõ DEBUG [23:55:52.635]: Main received result for PAR
      ‚è≥ MT TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for V (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - V: Initiating feature extraction for training.
  [DIAGNOSTIC] V: fetch_training_data - Initial data rows: 205
   ‚Ü≥ V: rows after features available: 126
üéØ V: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] V: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö V: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ V: Training LSTM (50 epochs)...
      ‚è≥ MT TCN: Epoch 30/50 (60%)
      ‚è≥ MT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.195449
         RMSE: 0.442096
         R¬≤ Score: 0.0491
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MT Random Forest: Starting GridSearchCV fit...
      ‚è≥ V LSTM: Epoch 10/50 (20%)
      ‚è≥ V LSTM: Epoch 20/50 (40%)
      ‚è≥ V LSTM: Epoch 30/50 (60%)
      ‚è≥ V LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.242548
         RMSE: 0.492492
         R¬≤ Score: -0.3812 (Poor - 38.1% variance explained)
      üîπ V: Training TCN (50 epochs)...
      ‚è≥ V TCN: Epoch 10/50 (20%)
      ‚è≥ V TCN: Epoch 20/50 (40%)
      ‚è≥ V TCN: Epoch 30/50 (60%)
      ‚è≥ V TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.181723
         RMSE: 0.426290
         R¬≤ Score: -0.0348
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä V: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ V Random Forest: Starting GridSearchCV fit...
       ‚úÖ MT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.7268 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=21.0570 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MT XGBoost: Starting GridSearchCV fit...
       ‚úÖ V Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5527 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ V LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ V LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.3090 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ V XGBoost: Starting GridSearchCV fit...
       ‚úÖ LTL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.8546 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.5661
    - TCN: MSE=0.4884
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4884
        ‚Ä¢ LSTM: MSE=0.5661
        ‚Ä¢ XGBoost: MSE=13.8546
        ‚Ä¢ Random Forest: MSE=20.1840
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3313
   ‚úÖ LTL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LTL (TargetReturn): TCN with MSE=0.4884
üêõ DEBUG: LTL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LTL.
üêõ DEBUG: LTL - Moving model to CPU before return...
üêõ DEBUG [23:56:20.192]: LTL - Returning result metadata...
üêõ DEBUG: train_worker started for COMM
üêõ DEBUG [23:56:20.193]: Main received result for LTL
  ‚öôÔ∏è Training models for COMM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - COMM: Initiating feature extraction for training.
  [DIAGNOSTIC] COMM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COMM: rows after features available: 126
üéØ COMM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COMM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COMM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COMM: Training LSTM (50 epochs)...
      ‚è≥ COMM LSTM: Epoch 10/50 (20%)
      ‚è≥ COMM LSTM: Epoch 20/50 (40%)
      ‚è≥ COMM LSTM: Epoch 30/50 (60%)
      ‚è≥ COMM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.633765
         RMSE: 0.796094
         R¬≤ Score: -1.2355 (Poor - 123.6% variance explained)
      üîπ COMM: Training TCN (50 epochs)...
      ‚è≥ COMM TCN: Epoch 10/50 (20%)
      ‚è≥ COMM TCN: Epoch 20/50 (40%)
      ‚è≥ COMM TCN: Epoch 30/50 (60%)
      ‚è≥ COMM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.282572
         RMSE: 0.531575
         R¬≤ Score: 0.0033
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COMM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COMM Random Forest: Starting GridSearchCV fit...
       ‚úÖ THFF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.8269 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.5442
    - TCN: MSE=0.2908
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2908
        ‚Ä¢ LSTM: MSE=0.5442
        ‚Ä¢ Random Forest: MSE=8.0324
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.4970
        ‚Ä¢ XGBoost: MSE=8.8269
   ‚úÖ THFF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for THFF (TargetReturn): TCN with MSE=0.2908
üêõ DEBUG: THFF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for THFF.
üêõ DEBUG: THFF - Moving model to CPU before return...
üêõ DEBUG [23:56:24.667]: THFF - Returning result metadata...
üêõ DEBUG: train_worker started for AGQ
üêõ DEBUG [23:56:24.668]: Main received result for THFF
  ‚öôÔ∏è Training models for AGQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - AGQ: Initiating feature extraction for training.
  [DIAGNOSTIC] AGQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AGQ: rows after features available: 126
üéØ AGQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AGQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AGQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AGQ: Training LSTM (50 epochs)...
      ‚è≥ AGQ LSTM: Epoch 10/50 (20%)
      ‚è≥ AGQ LSTM: Epoch 20/50 (40%)
      ‚è≥ AGQ LSTM: Epoch 30/50 (60%)
       ‚úÖ COMM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=216.0420 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COMM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AGQ LSTM: Epoch 40/50 (80%)
       ‚úÖ SMMT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=124.6756 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.1808
    - TCN: MSE=0.1496
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1496
        ‚Ä¢ LSTM: MSE=0.1808
        ‚Ä¢ LightGBM Regressor (CPU): MSE=71.3193
        ‚Ä¢ XGBoost: MSE=124.6756
        ‚Ä¢ Random Forest: MSE=125.0505
   ‚úÖ SMMT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMMT (TargetReturn): TCN with MSE=0.1496
üêõ DEBUG: SMMT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMMT.
üêõ DEBUG: SMMT - Moving model to CPU before return...
üêõ DEBUG [23:56:27.035]: SMMT - Returning result metadata...
üêõ DEBUG: train_worker started for GOLF
üêõ DEBUG [23:56:27.036]: Main received result for SMMT
üêõ DEBUG: Training progress: 688/959 done
      üìä LSTM Regression Metrics:
         MSE: 0.410005
         RMSE: 0.640317
         R¬≤ Score: -0.9331 (Poor - 93.3% variance explained)
      üîπ AGQ: Training TCN (50 epochs)...
  ‚öôÔ∏è Training models for GOLF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - GOLF: Initiating feature extraction for training.
  [DIAGNOSTIC] GOLF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GOLF: rows after features available: 126
üéØ GOLF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GOLF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GOLF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GOLF: Training LSTM (50 epochs)...
      ‚è≥ AGQ TCN: Epoch 10/50 (20%)
      ‚è≥ AGQ TCN: Epoch 20/50 (40%)
       ‚úÖ COMM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=180.2494 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COMM XGBoost: Starting GridSearchCV fit...
      ‚è≥ AGQ TCN: Epoch 30/50 (60%)
      ‚è≥ AGQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.337478
         RMSE: 0.580929
         R¬≤ Score: -0.5911
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AGQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AGQ Random Forest: Starting GridSearchCV fit...
      ‚è≥ GOLF LSTM: Epoch 10/50 (20%)
      ‚è≥ GOLF LSTM: Epoch 20/50 (40%)
      ‚è≥ GOLF LSTM: Epoch 30/50 (60%)
      ‚è≥ GOLF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.347685
         RMSE: 0.589648
         R¬≤ Score: -0.9632 (Poor - 96.3% variance explained)
      üîπ GOLF: Training TCN (50 epochs)...
      ‚è≥ GOLF TCN: Epoch 10/50 (20%)
      ‚è≥ GOLF TCN: Epoch 20/50 (40%)
      ‚è≥ GOLF TCN: Epoch 30/50 (60%)
      ‚è≥ GOLF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.232676
         RMSE: 0.482365
         R¬≤ Score: -0.3138
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GOLF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GOLF Random Forest: Starting GridSearchCV fit...
       ‚úÖ AGQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.0374 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AGQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AGQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=61.0489 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AGQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ GOLF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.9764 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GOLF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IMAX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.3634 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.5340
    - TCN: MSE=0.2424
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2424
        ‚Ä¢ LSTM: MSE=0.5340
        ‚Ä¢ Random Forest: MSE=9.0336
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.1049
        ‚Ä¢ XGBoost: MSE=10.3634
   ‚úÖ IMAX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IMAX (TargetReturn): TCN with MSE=0.2424
üêõ DEBUG: IMAX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IMAX.
üêõ DEBUG: IMAX - Moving model to CPU before return...
üêõ DEBUG [23:56:33.051]: IMAX - Returning result metadata...
üêõ DEBUG: train_worker started for HCI
üêõ DEBUG [23:56:33.054]: Main received result for IMAX
  ‚öôÔ∏è Training models for HCI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - HCI: Initiating feature extraction for training.
  [DIAGNOSTIC] HCI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HCI: rows after features available: 126
üéØ HCI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HCI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HCI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HCI: Training LSTM (50 epochs)...
      ‚è≥ HCI LSTM: Epoch 10/50 (20%)
       ‚úÖ GOLF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.6791 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GOLF XGBoost: Starting GridSearchCV fit...
      ‚è≥ HCI LSTM: Epoch 20/50 (40%)
      ‚è≥ HCI LSTM: Epoch 30/50 (60%)
      ‚è≥ HCI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.657474
         RMSE: 0.810848
         R¬≤ Score: -1.0710 (Poor - 107.1% variance explained)
      üîπ HCI: Training TCN (50 epochs)...
      ‚è≥ HCI TCN: Epoch 10/50 (20%)
      ‚è≥ HCI TCN: Epoch 20/50 (40%)
      ‚è≥ HCI TCN: Epoch 30/50 (60%)
      ‚è≥ HCI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.380399
         RMSE: 0.616765
         R¬≤ Score: -0.1982
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HCI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HCI Random Forest: Starting GridSearchCV fit...
       ‚úÖ HCI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.9837 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HCI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HCI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.5346 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HCI XGBoost: Starting GridSearchCV fit...
       ‚úÖ SMCI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=172.6150 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.0s
    - LSTM: MSE=0.1666
    - TCN: MSE=0.0760
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0760
        ‚Ä¢ LSTM: MSE=0.1666
        ‚Ä¢ XGBoost: MSE=172.6150
        ‚Ä¢ Random Forest: MSE=196.9603
        ‚Ä¢ LightGBM Regressor (CPU): MSE=232.5650
   ‚úÖ SMCI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMCI (TargetReturn): TCN with MSE=0.0760
üêõ DEBUG: SMCI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMCI.
üêõ DEBUG: SMCI - Moving model to CPU before return...
üêõ DEBUG [23:56:47.758]: SMCI - Returning result metadata...
üêõ DEBUG [23:56:47.758]: Main received result for SMCI
üêõ DEBUG: train_worker started for DTM
  ‚öôÔ∏è Training models for DTM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - DTM: Initiating feature extraction for training.
  [DIAGNOSTIC] DTM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DTM: rows after features available: 126
üéØ DTM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DTM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DTM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DTM: Training LSTM (50 epochs)...
      ‚è≥ DTM LSTM: Epoch 10/50 (20%)
      ‚è≥ DTM LSTM: Epoch 20/50 (40%)
      ‚è≥ DTM LSTM: Epoch 30/50 (60%)
      ‚è≥ DTM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.164453
         RMSE: 0.405528
         R¬≤ Score: -0.5494 (Poor - 54.9% variance explained)
      üîπ DTM: Training TCN (50 epochs)...
      ‚è≥ DTM TCN: Epoch 10/50 (20%)
      ‚è≥ DTM TCN: Epoch 20/50 (40%)
      ‚è≥ DTM TCN: Epoch 30/50 (60%)
      ‚è≥ DTM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.148010
         RMSE: 0.384721
         R¬≤ Score: -0.3944
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DTM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DTM Random Forest: Starting GridSearchCV fit...
       ‚úÖ DTM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.6645 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DTM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DTM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.7409 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DTM XGBoost: Starting GridSearchCV fit...
       ‚úÖ BGM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=173.1239 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.3267
    - TCN: MSE=0.2027
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2027
        ‚Ä¢ LSTM: MSE=0.3267
        ‚Ä¢ LightGBM Regressor (CPU): MSE=92.8535
        ‚Ä¢ Random Forest: MSE=112.4556
        ‚Ä¢ XGBoost: MSE=173.1239
   ‚úÖ BGM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BGM (TargetReturn): TCN with MSE=0.2027
üêõ DEBUG: BGM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BGM.
üêõ DEBUG: BGM - Moving model to CPU before return...
üêõ DEBUG [23:56:58.802]: BGM - Returning result metadata...
üêõ DEBUG [23:56:58.802]: Main received result for BGM
üêõ DEBUG: train_worker started for KBWB
  ‚öôÔ∏è Training models for KBWB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - KBWB: Initiating feature extraction for training.
  [DIAGNOSTIC] KBWB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KBWB: rows after features available: 126
üéØ KBWB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KBWB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KBWB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KBWB: Training LSTM (50 epochs)...
      ‚è≥ KBWB LSTM: Epoch 10/50 (20%)
      ‚è≥ KBWB LSTM: Epoch 20/50 (40%)
      ‚è≥ KBWB LSTM: Epoch 30/50 (60%)
      ‚è≥ KBWB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.648067
         RMSE: 0.805026
         R¬≤ Score: -1.0952 (Poor - 109.5% variance explained)
      üîπ KBWB: Training TCN (50 epochs)...
      ‚è≥ KBWB TCN: Epoch 10/50 (20%)
      ‚è≥ KBWB TCN: Epoch 20/50 (40%)
      ‚è≥ KBWB TCN: Epoch 30/50 (60%)
      ‚è≥ KBWB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.317280
         RMSE: 0.563276
         R¬≤ Score: -0.0258
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KBWB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KBWB Random Forest: Starting GridSearchCV fit...
       ‚úÖ NG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=113.8237 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.1722
    - TCN: MSE=0.1088
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1088
        ‚Ä¢ LSTM: MSE=0.1722
        ‚Ä¢ LightGBM Regressor (CPU): MSE=71.6973
        ‚Ä¢ Random Forest: MSE=76.3796
        ‚Ä¢ XGBoost: MSE=113.8237
   ‚úÖ NG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NG (TargetReturn): TCN with MSE=0.1088
üêõ DEBUG: NG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NG.
üêõ DEBUG: NG - Moving model to CPU before return...
üêõ DEBUG [23:57:02.384]: NG - Returning result metadata...
üêõ DEBUG [23:57:02.385]: Main received result for NG
üêõ DEBUG: Training progress: 692/959 done
üêõ DEBUG: train_worker started for APLD
  ‚öôÔ∏è Training models for APLD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - APLD: Initiating feature extraction for training.
  [DIAGNOSTIC] APLD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ APLD: rows after features available: 126
üéØ APLD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] APLD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö APLD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ APLD: Training LSTM (50 epochs)...
      ‚è≥ APLD LSTM: Epoch 10/50 (20%)
      ‚è≥ APLD LSTM: Epoch 20/50 (40%)
      ‚è≥ APLD LSTM: Epoch 30/50 (60%)
      ‚è≥ APLD LSTM: Epoch 40/50 (80%)
       ‚úÖ KBWB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.7193 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KBWB LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.704173
         RMSE: 0.839150
         R¬≤ Score: -1.4028 (Poor - 140.3% variance explained)
      üîπ APLD: Training TCN (50 epochs)...
      ‚è≥ APLD TCN: Epoch 10/50 (20%)
      ‚è≥ APLD TCN: Epoch 20/50 (40%)
       ‚úÖ KBWB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.7180 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KBWB XGBoost: Starting GridSearchCV fit...
      ‚è≥ APLD TCN: Epoch 30/50 (60%)
      ‚è≥ APLD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.490482
         RMSE: 0.700344
         R¬≤ Score: -0.6736
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä APLD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ APLD Random Forest: Starting GridSearchCV fit...
       ‚úÖ FN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=89.4547 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.3703
    - TCN: MSE=0.4327
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3703
        ‚Ä¢ TCN: MSE=0.4327
        ‚Ä¢ LightGBM Regressor (CPU): MSE=55.9334
        ‚Ä¢ Random Forest: MSE=60.0020
        ‚Ä¢ XGBoost: MSE=89.4547
   ‚úÖ FN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FN (TargetReturn): LSTM with MSE=0.3703
üêõ DEBUG: FN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FN.
üêõ DEBUG: FN - Moving model to CPU before return...
üêõ DEBUG [23:57:07.226]: FN - Returning result metadata...
üêõ DEBUG: train_worker started for NPKI
üêõ DEBUG [23:57:07.226]: Main received result for FN
  ‚öôÔ∏è Training models for NPKI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - NPKI: Initiating feature extraction for training.
  [DIAGNOSTIC] NPKI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NPKI: rows after features available: 126
üéØ NPKI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NPKI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NPKI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NPKI: Training LSTM (50 epochs)...
      ‚è≥ NPKI LSTM: Epoch 10/50 (20%)
       ‚úÖ APLD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=433.5790 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ APLD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NPKI LSTM: Epoch 20/50 (40%)
      ‚è≥ NPKI LSTM: Epoch 30/50 (60%)
       ‚úÖ APLD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=348.3758 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ APLD XGBoost: Starting GridSearchCV fit...
      ‚è≥ NPKI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.494150
         RMSE: 0.702958
         R¬≤ Score: -1.4303 (Poor - 143.0% variance explained)
      üîπ NPKI: Training TCN (50 epochs)...
      ‚è≥ NPKI TCN: Epoch 10/50 (20%)
      ‚è≥ NPKI TCN: Epoch 20/50 (40%)
      ‚è≥ NPKI TCN: Epoch 30/50 (60%)
      ‚è≥ NPKI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.244495
         RMSE: 0.494465
         R¬≤ Score: -0.2025
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NPKI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NPKI Random Forest: Starting GridSearchCV fit...
       ‚úÖ NPKI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.6341 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NPKI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NPKI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=31.7065 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NPKI XGBoost: Starting GridSearchCV fit...
       ‚úÖ UFCS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.1103 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.1269
    - TCN: MSE=0.1177
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1177
        ‚Ä¢ LSTM: MSE=0.1269
        ‚Ä¢ Random Forest: MSE=5.1744
        ‚Ä¢ XGBoost: MSE=8.1103
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.3029
   ‚úÖ UFCS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UFCS (TargetReturn): TCN with MSE=0.1177
üêõ DEBUG: UFCS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UFCS.
üêõ DEBUG: UFCS - Moving model to CPU before return...
üêõ DEBUG [23:57:15.684]: UFCS - Returning result metadata...
üêõ DEBUG [23:57:15.685]: Main received result for UFCS
üêõ DEBUG: train_worker started for OSK
  ‚öôÔ∏è Training models for OSK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - OSK: Initiating feature extraction for training.
  [DIAGNOSTIC] OSK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OSK: rows after features available: 126
üéØ OSK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OSK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OSK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OSK: Training LSTM (50 epochs)...
      ‚è≥ OSK LSTM: Epoch 10/50 (20%)
      ‚è≥ OSK LSTM: Epoch 20/50 (40%)
      ‚è≥ OSK LSTM: Epoch 30/50 (60%)
      ‚è≥ OSK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.623718
         RMSE: 0.789758
         R¬≤ Score: -0.8479 (Poor - 84.8% variance explained)
      üîπ OSK: Training TCN (50 epochs)...
      ‚è≥ OSK TCN: Epoch 10/50 (20%)
      ‚è≥ OSK TCN: Epoch 20/50 (40%)
      ‚è≥ OSK TCN: Epoch 30/50 (60%)
      ‚è≥ OSK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.562143
         RMSE: 0.749762
         R¬≤ Score: -0.6655
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OSK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OSK Random Forest: Starting GridSearchCV fit...
       ‚úÖ OSK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.9420 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OSK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OSK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.8733 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OSK XGBoost: Starting GridSearchCV fit...
       ‚úÖ DG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.4066 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.1248
    - TCN: MSE=0.0659
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0659
        ‚Ä¢ LSTM: MSE=0.1248
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.2774
        ‚Ä¢ Random Forest: MSE=31.4570
        ‚Ä¢ XGBoost: MSE=52.4066
   ‚úÖ DG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DG (TargetReturn): TCN with MSE=0.0659
üêõ DEBUG: DG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DG.
üêõ DEBUG: DG - Moving model to CPU before return...
üêõ DEBUG [23:57:40.964]: DG - Returning result metadata...
üêõ DEBUG: train_worker started for DDOG
  ‚öôÔ∏è Training models for DDOG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DDOG: Initiating feature extraction for training.
  [DIAGNOSTIC] DDOG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DDOG: rows after features available: 126
üéØ DDOG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DDOG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DDOG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DDOG: Training LSTM (50 epochs)...
      ‚è≥ DDOG LSTM: Epoch 10/50 (20%)
      ‚è≥ DDOG LSTM: Epoch 20/50 (40%)
      ‚è≥ DDOG LSTM: Epoch 30/50 (60%)
       ‚úÖ NOTV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=176.1030 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.1250
    - TCN: MSE=0.0789
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0789
        ‚Ä¢ LSTM: MSE=0.1250
        ‚Ä¢ Random Forest: MSE=79.2133
        ‚Ä¢ XGBoost: MSE=176.1030
        ‚Ä¢ LightGBM Regressor (CPU): MSE=206.4707
   ‚úÖ NOTV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NOTV (TargetReturn): TCN with MSE=0.0789
üêõ DEBUG: NOTV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NOTV.
üêõ DEBUG: NOTV - Moving model to CPU before return...
üêõ DEBUG [23:57:42.549]: NOTV - Returning result metadata...
üêõ DEBUG [23:57:42.550]: Main received result for NOTV
üêõ DEBUG [23:57:42.550]: Main received result for DG
üêõ DEBUG: Training progress: 696/959 done
üêõ DEBUG: train_worker started for VCTR
  ‚öôÔ∏è Training models for VCTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - VCTR: Initiating feature extraction for training.
  [DIAGNOSTIC] VCTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VCTR: rows after features available: 126
üéØ VCTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VCTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VCTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VCTR: Training LSTM (50 epochs)...
      ‚è≥ DDOG LSTM: Epoch 40/50 (80%)
      ‚è≥ VCTR LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.594838
         RMSE: 0.771257
         R¬≤ Score: -1.0784 (Poor - 107.8% variance explained)
      üîπ DDOG: Training TCN (50 epochs)...
      ‚è≥ DDOG TCN: Epoch 10/50 (20%)
      ‚è≥ VCTR LSTM: Epoch 20/50 (40%)
      ‚è≥ DDOG TCN: Epoch 20/50 (40%)
      ‚è≥ DDOG TCN: Epoch 30/50 (60%)
      ‚è≥ DDOG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.574095
         RMSE: 0.757690
         R¬≤ Score: -1.0059
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DDOG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DDOG Random Forest: Starting GridSearchCV fit...
      ‚è≥ VCTR LSTM: Epoch 30/50 (60%)
      ‚è≥ VCTR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.453065
         RMSE: 0.673101
         R¬≤ Score: -0.7667 (Poor - 76.7% variance explained)
      üîπ VCTR: Training TCN (50 epochs)...
      ‚è≥ VCTR TCN: Epoch 10/50 (20%)
      ‚è≥ VCTR TCN: Epoch 20/50 (40%)
      ‚è≥ VCTR TCN: Epoch 30/50 (60%)
       ‚úÖ SMFG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.1549 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.3559
    - TCN: MSE=0.2386
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2386
        ‚Ä¢ LSTM: MSE=0.3559
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.1543
        ‚Ä¢ Random Forest: MSE=9.2087
        ‚Ä¢ XGBoost: MSE=14.1549
   ‚úÖ SMFG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMFG (TargetReturn): TCN with MSE=0.2386
üêõ DEBUG: SMFG - train_and_evaluate_models completed
      ‚è≥ VCTR TCN: Epoch 40/50 (80%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMFG.
üêõ DEBUG: SMFG - Moving model to CPU before return...
üêõ DEBUG [23:57:45.474]: SMFG - Returning result metadata...
üêõ DEBUG [23:57:45.475]: Main received result for SMFG
üêõ DEBUG: train_worker started for SFST
  ‚öôÔ∏è Training models for SFST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - SFST: Initiating feature extraction for training.
  [DIAGNOSTIC] SFST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SFST: rows after features available: 126
üéØ SFST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SFST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SFST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SFST: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.329329
         RMSE: 0.573872
         R¬≤ Score: -0.2842
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VCTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VCTR Random Forest: Starting GridSearchCV fit...
      ‚è≥ SFST LSTM: Epoch 10/50 (20%)
      ‚è≥ SFST LSTM: Epoch 20/50 (40%)
       ‚úÖ DDOG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.9192 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DDOG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SFST LSTM: Epoch 30/50 (60%)
      ‚è≥ SFST LSTM: Epoch 40/50 (80%)
       ‚úÖ DDOG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=30.7622 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DDOG XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.375680
         RMSE: 0.612927
         R¬≤ Score: -1.4513 (Poor - 145.1% variance explained)
      üîπ SFST: Training TCN (50 epochs)...
      ‚è≥ SFST TCN: Epoch 10/50 (20%)
      ‚è≥ SFST TCN: Epoch 20/50 (40%)
      ‚è≥ SFST TCN: Epoch 30/50 (60%)
       ‚úÖ VCTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.3667 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VCTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SFST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.177281
         RMSE: 0.421048
         R¬≤ Score: -0.1568
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SFST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SFST Random Forest: Starting GridSearchCV fit...
       ‚úÖ CCEP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.7528 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.3844
    - TCN: MSE=0.1957
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1957
        ‚Ä¢ LSTM: MSE=0.3844
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.0064
        ‚Ä¢ Random Forest: MSE=4.4082
        ‚Ä¢ XGBoost: MSE=4.7528
   ‚úÖ CCEP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CCEP (TargetReturn): TCN with MSE=0.1957
üêõ DEBUG: CCEP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CCEP.
üêõ DEBUG: CCEP - Moving model to CPU before return...
üêõ DEBUG [23:57:48.369]: CCEP - Returning result metadata...
üêõ DEBUG: train_worker started for AGI
üêõ DEBUG [23:57:48.370]: Main received result for CCEP
  ‚öôÔ∏è Training models for AGI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - AGI: Initiating feature extraction for training.
  [DIAGNOSTIC] AGI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AGI: rows after features available: 126
üéØ AGI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AGI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AGI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AGI: Training LSTM (50 epochs)...
      ‚è≥ AGI LSTM: Epoch 10/50 (20%)
       ‚úÖ VCTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.7932 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VCTR XGBoost: Starting GridSearchCV fit...
      ‚è≥ AGI LSTM: Epoch 20/50 (40%)
      ‚è≥ AGI LSTM: Epoch 30/50 (60%)
      ‚è≥ AGI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.246443
         RMSE: 0.496431
         R¬≤ Score: -0.5311 (Poor - 53.1% variance explained)
      üîπ AGI: Training TCN (50 epochs)...
      ‚è≥ AGI TCN: Epoch 10/50 (20%)
      ‚è≥ AGI TCN: Epoch 20/50 (40%)
       ‚úÖ SFST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.1598 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SFST LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AGI TCN: Epoch 30/50 (60%)
      ‚è≥ AGI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.264044
         RMSE: 0.513852
         R¬≤ Score: -0.6405
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AGI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AGI Random Forest: Starting GridSearchCV fit...
       ‚úÖ SFST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.4583 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SFST XGBoost: Starting GridSearchCV fit...
       ‚úÖ MT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.1781 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.8s
    - LSTM: MSE=0.4504
    - TCN: MSE=0.1954
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1954
        ‚Ä¢ LSTM: MSE=0.4504
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.0570
        ‚Ä¢ XGBoost: MSE=21.1781
        ‚Ä¢ Random Forest: MSE=26.7268
   ‚úÖ MT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MT (TargetReturn): TCN with MSE=0.1954
üêõ DEBUG: MT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MT.
üêõ DEBUG: MT - Moving model to CPU before return...
üêõ DEBUG [23:57:53.542]: MT - Returning result metadata...
üêõ DEBUG: train_worker started for HAFC
üêõ DEBUG [23:57:53.543]: Main received result for MT
  ‚öôÔ∏è Training models for HAFC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - HAFC: Initiating feature extraction for training.
  [DIAGNOSTIC] HAFC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HAFC: rows after features available: 126
üéØ HAFC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HAFC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HAFC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HAFC: Training LSTM (50 epochs)...
       ‚úÖ AGI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.3769 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AGI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ HAFC LSTM: Epoch 10/50 (20%)
       ‚úÖ AGI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.7750 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AGI XGBoost: Starting GridSearchCV fit...
      ‚è≥ HAFC LSTM: Epoch 20/50 (40%)
      ‚è≥ HAFC LSTM: Epoch 30/50 (60%)
      ‚è≥ HAFC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.278823
         RMSE: 0.528037
         R¬≤ Score: -0.4816 (Poor - 48.2% variance explained)
      üîπ HAFC: Training TCN (50 epochs)...
      ‚è≥ HAFC TCN: Epoch 10/50 (20%)
      ‚è≥ HAFC TCN: Epoch 20/50 (40%)
      ‚è≥ HAFC TCN: Epoch 30/50 (60%)
      ‚è≥ HAFC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.220549
         RMSE: 0.469626
         R¬≤ Score: -0.1720
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HAFC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HAFC Random Forest: Starting GridSearchCV fit...
       ‚úÖ HAFC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.3017 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HAFC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HAFC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.2472 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HAFC XGBoost: Starting GridSearchCV fit...
       ‚úÖ V XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.6147 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.2425
    - TCN: MSE=0.1817
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1817
        ‚Ä¢ LSTM: MSE=0.2425
        ‚Ä¢ XGBoost: MSE=4.6147
        ‚Ä¢ Random Forest: MSE=5.5527
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.3090
   ‚úÖ V: Phase 3/3 - Model selection complete!
  üèÜ WINNER for V (TargetReturn): TCN with MSE=0.1817
üêõ DEBUG: V - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for V.
üêõ DEBUG: V - Moving model to CPU before return...
üêõ DEBUG [23:58:00.132]: V - Returning result metadata...
üêõ DEBUG: train_worker started for FEP
üêõ DEBUG [23:58:00.133]: Main received result for V
üêõ DEBUG: Training progress: 700/959 done
  ‚öôÔ∏è Training models for FEP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - FEP: Initiating feature extraction for training.
  [DIAGNOSTIC] FEP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FEP: rows after features available: 126
üéØ FEP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FEP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FEP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FEP: Training LSTM (50 epochs)...
      ‚è≥ FEP LSTM: Epoch 10/50 (20%)
      ‚è≥ FEP LSTM: Epoch 20/50 (40%)
      ‚è≥ FEP LSTM: Epoch 30/50 (60%)
      ‚è≥ FEP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.151123
         RMSE: 0.388745
         R¬≤ Score: -0.8179 (Poor - 81.8% variance explained)
      üîπ FEP: Training TCN (50 epochs)...
      ‚è≥ FEP TCN: Epoch 10/50 (20%)
      ‚è≥ FEP TCN: Epoch 20/50 (40%)
      ‚è≥ FEP TCN: Epoch 30/50 (60%)
      ‚è≥ FEP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.113555
         RMSE: 0.336979
         R¬≤ Score: -0.3660
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FEP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FEP Random Forest: Starting GridSearchCV fit...
       ‚úÖ FEP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.4658 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FEP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FEP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.9453 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FEP XGBoost: Starting GridSearchCV fit...
       ‚úÖ COMM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=192.1346 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.6338
    - TCN: MSE=0.2826
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2826
        ‚Ä¢ LSTM: MSE=0.6338
        ‚Ä¢ LightGBM Regressor (CPU): MSE=180.2494
        ‚Ä¢ XGBoost: MSE=192.1346
        ‚Ä¢ Random Forest: MSE=216.0420
   ‚úÖ COMM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COMM (TargetReturn): TCN with MSE=0.2826
üêõ DEBUG: COMM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COMM.
üêõ DEBUG: COMM - Moving model to CPU before return...
üêõ DEBUG [23:58:24.501]: COMM - Returning result metadata...
üêõ DEBUG: train_worker started for WMB
üêõ DEBUG [23:58:24.502]: Main received result for COMM
  ‚öôÔ∏è Training models for WMB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - WMB: Initiating feature extraction for training.
  [DIAGNOSTIC] WMB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WMB: rows after features available: 126
üéØ WMB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WMB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WMB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WMB: Training LSTM (50 epochs)...
      ‚è≥ WMB LSTM: Epoch 10/50 (20%)
      ‚è≥ WMB LSTM: Epoch 20/50 (40%)
      ‚è≥ WMB LSTM: Epoch 30/50 (60%)
      ‚è≥ WMB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.249977
         RMSE: 0.499977
         R¬≤ Score: -0.6349 (Poor - 63.5% variance explained)
      üîπ WMB: Training TCN (50 epochs)...
      ‚è≥ WMB TCN: Epoch 10/50 (20%)
      ‚è≥ WMB TCN: Epoch 20/50 (40%)
      ‚è≥ WMB TCN: Epoch 30/50 (60%)
      ‚è≥ WMB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.160478
         RMSE: 0.400598
         R¬≤ Score: -0.0496
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WMB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WMB Random Forest: Starting GridSearchCV fit...
       ‚úÖ WMB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.6563 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WMB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AGQ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=35.2360 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.4100
    - TCN: MSE=0.3375
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3375
        ‚Ä¢ LSTM: MSE=0.4100
        ‚Ä¢ XGBoost: MSE=35.2360
        ‚Ä¢ Random Forest: MSE=37.0374
        ‚Ä¢ LightGBM Regressor (CPU): MSE=61.0489
   ‚úÖ AGQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AGQ (TargetReturn): TCN with MSE=0.3375
üêõ DEBUG: AGQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AGQ.
üêõ DEBUG: AGQ - Moving model to CPU before return...
üêõ DEBUG [23:58:30.520]: AGQ - Returning result metadata...
üêõ DEBUG: train_worker started for ATEN
üêõ DEBUG [23:58:30.521]: Main received result for AGQ
  ‚öôÔ∏è Training models for ATEN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - ATEN: Initiating feature extraction for training.
  [DIAGNOSTIC] ATEN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ATEN: rows after features available: 126
üéØ ATEN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ATEN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ATEN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ATEN: Training LSTM (50 epochs)...
       ‚úÖ WMB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.4501 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WMB XGBoost: Starting GridSearchCV fit...
      ‚è≥ ATEN LSTM: Epoch 10/50 (20%)
      ‚è≥ ATEN LSTM: Epoch 20/50 (40%)
       ‚úÖ GOLF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.9098 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.3477
    - TCN: MSE=0.2327
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2327
        ‚Ä¢ LSTM: MSE=0.3477
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.6791
        ‚Ä¢ Random Forest: MSE=13.9764
        ‚Ä¢ XGBoost: MSE=16.9098
   ‚úÖ GOLF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GOLF (TargetReturn): TCN with MSE=0.2327
üêõ DEBUG: GOLF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GOLF.
üêõ DEBUG: GOLF - Moving model to CPU before return...
üêõ DEBUG [23:58:31.598]: GOLF - Returning result metadata...
üêõ DEBUG [23:58:31.599]: Main received result for GOLF
üêõ DEBUG: train_worker started for RZLT
  ‚öôÔ∏è Training models for RZLT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - RZLT: Initiating feature extraction for training.
  [DIAGNOSTIC] RZLT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RZLT: rows after features available: 126
üéØ RZLT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RZLT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RZLT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RZLT: Training LSTM (50 epochs)...
      ‚è≥ ATEN LSTM: Epoch 30/50 (60%)
      ‚è≥ RZLT LSTM: Epoch 10/50 (20%)
      ‚è≥ ATEN LSTM: Epoch 40/50 (80%)
      ‚è≥ RZLT LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.552762
         RMSE: 0.743480
         R¬≤ Score: -0.8197 (Poor - 82.0% variance explained)
      üîπ ATEN: Training TCN (50 epochs)...
      ‚è≥ RZLT LSTM: Epoch 30/50 (60%)
      ‚è≥ ATEN TCN: Epoch 10/50 (20%)
      ‚è≥ ATEN TCN: Epoch 20/50 (40%)
      ‚è≥ ATEN TCN: Epoch 30/50 (60%)
      ‚è≥ ATEN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.323053
         RMSE: 0.568378
         R¬≤ Score: -0.0635
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ATEN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ATEN Random Forest: Starting GridSearchCV fit...
      ‚è≥ RZLT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.581494
         RMSE: 0.762557
         R¬≤ Score: -1.1804 (Poor - 118.0% variance explained)
      üîπ RZLT: Training TCN (50 epochs)...
      ‚è≥ RZLT TCN: Epoch 10/50 (20%)
      ‚è≥ RZLT TCN: Epoch 20/50 (40%)
      ‚è≥ RZLT TCN: Epoch 30/50 (60%)
      ‚è≥ RZLT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.259743
         RMSE: 0.509650
         R¬≤ Score: 0.0261
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RZLT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RZLT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ATEN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.5980 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ATEN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HCI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.0415 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.6575
    - TCN: MSE=0.3804
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3804
        ‚Ä¢ LSTM: MSE=0.6575
        ‚Ä¢ Random Forest: MSE=8.9837
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5346
        ‚Ä¢ XGBoost: MSE=15.0415
   ‚úÖ HCI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HCI (TargetReturn): TCN with MSE=0.3804
üêõ DEBUG: HCI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HCI.
üêõ DEBUG: HCI - Moving model to CPU before return...
üêõ DEBUG [23:58:37.114]: HCI - Returning result metadata...
üêõ DEBUG [23:58:37.115]: Main received result for HCI
üêõ DEBUG: Training progress: 704/959 doneüêõ DEBUG: train_worker started for AL

  ‚öôÔ∏è Training models for AL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - AL: Initiating feature extraction for training.
  [DIAGNOSTIC] AL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AL: rows after features available: 126
üéØ AL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AL: Training LSTM (50 epochs)...
       ‚úÖ ATEN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.4807 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ATEN XGBoost: Starting GridSearchCV fit...
      ‚è≥ AL LSTM: Epoch 10/50 (20%)
       ‚úÖ RZLT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=55.3725 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RZLT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AL LSTM: Epoch 20/50 (40%)
       ‚úÖ RZLT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=78.4111 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RZLT XGBoost: Starting GridSearchCV fit...
      ‚è≥ AL LSTM: Epoch 30/50 (60%)
      ‚è≥ AL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.359730
         RMSE: 0.599775
         R¬≤ Score: -1.1123 (Poor - 111.2% variance explained)
      üîπ AL: Training TCN (50 epochs)...
      ‚è≥ AL TCN: Epoch 10/50 (20%)
      ‚è≥ AL TCN: Epoch 20/50 (40%)
      ‚è≥ AL TCN: Epoch 30/50 (60%)
      ‚è≥ AL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.200704
         RMSE: 0.448000
         R¬≤ Score: -0.1785
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AL Random Forest: Starting GridSearchCV fit...
       ‚úÖ AL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.7163 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.6075 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AL XGBoost: Starting GridSearchCV fit...
       ‚úÖ DTM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.1943 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.6s
    - LSTM: MSE=0.1645
    - TCN: MSE=0.1480
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1480
        ‚Ä¢ LSTM: MSE=0.1645
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.7409
        ‚Ä¢ Random Forest: MSE=12.6645
        ‚Ä¢ XGBoost: MSE=14.1943
   ‚úÖ DTM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DTM (TargetReturn): TCN with MSE=0.1480
üêõ DEBUG: DTM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DTM.
üêõ DEBUG: DTM - Moving model to CPU before return...
üêõ DEBUG [23:58:50.430]: DTM - Returning result metadata...
üêõ DEBUG [23:58:50.431]: Main received result for DTM
üêõ DEBUG: train_worker started for COR
  ‚öôÔ∏è Training models for COR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - COR: Initiating feature extraction for training.
  [DIAGNOSTIC] COR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COR: rows after features available: 126
üéØ COR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COR: Training LSTM (50 epochs)...
      ‚è≥ COR LSTM: Epoch 10/50 (20%)
      ‚è≥ COR LSTM: Epoch 20/50 (40%)
      ‚è≥ COR LSTM: Epoch 30/50 (60%)
      ‚è≥ COR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.279778
         RMSE: 0.528940
         R¬≤ Score: -1.0242 (Poor - 102.4% variance explained)
      üîπ COR: Training TCN (50 epochs)...
      ‚è≥ COR TCN: Epoch 10/50 (20%)
      ‚è≥ COR TCN: Epoch 20/50 (40%)
      ‚è≥ COR TCN: Epoch 30/50 (60%)
      ‚è≥ COR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.142019
         RMSE: 0.376854
         R¬≤ Score: -0.0275
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COR Random Forest: Starting GridSearchCV fit...
       ‚úÖ COR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.0483 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ COR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4690 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COR XGBoost: Starting GridSearchCV fit...
       ‚úÖ KBWB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.7267 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.3s
    - LSTM: MSE=0.6481
    - TCN: MSE=0.3173
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3173
        ‚Ä¢ LSTM: MSE=0.6481
        ‚Ä¢ Random Forest: MSE=11.7193
        ‚Ä¢ XGBoost: MSE=12.7267
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.7180
   ‚úÖ KBWB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KBWB (TargetReturn): TCN with MSE=0.3173
üêõ DEBUG: KBWB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KBWB.
üêõ DEBUG: KBWB - Moving model to CPU before return...
üêõ DEBUG [23:59:01.487]: KBWB - Returning result metadata...
üêõ DEBUG: train_worker started for ONC
üêõ DEBUG [23:59:01.490]: Main received result for KBWB
  ‚öôÔ∏è Training models for ONC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - ONC: Initiating feature extraction for training.
  [DIAGNOSTIC] ONC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ONC: rows after features available: 126
üéØ ONC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ONC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ONC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ONC: Training LSTM (50 epochs)...
      ‚è≥ ONC LSTM: Epoch 10/50 (20%)
      ‚è≥ ONC LSTM: Epoch 20/50 (40%)
      ‚è≥ ONC LSTM: Epoch 30/50 (60%)
      ‚è≥ ONC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.165017
         RMSE: 0.406223
         R¬≤ Score: -0.8432 (Poor - 84.3% variance explained)
      üîπ ONC: Training TCN (50 epochs)...
      ‚è≥ ONC TCN: Epoch 10/50 (20%)
      ‚è≥ ONC TCN: Epoch 20/50 (40%)
      ‚è≥ ONC TCN: Epoch 30/50 (60%)
      ‚è≥ ONC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.090700
         RMSE: 0.301164
         R¬≤ Score: -0.0131
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ONC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ONC Random Forest: Starting GridSearchCV fit...
       ‚úÖ ONC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.1855 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ONC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ONC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.9195 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ONC XGBoost: Starting GridSearchCV fit...
       ‚úÖ APLD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=531.4278 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.7042
    - TCN: MSE=0.4905
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4905
        ‚Ä¢ LSTM: MSE=0.7042
        ‚Ä¢ LightGBM Regressor (CPU): MSE=348.3758
        ‚Ä¢ Random Forest: MSE=433.5790
        ‚Ä¢ XGBoost: MSE=531.4278
   ‚úÖ APLD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for APLD (TargetReturn): TCN with MSE=0.4905
üêõ DEBUG: APLD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for APLD.
üêõ DEBUG: APLD - Moving model to CPU before return...
üêõ DEBUG [23:59:09.842]: APLD - Returning result metadata...
üêõ DEBUG: train_worker started for NEU
üêõ DEBUG [23:59:09.846]: Main received result for APLD
  ‚öôÔ∏è Training models for NEU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - NEU: Initiating feature extraction for training.
  [DIAGNOSTIC] NEU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NEU: rows after features available: 126
üéØ NEU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NEU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NEU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NEU: Training LSTM (50 epochs)...
      ‚è≥ NEU LSTM: Epoch 10/50 (20%)
       ‚úÖ NPKI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.9219 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.4941
    - TCN: MSE=0.2445
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2445
        ‚Ä¢ LSTM: MSE=0.4941
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.7065
        ‚Ä¢ Random Forest: MSE=39.6341
        ‚Ä¢ XGBoost: MSE=44.9219
   ‚úÖ NPKI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NPKI (TargetReturn): TCN with MSE=0.2445
üêõ DEBUG: NPKI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NPKI.
üêõ DEBUG: NPKI - Moving model to CPU before return...
üêõ DEBUG [23:59:10.528]: NPKI - Returning result metadata...
üêõ DEBUG: train_worker started for YINN
üêõ DEBUG [23:59:10.528]: Main received result for NPKI
üêõ DEBUG: Training progress: 708/959 done
  ‚öôÔ∏è Training models for YINN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - YINN: Initiating feature extraction for training.
  [DIAGNOSTIC] YINN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YINN: rows after features available: 126
üéØ YINN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YINN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YINN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YINN: Training LSTM (50 epochs)...
      ‚è≥ NEU LSTM: Epoch 20/50 (40%)
      ‚è≥ YINN LSTM: Epoch 10/50 (20%)
      ‚è≥ NEU LSTM: Epoch 30/50 (60%)
      ‚è≥ YINN LSTM: Epoch 20/50 (40%)
      ‚è≥ NEU LSTM: Epoch 40/50 (80%)
      ‚è≥ YINN LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.097302
         RMSE: 0.311933
         R¬≤ Score: -0.7189 (Poor - 71.9% variance explained)
      üîπ NEU: Training TCN (50 epochs)...
      ‚è≥ NEU TCN: Epoch 10/50 (20%)
      ‚è≥ NEU TCN: Epoch 20/50 (40%)
      ‚è≥ YINN LSTM: Epoch 40/50 (80%)
      ‚è≥ NEU TCN: Epoch 30/50 (60%)
      ‚è≥ NEU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.051753
         RMSE: 0.227492
         R¬≤ Score: 0.0858
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NEU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NEU Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.323719
         RMSE: 0.568963
         R¬≤ Score: -0.6219 (Poor - 62.2% variance explained)
      üîπ YINN: Training TCN (50 epochs)...
      ‚è≥ YINN TCN: Epoch 10/50 (20%)
      ‚è≥ YINN TCN: Epoch 20/50 (40%)
      ‚è≥ YINN TCN: Epoch 30/50 (60%)
      ‚è≥ YINN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.282668
         RMSE: 0.531665
         R¬≤ Score: -0.4163
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YINN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YINN Random Forest: Starting GridSearchCV fit...
       ‚úÖ NEU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.2026 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NEU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NEU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.8280 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NEU XGBoost: Starting GridSearchCV fit...
       ‚úÖ YINN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=125.5416 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YINN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ YINN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=114.4472 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YINN XGBoost: Starting GridSearchCV fit...
       ‚úÖ OSK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.4383 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.6237
    - TCN: MSE=0.5621
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5621
        ‚Ä¢ LSTM: MSE=0.6237
        ‚Ä¢ Random Forest: MSE=22.9420
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.8733
        ‚Ä¢ XGBoost: MSE=40.4383
   ‚úÖ OSK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OSK (TargetReturn): TCN with MSE=0.5621
üêõ DEBUG: OSK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OSK.
üêõ DEBUG: OSK - Moving model to CPU before return...
üêõ DEBUG [23:59:22.026]: OSK - Returning result metadata...
üêõ DEBUG: train_worker started for RBC
üêõ DEBUG [23:59:22.030]: Main received result for OSK
  ‚öôÔ∏è Training models for RBC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - RBC: Initiating feature extraction for training.
  [DIAGNOSTIC] RBC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RBC: rows after features available: 126
üéØ RBC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RBC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RBC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RBC: Training LSTM (50 epochs)...
      ‚è≥ RBC LSTM: Epoch 10/50 (20%)
      ‚è≥ RBC LSTM: Epoch 20/50 (40%)
      ‚è≥ RBC LSTM: Epoch 30/50 (60%)
      ‚è≥ RBC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.525905
         RMSE: 0.725193
         R¬≤ Score: -1.2468 (Poor - 124.7% variance explained)
      üîπ RBC: Training TCN (50 epochs)...
      ‚è≥ RBC TCN: Epoch 10/50 (20%)
      ‚è≥ RBC TCN: Epoch 20/50 (40%)
      ‚è≥ RBC TCN: Epoch 30/50 (60%)
      ‚è≥ RBC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.380679
         RMSE: 0.616992
         R¬≤ Score: -0.6263
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RBC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RBC Random Forest: Starting GridSearchCV fit...
       ‚úÖ RBC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.8577 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RBC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RBC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.8236 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RBC XGBoost: Starting GridSearchCV fit...
       ‚úÖ DDOG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.2859 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.5948
    - TCN: MSE=0.5741
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5741
        ‚Ä¢ LSTM: MSE=0.5948
        ‚Ä¢ XGBoost: MSE=17.2859
        ‚Ä¢ Random Forest: MSE=18.9192
        ‚Ä¢ LightGBM Regressor (CPU): MSE=30.7622
   ‚úÖ DDOG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DDOG (TargetReturn): TCN with MSE=0.5741
üêõ DEBUG: DDOG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DDOG.
üêõ DEBUG: DDOG - Moving model to CPU before return...
üêõ DEBUG [23:59:45.092]: DDOG - Returning result metadata...
üêõ DEBUG [23:59:45.092]: Main received result for DDOG
üêõ DEBUG: train_worker started for PRDO
  ‚öôÔ∏è Training models for PRDO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - PRDO: Initiating feature extraction for training.
  [DIAGNOSTIC] PRDO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRDO: rows after features available: 126
üéØ PRDO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRDO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRDO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRDO: Training LSTM (50 epochs)...
      ‚è≥ PRDO LSTM: Epoch 10/50 (20%)
      ‚è≥ PRDO LSTM: Epoch 20/50 (40%)
      ‚è≥ PRDO LSTM: Epoch 30/50 (60%)
      ‚è≥ PRDO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.712253
         RMSE: 0.843951
         R¬≤ Score: -1.4683 (Poor - 146.8% variance explained)
      üîπ PRDO: Training TCN (50 epochs)...
      ‚è≥ PRDO TCN: Epoch 10/50 (20%)
      ‚è≥ PRDO TCN: Epoch 20/50 (40%)
      ‚è≥ PRDO TCN: Epoch 30/50 (60%)
      ‚è≥ PRDO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.428043
         RMSE: 0.654250
         R¬≤ Score: -0.4833
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRDO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRDO Random Forest: Starting GridSearchCV fit...
       ‚úÖ SFST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.0743 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 117.8s
    - LSTM: MSE=0.3757
    - TCN: MSE=0.1773
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1773
        ‚Ä¢ LSTM: MSE=0.3757
        ‚Ä¢ Random Forest: MSE=9.1598
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.4583
        ‚Ä¢ XGBoost: MSE=13.0743
   ‚úÖ SFST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SFST (TargetReturn): TCN with MSE=0.1773
üêõ DEBUG: SFST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SFST.
üêõ DEBUG: SFST - Moving model to CPU before return...
üêõ DEBUG [23:59:49.564]: SFST - Returning result metadata...
üêõ DEBUG: train_worker started for CSGS
  ‚öôÔ∏è Training models for CSGS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CSGS: Initiating feature extraction for training.
  [DIAGNOSTIC] CSGS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CSGS: rows after features available: 126
üéØ CSGS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CSGS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CSGS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CSGS: Training LSTM (50 epochs)...
       ‚úÖ VCTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.2568 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.4531
    - TCN: MSE=0.3293
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3293
        ‚Ä¢ LSTM: MSE=0.4531
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.7932
        ‚Ä¢ XGBoost: MSE=14.2568
        ‚Ä¢ Random Forest: MSE=16.3667
   ‚úÖ VCTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VCTR (TargetReturn): TCN with MSE=0.3293
üêõ DEBUG: VCTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VCTR.
üêõ DEBUG: VCTR - Moving model to CPU before return...
üêõ DEBUG [23:59:49.957]: VCTR - Returning result metadata...
üêõ DEBUG [23:59:49.958]: Main received result for VCTR
üêõ DEBUG [23:59:49.958]: Main received result for SFSTüêõ DEBUG: train_worker started for EWS

üêõ DEBUG: Training progress: 712/959 done
  ‚öôÔ∏è Training models for EWS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - EWS: Initiating feature extraction for training.
  [DIAGNOSTIC] EWS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWS: rows after features available: 126
üéØ EWS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWS: Training LSTM (50 epochs)...
      ‚è≥ CSGS LSTM: Epoch 10/50 (20%)
      ‚è≥ EWS LSTM: Epoch 10/50 (20%)
      ‚è≥ CSGS LSTM: Epoch 20/50 (40%)
       ‚úÖ PRDO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.1773 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRDO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EWS LSTM: Epoch 20/50 (40%)
      ‚è≥ CSGS LSTM: Epoch 30/50 (60%)
       ‚úÖ PRDO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.3490 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRDO XGBoost: Starting GridSearchCV fit...
      ‚è≥ EWS LSTM: Epoch 30/50 (60%)
      ‚è≥ CSGS LSTM: Epoch 40/50 (80%)
      ‚è≥ EWS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.131182
         RMSE: 0.362191
         R¬≤ Score: -0.9449 (Poor - 94.5% variance explained)
      üîπ CSGS: Training TCN (50 epochs)...
      ‚è≥ CSGS TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.240492
         RMSE: 0.490399
         R¬≤ Score: -1.7285 (Poor - 172.9% variance explained)
      üîπ EWS: Training TCN (50 epochs)...
      ‚è≥ CSGS TCN: Epoch 20/50 (40%)
      ‚è≥ EWS TCN: Epoch 10/50 (20%)
      ‚è≥ CSGS TCN: Epoch 30/50 (60%)
      ‚è≥ EWS TCN: Epoch 20/50 (40%)
      ‚è≥ CSGS TCN: Epoch 40/50 (80%)
      ‚è≥ EWS TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.073052
         RMSE: 0.270282
         R¬≤ Score: -0.0831
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CSGS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CSGS Random Forest: Starting GridSearchCV fit...
      ‚è≥ EWS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.088167
         RMSE: 0.296929
         R¬≤ Score: -0.0003
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWS Random Forest: Starting GridSearchCV fit...
       ‚úÖ CSGS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.3655 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CSGS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.5111 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AGI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.0558 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.2464
    - TCN: MSE=0.2640
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2464
        ‚Ä¢ TCN: MSE=0.2640
        ‚Ä¢ Random Forest: MSE=15.3769
        ‚Ä¢ XGBoost: MSE=20.0558
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.7750
   ‚úÖ AGI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AGI (TargetReturn): LSTM with MSE=0.2464
üêõ DEBUG: AGI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AGI.
üêõ DEBUG: AGI - Moving model to CPU before return...
üêõ DEBUG [23:59:55.701]: AGI - Returning result metadata...
üêõ DEBUG [23:59:55.701]: Main received result for AGI
üêõ DEBUG: train_worker started for FWONA
  ‚öôÔ∏è Training models for FWONA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FWONA: Initiating feature extraction for training.
  [DIAGNOSTIC] FWONA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FWONA: rows after features available: 126
üéØ FWONA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FWONA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FWONA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FWONA: Training LSTM (50 epochs)...
      ‚è≥ FWONA LSTM: Epoch 10/50 (20%)
       ‚úÖ CSGS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.3618 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CSGS XGBoost: Starting GridSearchCV fit...
       ‚úÖ EWS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.8208 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWS XGBoost: Starting GridSearchCV fit...
      ‚è≥ FWONA LSTM: Epoch 20/50 (40%)
      ‚è≥ FWONA LSTM: Epoch 30/50 (60%)
      ‚è≥ FWONA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.770210
         RMSE: 0.877616
         R¬≤ Score: -1.9464 (Poor - 194.6% variance explained)
      üîπ FWONA: Training TCN (50 epochs)...
      ‚è≥ FWONA TCN: Epoch 10/50 (20%)
      ‚è≥ FWONA TCN: Epoch 20/50 (40%)
      ‚è≥ FWONA TCN: Epoch 30/50 (60%)
      ‚è≥ FWONA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.352976
         RMSE: 0.594117
         R¬≤ Score: -0.3503
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FWONA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FWONA Random Forest: Starting GridSearchCV fit...
       ‚úÖ HAFC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=22.2713 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.2788
    - TCN: MSE=0.2205
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2205
        ‚Ä¢ LSTM: MSE=0.2788
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.2472
        ‚Ä¢ Random Forest: MSE=17.3017
        ‚Ä¢ XGBoost: MSE=22.2713
   ‚úÖ HAFC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HAFC (TargetReturn): TCN with MSE=0.2205
üêõ DEBUG: HAFC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HAFC.
üêõ DEBUG: HAFC - Moving model to CPU before return...
üêõ DEBUG [23:59:59.609]: HAFC - Returning result metadata...
üêõ DEBUG [23:59:59.611]: Main received result for HAFC
üêõ DEBUG: train_worker started for BKR
  ‚öôÔ∏è Training models for BKR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - BKR: Initiating feature extraction for training.
  [DIAGNOSTIC] BKR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BKR: rows after features available: 126
üéØ BKR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BKR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BKR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BKR: Training LSTM (50 epochs)...
      ‚è≥ BKR LSTM: Epoch 10/50 (20%)
      ‚è≥ BKR LSTM: Epoch 20/50 (40%)
      ‚è≥ BKR LSTM: Epoch 30/50 (60%)
       ‚úÖ FWONA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6437 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FWONA LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BKR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.337391
         RMSE: 0.580853
         R¬≤ Score: -0.5255 (Poor - 52.5% variance explained)
      üîπ BKR: Training TCN (50 epochs)...
       ‚úÖ FWONA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.9822 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FWONA XGBoost: Starting GridSearchCV fit...
      ‚è≥ BKR TCN: Epoch 10/50 (20%)
      ‚è≥ BKR TCN: Epoch 20/50 (40%)
      ‚è≥ BKR TCN: Epoch 30/50 (60%)
      ‚è≥ BKR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.307378
         RMSE: 0.554417
         R¬≤ Score: -0.3898
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BKR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BKR Random Forest: Starting GridSearchCV fit...
       ‚úÖ BKR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.0781 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BKR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BKR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.1680 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BKR XGBoost: Starting GridSearchCV fit...
       ‚úÖ FEP XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.4501 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.7s
    - LSTM: MSE=0.1511
    - TCN: MSE=0.1136
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1136
        ‚Ä¢ LSTM: MSE=0.1511
        ‚Ä¢ XGBoost: MSE=6.4501
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.9453
        ‚Ä¢ Random Forest: MSE=7.4658
   ‚úÖ FEP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FEP (TargetReturn): TCN with MSE=0.1136
üêõ DEBUG: FEP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FEP.
üêõ DEBUG: FEP - Moving model to CPU before return...
üêõ DEBUG [00:00:08.986]: FEP - Returning result metadata...
üêõ DEBUG [00:00:08.987]: Main received result for FEP
üêõ DEBUG: train_worker started for NMG
  ‚öôÔ∏è Training models for NMG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - NMG: Initiating feature extraction for training.
  [DIAGNOSTIC] NMG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NMG: rows after features available: 126
üéØ NMG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NMG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NMG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NMG: Training LSTM (50 epochs)...
      ‚è≥ NMG LSTM: Epoch 10/50 (20%)
      ‚è≥ NMG LSTM: Epoch 20/50 (40%)
      ‚è≥ NMG LSTM: Epoch 30/50 (60%)
      ‚è≥ NMG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.170294
         RMSE: 0.412667
         R¬≤ Score: -1.6003 (Poor - 160.0% variance explained)
      üîπ NMG: Training TCN (50 epochs)...
      ‚è≥ NMG TCN: Epoch 10/50 (20%)
      ‚è≥ NMG TCN: Epoch 20/50 (40%)
      ‚è≥ NMG TCN: Epoch 30/50 (60%)
      ‚è≥ NMG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.071002
         RMSE: 0.266462
         R¬≤ Score: -0.0842
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NMG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NMG Random Forest: Starting GridSearchCV fit...
       ‚úÖ NMG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=48.1174 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NMG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NMG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=51.8234 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NMG XGBoost: Starting GridSearchCV fit...
       ‚úÖ WMB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.9879 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.2500
    - TCN: MSE=0.1605
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1605
        ‚Ä¢ LSTM: MSE=0.2500
        ‚Ä¢ Random Forest: MSE=7.6563
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.4501
        ‚Ä¢ XGBoost: MSE=8.9879
   ‚úÖ WMB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WMB (TargetReturn): TCN with MSE=0.1605
üêõ DEBUG: WMB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WMB.
üêõ DEBUG: WMB - Moving model to CPU before return...
üêõ DEBUG [00:00:27.140]: WMB - Returning result metadata...
üêõ DEBUG [00:00:27.141]: Main received result for WMB
üêõ DEBUG: Training progress: 716/959 done
üêõ DEBUG: train_worker started for MKL
  ‚öôÔ∏è Training models for MKL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - MKL: Initiating feature extraction for training.
  [DIAGNOSTIC] MKL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MKL: rows after features available: 126
üéØ MKL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MKL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MKL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MKL: Training LSTM (50 epochs)...
      ‚è≥ MKL LSTM: Epoch 10/50 (20%)
      ‚è≥ MKL LSTM: Epoch 20/50 (40%)
      ‚è≥ MKL LSTM: Epoch 30/50 (60%)
      ‚è≥ MKL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.198461
         RMSE: 0.445489
         R¬≤ Score: -0.6409 (Poor - 64.1% variance explained)
      üîπ MKL: Training TCN (50 epochs)...
      ‚è≥ MKL TCN: Epoch 10/50 (20%)
      ‚è≥ MKL TCN: Epoch 20/50 (40%)
      ‚è≥ MKL TCN: Epoch 30/50 (60%)
      ‚è≥ MKL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.212916
         RMSE: 0.461428
         R¬≤ Score: -0.7605
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MKL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MKL Random Forest: Starting GridSearchCV fit...
       ‚úÖ MKL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5836 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MKL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MKL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.7996 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MKL XGBoost: Starting GridSearchCV fit...
       ‚úÖ ATEN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.6722 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.1s
    - LSTM: MSE=0.5528
    - TCN: MSE=0.3231
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3231
        ‚Ä¢ LSTM: MSE=0.5528
        ‚Ä¢ Random Forest: MSE=14.5980
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.4807
        ‚Ä¢ XGBoost: MSE=20.6722
   ‚úÖ ATEN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ATEN (TargetReturn): TCN with MSE=0.3231
üêõ DEBUG: ATEN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ATEN.
üêõ DEBUG: ATEN - Moving model to CPU before return...
üêõ DEBUG [00:00:36.655]: ATEN - Returning result metadata...
üêõ DEBUG [00:00:36.656]: Main received result for ATEN
üêõ DEBUG: train_worker started for NTR
  ‚öôÔ∏è Training models for NTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NTR: Initiating feature extraction for training.
  [DIAGNOSTIC] NTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTR: rows after features available: 126
üéØ NTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTR: Training LSTM (50 epochs)...
      ‚è≥ NTR LSTM: Epoch 10/50 (20%)
      ‚è≥ NTR LSTM: Epoch 20/50 (40%)
      ‚è≥ NTR LSTM: Epoch 30/50 (60%)
      ‚è≥ NTR LSTM: Epoch 40/50 (80%)
       ‚úÖ RZLT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=59.3780 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.5815
    - TCN: MSE=0.2597
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2597
        ‚Ä¢ LSTM: MSE=0.5815
        ‚Ä¢ Random Forest: MSE=55.3725
        ‚Ä¢ XGBoost: MSE=59.3780
        ‚Ä¢ LightGBM Regressor (CPU): MSE=78.4111
   ‚úÖ RZLT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RZLT (TargetReturn): TCN with MSE=0.2597
üêõ DEBUG: RZLT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RZLT.
üêõ DEBUG: RZLT - Moving model to CPU before return...
üêõ DEBUG [00:00:38.614]: RZLT - Returning result metadata...
üêõ DEBUG: train_worker started for SONY
üêõ DEBUG [00:00:38.615]: Main received result for RZLT
  ‚öôÔ∏è Training models for SONY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - SONY: Initiating feature extraction for training.
  [DIAGNOSTIC] SONY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SONY: rows after features available: 126
üéØ SONY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SONY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SONY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SONY: Training LSTM (50 epochs)...
      ‚è≥ SONY LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.263633
         RMSE: 0.513452
         R¬≤ Score: -1.2188 (Poor - 121.9% variance explained)
      üîπ NTR: Training TCN (50 epochs)...
      ‚è≥ NTR TCN: Epoch 10/50 (20%)
      ‚è≥ NTR TCN: Epoch 20/50 (40%)
      ‚è≥ NTR TCN: Epoch 30/50 (60%)
      ‚è≥ NTR TCN: Epoch 40/50 (80%)
      ‚è≥ SONY LSTM: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.194408
         RMSE: 0.440917
         R¬≤ Score: -0.6362
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTR Random Forest: Starting GridSearchCV fit...
      ‚è≥ SONY LSTM: Epoch 30/50 (60%)
      ‚è≥ SONY LSTM: Epoch 40/50 (80%)
       ‚úÖ AL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.2272 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.3597
    - TCN: MSE=0.2007
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2007
        ‚Ä¢ LSTM: MSE=0.3597
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.6075
        ‚Ä¢ Random Forest: MSE=23.7163
        ‚Ä¢ XGBoost: MSE=31.2272
   ‚úÖ AL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AL (TargetReturn): TCN with MSE=0.2007
üêõ DEBUG: AL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AL.
üêõ DEBUG: AL - Moving model to CPU before return...
üêõ DEBUG [00:00:40.983]: AL - Returning result metadata...
üêõ DEBUG [00:00:40.984]: Main received result for AL
üêõ DEBUG: train_worker started for BETZ
  ‚öôÔ∏è Training models for BETZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BETZ: Initiating feature extraction for training.
  [DIAGNOSTIC] BETZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BETZ: rows after features available: 126
üéØ BETZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BETZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BETZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BETZ: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.242189
         RMSE: 0.492127
         R¬≤ Score: -0.7352 (Poor - 73.5% variance explained)
      üîπ SONY: Training TCN (50 epochs)...
      ‚è≥ SONY TCN: Epoch 10/50 (20%)
      ‚è≥ SONY TCN: Epoch 20/50 (40%)
      ‚è≥ SONY TCN: Epoch 30/50 (60%)
      ‚è≥ SONY TCN: Epoch 40/50 (80%)
      ‚è≥ BETZ LSTM: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.153396
         RMSE: 0.391658
         R¬≤ Score: -0.0990
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SONY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SONY Random Forest: Starting GridSearchCV fit...
      ‚è≥ BETZ LSTM: Epoch 20/50 (40%)
      ‚è≥ BETZ LSTM: Epoch 30/50 (60%)
       ‚úÖ NTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.5766 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BETZ LSTM: Epoch 40/50 (80%)
       ‚úÖ NTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.6114 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTR XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.458047
         RMSE: 0.676792
         R¬≤ Score: -0.8957 (Poor - 89.6% variance explained)
      üîπ BETZ: Training TCN (50 epochs)...
      ‚è≥ BETZ TCN: Epoch 10/50 (20%)
      ‚è≥ BETZ TCN: Epoch 20/50 (40%)
      ‚è≥ BETZ TCN: Epoch 30/50 (60%)
      ‚è≥ BETZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.435627
         RMSE: 0.660021
         R¬≤ Score: -0.8029
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BETZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BETZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ SONY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9257 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SONY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SONY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.6571 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SONY XGBoost: Starting GridSearchCV fit...
       ‚úÖ BETZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9335 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BETZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BETZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.1189 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BETZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ COR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.0407 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.2798
    - TCN: MSE=0.1420
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1420
        ‚Ä¢ LSTM: MSE=0.2798
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4690
        ‚Ä¢ Random Forest: MSE=7.0483
        ‚Ä¢ XGBoost: MSE=9.0407
   ‚úÖ COR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COR (TargetReturn): TCN with MSE=0.1420
üêõ DEBUG: COR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COR.
üêõ DEBUG: COR - Moving model to CPU before return...
üêõ DEBUG [00:00:57.441]: COR - Returning result metadata...
üêõ DEBUG: train_worker started for DUOL
üêõ DEBUG [00:00:57.442]: Main received result for COR
üêõ DEBUG: Training progress: 720/959 done
  ‚öôÔ∏è Training models for DUOL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - DUOL: Initiating feature extraction for training.
  [DIAGNOSTIC] DUOL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DUOL: rows after features available: 126
üéØ DUOL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DUOL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DUOL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DUOL: Training LSTM (50 epochs)...
      ‚è≥ DUOL LSTM: Epoch 10/50 (20%)
      ‚è≥ DUOL LSTM: Epoch 20/50 (40%)
      ‚è≥ DUOL LSTM: Epoch 30/50 (60%)
      ‚è≥ DUOL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.365247
         RMSE: 0.604357
         R¬≤ Score: -0.9195 (Poor - 91.9% variance explained)
      üîπ DUOL: Training TCN (50 epochs)...
      ‚è≥ DUOL TCN: Epoch 10/50 (20%)
      ‚è≥ DUOL TCN: Epoch 20/50 (40%)
      ‚è≥ DUOL TCN: Epoch 30/50 (60%)
      ‚è≥ DUOL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.201754
         RMSE: 0.449170
         R¬≤ Score: -0.0603
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DUOL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DUOL Random Forest: Starting GridSearchCV fit...
       ‚úÖ DUOL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=96.3853 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DUOL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DUOL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=41.1363 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DUOL XGBoost: Starting GridSearchCV fit...
       ‚úÖ ONC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.1814 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.1650
    - TCN: MSE=0.0907
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0907
        ‚Ä¢ LSTM: MSE=0.1650
        ‚Ä¢ Random Forest: MSE=35.1855
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.9195
        ‚Ä¢ XGBoost: MSE=37.1814
   ‚úÖ ONC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ONC (TargetReturn): TCN with MSE=0.0907
üêõ DEBUG: ONC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ONC.
üêõ DEBUG: ONC - Moving model to CPU before return...
üêõ DEBUG [00:01:06.381]: ONC - Returning result metadata...
üêõ DEBUG: train_worker started for AWI
üêõ DEBUG [00:01:06.385]: Main received result for ONC
  ‚öôÔ∏è Training models for AWI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - AWI: Initiating feature extraction for training.
  [DIAGNOSTIC] AWI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AWI: rows after features available: 126
üéØ AWI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AWI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AWI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AWI: Training LSTM (50 epochs)...
      ‚è≥ AWI LSTM: Epoch 10/50 (20%)
      ‚è≥ AWI LSTM: Epoch 20/50 (40%)
      ‚è≥ AWI LSTM: Epoch 30/50 (60%)
      ‚è≥ AWI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.394088
         RMSE: 0.627764
         R¬≤ Score: -0.7921 (Poor - 79.2% variance explained)
      üîπ AWI: Training TCN (50 epochs)...
      ‚è≥ AWI TCN: Epoch 10/50 (20%)
      ‚è≥ AWI TCN: Epoch 20/50 (40%)
      ‚è≥ AWI TCN: Epoch 30/50 (60%)
      ‚è≥ AWI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.374040
         RMSE: 0.611588
         R¬≤ Score: -0.7009
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AWI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AWI Random Forest: Starting GridSearchCV fit...
       ‚úÖ AWI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.2607 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AWI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AWI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.9573 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AWI XGBoost: Starting GridSearchCV fit...
       ‚úÖ YINN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=175.0184 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.3237
    - TCN: MSE=0.2827
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2827
        ‚Ä¢ LSTM: MSE=0.3237
        ‚Ä¢ LightGBM Regressor (CPU): MSE=114.4472
        ‚Ä¢ Random Forest: MSE=125.5416
        ‚Ä¢ XGBoost: MSE=175.0184
   ‚úÖ YINN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YINN (TargetReturn): TCN with MSE=0.2827
üêõ DEBUG: YINN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YINN.
üêõ DEBUG: YINN - Moving model to CPU before return...
üêõ DEBUG [00:01:16.882]: YINN - Returning result metadata...
üêõ DEBUG: train_worker started for BEN
  ‚öôÔ∏è Training models for BEN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - BEN: Initiating feature extraction for training.
  [DIAGNOSTIC] BEN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BEN: rows after features available: 126
üéØ BEN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BEN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BEN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BEN: Training LSTM (50 epochs)...
       ‚úÖ NEU XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.9581 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.0973
    - TCN: MSE=0.0518
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0518
        ‚Ä¢ LSTM: MSE=0.0973
        ‚Ä¢ XGBoost: MSE=10.9581
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8280
        ‚Ä¢ Random Forest: MSE=16.2026
   ‚úÖ NEU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NEU (TargetReturn): TCN with MSE=0.0518
üêõ DEBUG: NEU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NEU.
üêõ DEBUG: NEU - Moving model to CPU before return...
üêõ DEBUG [00:01:17.248]: NEU - Returning result metadata...
üêõ DEBUG: train_worker started for DAKT
üêõ DEBUG [00:01:17.249]: Main received result for NEU
üêõ DEBUG [00:01:17.249]: Main received result for YINN
  ‚öôÔ∏è Training models for DAKT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - DAKT: Initiating feature extraction for training.
  [DIAGNOSTIC] DAKT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DAKT: rows after features available: 126
üéØ DAKT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DAKT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DAKT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DAKT: Training LSTM (50 epochs)...
      ‚è≥ BEN LSTM: Epoch 10/50 (20%)
      ‚è≥ DAKT LSTM: Epoch 10/50 (20%)
      ‚è≥ BEN LSTM: Epoch 20/50 (40%)
      ‚è≥ DAKT LSTM: Epoch 20/50 (40%)
      ‚è≥ BEN LSTM: Epoch 30/50 (60%)
      ‚è≥ DAKT LSTM: Epoch 30/50 (60%)
      ‚è≥ BEN LSTM: Epoch 40/50 (80%)
      ‚è≥ DAKT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.416668
         RMSE: 0.645498
         R¬≤ Score: -0.6272 (Poor - 62.7% variance explained)
      üîπ BEN: Training TCN (50 epochs)...
      ‚è≥ BEN TCN: Epoch 10/50 (20%)
      ‚è≥ BEN TCN: Epoch 20/50 (40%)
      ‚è≥ BEN TCN: Epoch 30/50 (60%)
      ‚è≥ BEN TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.531918
         RMSE: 0.729327
         R¬≤ Score: -1.3106 (Poor - 131.1% variance explained)
      üîπ DAKT: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.389032
         RMSE: 0.623724
         R¬≤ Score: -0.5193
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BEN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BEN Random Forest: Starting GridSearchCV fit...
      ‚è≥ DAKT TCN: Epoch 10/50 (20%)
      ‚è≥ DAKT TCN: Epoch 20/50 (40%)
      ‚è≥ DAKT TCN: Epoch 30/50 (60%)
      ‚è≥ DAKT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.473921
         RMSE: 0.688419
         R¬≤ Score: -1.0587
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DAKT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DAKT Random Forest: Starting GridSearchCV fit...
       ‚úÖ BEN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.5588 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BEN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DAKT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.2712 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DAKT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BEN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.9904 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BEN XGBoost: Starting GridSearchCV fit...
       ‚úÖ DAKT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.1355 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DAKT XGBoost: Starting GridSearchCV fit...
       ‚úÖ RBC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.9521 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.5259
    - TCN: MSE=0.3807
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3807
        ‚Ä¢ LSTM: MSE=0.5259
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8236
        ‚Ä¢ Random Forest: MSE=15.8577
        ‚Ä¢ XGBoost: MSE=15.9521
   ‚úÖ RBC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RBC (TargetReturn): TCN with MSE=0.3807
üêõ DEBUG: RBC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RBC.
üêõ DEBUG: RBC - Moving model to CPU before return...
üêõ DEBUG [00:01:27.191]: RBC - Returning result metadata...
üêõ DEBUG: train_worker started for DBD
üêõ DEBUG [00:01:27.192]: Main received result for RBC
üêõ DEBUG: Training progress: 724/959 done
  ‚öôÔ∏è Training models for DBD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - DBD: Initiating feature extraction for training.
  [DIAGNOSTIC] DBD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DBD: rows after features available: 126
üéØ DBD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DBD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DBD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DBD: Training LSTM (50 epochs)...
      ‚è≥ DBD LSTM: Epoch 10/50 (20%)
      ‚è≥ DBD LSTM: Epoch 20/50 (40%)
      ‚è≥ DBD LSTM: Epoch 30/50 (60%)
      ‚è≥ DBD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.380843
         RMSE: 0.617125
         R¬≤ Score: -1.0254 (Poor - 102.5% variance explained)
      üîπ DBD: Training TCN (50 epochs)...
      ‚è≥ DBD TCN: Epoch 10/50 (20%)
      ‚è≥ DBD TCN: Epoch 20/50 (40%)
      ‚è≥ DBD TCN: Epoch 30/50 (60%)
      ‚è≥ DBD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.273073
         RMSE: 0.522564
         R¬≤ Score: -0.4522
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DBD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DBD Random Forest: Starting GridSearchCV fit...
       ‚úÖ DBD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.6907 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DBD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DBD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=17.1528 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DBD XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRDO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.8159 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.0s
    - LSTM: MSE=0.7123
    - TCN: MSE=0.4280
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4280
        ‚Ä¢ LSTM: MSE=0.7123
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.3490
        ‚Ä¢ Random Forest: MSE=20.1773
        ‚Ä¢ XGBoost: MSE=23.8159
   ‚úÖ PRDO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRDO (TargetReturn): TCN with MSE=0.4280
üêõ DEBUG: PRDO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRDO.
üêõ DEBUG: PRDO - Moving model to CPU before return...
üêõ DEBUG [00:01:46.373]: PRDO - Returning result metadata...
üêõ DEBUG: train_worker started for NPO
üêõ DEBUG [00:01:46.377]: Main received result for PRDO
  ‚öôÔ∏è Training models for NPO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - NPO: Initiating feature extraction for training.
  [DIAGNOSTIC] NPO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NPO: rows after features available: 126
üéØ NPO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NPO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NPO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NPO: Training LSTM (50 epochs)...
      ‚è≥ NPO LSTM: Epoch 10/50 (20%)
      ‚è≥ NPO LSTM: Epoch 20/50 (40%)
      ‚è≥ NPO LSTM: Epoch 30/50 (60%)
      ‚è≥ NPO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.717754
         RMSE: 0.847204
         R¬≤ Score: -1.1545 (Poor - 115.4% variance explained)
      üîπ NPO: Training TCN (50 epochs)...
      ‚è≥ NPO TCN: Epoch 10/50 (20%)
      ‚è≥ NPO TCN: Epoch 20/50 (40%)
      ‚è≥ NPO TCN: Epoch 30/50 (60%)
      ‚è≥ NPO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.361889
         RMSE: 0.601572
         R¬≤ Score: -0.0863
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NPO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NPO Random Forest: Starting GridSearchCV fit...
       ‚úÖ NPO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.2607 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NPO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NPO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=28.6695 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NPO XGBoost: Starting GridSearchCV fit...
       ‚úÖ CSGS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.2052 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.1312
    - TCN: MSE=0.0731
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0731
        ‚Ä¢ LSTM: MSE=0.1312
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.3618
        ‚Ä¢ Random Forest: MSE=8.3655
        ‚Ä¢ XGBoost: MSE=9.2052
   ‚úÖ CSGS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CSGS (TargetReturn): TCN with MSE=0.0731
üêõ DEBUG: CSGS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CSGS.
üêõ DEBUG: CSGS - Moving model to CPU before return...
üêõ DEBUG [00:01:55.058]: CSGS - Returning result metadata...
üêõ DEBUG [00:01:55.058]: Main received result for CSGS
üêõ DEBUG: train_worker started for VNM
  ‚öôÔ∏è Training models for VNM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - VNM: Initiating feature extraction for training.
  [DIAGNOSTIC] VNM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VNM: rows after features available: 126
üéØ VNM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VNM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VNM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VNM: Training LSTM (50 epochs)...
      ‚è≥ VNM LSTM: Epoch 10/50 (20%)
      ‚è≥ VNM LSTM: Epoch 20/50 (40%)
      ‚è≥ VNM LSTM: Epoch 30/50 (60%)
       ‚úÖ EWS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.0321 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.2405
    - TCN: MSE=0.0882
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0882
        ‚Ä¢ LSTM: MSE=0.2405
        ‚Ä¢ Random Forest: MSE=4.5111
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.8208
        ‚Ä¢ XGBoost: MSE=6.0321
   ‚úÖ EWS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EWS (TargetReturn): TCN with MSE=0.0882
üêõ DEBUG: EWS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWS.
üêõ DEBUG: EWS - Moving model to CPU before return...
üêõ DEBUG [00:01:56.924]: EWS - Returning result metadata...
üêõ DEBUG: train_worker started for NVDY
üêõ DEBUG [00:01:56.926]: Main received result for EWS
  ‚öôÔ∏è Training models for NVDY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - NVDY: Initiating feature extraction for training.
  [DIAGNOSTIC] NVDY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVDY: rows after features available: 126
üéØ NVDY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVDY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVDY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVDY: Training LSTM (50 epochs)...
      ‚è≥ VNM LSTM: Epoch 40/50 (80%)
      ‚è≥ NVDY LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.339708
         RMSE: 0.582845
         R¬≤ Score: -0.7708 (Poor - 77.1% variance explained)
      üîπ VNM: Training TCN (50 epochs)...
      ‚è≥ VNM TCN: Epoch 10/50 (20%)
      ‚è≥ VNM TCN: Epoch 20/50 (40%)
      ‚è≥ VNM TCN: Epoch 30/50 (60%)
      ‚è≥ NVDY LSTM: Epoch 20/50 (40%)
      ‚è≥ VNM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.313612
         RMSE: 0.560011
         R¬≤ Score: -0.6348
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VNM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VNM Random Forest: Starting GridSearchCV fit...
      ‚è≥ NVDY LSTM: Epoch 30/50 (60%)
      ‚è≥ NVDY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.423004
         RMSE: 0.650388
         R¬≤ Score: -0.7748 (Poor - 77.5% variance explained)
      üîπ NVDY: Training TCN (50 epochs)...
      ‚è≥ NVDY TCN: Epoch 10/50 (20%)
      ‚è≥ NVDY TCN: Epoch 20/50 (40%)
      ‚è≥ NVDY TCN: Epoch 30/50 (60%)
      ‚è≥ NVDY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.493078
         RMSE: 0.702195
         R¬≤ Score: -1.0689
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVDY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVDY Random Forest: Starting GridSearchCV fit...
       ‚úÖ VNM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.3531 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VNM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VNM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.1638 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VNM XGBoost: Starting GridSearchCV fit...
       ‚úÖ FWONA XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.5386 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.7702
    - TCN: MSE=0.3530
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3530
        ‚Ä¢ LSTM: MSE=0.7702
        ‚Ä¢ XGBoost: MSE=11.5386
        ‚Ä¢ Random Forest: MSE=11.6437
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.9822
   ‚úÖ FWONA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FWONA (TargetReturn): TCN with MSE=0.3530
üêõ DEBUG: FWONA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FWONA.
üêõ DEBUG: FWONA - Moving model to CPU before return...
üêõ DEBUG [00:02:01.840]: FWONA - Returning result metadata...
üêõ DEBUG: train_worker started for BMBL
üêõ DEBUG [00:02:01.842]: Main received result for FWONA
üêõ DEBUG: Training progress: 728/959 done
  ‚öôÔ∏è Training models for BMBL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - BMBL: Initiating feature extraction for training.
  [DIAGNOSTIC] BMBL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BMBL: rows after features available: 126
üéØ BMBL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BMBL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BMBL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BMBL: Training LSTM (50 epochs)...
      ‚è≥ BMBL LSTM: Epoch 10/50 (20%)
       ‚úÖ NVDY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.2843 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVDY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BKR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.2348 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.3374
    - TCN: MSE=0.3074
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3074
        ‚Ä¢ LSTM: MSE=0.3374
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.1680
        ‚Ä¢ XGBoost: MSE=18.2348
        ‚Ä¢ Random Forest: MSE=21.0781
   ‚úÖ BKR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BKR (TargetReturn): TCN with MSE=0.3074
üêõ DEBUG: BKR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BKR.
üêõ DEBUG: BKR - Moving model to CPU before return...
üêõ DEBUG [00:02:02.819]: BKR - Returning result metadata...
üêõ DEBUG: train_worker started for NPCE
üêõ DEBUG [00:02:02.820]: Main received result for BKR
  ‚öôÔ∏è Training models for NPCE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NPCE: Initiating feature extraction for training.
  [DIAGNOSTIC] NPCE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NPCE: rows after features available: 126
üéØ NPCE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NPCE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NPCE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NPCE: Training LSTM (50 epochs)...
      ‚è≥ BMBL LSTM: Epoch 20/50 (40%)
       ‚úÖ NVDY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.8473 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVDY XGBoost: Starting GridSearchCV fit...
      ‚è≥ NPCE LSTM: Epoch 10/50 (20%)
      ‚è≥ BMBL LSTM: Epoch 30/50 (60%)
      ‚è≥ NPCE LSTM: Epoch 20/50 (40%)
      ‚è≥ BMBL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.725504
         RMSE: 0.851765
         R¬≤ Score: -1.1388 (Poor - 113.9% variance explained)
      üîπ BMBL: Training TCN (50 epochs)...
      ‚è≥ NPCE LSTM: Epoch 30/50 (60%)
      ‚è≥ BMBL TCN: Epoch 10/50 (20%)
      ‚è≥ BMBL TCN: Epoch 20/50 (40%)
      ‚è≥ BMBL TCN: Epoch 30/50 (60%)
      ‚è≥ BMBL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.668326
         RMSE: 0.817512
         R¬≤ Score: -0.9703
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BMBL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BMBL Random Forest: Starting GridSearchCV fit...
      ‚è≥ NPCE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.099042
         RMSE: 0.314709
         R¬≤ Score: -0.9471 (Poor - 94.7% variance explained)
      üîπ NPCE: Training TCN (50 epochs)...
      ‚è≥ NPCE TCN: Epoch 10/50 (20%)
      ‚è≥ NPCE TCN: Epoch 20/50 (40%)
      ‚è≥ NPCE TCN: Epoch 30/50 (60%)
      ‚è≥ NPCE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.051026
         RMSE: 0.225890
         R¬≤ Score: -0.0031
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NPCE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NPCE Random Forest: Starting GridSearchCV fit...
       ‚úÖ BMBL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=56.1713 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BMBL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BMBL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=67.6158 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BMBL XGBoost: Starting GridSearchCV fit...
       ‚úÖ NPCE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=74.3725 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NPCE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NPCE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=112.0762 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NPCE XGBoost: Starting GridSearchCV fit...
       ‚úÖ NMG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=67.5475 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 119.5s
    - LSTM: MSE=0.1703
    - TCN: MSE=0.0710
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0710
        ‚Ä¢ LSTM: MSE=0.1703
        ‚Ä¢ Random Forest: MSE=48.1174
        ‚Ä¢ LightGBM Regressor (CPU): MSE=51.8234
        ‚Ä¢ XGBoost: MSE=67.5475
   ‚úÖ NMG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NMG (TargetReturn): TCN with MSE=0.0710
üêõ DEBUG: NMG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NMG.
üêõ DEBUG: NMG - Moving model to CPU before return...
üêõ DEBUG [00:02:14.616]: NMG - Returning result metadata...
üêõ DEBUG: train_worker started for IVZ
üêõ DEBUG [00:02:14.621]: Main received result for NMG
  ‚öôÔ∏è Training models for IVZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - IVZ: Initiating feature extraction for training.
  [DIAGNOSTIC] IVZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IVZ: rows after features available: 126
üéØ IVZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IVZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IVZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IVZ: Training LSTM (50 epochs)...
      ‚è≥ IVZ LSTM: Epoch 10/50 (20%)
      ‚è≥ IVZ LSTM: Epoch 20/50 (40%)
      ‚è≥ IVZ LSTM: Epoch 30/50 (60%)
      ‚è≥ IVZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.616998
         RMSE: 0.785492
         R¬≤ Score: -0.8412 (Poor - 84.1% variance explained)
      üîπ IVZ: Training TCN (50 epochs)...
      ‚è≥ IVZ TCN: Epoch 10/50 (20%)
      ‚è≥ IVZ TCN: Epoch 20/50 (40%)
      ‚è≥ IVZ TCN: Epoch 30/50 (60%)
      ‚è≥ IVZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.551112
         RMSE: 0.742369
         R¬≤ Score: -0.6446
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IVZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IVZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ IVZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.2714 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IVZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IVZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=45.8572 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IVZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ MKL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.6942 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.5s
    - LSTM: MSE=0.1985
    - TCN: MSE=0.2129
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1985
        ‚Ä¢ TCN: MSE=0.2129
        ‚Ä¢ Random Forest: MSE=5.5836
        ‚Ä¢ XGBoost: MSE=5.6942
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.7996
   ‚úÖ MKL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MKL (TargetReturn): LSTM with MSE=0.1985
üêõ DEBUG: MKL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MKL.
üêõ DEBUG: MKL - Moving model to CPU before return...
üêõ DEBUG [00:02:29.905]: MKL - Returning result metadata...
üêõ DEBUG: train_worker started for SILJ
üêõ DEBUG [00:02:29.907]: Main received result for MKL
  ‚öôÔ∏è Training models for SILJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SILJ: Initiating feature extraction for training.
  [DIAGNOSTIC] SILJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SILJ: rows after features available: 126
üéØ SILJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SILJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SILJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SILJ: Training LSTM (50 epochs)...
      ‚è≥ SILJ LSTM: Epoch 10/50 (20%)
      ‚è≥ SILJ LSTM: Epoch 20/50 (40%)
      ‚è≥ SILJ LSTM: Epoch 30/50 (60%)
      ‚è≥ SILJ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.153184
         RMSE: 0.391387
         R¬≤ Score: -1.1055 (Poor - 110.6% variance explained)
      üîπ SILJ: Training TCN (50 epochs)...
      ‚è≥ SILJ TCN: Epoch 10/50 (20%)
      ‚è≥ SILJ TCN: Epoch 20/50 (40%)
      ‚è≥ SILJ TCN: Epoch 30/50 (60%)
      ‚è≥ SILJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.089725
         RMSE: 0.299541
         R¬≤ Score: -0.2333
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SILJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SILJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ SILJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.0196 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SILJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SILJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.8017 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SILJ XGBoost: Starting GridSearchCV fit...
       ‚úÖ NTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.0097 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.2636
    - TCN: MSE=0.1944
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1944
        ‚Ä¢ LSTM: MSE=0.2636
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.6114
        ‚Ä¢ Random Forest: MSE=8.5766
        ‚Ä¢ XGBoost: MSE=9.0097
   ‚úÖ NTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTR (TargetReturn): TCN with MSE=0.1944
üêõ DEBUG: NTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTR.
üêõ DEBUG: NTR - Moving model to CPU before return...
üêõ DEBUG [00:02:44.070]: NTR - Returning result metadata...
üêõ DEBUG: train_worker started for PPA
üêõ DEBUG [00:02:44.072]: Main received result for NTR
üêõ DEBUG: Training progress: 732/959 done
  ‚öôÔ∏è Training models for PPA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - PPA: Initiating feature extraction for training.
  [DIAGNOSTIC] PPA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PPA: rows after features available: 126
üéØ PPA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PPA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PPA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PPA: Training LSTM (50 epochs)...
      ‚è≥ PPA LSTM: Epoch 10/50 (20%)
       ‚úÖ SONY XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.6430 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.2422
    - TCN: MSE=0.1534
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1534
        ‚Ä¢ LSTM: MSE=0.2422
        ‚Ä¢ XGBoost: MSE=10.6430
        ‚Ä¢ Random Forest: MSE=10.9257
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6571
   ‚úÖ SONY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SONY (TargetReturn): TCN with MSE=0.1534
üêõ DEBUG: SONY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SONY.
üêõ DEBUG: SONY - Moving model to CPU before return...
üêõ DEBUG [00:02:44.705]: SONY - Returning result metadata...
üêõ DEBUG [00:02:44.705]: Main received result for SONYüêõ DEBUG: train_worker started for GL

  ‚öôÔ∏è Training models for GL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - GL: Initiating feature extraction for training.
  [DIAGNOSTIC] GL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GL: rows after features available: 126
üéØ GL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GL: Training LSTM (50 epochs)...
       ‚úÖ BETZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.2365 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.4580
    - TCN: MSE=0.4356
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4356
        ‚Ä¢ LSTM: MSE=0.4580
        ‚Ä¢ Random Forest: MSE=6.9335
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.1189
        ‚Ä¢ XGBoost: MSE=7.2365
   ‚úÖ BETZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BETZ (TargetReturn): TCN with MSE=0.4356
üêõ DEBUG: BETZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BETZ.
üêõ DEBUG: BETZ - Moving model to CPU before return...
üêõ DEBUG [00:02:45.048]: BETZ - Returning result metadata...
üêõ DEBUG: train_worker started for XPO
üêõ DEBUG [00:02:45.049]: Main received result for BETZ
      ‚è≥ PPA LSTM: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for XPO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - XPO: Initiating feature extraction for training.
  [DIAGNOSTIC] XPO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XPO: rows after features available: 126
üéØ XPO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XPO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XPO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XPO: Training LSTM (50 epochs)...
      ‚è≥ GL LSTM: Epoch 10/50 (20%)
      ‚è≥ PPA LSTM: Epoch 30/50 (60%)
      ‚è≥ XPO LSTM: Epoch 10/50 (20%)
      ‚è≥ GL LSTM: Epoch 20/50 (40%)
      ‚è≥ XPO LSTM: Epoch 20/50 (40%)
      ‚è≥ PPA LSTM: Epoch 40/50 (80%)
      ‚è≥ GL LSTM: Epoch 30/50 (60%)
      ‚è≥ XPO LSTM: Epoch 30/50 (60%)
      ‚è≥ GL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.301285
         RMSE: 0.548894
         R¬≤ Score: -0.6650 (Poor - 66.5% variance explained)
      üîπ PPA: Training TCN (50 epochs)...
      ‚è≥ PPA TCN: Epoch 10/50 (20%)
      ‚è≥ PPA TCN: Epoch 20/50 (40%)
      ‚è≥ PPA TCN: Epoch 30/50 (60%)
      ‚è≥ PPA TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.135186
         RMSE: 0.367676
         R¬≤ Score: -1.3980 (Poor - 139.8% variance explained)
      üîπ GL: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.192278
         RMSE: 0.438495
         R¬≤ Score: -0.0626
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PPA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PPA Random Forest: Starting GridSearchCV fit...
      ‚è≥ XPO LSTM: Epoch 40/50 (80%)
      ‚è≥ GL TCN: Epoch 10/50 (20%)
      ‚è≥ GL TCN: Epoch 20/50 (40%)
      ‚è≥ GL TCN: Epoch 30/50 (60%)
      ‚è≥ GL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.061438
         RMSE: 0.247868
         R¬≤ Score: -0.0899
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GL Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.780531
         RMSE: 0.883476
         R¬≤ Score: -1.2883 (Poor - 128.8% variance explained)
      üîπ XPO: Training TCN (50 epochs)...
      ‚è≥ XPO TCN: Epoch 10/50 (20%)
      ‚è≥ XPO TCN: Epoch 20/50 (40%)
      ‚è≥ XPO TCN: Epoch 30/50 (60%)
      ‚è≥ XPO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.553056
         RMSE: 0.743677
         R¬≤ Score: -0.6214
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XPO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XPO Random Forest: Starting GridSearchCV fit...
       ‚úÖ GL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5043 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PPA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9781 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PPA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ XPO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.9756 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XPO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.0501 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GL XGBoost: Starting GridSearchCV fit...
       ‚úÖ PPA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=14.9899 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PPA XGBoost: Starting GridSearchCV fit...
       ‚úÖ XPO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.6140 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XPO XGBoost: Starting GridSearchCV fit...
       ‚úÖ DUOL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=175.0033 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.9s
    - LSTM: MSE=0.3652
    - TCN: MSE=0.2018
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2018
        ‚Ä¢ LSTM: MSE=0.3652
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.1363
        ‚Ä¢ Random Forest: MSE=96.3853
        ‚Ä¢ XGBoost: MSE=175.0033
   ‚úÖ DUOL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DUOL (TargetReturn): TCN with MSE=0.2018
üêõ DEBUG: DUOL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DUOL.
üêõ DEBUG: DUOL - Moving model to CPU before return...
üêõ DEBUG [00:03:03.661]: DUOL - Returning result metadata...
üêõ DEBUG: train_worker started for DOCU
üêõ DEBUG [00:03:03.662]: Main received result for DUOL
  ‚öôÔ∏è Training models for DOCU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - DOCU: Initiating feature extraction for training.
  [DIAGNOSTIC] DOCU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DOCU: rows after features available: 126
üéØ DOCU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DOCU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DOCU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DOCU: Training LSTM (50 epochs)...
      ‚è≥ DOCU LSTM: Epoch 10/50 (20%)
      ‚è≥ DOCU LSTM: Epoch 20/50 (40%)
      ‚è≥ DOCU LSTM: Epoch 30/50 (60%)
      ‚è≥ DOCU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.134636
         RMSE: 0.366928
         R¬≤ Score: -0.5190 (Poor - 51.9% variance explained)
      üîπ DOCU: Training TCN (50 epochs)...
      ‚è≥ DOCU TCN: Epoch 10/50 (20%)
      ‚è≥ DOCU TCN: Epoch 20/50 (40%)
      ‚è≥ DOCU TCN: Epoch 30/50 (60%)
      ‚è≥ DOCU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.093273
         RMSE: 0.305406
         R¬≤ Score: -0.0523
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DOCU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DOCU Random Forest: Starting GridSearchCV fit...
       ‚úÖ DOCU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.3259 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DOCU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DOCU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.1269 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DOCU XGBoost: Starting GridSearchCV fit...
       ‚úÖ AWI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.4339 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.3s
    - LSTM: MSE=0.3941
    - TCN: MSE=0.3740
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3740
        ‚Ä¢ LSTM: MSE=0.3941
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.9573
        ‚Ä¢ Random Forest: MSE=7.2607
        ‚Ä¢ XGBoost: MSE=8.4339
   ‚úÖ AWI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AWI (TargetReturn): TCN with MSE=0.3740
üêõ DEBUG: AWI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AWI.
üêõ DEBUG: AWI - Moving model to CPU before return...
üêõ DEBUG [00:03:10.277]: AWI - Returning result metadata...
üêõ DEBUG: train_worker started for PBYI
üêõ DEBUG [00:03:10.278]: Main received result for AWI
üêõ DEBUG: Training progress: 736/959 done
  ‚öôÔ∏è Training models for PBYI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - PBYI: Initiating feature extraction for training.
  [DIAGNOSTIC] PBYI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PBYI: rows after features available: 126
üéØ PBYI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PBYI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PBYI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PBYI: Training LSTM (50 epochs)...
      ‚è≥ PBYI LSTM: Epoch 10/50 (20%)
      ‚è≥ PBYI LSTM: Epoch 20/50 (40%)
      ‚è≥ PBYI LSTM: Epoch 30/50 (60%)
      ‚è≥ PBYI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.201344
         RMSE: 0.448713
         R¬≤ Score: -0.7527 (Poor - 75.3% variance explained)
      üîπ PBYI: Training TCN (50 epochs)...
      ‚è≥ PBYI TCN: Epoch 10/50 (20%)
      ‚è≥ PBYI TCN: Epoch 20/50 (40%)
      ‚è≥ PBYI TCN: Epoch 30/50 (60%)
      ‚è≥ PBYI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.116274
         RMSE: 0.340989
         R¬≤ Score: -0.0122
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PBYI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PBYI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PBYI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.8479 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PBYI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PBYI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.4061 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PBYI XGBoost: Starting GridSearchCV fit...
       ‚úÖ BEN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=36.8553 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.4167
    - TCN: MSE=0.3890
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3890
        ‚Ä¢ LSTM: MSE=0.4167
        ‚Ä¢ Random Forest: MSE=23.5588
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.9904
        ‚Ä¢ XGBoost: MSE=36.8553
   ‚úÖ BEN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BEN (TargetReturn): TCN with MSE=0.3890
üêõ DEBUG: BEN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BEN.
üêõ DEBUG: BEN - Moving model to CPU before return...
üêõ DEBUG [00:03:21.515]: BEN - Returning result metadata...
üêõ DEBUG: train_worker started for WCC
üêõ DEBUG [00:03:21.520]: Main received result for BEN
  ‚öôÔ∏è Training models for WCC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - WCC: Initiating feature extraction for training.
  [DIAGNOSTIC] WCC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WCC: rows after features available: 126
üéØ WCC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WCC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WCC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WCC: Training LSTM (50 epochs)...
      ‚è≥ WCC LSTM: Epoch 10/50 (20%)
      ‚è≥ WCC LSTM: Epoch 20/50 (40%)
      ‚è≥ WCC LSTM: Epoch 30/50 (60%)
       ‚úÖ DAKT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.1392 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.5319
    - TCN: MSE=0.4739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4739
        ‚Ä¢ LSTM: MSE=0.5319
        ‚Ä¢ XGBoost: MSE=13.1392
        ‚Ä¢ Random Forest: MSE=17.2712
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.1355
   ‚úÖ DAKT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DAKT (TargetReturn): TCN with MSE=0.4739
üêõ DEBUG: DAKT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DAKT.
üêõ DEBUG: DAKT - Moving model to CPU before return...
üêõ DEBUG [00:03:23.138]: DAKT - Returning result metadata...
üêõ DEBUG [00:03:23.139]: Main received result for DAKT
üêõ DEBUG: train_worker started for EWG
  ‚öôÔ∏è Training models for EWG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - EWG: Initiating feature extraction for training.
  [DIAGNOSTIC] EWG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWG: rows after features available: 126
üéØ EWG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWG: Training LSTM (50 epochs)...
      ‚è≥ WCC LSTM: Epoch 40/50 (80%)
      ‚è≥ EWG LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.771505
         RMSE: 0.878354
         R¬≤ Score: -1.5298 (Poor - 153.0% variance explained)
      üîπ WCC: Training TCN (50 epochs)...
      ‚è≥ WCC TCN: Epoch 10/50 (20%)
      ‚è≥ WCC TCN: Epoch 20/50 (40%)
      ‚è≥ EWG LSTM: Epoch 20/50 (40%)
      ‚è≥ WCC TCN: Epoch 30/50 (60%)
      ‚è≥ WCC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.474144
         RMSE: 0.688581
         R¬≤ Score: -0.5547
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WCC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WCC Random Forest: Starting GridSearchCV fit...
      ‚è≥ EWG LSTM: Epoch 30/50 (60%)
      ‚è≥ EWG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.173274
         RMSE: 0.416262
         R¬≤ Score: -0.4408 (Poor - 44.1% variance explained)
      üîπ EWG: Training TCN (50 epochs)...
      ‚è≥ EWG TCN: Epoch 10/50 (20%)
      ‚è≥ EWG TCN: Epoch 20/50 (40%)
      ‚è≥ EWG TCN: Epoch 30/50 (60%)
      ‚è≥ EWG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.120118
         RMSE: 0.346581
         R¬≤ Score: 0.0012
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWG Random Forest: Starting GridSearchCV fit...
       ‚úÖ WCC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.3134 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WCC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WCC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.4842 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WCC XGBoost: Starting GridSearchCV fit...
       ‚úÖ EWG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.7745 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.0741 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWG XGBoost: Starting GridSearchCV fit...
       ‚úÖ DBD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.6873 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.3808
    - TCN: MSE=0.2731
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2731
        ‚Ä¢ LSTM: MSE=0.3808
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.1528
        ‚Ä¢ XGBoost: MSE=21.6873
        ‚Ä¢ Random Forest: MSE=22.6907
   ‚úÖ DBD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DBD (TargetReturn): TCN with MSE=0.2731
üêõ DEBUG: DBD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DBD.
üêõ DEBUG: DBD - Moving model to CPU before return...
üêõ DEBUG [00:03:31.162]: DBD - Returning result metadata...
üêõ DEBUG: train_worker started for FDN
üêõ DEBUG [00:03:31.166]: Main received result for DBD
  ‚öôÔ∏è Training models for FDN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - FDN: Initiating feature extraction for training.
  [DIAGNOSTIC] FDN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDN: rows after features available: 126
üéØ FDN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDN: Training LSTM (50 epochs)...
      ‚è≥ FDN LSTM: Epoch 10/50 (20%)
      ‚è≥ FDN LSTM: Epoch 20/50 (40%)
      ‚è≥ FDN LSTM: Epoch 30/50 (60%)
      ‚è≥ FDN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.412575
         RMSE: 0.642320
         R¬≤ Score: -0.5678 (Poor - 56.8% variance explained)
      üîπ FDN: Training TCN (50 epochs)...
      ‚è≥ FDN TCN: Epoch 10/50 (20%)
      ‚è≥ FDN TCN: Epoch 20/50 (40%)
      ‚è≥ FDN TCN: Epoch 30/50 (60%)
      ‚è≥ FDN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.482766
         RMSE: 0.694813
         R¬≤ Score: -0.8345
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDN Random Forest: Starting GridSearchCV fit...
       ‚úÖ FDN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.0478 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.5615 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDN XGBoost: Starting GridSearchCV fit...
       ‚úÖ NPO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.4041 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.2s
    - LSTM: MSE=0.7178
    - TCN: MSE=0.3619
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3619
        ‚Ä¢ LSTM: MSE=0.7178
        ‚Ä¢ Random Forest: MSE=22.2607
        ‚Ä¢ XGBoost: MSE=24.4041
        ‚Ä¢ LightGBM Regressor (CPU): MSE=28.6695
   ‚úÖ NPO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NPO (TargetReturn): TCN with MSE=0.3619
üêõ DEBUG: NPO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NPO.
üêõ DEBUG: NPO - Moving model to CPU before return...
üêõ DEBUG [00:03:48.306]: NPO - Returning result metadata...
üêõ DEBUG: train_worker started for DRTS
üêõ DEBUG [00:03:48.306]: Main received result for NPO
üêõ DEBUG: Training progress: 740/959 done
  ‚öôÔ∏è Training models for DRTS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - DRTS: Initiating feature extraction for training.
  [DIAGNOSTIC] DRTS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DRTS: rows after features available: 126
üéØ DRTS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DRTS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DRTS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DRTS: Training LSTM (50 epochs)...
      ‚è≥ DRTS LSTM: Epoch 10/50 (20%)
      ‚è≥ DRTS LSTM: Epoch 20/50 (40%)
      ‚è≥ DRTS LSTM: Epoch 30/50 (60%)
      ‚è≥ DRTS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.163788
         RMSE: 0.404707
         R¬≤ Score: -1.1065 (Poor - 110.6% variance explained)
      üîπ DRTS: Training TCN (50 epochs)...
      ‚è≥ DRTS TCN: Epoch 10/50 (20%)
      ‚è≥ DRTS TCN: Epoch 20/50 (40%)
      ‚è≥ DRTS TCN: Epoch 30/50 (60%)
      ‚è≥ DRTS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.112530
         RMSE: 0.335455
         R¬≤ Score: -0.4472
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DRTS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DRTS Random Forest: Starting GridSearchCV fit...
       ‚úÖ DRTS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.8143 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DRTS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DRTS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=33.9620 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DRTS XGBoost: Starting GridSearchCV fit...
       ‚úÖ VNM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.9524 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.3397
    - TCN: MSE=0.3136
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3136
        ‚Ä¢ LSTM: MSE=0.3397
        ‚Ä¢ Random Forest: MSE=7.3531
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.1638
        ‚Ä¢ XGBoost: MSE=12.9524
   ‚úÖ VNM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VNM (TargetReturn): TCN with MSE=0.3136
üêõ DEBUG: VNM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VNM.
üêõ DEBUG: VNM - Moving model to CPU before return...
üêõ DEBUG [00:03:59.272]: VNM - Returning result metadata...
üêõ DEBUG: train_worker started for GLAD
üêõ DEBUG [00:03:59.274]: Main received result for VNM
  ‚öôÔ∏è Training models for GLAD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - GLAD: Initiating feature extraction for training.
  [DIAGNOSTIC] GLAD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GLAD: rows after features available: 126
üéØ GLAD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GLAD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GLAD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GLAD: Training LSTM (50 epochs)...
      ‚è≥ GLAD LSTM: Epoch 10/50 (20%)
      ‚è≥ GLAD LSTM: Epoch 20/50 (40%)
      ‚è≥ GLAD LSTM: Epoch 30/50 (60%)
      ‚è≥ GLAD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.535748
         RMSE: 0.731948
         R¬≤ Score: -1.4322 (Poor - 143.2% variance explained)
      üîπ GLAD: Training TCN (50 epochs)...
      ‚è≥ GLAD TCN: Epoch 10/50 (20%)
      ‚è≥ GLAD TCN: Epoch 20/50 (40%)
      ‚è≥ GLAD TCN: Epoch 30/50 (60%)
      ‚è≥ GLAD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.322523
         RMSE: 0.567911
         R¬≤ Score: -0.4642
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GLAD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GLAD Random Forest: Starting GridSearchCV fit...
       ‚úÖ NVDY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.7191 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.4230
    - TCN: MSE=0.4931
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4230
        ‚Ä¢ TCN: MSE=0.4931
        ‚Ä¢ Random Forest: MSE=21.2843
        ‚Ä¢ XGBoost: MSE=23.7191
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.8473
   ‚úÖ NVDY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVDY (TargetReturn): LSTM with MSE=0.4230
üêõ DEBUG: NVDY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVDY.
üêõ DEBUG: NVDY - Moving model to CPU before return...
üêõ DEBUG [00:04:05.119]: NVDY - Returning result metadata...
üêõ DEBUG: train_worker started for FLUT
üêõ DEBUG [00:04:05.119]: Main received result for NVDY
  ‚öôÔ∏è Training models for FLUT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - FLUT: Initiating feature extraction for training.
  [DIAGNOSTIC] FLUT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FLUT: rows after features available: 126
üéØ FLUT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FLUT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FLUT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FLUT: Training LSTM (50 epochs)...
       ‚úÖ GLAD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5240 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GLAD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FLUT LSTM: Epoch 10/50 (20%)
      ‚è≥ FLUT LSTM: Epoch 20/50 (40%)
       ‚úÖ GLAD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.6895 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GLAD XGBoost: Starting GridSearchCV fit...
       ‚úÖ NPCE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=82.8713 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 116.5s
    - LSTM: MSE=0.0990
    - TCN: MSE=0.0510
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0510
        ‚Ä¢ LSTM: MSE=0.0990
        ‚Ä¢ Random Forest: MSE=74.3725
        ‚Ä¢ XGBoost: MSE=82.8713
        ‚Ä¢ LightGBM Regressor (CPU): MSE=112.0762
   ‚úÖ NPCE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NPCE (TargetReturn): TCN with MSE=0.0510
üêõ DEBUG: NPCE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NPCE.
üêõ DEBUG: NPCE - Moving model to CPU before return...
üêõ DEBUG [00:04:06.125]: NPCE - Returning result metadata...
üêõ DEBUG: train_worker started for NTGR
  ‚öôÔ∏è Training models for NTGR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NTGR: Initiating feature extraction for training.
  [DIAGNOSTIC] NTGR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTGR: rows after features available: 126
üéØ NTGR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTGR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTGR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTGR: Training LSTM (50 epochs)...
      ‚è≥ FLUT LSTM: Epoch 30/50 (60%)
      ‚è≥ NTGR LSTM: Epoch 10/50 (20%)
      ‚è≥ FLUT LSTM: Epoch 40/50 (80%)
      ‚è≥ NTGR LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.672790
         RMSE: 0.820238
         R¬≤ Score: -0.7157 (Poor - 71.6% variance explained)
      üîπ FLUT: Training TCN (50 epochs)...
      ‚è≥ FLUT TCN: Epoch 10/50 (20%)
      ‚è≥ FLUT TCN: Epoch 20/50 (40%)
      ‚è≥ FLUT TCN: Epoch 30/50 (60%)
      ‚è≥ NTGR LSTM: Epoch 30/50 (60%)
      ‚è≥ FLUT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.770365
         RMSE: 0.877704
         R¬≤ Score: -0.9645
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FLUT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FLUT Random Forest: Starting GridSearchCV fit...
      ‚è≥ NTGR LSTM: Epoch 40/50 (80%)
       ‚úÖ BMBL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=78.8777 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.5s
    - LSTM: MSE=0.7255
    - TCN: MSE=0.6683
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6683
        ‚Ä¢ LSTM: MSE=0.7255
        ‚Ä¢ Random Forest: MSE=56.1713
        ‚Ä¢ LightGBM Regressor (CPU): MSE=67.6158
        ‚Ä¢ XGBoost: MSE=78.8777
   ‚úÖ BMBL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BMBL (TargetReturn): TCN with MSE=0.6683
üêõ DEBUG: BMBL - train_and_evaluate_models completed
      üìä LSTM Regression Metrics:
         MSE: 0.223054
         RMSE: 0.472286
         R¬≤ Score: -0.2341 (Poor - 23.4% variance explained)
      üîπ NTGR: Training TCN (50 epochs)...
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BMBL.
üêõ DEBUG: BMBL - Moving model to CPU before return...
üêõ DEBUG [00:04:08.798]: BMBL - Returning result metadata...
üêõ DEBUG [00:04:08.800]: Main received result for BMBL
üêõ DEBUG [00:04:08.800]: Main received result for NPCE
üêõ DEBUG: Training progress: 744/959 done
üêõ DEBUG: train_worker started for AFK
  ‚öôÔ∏è Training models for AFK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - AFK: Initiating feature extraction for training.
  [DIAGNOSTIC] AFK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AFK: rows after features available: 126
üéØ AFK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AFK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AFK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AFK: Training LSTM (50 epochs)...
      ‚è≥ NTGR TCN: Epoch 10/50 (20%)
      ‚è≥ NTGR TCN: Epoch 20/50 (40%)
      ‚è≥ NTGR TCN: Epoch 30/50 (60%)
      ‚è≥ NTGR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.194291
         RMSE: 0.440785
         R¬≤ Score: -0.0750
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTGR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTGR Random Forest: Starting GridSearchCV fit...
      ‚è≥ AFK LSTM: Epoch 10/50 (20%)
      ‚è≥ AFK LSTM: Epoch 20/50 (40%)
      ‚è≥ AFK LSTM: Epoch 30/50 (60%)
       ‚úÖ FLUT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.6866 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FLUT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AFK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.127533
         RMSE: 0.357117
         R¬≤ Score: -0.0605 (Poor - 6.1% variance explained)
      üîπ AFK: Training TCN (50 epochs)...
       ‚úÖ FLUT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.7988 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FLUT XGBoost: Starting GridSearchCV fit...
      ‚è≥ AFK TCN: Epoch 10/50 (20%)
      ‚è≥ AFK TCN: Epoch 20/50 (40%)
      ‚è≥ AFK TCN: Epoch 30/50 (60%)
      ‚è≥ AFK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180161
         RMSE: 0.424453
         R¬≤ Score: -0.4981
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AFK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AFK Random Forest: Starting GridSearchCV fit...
       ‚úÖ NTGR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=34.2381 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTGR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NTGR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.7986 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTGR XGBoost: Starting GridSearchCV fit...
       ‚úÖ AFK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.8296 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AFK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AFK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.6972 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AFK XGBoost: Starting GridSearchCV fit...
       ‚úÖ IVZ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=24.8575 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.6170
    - TCN: MSE=0.5511
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5511
        ‚Ä¢ LSTM: MSE=0.6170
        ‚Ä¢ XGBoost: MSE=24.8575
        ‚Ä¢ Random Forest: MSE=25.2714
        ‚Ä¢ LightGBM Regressor (CPU): MSE=45.8572
   ‚úÖ IVZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IVZ (TargetReturn): TCN with MSE=0.5511
üêõ DEBUG: IVZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IVZ.
üêõ DEBUG: IVZ - Moving model to CPU before return...
üêõ DEBUG [00:04:21.499]: IVZ - Returning result metadata...
üêõ DEBUG [00:04:21.499]: Main received result for IVZ
üêõ DEBUG: train_worker started for WAY
  ‚öôÔ∏è Training models for WAY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - WAY: Initiating feature extraction for training.
  [DIAGNOSTIC] WAY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WAY: rows after features available: 126
üéØ WAY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WAY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WAY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WAY: Training LSTM (50 epochs)...
      ‚è≥ WAY LSTM: Epoch 10/50 (20%)
      ‚è≥ WAY LSTM: Epoch 20/50 (40%)
      ‚è≥ WAY LSTM: Epoch 30/50 (60%)
      ‚è≥ WAY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.081957
         RMSE: 0.286281
         R¬≤ Score: -0.5887 (Poor - 58.9% variance explained)
      üîπ WAY: Training TCN (50 epochs)...
      ‚è≥ WAY TCN: Epoch 10/50 (20%)
      ‚è≥ WAY TCN: Epoch 20/50 (40%)
      ‚è≥ WAY TCN: Epoch 30/50 (60%)
      ‚è≥ WAY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.057765
         RMSE: 0.240343
         R¬≤ Score: -0.1198
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WAY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WAY Random Forest: Starting GridSearchCV fit...
       ‚úÖ WAY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.6838 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WAY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WAY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.8880 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WAY XGBoost: Starting GridSearchCV fit...
       ‚úÖ SILJ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=20.3533 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.3s
    - LSTM: MSE=0.1532
    - TCN: MSE=0.0897
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0897
        ‚Ä¢ LSTM: MSE=0.1532
        ‚Ä¢ XGBoost: MSE=20.3533
        ‚Ä¢ Random Forest: MSE=24.0196
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.8017
   ‚úÖ SILJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SILJ (TargetReturn): TCN with MSE=0.0897
üêõ DEBUG: SILJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SILJ.
üêõ DEBUG: SILJ - Moving model to CPU before return...
üêõ DEBUG [00:04:31.993]: SILJ - Returning result metadata...
üêõ DEBUG [00:04:31.993]: Main received result for SILJ
üêõ DEBUG: train_worker started for DRI
  ‚öôÔ∏è Training models for DRI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - DRI: Initiating feature extraction for training.
  [DIAGNOSTIC] DRI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DRI: rows after features available: 126
üéØ DRI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DRI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DRI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DRI: Training LSTM (50 epochs)...
      ‚è≥ DRI LSTM: Epoch 10/50 (20%)
      ‚è≥ DRI LSTM: Epoch 20/50 (40%)
      ‚è≥ DRI LSTM: Epoch 30/50 (60%)
      ‚è≥ DRI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.304930
         RMSE: 0.552205
         R¬≤ Score: -0.9655 (Poor - 96.6% variance explained)
      üîπ DRI: Training TCN (50 epochs)...
      ‚è≥ DRI TCN: Epoch 10/50 (20%)
      ‚è≥ DRI TCN: Epoch 20/50 (40%)
      ‚è≥ DRI TCN: Epoch 30/50 (60%)
      ‚è≥ DRI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.170403
         RMSE: 0.412799
         R¬≤ Score: -0.0984
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DRI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DRI Random Forest: Starting GridSearchCV fit...
       ‚úÖ DRI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.0722 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DRI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DRI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.3553 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DRI XGBoost: Starting GridSearchCV fit...
       ‚úÖ GL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.9400 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.1s
    - LSTM: MSE=0.1352
    - TCN: MSE=0.0614
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0614
        ‚Ä¢ LSTM: MSE=0.1352
        ‚Ä¢ Random Forest: MSE=5.5043
        ‚Ä¢ XGBoost: MSE=5.9400
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.0501
   ‚úÖ GL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GL (TargetReturn): TCN with MSE=0.0614
üêõ DEBUG: GL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GL.
üêõ DEBUG: GL - Moving model to CPU before return...
üêõ DEBUG [00:04:47.416]: GL - Returning result metadata...
üêõ DEBUG: train_worker started for ALV
  ‚öôÔ∏è Training models for ALV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ALV: Initiating feature extraction for training.
  [DIAGNOSTIC] ALV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ALV: rows after features available: 126
üéØ ALV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ALV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ALV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ALV: Training LSTM (50 epochs)...
      ‚è≥ ALV LSTM: Epoch 10/50 (20%)
      ‚è≥ ALV LSTM: Epoch 20/50 (40%)
      ‚è≥ ALV LSTM: Epoch 30/50 (60%)
      ‚è≥ ALV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.435021
         RMSE: 0.659561
         R¬≤ Score: -0.9870 (Poor - 98.7% variance explained)
      üîπ ALV: Training TCN (50 epochs)...
      ‚è≥ ALV TCN: Epoch 10/50 (20%)
      ‚è≥ ALV TCN: Epoch 20/50 (40%)
       ‚úÖ XPO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.3346 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.7805
    - TCN: MSE=0.5531
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5531
        ‚Ä¢ LSTM: MSE=0.7805
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.6140
        ‚Ä¢ Random Forest: MSE=24.9756
        ‚Ä¢ XGBoost: MSE=29.3346
   ‚úÖ XPO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XPO (TargetReturn): TCN with MSE=0.5531
üêõ DEBUG: XPO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XPO.
üêõ DEBUG: XPO - Moving model to CPU before return...
üêõ DEBUG [00:04:50.214]: XPO - Returning result metadata...
üêõ DEBUG: train_worker started for SYBT
  ‚öôÔ∏è Training models for SYBT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SYBT: Initiating feature extraction for training.
  [DIAGNOSTIC] SYBT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SYBT: rows after features available: 126
üéØ SYBT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SYBT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SYBT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SYBT: Training LSTM (50 epochs)...
      ‚è≥ ALV TCN: Epoch 30/50 (60%)
      ‚è≥ ALV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.329939
         RMSE: 0.574403
         R¬≤ Score: -0.5070
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ALV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ALV Random Forest: Starting GridSearchCV fit...
      ‚è≥ SYBT LSTM: Epoch 10/50 (20%)
      ‚è≥ SYBT LSTM: Epoch 20/50 (40%)
       ‚úÖ PPA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.5188 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.3013
    - TCN: MSE=0.1923
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1923
        ‚Ä¢ LSTM: MSE=0.3013
        ‚Ä¢ Random Forest: MSE=10.9781
        ‚Ä¢ XGBoost: MSE=11.5188
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.9899
   ‚úÖ PPA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PPA (TargetReturn): TCN with MSE=0.1923
üêõ DEBUG: PPA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PPA.
üêõ DEBUG: PPA - Moving model to CPU before return...
üêõ DEBUG [00:04:51.246]: PPA - Returning result metadata...
üêõ DEBUG: train_worker started for UBS
üêõ DEBUG [00:04:51.257]: Main received result for PPA
üêõ DEBUG [00:04:51.257]: Main received result for GL
üêõ DEBUG: Training progress: 748/959 done
üêõ DEBUG [00:04:51.257]: Main received result for XPO
  ‚öôÔ∏è Training models for UBS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - UBS: Initiating feature extraction for training.
  [DIAGNOSTIC] UBS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UBS: rows after features available: 126
üéØ UBS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UBS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UBS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UBS: Training LSTM (50 epochs)...
      ‚è≥ SYBT LSTM: Epoch 30/50 (60%)
      ‚è≥ UBS LSTM: Epoch 10/50 (20%)
      ‚è≥ SYBT LSTM: Epoch 40/50 (80%)
      ‚è≥ UBS LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.295142
         RMSE: 0.543269
         R¬≤ Score: -0.8436 (Poor - 84.4% variance explained)
      üîπ SYBT: Training TCN (50 epochs)...
      ‚è≥ SYBT TCN: Epoch 10/50 (20%)
      ‚è≥ SYBT TCN: Epoch 20/50 (40%)
      ‚è≥ SYBT TCN: Epoch 30/50 (60%)
      ‚è≥ UBS LSTM: Epoch 30/50 (60%)
      ‚è≥ SYBT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.206267
         RMSE: 0.454166
         R¬≤ Score: -0.2884
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SYBT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SYBT Random Forest: Starting GridSearchCV fit...
      ‚è≥ UBS LSTM: Epoch 40/50 (80%)
       ‚úÖ ALV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.2939 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ALV LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.678666
         RMSE: 0.823812
         R¬≤ Score: -1.2997 (Poor - 130.0% variance explained)
      üîπ UBS: Training TCN (50 epochs)...
      ‚è≥ UBS TCN: Epoch 10/50 (20%)
      ‚è≥ UBS TCN: Epoch 20/50 (40%)
      ‚è≥ UBS TCN: Epoch 30/50 (60%)
      ‚è≥ UBS TCN: Epoch 40/50 (80%)
       ‚úÖ ALV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=18.6370 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ALV XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.423276
         RMSE: 0.650597
         R¬≤ Score: -0.4343
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UBS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UBS Random Forest: Starting GridSearchCV fit...
       ‚úÖ SYBT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.5502 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SYBT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SYBT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.8450 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SYBT XGBoost: Starting GridSearchCV fit...
       ‚úÖ UBS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.8084 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UBS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UBS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.3999 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UBS XGBoost: Starting GridSearchCV fit...
       ‚úÖ DOCU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.5824 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=0.1346
    - TCN: MSE=0.0933
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0933
        ‚Ä¢ LSTM: MSE=0.1346
        ‚Ä¢ Random Forest: MSE=18.3259
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.1269
        ‚Ä¢ XGBoost: MSE=23.5824
   ‚úÖ DOCU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DOCU (TargetReturn): TCN with MSE=0.0933
üêõ DEBUG: DOCU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DOCU.
üêõ DEBUG: DOCU - Moving model to CPU before return...
üêõ DEBUG [00:05:10.121]: DOCU - Returning result metadata...
üêõ DEBUG [00:05:10.122]: Main received result for DOCU
üêõ DEBUG: train_worker started for SHCO
  ‚öôÔ∏è Training models for SHCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - SHCO: Initiating feature extraction for training.
  [DIAGNOSTIC] SHCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SHCO: rows after features available: 126
üéØ SHCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SHCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SHCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SHCO: Training LSTM (50 epochs)...
      ‚è≥ SHCO LSTM: Epoch 10/50 (20%)
      ‚è≥ SHCO LSTM: Epoch 20/50 (40%)
      ‚è≥ SHCO LSTM: Epoch 30/50 (60%)
      ‚è≥ SHCO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.380311
         RMSE: 0.616693
         R¬≤ Score: -0.7649 (Poor - 76.5% variance explained)
      üîπ SHCO: Training TCN (50 epochs)...
      ‚è≥ SHCO TCN: Epoch 10/50 (20%)
      ‚è≥ SHCO TCN: Epoch 20/50 (40%)
      ‚è≥ SHCO TCN: Epoch 30/50 (60%)
      ‚è≥ SHCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.318751
         RMSE: 0.564581
         R¬≤ Score: -0.4792
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SHCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SHCO Random Forest: Starting GridSearchCV fit...
       ‚úÖ PBYI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=23.8091 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.2013
    - TCN: MSE=0.1163
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1163
        ‚Ä¢ LSTM: MSE=0.2013
        ‚Ä¢ XGBoost: MSE=23.8091
        ‚Ä¢ Random Forest: MSE=23.8479
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.4061
   ‚úÖ PBYI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PBYI (TargetReturn): TCN with MSE=0.1163
üêõ DEBUG: PBYI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PBYI.
üêõ DEBUG: PBYI - Moving model to CPU before return...
üêõ DEBUG [00:05:15.480]: PBYI - Returning result metadata...
üêõ DEBUG [00:05:15.480]: Main received result for PBYI
üêõ DEBUG: train_worker started for FSCO
  ‚öôÔ∏è Training models for FSCO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FSCO: Initiating feature extraction for training.
  [DIAGNOSTIC] FSCO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FSCO: rows after features available: 126
üéØ FSCO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FSCO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FSCO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FSCO: Training LSTM (50 epochs)...
       ‚úÖ SHCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.1608 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SHCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FSCO LSTM: Epoch 10/50 (20%)
       ‚úÖ SHCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=22.5078 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SHCO XGBoost: Starting GridSearchCV fit...
      ‚è≥ FSCO LSTM: Epoch 20/50 (40%)
      ‚è≥ FSCO LSTM: Epoch 30/50 (60%)
      ‚è≥ FSCO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.127104
         RMSE: 0.356516
         R¬≤ Score: -0.5459 (Poor - 54.6% variance explained)
      üîπ FSCO: Training TCN (50 epochs)...
      ‚è≥ FSCO TCN: Epoch 10/50 (20%)
      ‚è≥ FSCO TCN: Epoch 20/50 (40%)
      ‚è≥ FSCO TCN: Epoch 30/50 (60%)
      ‚è≥ FSCO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.128920
         RMSE: 0.359054
         R¬≤ Score: -0.5680
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FSCO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FSCO Random Forest: Starting GridSearchCV fit...
       ‚úÖ FSCO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.6451 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FSCO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FSCO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.1069 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FSCO XGBoost: Starting GridSearchCV fit...
       ‚úÖ WCC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.4531 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.5s
    - LSTM: MSE=0.7715
    - TCN: MSE=0.4741
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4741
        ‚Ä¢ LSTM: MSE=0.7715
        ‚Ä¢ Random Forest: MSE=15.3134
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.4842
        ‚Ä¢ XGBoost: MSE=19.4531
   ‚úÖ WCC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WCC (TargetReturn): TCN with MSE=0.4741
üêõ DEBUG: WCC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WCC.
üêõ DEBUG: WCC - Moving model to CPU before return...
üêõ DEBUG [00:05:26.460]: WCC - Returning result metadata...
üêõ DEBUG [00:05:26.461]: Main received result for WCC
üêõ DEBUG: Training progress: 752/959 done
üêõ DEBUG: train_worker started for KMI
  ‚öôÔ∏è Training models for KMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - KMI: Initiating feature extraction for training.
  [DIAGNOSTIC] KMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KMI: rows after features available: 126
üéØ KMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KMI: Training LSTM (50 epochs)...
      ‚è≥ KMI LSTM: Epoch 10/50 (20%)
      ‚è≥ KMI LSTM: Epoch 20/50 (40%)
      ‚è≥ KMI LSTM: Epoch 30/50 (60%)
      ‚è≥ KMI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.075887
         RMSE: 0.275476
         R¬≤ Score: -0.3666 (Poor - 36.7% variance explained)
      üîπ KMI: Training TCN (50 epochs)...
      ‚è≥ KMI TCN: Epoch 10/50 (20%)
      ‚è≥ KMI TCN: Epoch 20/50 (40%)
      ‚è≥ KMI TCN: Epoch 30/50 (60%)
       ‚úÖ EWG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.5737 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.1733
    - TCN: MSE=0.1201
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1201
        ‚Ä¢ LSTM: MSE=0.1733
        ‚Ä¢ XGBoost: MSE=6.5737
        ‚Ä¢ Random Forest: MSE=6.7745
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0741
   ‚úÖ EWG: Phase 3/3 - Model selection complete!
      ‚è≥ KMI TCN: Epoch 40/50 (80%)
  üèÜ WINNER for EWG (TargetReturn): TCN with MSE=0.1201
üêõ DEBUG: EWG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWG.
üêõ DEBUG: EWG - Moving model to CPU before return...
üêõ DEBUG [00:05:29.074]: EWG - Returning result metadata...
üêõ DEBUG [00:05:29.075]: Main received result for EWG
üêõ DEBUG: train_worker started for SPPP
  ‚öôÔ∏è Training models for SPPP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - SPPP: Initiating feature extraction for training.
  [DIAGNOSTIC] SPPP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPPP: rows after features available: 126
üéØ SPPP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPPP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPPP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPPP: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.062064
         RMSE: 0.249127
         R¬≤ Score: -0.1177
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KMI Random Forest: Starting GridSearchCV fit...
      ‚è≥ SPPP LSTM: Epoch 10/50 (20%)
      ‚è≥ SPPP LSTM: Epoch 20/50 (40%)
      ‚è≥ SPPP LSTM: Epoch 30/50 (60%)
      ‚è≥ SPPP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.526724
         RMSE: 0.725758
         R¬≤ Score: -1.2657 (Poor - 126.6% variance explained)
      üîπ SPPP: Training TCN (50 epochs)...
      ‚è≥ SPPP TCN: Epoch 10/50 (20%)
      ‚è≥ SPPP TCN: Epoch 20/50 (40%)
      ‚è≥ SPPP TCN: Epoch 30/50 (60%)
      ‚è≥ SPPP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.345854
         RMSE: 0.588094
         R¬≤ Score: -0.4877
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPPP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPPP Random Forest: Starting GridSearchCV fit...
       ‚úÖ KMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.1554 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=10.0864 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KMI XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.2564 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.4126
    - TCN: MSE=0.4828
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4126
        ‚Ä¢ TCN: MSE=0.4828
        ‚Ä¢ XGBoost: MSE=6.2564
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.5615
        ‚Ä¢ Random Forest: MSE=8.0478
   ‚úÖ FDN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDN (TargetReturn): LSTM with MSE=0.4126
üêõ DEBUG: FDN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDN.
üêõ DEBUG: FDN - Moving model to CPU before return...
üêõ DEBUG [00:05:34.266]: FDN - Returning result metadata...
üêõ DEBUG: train_worker started for OPPE
üêõ DEBUG [00:05:34.267]: Main received result for FDN
  ‚öôÔ∏è Training models for OPPE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - OPPE: Initiating feature extraction for training.
  [DIAGNOSTIC] OPPE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPPE: rows after features available: 126
üéØ OPPE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPPE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPPE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPPE: Training LSTM (50 epochs)...
       ‚úÖ SPPP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7885 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPPP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ OPPE LSTM: Epoch 10/50 (20%)
      ‚è≥ OPPE LSTM: Epoch 20/50 (40%)
       ‚úÖ SPPP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.5146 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPPP XGBoost: Starting GridSearchCV fit...
      ‚è≥ OPPE LSTM: Epoch 30/50 (60%)
      ‚è≥ OPPE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.272565
         RMSE: 0.522078
         R¬≤ Score: -1.1543 (Poor - 115.4% variance explained)
      üîπ OPPE: Training TCN (50 epochs)...
      ‚è≥ OPPE TCN: Epoch 10/50 (20%)
      ‚è≥ OPPE TCN: Epoch 20/50 (40%)
      ‚è≥ OPPE TCN: Epoch 30/50 (60%)
      ‚è≥ OPPE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.178609
         RMSE: 0.422622
         R¬≤ Score: -0.4117
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPPE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPPE Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPPE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.1272 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPPE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OPPE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.6416 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPPE XGBoost: Starting GridSearchCV fit...
       ‚úÖ DRTS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=32.7783 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.0s
    - LSTM: MSE=0.1638
    - TCN: MSE=0.1125
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1125
        ‚Ä¢ LSTM: MSE=0.1638
        ‚Ä¢ Random Forest: MSE=26.8143
        ‚Ä¢ XGBoost: MSE=32.7783
        ‚Ä¢ LightGBM Regressor (CPU): MSE=33.9620
   ‚úÖ DRTS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DRTS (TargetReturn): TCN with MSE=0.1125
üêõ DEBUG: DRTS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DRTS.
üêõ DEBUG: DRTS - Moving model to CPU before return...
üêõ DEBUG [00:05:51.631]: DRTS - Returning result metadata...
üêõ DEBUG: train_worker started for WELL
üêõ DEBUG [00:05:51.633]: Main received result for DRTS
  ‚öôÔ∏è Training models for WELL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - WELL: Initiating feature extraction for training.
  [DIAGNOSTIC] WELL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WELL: rows after features available: 126
üéØ WELL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WELL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WELL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WELL: Training LSTM (50 epochs)...
      ‚è≥ WELL LSTM: Epoch 10/50 (20%)
      ‚è≥ WELL LSTM: Epoch 20/50 (40%)
      ‚è≥ WELL LSTM: Epoch 30/50 (60%)
      ‚è≥ WELL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.173399
         RMSE: 0.416412
         R¬≤ Score: -0.9478 (Poor - 94.8% variance explained)
      üîπ WELL: Training TCN (50 epochs)...
      ‚è≥ WELL TCN: Epoch 10/50 (20%)
      ‚è≥ WELL TCN: Epoch 20/50 (40%)
      ‚è≥ WELL TCN: Epoch 30/50 (60%)
      ‚è≥ WELL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.106231
         RMSE: 0.325932
         R¬≤ Score: -0.1933
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WELL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WELL Random Forest: Starting GridSearchCV fit...
       ‚úÖ WELL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.6959 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WELL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WELL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.5211 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WELL XGBoost: Starting GridSearchCV fit...
       ‚úÖ GLAD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.8384 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.5357
    - TCN: MSE=0.3225
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3225
        ‚Ä¢ LSTM: MSE=0.5357
        ‚Ä¢ Random Forest: MSE=5.5240
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.6895
        ‚Ä¢ XGBoost: MSE=5.8384
   ‚úÖ GLAD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GLAD (TargetReturn): TCN with MSE=0.3225
üêõ DEBUG: GLAD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GLAD.
üêõ DEBUG: GLAD - Moving model to CPU before return...
üêõ DEBUG [00:06:04.229]: GLAD - Returning result metadata...
üêõ DEBUG: train_worker started for YMM
üêõ DEBUG [00:06:04.232]: Main received result for GLAD
üêõ DEBUG: Training progress: 756/959 done
  ‚öôÔ∏è Training models for YMM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - YMM: Initiating feature extraction for training.
  [DIAGNOSTIC] YMM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YMM: rows after features available: 126
üéØ YMM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YMM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YMM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YMM: Training LSTM (50 epochs)...
      ‚è≥ YMM LSTM: Epoch 10/50 (20%)
      ‚è≥ YMM LSTM: Epoch 20/50 (40%)
      ‚è≥ YMM LSTM: Epoch 30/50 (60%)
      ‚è≥ YMM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.149538
         RMSE: 0.386702
         R¬≤ Score: -0.4531 (Poor - 45.3% variance explained)
      üîπ YMM: Training TCN (50 epochs)...
      ‚è≥ YMM TCN: Epoch 10/50 (20%)
      ‚è≥ YMM TCN: Epoch 20/50 (40%)
      ‚è≥ YMM TCN: Epoch 30/50 (60%)
      ‚è≥ YMM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.104293
         RMSE: 0.322944
         R¬≤ Score: -0.0134
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YMM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YMM Random Forest: Starting GridSearchCV fit...
       ‚úÖ YMM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.5798 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YMM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ YMM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.4697 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YMM XGBoost: Starting GridSearchCV fit...
       ‚úÖ NTGR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.9108 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.2231
    - TCN: MSE=0.1943
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1943
        ‚Ä¢ LSTM: MSE=0.2231
        ‚Ä¢ Random Forest: MSE=34.2381
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.7986
        ‚Ä¢ XGBoost: MSE=47.9108
   ‚úÖ NTGR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTGR (TargetReturn): TCN with MSE=0.1943
üêõ DEBUG: NTGR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTGR.
üêõ DEBUG: NTGR - Moving model to CPU before return...
üêõ DEBUG [00:06:11.335]: NTGR - Returning result metadata...
üêõ DEBUG: train_worker started for RDWR
  ‚öôÔ∏è Training models for RDWR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - RDWR: Initiating feature extraction for training.
  [DIAGNOSTIC] RDWR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RDWR: rows after features available: 126
üéØ RDWR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RDWR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RDWR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RDWR: Training LSTM (50 epochs)...
       ‚úÖ FLUT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.4077 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=0.6728
    - TCN: MSE=0.7704
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.6728
        ‚Ä¢ TCN: MSE=0.7704
        ‚Ä¢ Random Forest: MSE=9.6866
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.7988
        ‚Ä¢ XGBoost: MSE=10.4077
   ‚úÖ FLUT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FLUT (TargetReturn): LSTM with MSE=0.6728
üêõ DEBUG: FLUT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FLUT.
üêõ DEBUG: FLUT - Moving model to CPU before return...
üêõ DEBUG [00:06:11.695]: FLUT - Returning result metadata...
üêõ DEBUG [00:06:11.696]: Main received result for FLUT
üêõ DEBUG [00:06:11.696]: Main received result for NTGR
üêõ DEBUG: train_worker started for BNT
  ‚öôÔ∏è Training models for BNT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - BNT: Initiating feature extraction for training.
  [DIAGNOSTIC] BNT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BNT: rows after features available: 126
üéØ BNT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BNT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BNT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BNT: Training LSTM (50 epochs)...
      ‚è≥ RDWR LSTM: Epoch 10/50 (20%)
      ‚è≥ BNT LSTM: Epoch 10/50 (20%)
      ‚è≥ RDWR LSTM: Epoch 20/50 (40%)
      ‚è≥ BNT LSTM: Epoch 20/50 (40%)
      ‚è≥ RDWR LSTM: Epoch 30/50 (60%)
      ‚è≥ BNT LSTM: Epoch 30/50 (60%)
      ‚è≥ RDWR LSTM: Epoch 40/50 (80%)
      ‚è≥ BNT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.441575
         RMSE: 0.664511
         R¬≤ Score: -0.7855 (Poor - 78.5% variance explained)
      üîπ RDWR: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.460489
         RMSE: 0.678593
         R¬≤ Score: -0.9040 (Poor - 90.4% variance explained)
      üîπ BNT: Training TCN (50 epochs)...
      ‚è≥ RDWR TCN: Epoch 10/50 (20%)
      ‚è≥ RDWR TCN: Epoch 20/50 (40%)
      ‚è≥ BNT TCN: Epoch 10/50 (20%)
      ‚è≥ RDWR TCN: Epoch 30/50 (60%)
      ‚è≥ BNT TCN: Epoch 20/50 (40%)
      ‚è≥ BNT TCN: Epoch 30/50 (60%)
      ‚è≥ RDWR TCN: Epoch 40/50 (80%)
      ‚è≥ BNT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.340338
         RMSE: 0.583385
         R¬≤ Score: -0.3761
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RDWR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RDWR Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.349361
         RMSE: 0.591067
         R¬≤ Score: -0.4445
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BNT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BNT Random Forest: Starting GridSearchCV fit...
       ‚úÖ AFK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.9443 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.1275
    - TCN: MSE=0.1802
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1275
        ‚Ä¢ TCN: MSE=0.1802
        ‚Ä¢ Random Forest: MSE=6.8296
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.6972
        ‚Ä¢ XGBoost: MSE=7.9443
   ‚úÖ AFK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AFK (TargetReturn): LSTM with MSE=0.1275
üêõ DEBUG: AFK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AFK.
üêõ DEBUG: AFK - Moving model to CPU before return...
üêõ DEBUG [00:06:16.183]: AFK - Returning result metadata...
üêõ DEBUG: train_worker started for EGAN
üêõ DEBUG [00:06:16.186]: Main received result for AFK
  ‚öôÔ∏è Training models for EGAN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - EGAN: Initiating feature extraction for training.
  [DIAGNOSTIC] EGAN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EGAN: rows after features available: 126
üéØ EGAN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EGAN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EGAN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EGAN: Training LSTM (50 epochs)...
      ‚è≥ EGAN LSTM: Epoch 10/50 (20%)
      ‚è≥ EGAN LSTM: Epoch 20/50 (40%)
       ‚úÖ RDWR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.4226 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RDWR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EGAN LSTM: Epoch 30/50 (60%)
       ‚úÖ BNT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.0170 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BNT LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EGAN LSTM: Epoch 40/50 (80%)
       ‚úÖ RDWR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=49.9054 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RDWR XGBoost: Starting GridSearchCV fit...
       ‚úÖ BNT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.2177 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BNT XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.567152
         RMSE: 0.753095
         R¬≤ Score: -1.0202 (Poor - 102.0% variance explained)
      üîπ EGAN: Training TCN (50 epochs)...
      ‚è≥ EGAN TCN: Epoch 10/50 (20%)
      ‚è≥ EGAN TCN: Epoch 20/50 (40%)
      ‚è≥ EGAN TCN: Epoch 30/50 (60%)
      ‚è≥ EGAN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.489988
         RMSE: 0.699991
         R¬≤ Score: -0.7454
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EGAN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EGAN Random Forest: Starting GridSearchCV fit...
       ‚úÖ EGAN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.6336 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EGAN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EGAN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.1641 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EGAN XGBoost: Starting GridSearchCV fit...
       ‚úÖ WAY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.0405 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.0820
    - TCN: MSE=0.0578
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0578
        ‚Ä¢ LSTM: MSE=0.0820
        ‚Ä¢ Random Forest: MSE=16.6838
        ‚Ä¢ XGBoost: MSE=19.0405
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.8880
   ‚úÖ WAY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WAY (TargetReturn): TCN with MSE=0.0578
üêõ DEBUG: WAY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WAY.
üêõ DEBUG: WAY - Moving model to CPU before return...
üêõ DEBUG [00:06:28.904]: WAY - Returning result metadata...
üêõ DEBUG: train_worker started for BN
üêõ DEBUG [00:06:28.910]: Main received result for WAY
üêõ DEBUG: Training progress: 760/959 done
  ‚öôÔ∏è Training models for BN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - BN: Initiating feature extraction for training.
  [DIAGNOSTIC] BN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BN: rows after features available: 126
üéØ BN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BN: Training LSTM (50 epochs)...
      ‚è≥ BN LSTM: Epoch 10/50 (20%)
      ‚è≥ BN LSTM: Epoch 20/50 (40%)
      ‚è≥ BN LSTM: Epoch 30/50 (60%)
      ‚è≥ BN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.504295
         RMSE: 0.710137
         R¬≤ Score: -1.0602 (Poor - 106.0% variance explained)
      üîπ BN: Training TCN (50 epochs)...
      ‚è≥ BN TCN: Epoch 10/50 (20%)
      ‚è≥ BN TCN: Epoch 20/50 (40%)
      ‚è≥ BN TCN: Epoch 30/50 (60%)
      ‚è≥ BN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.402141
         RMSE: 0.634146
         R¬≤ Score: -0.6429
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BN Random Forest: Starting GridSearchCV fit...
       ‚úÖ BN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.0928 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.1201 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BN XGBoost: Starting GridSearchCV fit...
       ‚úÖ DRI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.8635 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.3049
    - TCN: MSE=0.1704
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1704
        ‚Ä¢ LSTM: MSE=0.3049
        ‚Ä¢ XGBoost: MSE=6.8635
        ‚Ä¢ Random Forest: MSE=7.0722
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.3553
   ‚úÖ DRI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DRI (TargetReturn): TCN with MSE=0.1704
üêõ DEBUG: DRI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DRI.
üêõ DEBUG: DRI - Moving model to CPU before return...
üêõ DEBUG [00:06:36.013]: DRI - Returning result metadata...
üêõ DEBUG: train_worker started for SPMO
üêõ DEBUG [00:06:36.014]: Main received result for DRI
  ‚öôÔ∏è Training models for SPMO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SPMO: Initiating feature extraction for training.
  [DIAGNOSTIC] SPMO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPMO: rows after features available: 126
üéØ SPMO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPMO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPMO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPMO: Training LSTM (50 epochs)...
      ‚è≥ SPMO LSTM: Epoch 10/50 (20%)
      ‚è≥ SPMO LSTM: Epoch 20/50 (40%)
      ‚è≥ SPMO LSTM: Epoch 30/50 (60%)
      ‚è≥ SPMO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.424836
         RMSE: 0.651794
         R¬≤ Score: -0.7875 (Poor - 78.7% variance explained)
      üîπ SPMO: Training TCN (50 epochs)...
      ‚è≥ SPMO TCN: Epoch 10/50 (20%)
      ‚è≥ SPMO TCN: Epoch 20/50 (40%)
      ‚è≥ SPMO TCN: Epoch 30/50 (60%)
      ‚è≥ SPMO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.449697
         RMSE: 0.670594
         R¬≤ Score: -0.8921
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPMO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPMO Random Forest: Starting GridSearchCV fit...
       ‚úÖ SPMO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.1159 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPMO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SPMO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.7077 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPMO XGBoost: Starting GridSearchCV fit...
       ‚úÖ SYBT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.8961 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.2951
    - TCN: MSE=0.2063
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2063
        ‚Ä¢ LSTM: MSE=0.2951
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.8450
        ‚Ä¢ XGBoost: MSE=10.8961
        ‚Ä¢ Random Forest: MSE=11.5502
   ‚úÖ SYBT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SYBT (TargetReturn): TCN with MSE=0.2063
üêõ DEBUG: SYBT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SYBT.
üêõ DEBUG: SYBT - Moving model to CPU before return...
üêõ DEBUG [00:06:53.841]: SYBT - Returning result metadata...
üêõ DEBUG: train_worker started for FMS
  ‚öôÔ∏è Training models for FMS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - FMS: Initiating feature extraction for training.
  [DIAGNOSTIC] FMS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FMS: rows after features available: 126
üéØ FMS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FMS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FMS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FMS: Training LSTM (50 epochs)...
      ‚è≥ FMS LSTM: Epoch 10/50 (20%)
      ‚è≥ FMS LSTM: Epoch 20/50 (40%)
      ‚è≥ FMS LSTM: Epoch 30/50 (60%)
       ‚úÖ ALV XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.3452 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.4350
    - TCN: MSE=0.3299
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3299
        ‚Ä¢ LSTM: MSE=0.4350
        ‚Ä¢ XGBoost: MSE=15.3452
        ‚Ä¢ Random Forest: MSE=16.2939
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.6370
   ‚úÖ ALV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ALV (TargetReturn): TCN with MSE=0.3299
üêõ DEBUG: ALV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ALV.
üêõ DEBUG: ALV - Moving model to CPU before return...
üêõ DEBUG [00:06:55.691]: ALV - Returning result metadata...
üêõ DEBUG: train_worker started for SKWD
üêõ DEBUG [00:06:55.692]: Main received result for ALV
üêõ DEBUG [00:06:55.693]: Main received result for SYBT
  ‚öôÔ∏è Training models for SKWD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - SKWD: Initiating feature extraction for training.
  [DIAGNOSTIC] SKWD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SKWD: rows after features available: 126
üéØ SKWD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SKWD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SKWD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SKWD: Training LSTM (50 epochs)...
      ‚è≥ FMS LSTM: Epoch 40/50 (80%)
      ‚è≥ SKWD LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.526861
         RMSE: 0.725852
         R¬≤ Score: -1.2576 (Poor - 125.8% variance explained)
      üîπ FMS: Training TCN (50 epochs)...
      ‚è≥ FMS TCN: Epoch 10/50 (20%)
      ‚è≥ FMS TCN: Epoch 20/50 (40%)
      ‚è≥ FMS TCN: Epoch 30/50 (60%)
      ‚è≥ SKWD LSTM: Epoch 20/50 (40%)
      ‚è≥ FMS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.296021
         RMSE: 0.544078
         R¬≤ Score: -0.2685
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FMS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FMS Random Forest: Starting GridSearchCV fit...
      ‚è≥ SKWD LSTM: Epoch 30/50 (60%)
      ‚è≥ SKWD LSTM: Epoch 40/50 (80%)
       ‚úÖ UBS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.3760 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.6787
    - TCN: MSE=0.4233
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4233
        ‚Ä¢ LSTM: MSE=0.6787
        ‚Ä¢ Random Forest: MSE=13.8084
        ‚Ä¢ XGBoost: MSE=16.3760
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.3999
   ‚úÖ UBS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UBS (TargetReturn): TCN with MSE=0.4233
üêõ DEBUG: UBS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UBS.
üêõ DEBUG: UBS - Moving model to CPU before return...
üêõ DEBUG [00:06:57.950]: UBS - Returning result metadata...
üêõ DEBUG [00:06:57.951]: Main received result for UBS
üêõ DEBUG: train_worker started for BSVN
üêõ DEBUG: Training progress: 764/959 done
  ‚öôÔ∏è Training models for BSVN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - BSVN: Initiating feature extraction for training.
  [DIAGNOSTIC] BSVN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BSVN: rows after features available: 126
üéØ BSVN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BSVN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BSVN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BSVN: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.377629
         RMSE: 0.614515
         R¬≤ Score: -0.7543 (Poor - 75.4% variance explained)
      üîπ SKWD: Training TCN (50 epochs)...
      ‚è≥ SKWD TCN: Epoch 10/50 (20%)
      ‚è≥ SKWD TCN: Epoch 20/50 (40%)
      ‚è≥ BSVN LSTM: Epoch 10/50 (20%)
      ‚è≥ SKWD TCN: Epoch 30/50 (60%)
      ‚è≥ SKWD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.280356
         RMSE: 0.529487
         R¬≤ Score: -0.3024
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SKWD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SKWD Random Forest: Starting GridSearchCV fit...
      ‚è≥ BSVN LSTM: Epoch 20/50 (40%)
      ‚è≥ BSVN LSTM: Epoch 30/50 (60%)
       ‚úÖ FMS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.9498 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FMS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BSVN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.554732
         RMSE: 0.744803
         R¬≤ Score: -0.8789 (Poor - 87.9% variance explained)
      üîπ BSVN: Training TCN (50 epochs)...
       ‚úÖ FMS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.4532 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FMS XGBoost: Starting GridSearchCV fit...
      ‚è≥ BSVN TCN: Epoch 10/50 (20%)
      ‚è≥ BSVN TCN: Epoch 20/50 (40%)
      ‚è≥ BSVN TCN: Epoch 30/50 (60%)
      ‚è≥ BSVN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.383817
         RMSE: 0.619530
         R¬≤ Score: -0.3000
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BSVN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BSVN Random Forest: Starting GridSearchCV fit...
       ‚úÖ SKWD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.2608 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SKWD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SKWD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=18.9220 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SKWD XGBoost: Starting GridSearchCV fit...
       ‚úÖ BSVN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.4423 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BSVN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BSVN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.0046 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BSVN XGBoost: Starting GridSearchCV fit...
       ‚úÖ SHCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=47.7900 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.3803
    - TCN: MSE=0.3188
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3188
        ‚Ä¢ LSTM: MSE=0.3803
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.5078
        ‚Ä¢ Random Forest: MSE=32.1608
        ‚Ä¢ XGBoost: MSE=47.7900
   ‚úÖ SHCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SHCO (TargetReturn): TCN with MSE=0.3188
üêõ DEBUG: SHCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SHCO.
üêõ DEBUG: SHCO - Moving model to CPU before return...
üêõ DEBUG [00:07:16.510]: SHCO - Returning result metadata...
üêõ DEBUG [00:07:16.510]: Main received result for SHCO
üêõ DEBUG: train_worker started for EWBC
  ‚öôÔ∏è Training models for EWBC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - EWBC: Initiating feature extraction for training.
  [DIAGNOSTIC] EWBC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EWBC: rows after features available: 126
üéØ EWBC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EWBC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EWBC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EWBC: Training LSTM (50 epochs)...
      ‚è≥ EWBC LSTM: Epoch 10/50 (20%)
      ‚è≥ EWBC LSTM: Epoch 20/50 (40%)
      ‚è≥ EWBC LSTM: Epoch 30/50 (60%)
      ‚è≥ EWBC LSTM: Epoch 40/50 (80%)
       ‚úÖ FSCO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.6802 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.1271
    - TCN: MSE=0.1289
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1271
        ‚Ä¢ TCN: MSE=0.1289
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.1069
        ‚Ä¢ XGBoost: MSE=5.6802
        ‚Ä¢ Random Forest: MSE=6.6451
   ‚úÖ FSCO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FSCO (TargetReturn): LSTM with MSE=0.1271
üêõ DEBUG: FSCO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FSCO.
üêõ DEBUG: FSCO - Moving model to CPU before return...
üêõ DEBUG [00:07:18.497]: FSCO - Returning result metadata...
üêõ DEBUG [00:07:18.498]: Main received result for FSCO
üêõ DEBUG: train_worker started for PWRD
  ‚öôÔ∏è Training models for PWRD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - PWRD: Initiating feature extraction for training.
  [DIAGNOSTIC] PWRD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PWRD: rows after features available: 126
üéØ PWRD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PWRD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PWRD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PWRD: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.446670
         RMSE: 0.668334
         R¬≤ Score: -0.7493 (Poor - 74.9% variance explained)
      üîπ EWBC: Training TCN (50 epochs)...
      ‚è≥ PWRD LSTM: Epoch 10/50 (20%)
      ‚è≥ EWBC TCN: Epoch 10/50 (20%)
      ‚è≥ EWBC TCN: Epoch 20/50 (40%)
      ‚è≥ EWBC TCN: Epoch 30/50 (60%)
      ‚è≥ EWBC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.346390
         RMSE: 0.588549
         R¬≤ Score: -0.3566
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EWBC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EWBC Random Forest: Starting GridSearchCV fit...
      ‚è≥ PWRD LSTM: Epoch 20/50 (40%)
      ‚è≥ PWRD LSTM: Epoch 30/50 (60%)
      ‚è≥ PWRD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.559739
         RMSE: 0.748157
         R¬≤ Score: -1.4902 (Poor - 149.0% variance explained)
      üîπ PWRD: Training TCN (50 epochs)...
      ‚è≥ PWRD TCN: Epoch 10/50 (20%)
      ‚è≥ PWRD TCN: Epoch 20/50 (40%)
      ‚è≥ PWRD TCN: Epoch 30/50 (60%)
      ‚è≥ PWRD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.389558
         RMSE: 0.624146
         R¬≤ Score: -0.7331
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PWRD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PWRD Random Forest: Starting GridSearchCV fit...
       ‚úÖ EWBC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=27.6326 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EWBC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EWBC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=18.6589 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EWBC XGBoost: Starting GridSearchCV fit...
       ‚úÖ PWRD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.0352 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PWRD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PWRD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.1605 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PWRD XGBoost: Starting GridSearchCV fit...
       ‚úÖ KMI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.4745 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.7s
    - LSTM: MSE=0.0759
    - TCN: MSE=0.0621
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0621
        ‚Ä¢ LSTM: MSE=0.0759
        ‚Ä¢ Random Forest: MSE=9.1554
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.0864
        ‚Ä¢ XGBoost: MSE=14.4745
   ‚úÖ KMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KMI (TargetReturn): TCN with MSE=0.0621
üêõ DEBUG: KMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KMI.
üêõ DEBUG: KMI - Moving model to CPU before return...
üêõ DEBUG [00:07:27.248]: KMI - Returning result metadata...
üêõ DEBUG [00:07:27.249]: Main received result for KMI
üêõ DEBUG: train_worker started for CIO
  ‚öôÔ∏è Training models for CIO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CIO: Initiating feature extraction for training.
  [DIAGNOSTIC] CIO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CIO: rows after features available: 126
üéØ CIO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CIO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CIO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CIO: Training LSTM (50 epochs)...
      ‚è≥ CIO LSTM: Epoch 10/50 (20%)
      ‚è≥ CIO LSTM: Epoch 20/50 (40%)
      ‚è≥ CIO LSTM: Epoch 30/50 (60%)
      ‚è≥ CIO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.217991
         RMSE: 0.466895
         R¬≤ Score: -0.3297 (Poor - 33.0% variance explained)
      üîπ CIO: Training TCN (50 epochs)...
      ‚è≥ CIO TCN: Epoch 10/50 (20%)
      ‚è≥ CIO TCN: Epoch 20/50 (40%)
      ‚è≥ CIO TCN: Epoch 30/50 (60%)
      ‚è≥ CIO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.248345
         RMSE: 0.498342
         R¬≤ Score: -0.5149
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CIO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CIO Random Forest: Starting GridSearchCV fit...
       ‚úÖ CIO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.6411 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CIO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CIO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.7663 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.4s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CIO XGBoost: Starting GridSearchCV fit...
       ‚úÖ SPPP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.4658 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.5267
    - TCN: MSE=0.3459
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3459
        ‚Ä¢ LSTM: MSE=0.5267
        ‚Ä¢ Random Forest: MSE=7.7885
        ‚Ä¢ XGBoost: MSE=8.4658
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.5146
   ‚úÖ SPPP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPPP (TargetReturn): TCN with MSE=0.3459
üêõ DEBUG: SPPP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPPP.
üêõ DEBUG: SPPP - Moving model to CPU before return...
üêõ DEBUG [00:07:36.691]: SPPP - Returning result metadata...
üêõ DEBUG [00:07:36.691]: Main received result for SPPP
üêõ DEBUG: train_worker started for BBUC
üêõ DEBUG: Training progress: 768/959 done
  ‚öôÔ∏è Training models for BBUC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - BBUC: Initiating feature extraction for training.
  [DIAGNOSTIC] BBUC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBUC: rows after features available: 126
üéØ BBUC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BBUC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBUC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBUC: Training LSTM (50 epochs)...
      ‚è≥ BBUC LSTM: Epoch 10/50 (20%)
      ‚è≥ BBUC LSTM: Epoch 20/50 (40%)
      ‚è≥ BBUC LSTM: Epoch 30/50 (60%)
      ‚è≥ BBUC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.193939
         RMSE: 0.440385
         R¬≤ Score: -0.5196 (Poor - 52.0% variance explained)
      üîπ BBUC: Training TCN (50 epochs)...
      ‚è≥ BBUC TCN: Epoch 10/50 (20%)
      ‚è≥ BBUC TCN: Epoch 20/50 (40%)
      ‚è≥ BBUC TCN: Epoch 30/50 (60%)
      ‚è≥ BBUC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.178281
         RMSE: 0.422233
         R¬≤ Score: -0.3969
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBUC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBUC Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPPE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.8563 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.2726
    - TCN: MSE=0.1786
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1786
        ‚Ä¢ LSTM: MSE=0.2726
        ‚Ä¢ Random Forest: MSE=4.1272
        ‚Ä¢ XGBoost: MSE=4.8563
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.6416
   ‚úÖ OPPE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPPE (TargetReturn): TCN with MSE=0.1786
üêõ DEBUG: OPPE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPPE.
üêõ DEBUG: OPPE - Moving model to CPU before return...
üêõ DEBUG [00:07:40.018]: OPPE - Returning result metadata...
üêõ DEBUG: train_worker started for FET
üêõ DEBUG [00:07:40.019]: Main received result for OPPE
  ‚öôÔ∏è Training models for FET (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - FET: Initiating feature extraction for training.
  [DIAGNOSTIC] FET: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FET: rows after features available: 126
üéØ FET: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FET: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FET: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FET: Training LSTM (50 epochs)...
      ‚è≥ FET LSTM: Epoch 10/50 (20%)
      ‚è≥ FET LSTM: Epoch 20/50 (40%)
      ‚è≥ FET LSTM: Epoch 30/50 (60%)
      ‚è≥ FET LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.638374
         RMSE: 0.798983
         R¬≤ Score: -1.2758 (Poor - 127.6% variance explained)
      üîπ FET: Training TCN (50 epochs)...
       ‚úÖ BBUC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.5309 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBUC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FET TCN: Epoch 10/50 (20%)
      ‚è≥ FET TCN: Epoch 20/50 (40%)
      ‚è≥ FET TCN: Epoch 30/50 (60%)
      ‚è≥ FET TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.426244
         RMSE: 0.652874
         R¬≤ Score: -0.5196
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FET: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FET Random Forest: Starting GridSearchCV fit...
       ‚úÖ BBUC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.1216 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBUC XGBoost: Starting GridSearchCV fit...
       ‚úÖ FET Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=37.9630 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FET LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FET LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=40.4492 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FET XGBoost: Starting GridSearchCV fit...
       ‚úÖ WELL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.8547 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.6s
    - LSTM: MSE=0.1734
    - TCN: MSE=0.1062
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1062
        ‚Ä¢ LSTM: MSE=0.1734
        ‚Ä¢ Random Forest: MSE=3.6959
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.5211
        ‚Ä¢ XGBoost: MSE=5.8547
   ‚úÖ WELL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WELL (TargetReturn): TCN with MSE=0.1062
üêõ DEBUG: WELL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WELL.
üêõ DEBUG: WELL - Moving model to CPU before return...
üêõ DEBUG [00:07:53.370]: WELL - Returning result metadata...
üêõ DEBUG [00:07:53.371]: Main received result for WELL
üêõ DEBUG: train_worker started for NGS
  ‚öôÔ∏è Training models for NGS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - NGS: Initiating feature extraction for training.
  [DIAGNOSTIC] NGS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NGS: rows after features available: 126
üéØ NGS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NGS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NGS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NGS: Training LSTM (50 epochs)...
      ‚è≥ NGS LSTM: Epoch 10/50 (20%)
      ‚è≥ NGS LSTM: Epoch 20/50 (40%)
      ‚è≥ NGS LSTM: Epoch 30/50 (60%)
      ‚è≥ NGS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.573902
         RMSE: 0.757563
         R¬≤ Score: -0.8090 (Poor - 80.9% variance explained)
      üîπ NGS: Training TCN (50 epochs)...
      ‚è≥ NGS TCN: Epoch 10/50 (20%)
      ‚è≥ NGS TCN: Epoch 20/50 (40%)
      ‚è≥ NGS TCN: Epoch 30/50 (60%)
      ‚è≥ NGS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.643780
         RMSE: 0.802359
         R¬≤ Score: -1.0292
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NGS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NGS Random Forest: Starting GridSearchCV fit...
       ‚úÖ NGS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.5520 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NGS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NGS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.5944 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NGS XGBoost: Starting GridSearchCV fit...
       ‚úÖ YMM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.8477 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.1495
    - TCN: MSE=0.1043
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1043
        ‚Ä¢ LSTM: MSE=0.1495
        ‚Ä¢ Random Forest: MSE=21.5798
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.4697
        ‚Ä¢ XGBoost: MSE=31.8477
   ‚úÖ YMM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YMM (TargetReturn): TCN with MSE=0.1043
üêõ DEBUG: YMM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YMM.
üêõ DEBUG: YMM - Moving model to CPU before return...
üêõ DEBUG [00:08:10.187]: YMM - Returning result metadata...
üêõ DEBUG [00:08:10.187]: Main received result for YMM
üêõ DEBUG: train_worker started for DIS
  ‚öôÔ∏è Training models for DIS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - DIS: Initiating feature extraction for training.
  [DIAGNOSTIC] DIS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DIS: rows after features available: 126
üéØ DIS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DIS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DIS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DIS: Training LSTM (50 epochs)...
      ‚è≥ DIS LSTM: Epoch 10/50 (20%)
      ‚è≥ DIS LSTM: Epoch 20/50 (40%)
      ‚è≥ DIS LSTM: Epoch 30/50 (60%)
      ‚è≥ DIS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.630148
         RMSE: 0.793819
         R¬≤ Score: -0.9649 (Poor - 96.5% variance explained)
      üîπ DIS: Training TCN (50 epochs)...
      ‚è≥ DIS TCN: Epoch 10/50 (20%)
      ‚è≥ DIS TCN: Epoch 20/50 (40%)
      ‚è≥ DIS TCN: Epoch 30/50 (60%)
      ‚è≥ DIS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.550342
         RMSE: 0.741850
         R¬≤ Score: -0.7160
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DIS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DIS Random Forest: Starting GridSearchCV fit...
       ‚úÖ DIS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.1924 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DIS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RDWR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.0162 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.4416
    - TCN: MSE=0.3403
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3403
        ‚Ä¢ LSTM: MSE=0.4416
        ‚Ä¢ Random Forest: MSE=13.4226
        ‚Ä¢ XGBoost: MSE=20.0162
        ‚Ä¢ LightGBM Regressor (CPU): MSE=49.9054
   ‚úÖ RDWR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RDWR (TargetReturn): TCN with MSE=0.3403
üêõ DEBUG: RDWR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RDWR.
üêõ DEBUG: RDWR - Moving model to CPU before return...
üêõ DEBUG [00:08:15.791]: RDWR - Returning result metadata...
üêõ DEBUG: train_worker started for INTU
üêõ DEBUG [00:08:15.792]: Main received result for RDWR
üêõ DEBUG: Training progress: 772/959 done
  ‚öôÔ∏è Training models for INTU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - INTU: Initiating feature extraction for training.
  [DIAGNOSTIC] INTU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ INTU: rows after features available: 126
üéØ INTU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] INTU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö INTU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ INTU: Training LSTM (50 epochs)...
       ‚úÖ DIS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.1410 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DIS XGBoost: Starting GridSearchCV fit...
      ‚è≥ INTU LSTM: Epoch 10/50 (20%)
      ‚è≥ INTU LSTM: Epoch 20/50 (40%)
      ‚è≥ INTU LSTM: Epoch 30/50 (60%)
      ‚è≥ INTU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.190131
         RMSE: 0.436040
         R¬≤ Score: -0.7241 (Poor - 72.4% variance explained)
      üîπ INTU: Training TCN (50 epochs)...
      ‚è≥ INTU TCN: Epoch 10/50 (20%)
      ‚è≥ INTU TCN: Epoch 20/50 (40%)
      ‚è≥ INTU TCN: Epoch 30/50 (60%)
      ‚è≥ INTU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.205593
         RMSE: 0.453424
         R¬≤ Score: -0.8643
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä INTU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ INTU Random Forest: Starting GridSearchCV fit...
       ‚úÖ EGAN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=30.1308 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 118.2s
    - LSTM: MSE=0.5672
    - TCN: MSE=0.4900
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4900
        ‚Ä¢ LSTM: MSE=0.5672
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.1641
        ‚Ä¢ Random Forest: MSE=19.6336
        ‚Ä¢ XGBoost: MSE=30.1308
   ‚úÖ EGAN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EGAN (TargetReturn): TCN with MSE=0.4900
üêõ DEBUG: EGAN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EGAN.
üêõ DEBUG: EGAN - Moving model to CPU before return...
üêõ DEBUG [00:08:20.819]: EGAN - Returning result metadata...
üêõ DEBUG: train_worker started for GVAL
  ‚öôÔ∏è Training models for GVAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - GVAL: Initiating feature extraction for training.
  [DIAGNOSTIC] GVAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GVAL: rows after features available: 126
üéØ GVAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GVAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GVAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GVAL: Training LSTM (50 epochs)...
       ‚úÖ BNT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=9.3654 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.4s
    - LSTM: MSE=0.4605
    - TCN: MSE=0.3494
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3494
        ‚Ä¢ LSTM: MSE=0.4605
        ‚Ä¢ XGBoost: MSE=9.3654
        ‚Ä¢ Random Forest: MSE=14.0170
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.2177
   ‚úÖ BNT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BNT (TargetReturn): TCN with MSE=0.3494
üêõ DEBUG: BNT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BNT.
üêõ DEBUG: BNT - Moving model to CPU before return...
üêõ DEBUG [00:08:21.054]: BNT - Returning result metadata...
üêõ DEBUG [00:08:21.055]: Main received result for BNTüêõ DEBUG: train_worker started for PAX

üêõ DEBUG [00:08:21.055]: Main received result for EGAN
  ‚öôÔ∏è Training models for PAX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - PAX: Initiating feature extraction for training.
  [DIAGNOSTIC] PAX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAX: rows after features available: 126
üéØ PAX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAX: Training LSTM (50 epochs)...
       ‚úÖ INTU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.6881 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ INTU LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GVAL LSTM: Epoch 10/50 (20%)
      ‚è≥ PAX LSTM: Epoch 10/50 (20%)
      ‚è≥ GVAL LSTM: Epoch 20/50 (40%)
       ‚úÖ INTU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.6628 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ INTU XGBoost: Starting GridSearchCV fit...
      ‚è≥ PAX LSTM: Epoch 20/50 (40%)
      ‚è≥ GVAL LSTM: Epoch 30/50 (60%)
      ‚è≥ PAX LSTM: Epoch 30/50 (60%)
      ‚è≥ GVAL LSTM: Epoch 40/50 (80%)
      ‚è≥ PAX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.150875
         RMSE: 0.388426
         R¬≤ Score: -0.4508 (Poor - 45.1% variance explained)
      üîπ GVAL: Training TCN (50 epochs)...
      ‚è≥ GVAL TCN: Epoch 10/50 (20%)
      ‚è≥ GVAL TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.480873
         RMSE: 0.693450
         R¬≤ Score: -0.8469 (Poor - 84.7% variance explained)
      üîπ PAX: Training TCN (50 epochs)...
      ‚è≥ GVAL TCN: Epoch 30/50 (60%)
      ‚è≥ PAX TCN: Epoch 10/50 (20%)
      ‚è≥ GVAL TCN: Epoch 40/50 (80%)
      ‚è≥ PAX TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.107578
         RMSE: 0.327991
         R¬≤ Score: -0.0344
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GVAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GVAL Random Forest: Starting GridSearchCV fit...
      ‚è≥ PAX TCN: Epoch 30/50 (60%)
      ‚è≥ PAX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.288283
         RMSE: 0.536920
         R¬≤ Score: -0.1072
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAX Random Forest: Starting GridSearchCV fit...
       ‚úÖ GVAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9927 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GVAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.5564 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GVAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.7904 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GVAL XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=29.2438 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAX XGBoost: Starting GridSearchCV fit...
       ‚úÖ BN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.1760 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.6s
    - LSTM: MSE=0.5043
    - TCN: MSE=0.4021
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4021
        ‚Ä¢ LSTM: MSE=0.5043
        ‚Ä¢ XGBoost: MSE=10.1760
        ‚Ä¢ Random Forest: MSE=12.0928
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.1201
   ‚úÖ BN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BN (TargetReturn): TCN with MSE=0.4021
üêõ DEBUG: BN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BN.
üêõ DEBUG: BN - Moving model to CPU before return...
üêõ DEBUG [00:08:37.494]: BN - Returning result metadata...
üêõ DEBUG [00:08:37.495]: Main received result for BN
üêõ DEBUG: train_worker started for ALTG
  ‚öôÔ∏è Training models for ALTG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - ALTG: Initiating feature extraction for training.
  [DIAGNOSTIC] ALTG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ALTG: rows after features available: 126
üéØ ALTG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ALTG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ALTG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ALTG: Training LSTM (50 epochs)...
      ‚è≥ ALTG LSTM: Epoch 10/50 (20%)
      ‚è≥ ALTG LSTM: Epoch 20/50 (40%)
      ‚è≥ ALTG LSTM: Epoch 30/50 (60%)
       ‚úÖ SPMO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.1370 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.3s
    - LSTM: MSE=0.4248
    - TCN: MSE=0.4497
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4248
        ‚Ä¢ TCN: MSE=0.4497
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.7077
        ‚Ä¢ Random Forest: MSE=11.1159
        ‚Ä¢ XGBoost: MSE=14.1370
   ‚úÖ SPMO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPMO (TargetReturn): LSTM with MSE=0.4248
üêõ DEBUG: SPMO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPMO.
üêõ DEBUG: SPMO - Moving model to CPU before return...
üêõ DEBUG [00:08:39.528]: SPMO - Returning result metadata...
üêõ DEBUG: train_worker started for ITRI
üêõ DEBUG [00:08:39.529]: Main received result for SPMO
üêõ DEBUG: Training progress: 776/959 done
  ‚öôÔ∏è Training models for ITRI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - ITRI: Initiating feature extraction for training.
  [DIAGNOSTIC] ITRI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ITRI: rows after features available: 126
üéØ ITRI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ITRI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ITRI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ITRI: Training LSTM (50 epochs)...
      ‚è≥ ALTG LSTM: Epoch 40/50 (80%)
      ‚è≥ ITRI LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.766918
         RMSE: 0.875739
         R¬≤ Score: -0.9784 (Poor - 97.8% variance explained)
      üîπ ALTG: Training TCN (50 epochs)...
      ‚è≥ ITRI LSTM: Epoch 20/50 (40%)
      ‚è≥ ALTG TCN: Epoch 10/50 (20%)
      ‚è≥ ALTG TCN: Epoch 20/50 (40%)
      ‚è≥ ALTG TCN: Epoch 30/50 (60%)
      ‚è≥ ALTG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.663781
         RMSE: 0.814728
         R¬≤ Score: -0.7123
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ALTG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ALTG Random Forest: Starting GridSearchCV fit...
      ‚è≥ ITRI LSTM: Epoch 30/50 (60%)
      ‚è≥ ITRI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.250453
         RMSE: 0.500453
         R¬≤ Score: -1.0626 (Poor - 106.3% variance explained)
      üîπ ITRI: Training TCN (50 epochs)...
      ‚è≥ ITRI TCN: Epoch 10/50 (20%)
      ‚è≥ ITRI TCN: Epoch 20/50 (40%)
      ‚è≥ ITRI TCN: Epoch 30/50 (60%)
      ‚è≥ ITRI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.181207
         RMSE: 0.425684
         R¬≤ Score: -0.4923
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ITRI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ITRI Random Forest: Starting GridSearchCV fit...
       ‚úÖ ALTG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.6255 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ALTG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ALTG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=89.9726 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ALTG XGBoost: Starting GridSearchCV fit...
       ‚úÖ ITRI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.3173 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ITRI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ITRI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.8304 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ITRI XGBoost: Starting GridSearchCV fit...
       ‚úÖ FMS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=8.3477 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.5269
    - TCN: MSE=0.2960
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2960
        ‚Ä¢ LSTM: MSE=0.5269
        ‚Ä¢ XGBoost: MSE=8.3477
        ‚Ä¢ Random Forest: MSE=8.9498
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.4532
   ‚úÖ FMS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FMS (TargetReturn): TCN with MSE=0.2960
üêõ DEBUG: FMS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FMS.
üêõ DEBUG: FMS - Moving model to CPU before return...
üêõ DEBUG [00:08:57.988]: FMS - Returning result metadata...
üêõ DEBUG: train_worker started for ORLY
üêõ DEBUG [00:08:57.989]: Main received result for FMS
  ‚öôÔ∏è Training models for ORLY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - ORLY: Initiating feature extraction for training.
  [DIAGNOSTIC] ORLY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ORLY: rows after features available: 126
üéØ ORLY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ORLY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ORLY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ORLY: Training LSTM (50 epochs)...
      ‚è≥ ORLY LSTM: Epoch 10/50 (20%)
      ‚è≥ ORLY LSTM: Epoch 20/50 (40%)
      ‚è≥ ORLY LSTM: Epoch 30/50 (60%)
      ‚è≥ ORLY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.205579
         RMSE: 0.453408
         R¬≤ Score: -1.6634 (Poor - 166.3% variance explained)
      üîπ ORLY: Training TCN (50 epochs)...
      ‚è≥ ORLY TCN: Epoch 10/50 (20%)
      ‚è≥ ORLY TCN: Epoch 20/50 (40%)
      ‚è≥ ORLY TCN: Epoch 30/50 (60%)
      ‚è≥ ORLY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.082257
         RMSE: 0.286806
         R¬≤ Score: -0.0657
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ORLY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ORLY Random Forest: Starting GridSearchCV fit...
       ‚úÖ ORLY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.5205 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ORLY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SKWD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.0794 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.3776
    - TCN: MSE=0.2804
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2804
        ‚Ä¢ LSTM: MSE=0.3776
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.9220
        ‚Ä¢ Random Forest: MSE=23.2608
        ‚Ä¢ XGBoost: MSE=33.0794
   ‚úÖ SKWD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SKWD (TargetReturn): TCN with MSE=0.2804
üêõ DEBUG: SKWD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SKWD.
üêõ DEBUG: SKWD - Moving model to CPU before return...
üêõ DEBUG [00:09:03.560]: SKWD - Returning result metadata...
üêõ DEBUG: train_worker started for VALN
üêõ DEBUG [00:09:03.561]: Main received result for SKWD
  ‚öôÔ∏è Training models for VALN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - VALN: Initiating feature extraction for training.
  [DIAGNOSTIC] VALN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VALN: rows after features available: 126
üéØ VALN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VALN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VALN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VALN: Training LSTM (50 epochs)...
       ‚úÖ BSVN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.0360 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.5547
    - TCN: MSE=0.3838
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3838
        ‚Ä¢ LSTM: MSE=0.5547
        ‚Ä¢ Random Forest: MSE=7.4423
        ‚Ä¢ XGBoost: MSE=8.0360
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.0046
   ‚úÖ BSVN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BSVN (TargetReturn): TCN with MSE=0.3838
üêõ DEBUG: BSVN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BSVN.
üêõ DEBUG: BSVN - Moving model to CPU before return...
üêõ DEBUG [00:09:03.898]: BSVN - Returning result metadata...
üêõ DEBUG: train_worker started for FFTY
üêõ DEBUG [00:09:03.899]: Main received result for BSVN
  ‚öôÔ∏è Training models for FFTY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - FFTY: Initiating feature extraction for training.
  [DIAGNOSTIC] FFTY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FFTY: rows after features available: 126
üéØ FFTY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FFTY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FFTY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FFTY: Training LSTM (50 epochs)...
      ‚è≥ VALN LSTM: Epoch 10/50 (20%)
       ‚úÖ ORLY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=3.2443 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ORLY XGBoost: Starting GridSearchCV fit...
      ‚è≥ FFTY LSTM: Epoch 10/50 (20%)
      ‚è≥ VALN LSTM: Epoch 20/50 (40%)
      ‚è≥ FFTY LSTM: Epoch 20/50 (40%)
      ‚è≥ VALN LSTM: Epoch 30/50 (60%)
      ‚è≥ FFTY LSTM: Epoch 30/50 (60%)
      ‚è≥ VALN LSTM: Epoch 40/50 (80%)
      ‚è≥ FFTY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.271242
         RMSE: 0.520809
         R¬≤ Score: -2.2473 (Poor - 224.7% variance explained)
      üîπ VALN: Training TCN (50 epochs)...
      ‚è≥ VALN TCN: Epoch 10/50 (20%)
      ‚è≥ VALN TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.504390
         RMSE: 0.710204
         R¬≤ Score: -0.7599 (Poor - 76.0% variance explained)
      üîπ FFTY: Training TCN (50 epochs)...
      ‚è≥ VALN TCN: Epoch 30/50 (60%)
      ‚è≥ FFTY TCN: Epoch 10/50 (20%)
      ‚è≥ VALN TCN: Epoch 40/50 (80%)
      ‚è≥ FFTY TCN: Epoch 20/50 (40%)
      ‚è≥ FFTY TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.087173
         RMSE: 0.295250
         R¬≤ Score: -0.0436
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VALN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VALN Random Forest: Starting GridSearchCV fit...
      ‚è≥ FFTY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.555806
         RMSE: 0.745524
         R¬≤ Score: -0.9393
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FFTY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FFTY Random Forest: Starting GridSearchCV fit...
       ‚úÖ VALN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=50.7120 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VALN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FFTY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.7955 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FFTY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VALN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=39.6798 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VALN XGBoost: Starting GridSearchCV fit...
       ‚úÖ FFTY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.6260 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FFTY XGBoost: Starting GridSearchCV fit...
       ‚úÖ EWBC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.2544 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=0.4467
    - TCN: MSE=0.3464
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3464
        ‚Ä¢ LSTM: MSE=0.4467
        ‚Ä¢ LightGBM Regressor (CPU): MSE=18.6589
        ‚Ä¢ Random Forest: MSE=27.6326
        ‚Ä¢ XGBoost: MSE=31.2544
   ‚úÖ EWBC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EWBC (TargetReturn): TCN with MSE=0.3464
üêõ DEBUG: EWBC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EWBC.
üêõ DEBUG: EWBC - Moving model to CPU before return...
üêõ DEBUG [00:09:23.297]: EWBC - Returning result metadata...
üêõ DEBUG [00:09:23.297]: Main received result for EWBC
üêõ DEBUG: Training progress: 780/959 done
üêõ DEBUG: train_worker started for R
  ‚öôÔ∏è Training models for R (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - R: Initiating feature extraction for training.
  [DIAGNOSTIC] R: fetch_training_data - Initial data rows: 205
   ‚Ü≥ R: rows after features available: 126
üéØ R: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] R: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö R: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ R: Training LSTM (50 epochs)...
      ‚è≥ R LSTM: Epoch 10/50 (20%)
       ‚úÖ PWRD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.1326 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.5597
    - TCN: MSE=0.3896
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3896
        ‚Ä¢ LSTM: MSE=0.5597
        ‚Ä¢ Random Forest: MSE=15.0352
        ‚Ä¢ XGBoost: MSE=15.1326
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.1605
   ‚úÖ PWRD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PWRD (TargetReturn): TCN with MSE=0.3896
üêõ DEBUG: PWRD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PWRD.
üêõ DEBUG: PWRD - Moving model to CPU before return...
üêõ DEBUG [00:09:24.203]: PWRD - Returning result metadata...
üêõ DEBUG [00:09:24.204]: Main received result for PWRD
üêõ DEBUG: train_worker started for EXPE
      ‚è≥ R LSTM: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for EXPE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - EXPE: Initiating feature extraction for training.
  [DIAGNOSTIC] EXPE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EXPE: rows after features available: 126
üéØ EXPE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EXPE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EXPE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EXPE: Training LSTM (50 epochs)...
      ‚è≥ R LSTM: Epoch 30/50 (60%)
      ‚è≥ EXPE LSTM: Epoch 10/50 (20%)
      ‚è≥ R LSTM: Epoch 40/50 (80%)
      ‚è≥ EXPE LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.531422
         RMSE: 0.728987
         R¬≤ Score: -0.6645 (Poor - 66.5% variance explained)
      üîπ R: Training TCN (50 epochs)...
      ‚è≥ EXPE LSTM: Epoch 30/50 (60%)
      ‚è≥ R TCN: Epoch 10/50 (20%)
      ‚è≥ R TCN: Epoch 20/50 (40%)
      ‚è≥ R TCN: Epoch 30/50 (60%)
      ‚è≥ R TCN: Epoch 40/50 (80%)
      ‚è≥ EXPE LSTM: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.624870
         RMSE: 0.790487
         R¬≤ Score: -0.9572
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä R: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ R Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.563879
         RMSE: 0.750919
         R¬≤ Score: -0.9439 (Poor - 94.4% variance explained)
      üîπ EXPE: Training TCN (50 epochs)...
      ‚è≥ EXPE TCN: Epoch 10/50 (20%)
      ‚è≥ EXPE TCN: Epoch 20/50 (40%)
      ‚è≥ EXPE TCN: Epoch 30/50 (60%)
      ‚è≥ EXPE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.381611
         RMSE: 0.617747
         R¬≤ Score: -0.3156
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EXPE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EXPE Random Forest: Starting GridSearchCV fit...
       ‚úÖ R Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0551 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ R LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EXPE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.4607 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EXPE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ R LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.2525 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ R XGBoost: Starting GridSearchCV fit...
       ‚úÖ EXPE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.6789 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EXPE XGBoost: Starting GridSearchCV fit...
       ‚úÖ CIO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.0211 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.2180
    - TCN: MSE=0.2483
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2180
        ‚Ä¢ TCN: MSE=0.2483
        ‚Ä¢ XGBoost: MSE=16.0211
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.7663
        ‚Ä¢ Random Forest: MSE=17.6411
   ‚úÖ CIO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CIO (TargetReturn): LSTM with MSE=0.2180
üêõ DEBUG: CIO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CIO.
üêõ DEBUG: CIO - Moving model to CPU before return...
üêõ DEBUG [00:09:31.616]: CIO - Returning result metadata...
üêõ DEBUG [00:09:31.616]: Main received result for CIO
üêõ DEBUG: train_worker started for Z
  ‚öôÔ∏è Training models for Z (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - Z: Initiating feature extraction for training.
  [DIAGNOSTIC] Z: fetch_training_data - Initial data rows: 205
   ‚Ü≥ Z: rows after features available: 126
üéØ Z: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] Z: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö Z: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ Z: Training LSTM (50 epochs)...
      ‚è≥ Z LSTM: Epoch 10/50 (20%)
      ‚è≥ Z LSTM: Epoch 20/50 (40%)
      ‚è≥ Z LSTM: Epoch 30/50 (60%)
      ‚è≥ Z LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.428993
         RMSE: 0.654975
         R¬≤ Score: -1.2281 (Poor - 122.8% variance explained)
      üîπ Z: Training TCN (50 epochs)...
      ‚è≥ Z TCN: Epoch 10/50 (20%)
      ‚è≥ Z TCN: Epoch 20/50 (40%)
      ‚è≥ Z TCN: Epoch 30/50 (60%)
      ‚è≥ Z TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.208210
         RMSE: 0.456300
         R¬≤ Score: -0.0814
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä Z: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ Z Random Forest: Starting GridSearchCV fit...
       ‚úÖ Z Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.9781 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ Z LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ Z LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.1544 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ Z XGBoost: Starting GridSearchCV fit...
       ‚úÖ FET XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.3195 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.6384
    - TCN: MSE=0.4262
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4262
        ‚Ä¢ LSTM: MSE=0.6384
        ‚Ä¢ Random Forest: MSE=37.9630
        ‚Ä¢ LightGBM Regressor (CPU): MSE=40.4492
        ‚Ä¢ XGBoost: MSE=41.3195
   ‚úÖ FET: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FET (TargetReturn): TCN with MSE=0.4262
üêõ DEBUG: FET - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FET.
üêõ DEBUG: FET - Moving model to CPU before return...
üêõ DEBUG [00:09:44.834]: FET - Returning result metadata...
üêõ DEBUG: train_worker started for EQH
  ‚öôÔ∏è Training models for EQH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - EQH: Initiating feature extraction for training.
  [DIAGNOSTIC] EQH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EQH: rows after features available: 126
üéØ EQH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EQH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EQH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EQH: Training LSTM (50 epochs)...
       ‚úÖ BBUC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.3806 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.2s
    - LSTM: MSE=0.1939
    - TCN: MSE=0.1783
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1783
        ‚Ä¢ LSTM: MSE=0.1939
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.1216
        ‚Ä¢ XGBoost: MSE=18.3806
        ‚Ä¢ Random Forest: MSE=21.5309
   ‚úÖ BBUC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBUC (TargetReturn): TCN with MSE=0.1783
üêõ DEBUG: BBUC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBUC.
üêõ DEBUG: BBUC - Moving model to CPU before return...
üêõ DEBUG [00:09:45.156]: BBUC - Returning result metadata...
üêõ DEBUG: train_worker started for TDY
üêõ DEBUG [00:09:45.162]: Main received result for BBUC
üêõ DEBUG [00:09:45.162]: Main received result for FET
üêõ DEBUG: Training progress: 784/959 done
  ‚öôÔ∏è Training models for TDY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - TDY: Initiating feature extraction for training.
  [DIAGNOSTIC] TDY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TDY: rows after features available: 126
üéØ TDY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TDY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TDY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TDY: Training LSTM (50 epochs)...
      ‚è≥ EQH LSTM: Epoch 10/50 (20%)
      ‚è≥ TDY LSTM: Epoch 10/50 (20%)
      ‚è≥ EQH LSTM: Epoch 20/50 (40%)
      ‚è≥ TDY LSTM: Epoch 20/50 (40%)
      ‚è≥ EQH LSTM: Epoch 30/50 (60%)
      ‚è≥ TDY LSTM: Epoch 30/50 (60%)
      ‚è≥ EQH LSTM: Epoch 40/50 (80%)
      ‚è≥ TDY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.328807
         RMSE: 0.573417
         R¬≤ Score: -0.8502 (Poor - 85.0% variance explained)
      üîπ EQH: Training TCN (50 epochs)...
      ‚è≥ EQH TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.540024
         RMSE: 0.734863
         R¬≤ Score: -0.8852 (Poor - 88.5% variance explained)
      üîπ TDY: Training TCN (50 epochs)...
      ‚è≥ EQH TCN: Epoch 20/50 (40%)
      ‚è≥ TDY TCN: Epoch 10/50 (20%)
      ‚è≥ EQH TCN: Epoch 30/50 (60%)
      ‚è≥ TDY TCN: Epoch 20/50 (40%)
      ‚è≥ EQH TCN: Epoch 40/50 (80%)
      ‚è≥ TDY TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.319856
         RMSE: 0.565558
         R¬≤ Score: -0.7998
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EQH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EQH Random Forest: Starting GridSearchCV fit...
      ‚è≥ TDY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.547597
         RMSE: 0.739998
         R¬≤ Score: -0.9116
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TDY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TDY Random Forest: Starting GridSearchCV fit...
       ‚úÖ TDY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.7185 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TDY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EQH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.5039 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EQH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TDY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.7539 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TDY XGBoost: Starting GridSearchCV fit...
       ‚úÖ EQH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.3645 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EQH XGBoost: Starting GridSearchCV fit...
       ‚úÖ NGS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=31.7088 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.6s
    - LSTM: MSE=0.5739
    - TCN: MSE=0.6438
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5739
        ‚Ä¢ TCN: MSE=0.6438
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.5944
        ‚Ä¢ Random Forest: MSE=26.5520
        ‚Ä¢ XGBoost: MSE=31.7088
   ‚úÖ NGS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NGS (TargetReturn): LSTM with MSE=0.5739
üêõ DEBUG: NGS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NGS.
üêõ DEBUG: NGS - Moving model to CPU before return...
üêõ DEBUG [00:09:55.200]: NGS - Returning result metadata...
üêõ DEBUG: train_worker started for OPEN
üêõ DEBUG [00:09:55.201]: Main received result for NGS
  ‚öôÔ∏è Training models for OPEN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - OPEN: Initiating feature extraction for training.
  [DIAGNOSTIC] OPEN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OPEN: rows after features available: 126
üéØ OPEN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OPEN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OPEN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OPEN: Training LSTM (50 epochs)...
      ‚è≥ OPEN LSTM: Epoch 10/50 (20%)
      ‚è≥ OPEN LSTM: Epoch 20/50 (40%)
      ‚è≥ OPEN LSTM: Epoch 30/50 (60%)
      ‚è≥ OPEN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.478594
         RMSE: 0.691805
         R¬≤ Score: -1.1531 (Poor - 115.3% variance explained)
      üîπ OPEN: Training TCN (50 epochs)...
      ‚è≥ OPEN TCN: Epoch 10/50 (20%)
      ‚è≥ OPEN TCN: Epoch 20/50 (40%)
      ‚è≥ OPEN TCN: Epoch 30/50 (60%)
      ‚è≥ OPEN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.256682
         RMSE: 0.506638
         R¬≤ Score: -0.1547
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OPEN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OPEN Random Forest: Starting GridSearchCV fit...
       ‚úÖ OPEN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=588.4943 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OPEN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OPEN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=961.0696 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OPEN XGBoost: Starting GridSearchCV fit...
       ‚úÖ DIS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.4331 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.6s
    - LSTM: MSE=0.6301
    - TCN: MSE=0.5503
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5503
        ‚Ä¢ LSTM: MSE=0.6301
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.1410
        ‚Ä¢ Random Forest: MSE=16.1924
        ‚Ä¢ XGBoost: MSE=26.4331
   ‚úÖ DIS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DIS (TargetReturn): TCN with MSE=0.5503
üêõ DEBUG: DIS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DIS.
üêõ DEBUG: DIS - Moving model to CPU before return...
üêõ DEBUG [00:10:10.521]: DIS - Returning result metadata...
üêõ DEBUG [00:10:10.521]: Main received result for DIS
üêõ DEBUG: train_worker started for FDT
  ‚öôÔ∏è Training models for FDT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - FDT: Initiating feature extraction for training.
  [DIAGNOSTIC] FDT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDT: rows after features available: 126
üéØ FDT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDT: Training LSTM (50 epochs)...
      ‚è≥ FDT LSTM: Epoch 10/50 (20%)
      ‚è≥ FDT LSTM: Epoch 20/50 (40%)
      ‚è≥ FDT LSTM: Epoch 30/50 (60%)
      ‚è≥ FDT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.205600
         RMSE: 0.453431
         R¬≤ Score: -1.0061 (Poor - 100.6% variance explained)
      üîπ FDT: Training TCN (50 epochs)...
      ‚è≥ FDT TCN: Epoch 10/50 (20%)
      ‚è≥ FDT TCN: Epoch 20/50 (40%)
      ‚è≥ FDT TCN: Epoch 30/50 (60%)
      ‚è≥ FDT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.133830
         RMSE: 0.365827
         R¬≤ Score: -0.3058
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDT Random Forest: Starting GridSearchCV fit...
       ‚úÖ FDT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.1623 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.7278 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDT XGBoost: Starting GridSearchCV fit...
       ‚úÖ INTU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.3460 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.1901
    - TCN: MSE=0.2056
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1901
        ‚Ä¢ TCN: MSE=0.2056
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.6628
        ‚Ä¢ Random Forest: MSE=10.6881
        ‚Ä¢ XGBoost: MSE=17.3460
   ‚úÖ INTU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for INTU (TargetReturn): LSTM with MSE=0.1901
üêõ DEBUG: INTU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for INTU.
üêõ DEBUG: INTU - Moving model to CPU before return...
üêõ DEBUG [00:10:19.090]: INTU - Returning result metadata...
üêõ DEBUG: train_worker started for SAP
üêõ DEBUG [00:10:19.092]: Main received result for INTU
  ‚öôÔ∏è Training models for SAP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - SAP: Initiating feature extraction for training.
  [DIAGNOSTIC] SAP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SAP: rows after features available: 126
üéØ SAP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SAP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SAP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SAP: Training LSTM (50 epochs)...
      ‚è≥ SAP LSTM: Epoch 10/50 (20%)
      ‚è≥ SAP LSTM: Epoch 20/50 (40%)
      ‚è≥ SAP LSTM: Epoch 30/50 (60%)
      ‚è≥ SAP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.280610
         RMSE: 0.529726
         R¬≤ Score: -0.7640 (Poor - 76.4% variance explained)
      üîπ SAP: Training TCN (50 epochs)...
      ‚è≥ SAP TCN: Epoch 10/50 (20%)
      ‚è≥ SAP TCN: Epoch 20/50 (40%)
      ‚è≥ SAP TCN: Epoch 30/50 (60%)
      ‚è≥ SAP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180689
         RMSE: 0.425076
         R¬≤ Score: -0.1359
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SAP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SAP Random Forest: Starting GridSearchCV fit...
       ‚úÖ SAP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.2211 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SAP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SAP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.8572 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SAP XGBoost: Starting GridSearchCV fit...
       ‚úÖ GVAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.4754 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.8s
    - LSTM: MSE=0.1509
    - TCN: MSE=0.1076
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1076
        ‚Ä¢ LSTM: MSE=0.1509
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7904
        ‚Ä¢ XGBoost: MSE=10.4754
        ‚Ä¢ Random Forest: MSE=10.9927
   ‚úÖ GVAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GVAL (TargetReturn): TCN with MSE=0.1076
üêõ DEBUG: GVAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GVAL.
üêõ DEBUG: GVAL - Moving model to CPU before return...
üêõ DEBUG [00:10:28.281]: GVAL - Returning result metadata...
üêõ DEBUG [00:10:28.282]: Main received result for GVAL
üêõ DEBUG: Training progress: 788/959 done
üêõ DEBUG: train_worker started for TEO
  ‚öôÔ∏è Training models for TEO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TEO: Initiating feature extraction for training.
  [DIAGNOSTIC] TEO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TEO: rows after features available: 126
üéØ TEO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TEO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TEO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TEO: Training LSTM (50 epochs)...
      ‚è≥ TEO LSTM: Epoch 10/50 (20%)
       ‚úÖ PAX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.9048 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.4809
    - TCN: MSE=0.2883
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2883
        ‚Ä¢ LSTM: MSE=0.4809
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.2438
        ‚Ä¢ Random Forest: MSE=35.5564
        ‚Ä¢ XGBoost: MSE=41.9048
   ‚úÖ PAX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAX (TargetReturn): TCN with MSE=0.2883
üêõ DEBUG: PAX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAX.
üêõ DEBUG: PAX - Moving model to CPU before return...
üêõ DEBUG [00:10:28.854]: PAX - Returning result metadata...
üêõ DEBUG: train_worker started for RELY
üêõ DEBUG [00:10:28.854]: Main received result for PAX
  ‚öôÔ∏è Training models for RELY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - RELY: Initiating feature extraction for training.
  [DIAGNOSTIC] RELY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RELY: rows after features available: 126
üéØ RELY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RELY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RELY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RELY: Training LSTM (50 epochs)...
      ‚è≥ TEO LSTM: Epoch 20/50 (40%)
      ‚è≥ RELY LSTM: Epoch 10/50 (20%)
      ‚è≥ TEO LSTM: Epoch 30/50 (60%)
      ‚è≥ RELY LSTM: Epoch 20/50 (40%)
      ‚è≥ TEO LSTM: Epoch 40/50 (80%)
      ‚è≥ RELY LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.046919
         RMSE: 0.216608
         R¬≤ Score: -0.9772 (Poor - 97.7% variance explained)
      üîπ TEO: Training TCN (50 epochs)...
      ‚è≥ RELY LSTM: Epoch 40/50 (80%)
      ‚è≥ TEO TCN: Epoch 10/50 (20%)
      ‚è≥ TEO TCN: Epoch 20/50 (40%)
      ‚è≥ TEO TCN: Epoch 30/50 (60%)
      ‚è≥ TEO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.027796
         RMSE: 0.166720
         R¬≤ Score: -0.1713
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TEO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TEO Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.049810
         RMSE: 0.223183
         R¬≤ Score: -0.8707 (Poor - 87.1% variance explained)
      üîπ RELY: Training TCN (50 epochs)...
      ‚è≥ RELY TCN: Epoch 10/50 (20%)
      ‚è≥ RELY TCN: Epoch 20/50 (40%)
      ‚è≥ RELY TCN: Epoch 30/50 (60%)
      ‚è≥ RELY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.026905
         RMSE: 0.164027
         R¬≤ Score: -0.0104
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RELY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RELY Random Forest: Starting GridSearchCV fit...
       ‚úÖ TEO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.0701 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TEO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RELY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.3772 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RELY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TEO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=36.3884 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TEO XGBoost: Starting GridSearchCV fit...
       ‚úÖ RELY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=21.9901 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RELY XGBoost: Starting GridSearchCV fit...
       ‚úÖ ITRI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.1758 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.2505
    - TCN: MSE=0.1812
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1812
        ‚Ä¢ LSTM: MSE=0.2505
        ‚Ä¢ Random Forest: MSE=9.3173
        ‚Ä¢ XGBoost: MSE=11.1758
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8304
   ‚úÖ ITRI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ITRI (TargetReturn): TCN with MSE=0.1812
üêõ DEBUG: ITRI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ITRI.
üêõ DEBUG: ITRI - Moving model to CPU before return...
üêõ DEBUG [00:10:42.780]: ITRI - Returning result metadata...
üêõ DEBUG: train_worker started for IAUM
  ‚öôÔ∏è Training models for IAUM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - IAUM: Initiating feature extraction for training.
  [DIAGNOSTIC] IAUM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IAUM: rows after features available: 126
üéØ IAUM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IAUM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IAUM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IAUM: Training LSTM (50 epochs)...
      ‚è≥ IAUM LSTM: Epoch 10/50 (20%)
      ‚è≥ IAUM LSTM: Epoch 20/50 (40%)
      ‚è≥ IAUM LSTM: Epoch 30/50 (60%)
      ‚è≥ IAUM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.308531
         RMSE: 0.555456
         R¬≤ Score: -0.6473 (Poor - 64.7% variance explained)
      üîπ IAUM: Training TCN (50 epochs)...
      ‚è≥ IAUM TCN: Epoch 10/50 (20%)
      ‚è≥ IAUM TCN: Epoch 20/50 (40%)
      ‚è≥ IAUM TCN: Epoch 30/50 (60%)
      ‚è≥ IAUM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.291237
         RMSE: 0.539663
         R¬≤ Score: -0.5550
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IAUM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IAUM Random Forest: Starting GridSearchCV fit...
       ‚úÖ ALTG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=48.4569 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.7669
    - TCN: MSE=0.6638
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6638
        ‚Ä¢ LSTM: MSE=0.7669
        ‚Ä¢ Random Forest: MSE=35.6255
        ‚Ä¢ XGBoost: MSE=48.4569
        ‚Ä¢ LightGBM Regressor (CPU): MSE=89.9726
   ‚úÖ ALTG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ALTG (TargetReturn): TCN with MSE=0.6638
üêõ DEBUG: ALTG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ALTG.
üêõ DEBUG: ALTG - Moving model to CPU before return...
üêõ DEBUG [00:10:45.805]: ALTG - Returning result metadata...
üêõ DEBUG: train_worker started for GLDM
üêõ DEBUG [00:10:45.806]: Main received result for ALTG
üêõ DEBUG [00:10:45.806]: Main received result for ITRI
  ‚öôÔ∏è Training models for GLDM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - GLDM: Initiating feature extraction for training.
  [DIAGNOSTIC] GLDM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GLDM: rows after features available: 126
üéØ GLDM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GLDM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GLDM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GLDM: Training LSTM (50 epochs)...
      ‚è≥ GLDM LSTM: Epoch 10/50 (20%)
      ‚è≥ GLDM LSTM: Epoch 20/50 (40%)
      ‚è≥ GLDM LSTM: Epoch 30/50 (60%)
      ‚è≥ GLDM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.307124
         RMSE: 0.554187
         R¬≤ Score: -0.6388 (Poor - 63.9% variance explained)
      üîπ GLDM: Training TCN (50 epochs)...
      ‚è≥ GLDM TCN: Epoch 10/50 (20%)
       ‚úÖ IAUM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.4214 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IAUM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GLDM TCN: Epoch 20/50 (40%)
      ‚è≥ GLDM TCN: Epoch 30/50 (60%)
      ‚è≥ GLDM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.306600
         RMSE: 0.553715
         R¬≤ Score: -0.6360
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GLDM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GLDM Random Forest: Starting GridSearchCV fit...
       ‚úÖ IAUM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.4986 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IAUM XGBoost: Starting GridSearchCV fit...
       ‚úÖ GLDM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.0021 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GLDM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GLDM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.4485 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GLDM XGBoost: Starting GridSearchCV fit...
       ‚úÖ ORLY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3.7932 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.2056
    - TCN: MSE=0.0823
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0823
        ‚Ä¢ LSTM: MSE=0.2056
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.2443
        ‚Ä¢ Random Forest: MSE=3.5205
        ‚Ä¢ XGBoost: MSE=3.7932
   ‚úÖ ORLY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ORLY (TargetReturn): TCN with MSE=0.0823
üêõ DEBUG: ORLY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ORLY.
üêõ DEBUG: ORLY - Moving model to CPU before return...
üêõ DEBUG [00:11:00.792]: ORLY - Returning result metadata...
üêõ DEBUG: train_worker started for BAR
üêõ DEBUG [00:11:00.793]: Main received result for ORLY
üêõ DEBUG: Training progress: 792/959 done
  ‚öôÔ∏è Training models for BAR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BAR: Initiating feature extraction for training.
  [DIAGNOSTIC] BAR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BAR: rows after features available: 126
üéØ BAR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BAR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BAR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BAR: Training LSTM (50 epochs)...
      ‚è≥ BAR LSTM: Epoch 10/50 (20%)
      ‚è≥ BAR LSTM: Epoch 20/50 (40%)
      ‚è≥ BAR LSTM: Epoch 30/50 (60%)
      ‚è≥ BAR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.408041
         RMSE: 0.638781
         R¬≤ Score: -1.1783 (Poor - 117.8% variance explained)
      üîπ BAR: Training TCN (50 epochs)...
      ‚è≥ BAR TCN: Epoch 10/50 (20%)
      ‚è≥ BAR TCN: Epoch 20/50 (40%)
      ‚è≥ BAR TCN: Epoch 30/50 (60%)
      ‚è≥ BAR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.295267
         RMSE: 0.543385
         R¬≤ Score: -0.5763
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BAR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BAR Random Forest: Starting GridSearchCV fit...
       ‚úÖ BAR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.4454 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BAR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BAR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.4749 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BAR XGBoost: Starting GridSearchCV fit...
       ‚úÖ FFTY XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.5008 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.5044
    - TCN: MSE=0.5558
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5044
        ‚Ä¢ TCN: MSE=0.5558
        ‚Ä¢ XGBoost: MSE=13.5008
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.6260
        ‚Ä¢ Random Forest: MSE=14.7955
   ‚úÖ FFTY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FFTY (TargetReturn): LSTM with MSE=0.5044
üêõ DEBUG: FFTY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FFTY.
üêõ DEBUG: FFTY - Moving model to CPU before return...
üêõ DEBUG [00:11:09.468]: FFTY - Returning result metadata...
üêõ DEBUG: train_worker started for DK
  ‚öôÔ∏è Training models for DK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - DK: Initiating feature extraction for training.
  [DIAGNOSTIC] DK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DK: rows after features available: 126
üéØ DK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DK: Training LSTM (50 epochs)...
       ‚úÖ VALN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=54.9081 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.2712
    - TCN: MSE=0.0872
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0872
        ‚Ä¢ LSTM: MSE=0.2712
        ‚Ä¢ LightGBM Regressor (CPU): MSE=39.6798
        ‚Ä¢ Random Forest: MSE=50.7120
        ‚Ä¢ XGBoost: MSE=54.9081
   ‚úÖ VALN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VALN (TargetReturn): TCN with MSE=0.0872
üêõ DEBUG: VALN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VALN.
üêõ DEBUG: VALN - Moving model to CPU before return...
üêõ DEBUG [00:11:09.675]: VALN - Returning result metadata...
üêõ DEBUG [00:11:09.676]: Main received result for VALN
üêõ DEBUG [00:11:09.677]: Main received result for FFTY
üêõ DEBUG: train_worker started for BULZ
  ‚öôÔ∏è Training models for BULZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - BULZ: Initiating feature extraction for training.
  [DIAGNOSTIC] BULZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BULZ: rows after features available: 126
üéØ BULZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BULZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BULZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BULZ: Training LSTM (50 epochs)...
      ‚è≥ DK LSTM: Epoch 10/50 (20%)
      ‚è≥ BULZ LSTM: Epoch 10/50 (20%)
      ‚è≥ DK LSTM: Epoch 20/50 (40%)
      ‚è≥ BULZ LSTM: Epoch 20/50 (40%)
      ‚è≥ DK LSTM: Epoch 30/50 (60%)
      ‚è≥ BULZ LSTM: Epoch 30/50 (60%)
      ‚è≥ DK LSTM: Epoch 40/50 (80%)
      ‚è≥ BULZ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.558879
         RMSE: 0.747582
         R¬≤ Score: -0.7701 (Poor - 77.0% variance explained)
      üîπ DK: Training TCN (50 epochs)...
      ‚è≥ DK TCN: Epoch 10/50 (20%)
      ‚è≥ DK TCN: Epoch 20/50 (40%)
      ‚è≥ DK TCN: Epoch 30/50 (60%)
      ‚è≥ DK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.339430
         RMSE: 0.582606
         R¬≤ Score: -0.0751
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DK Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.548118
         RMSE: 0.740350
         R¬≤ Score: -0.8722 (Poor - 87.2% variance explained)
      üîπ BULZ: Training TCN (50 epochs)...
      ‚è≥ BULZ TCN: Epoch 10/50 (20%)
      ‚è≥ BULZ TCN: Epoch 20/50 (40%)
      ‚è≥ BULZ TCN: Epoch 30/50 (60%)
      ‚è≥ BULZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.359191
         RMSE: 0.599325
         R¬≤ Score: -0.2269
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BULZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BULZ Random Forest: Starting GridSearchCV fit...
       ‚úÖ DK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.5461 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BULZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=91.4914 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BULZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=106.5717 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DK XGBoost: Starting GridSearchCV fit...
       ‚úÖ BULZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=228.8818 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BULZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ EXPE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.9305 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.5639
    - TCN: MSE=0.3816
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3816
        ‚Ä¢ LSTM: MSE=0.5639
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.6789
        ‚Ä¢ XGBoost: MSE=14.9305
        ‚Ä¢ Random Forest: MSE=15.4607
   ‚úÖ EXPE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EXPE (TargetReturn): TCN with MSE=0.3816
üêõ DEBUG: EXPE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EXPE.
üêõ DEBUG: EXPE - Moving model to CPU before return...
üêõ DEBUG [00:11:28.692]: EXPE - Returning result metadata...
üêõ DEBUG: train_worker started for SGOL
  ‚öôÔ∏è Training models for SGOL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - SGOL: Initiating feature extraction for training.
  [DIAGNOSTIC] SGOL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SGOL: rows after features available: 126
üéØ SGOL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SGOL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SGOL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SGOL: Training LSTM (50 epochs)...
      ‚è≥ SGOL LSTM: Epoch 10/50 (20%)
      ‚è≥ SGOL LSTM: Epoch 20/50 (40%)
      ‚è≥ SGOL LSTM: Epoch 30/50 (60%)
       ‚úÖ R XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.5828 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.5314
    - TCN: MSE=0.6249
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5314
        ‚Ä¢ TCN: MSE=0.6249
        ‚Ä¢ Random Forest: MSE=9.0551
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.2525
        ‚Ä¢ XGBoost: MSE=17.5828
   ‚úÖ R: Phase 3/3 - Model selection complete!
  üèÜ WINNER for R (TargetReturn): LSTM with MSE=0.5314
üêõ DEBUG: R - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for R.
üêõ DEBUG: R - Moving model to CPU before return...
üêõ DEBUG [00:11:30.503]: R - Returning result metadata...
üêõ DEBUG: train_worker started for EBAY
üêõ DEBUG [00:11:30.504]: Main received result for R
üêõ DEBUG [00:11:30.505]: Main received result for EXPE
üêõ DEBUG: Training progress: 796/959 done
  ‚öôÔ∏è Training models for EBAY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - EBAY: Initiating feature extraction for training.
  [DIAGNOSTIC] EBAY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EBAY: rows after features available: 126
üéØ EBAY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EBAY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EBAY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EBAY: Training LSTM (50 epochs)...
      ‚è≥ SGOL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.345262
         RMSE: 0.587590
         R¬≤ Score: -0.8334 (Poor - 83.3% variance explained)
      üîπ SGOL: Training TCN (50 epochs)...
      ‚è≥ EBAY LSTM: Epoch 10/50 (20%)
      ‚è≥ SGOL TCN: Epoch 10/50 (20%)
      ‚è≥ SGOL TCN: Epoch 20/50 (40%)
      ‚è≥ SGOL TCN: Epoch 30/50 (60%)
      ‚è≥ SGOL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.293512
         RMSE: 0.541767
         R¬≤ Score: -0.5586
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SGOL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SGOL Random Forest: Starting GridSearchCV fit...
      ‚è≥ EBAY LSTM: Epoch 20/50 (40%)
      ‚è≥ EBAY LSTM: Epoch 30/50 (60%)
      ‚è≥ EBAY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.297482
         RMSE: 0.545420
         R¬≤ Score: -0.7053 (Poor - 70.5% variance explained)
      üîπ EBAY: Training TCN (50 epochs)...
      ‚è≥ EBAY TCN: Epoch 10/50 (20%)
      ‚è≥ EBAY TCN: Epoch 20/50 (40%)
      ‚è≥ EBAY TCN: Epoch 30/50 (60%)
      ‚è≥ EBAY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.319121
         RMSE: 0.564908
         R¬≤ Score: -0.8294
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EBAY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EBAY Random Forest: Starting GridSearchCV fit...
       ‚úÖ SGOL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.4733 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SGOL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SGOL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.8291 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SGOL XGBoost: Starting GridSearchCV fit...
       ‚úÖ Z XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.4018 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.4290
    - TCN: MSE=0.2082
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2082
        ‚Ä¢ LSTM: MSE=0.4290
        ‚Ä¢ Random Forest: MSE=12.9781
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.1544
        ‚Ä¢ XGBoost: MSE=17.4018
   ‚úÖ Z: Phase 3/3 - Model selection complete!
  üèÜ WINNER for Z (TargetReturn): TCN with MSE=0.2082
üêõ DEBUG: Z - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for Z.
üêõ DEBUG: Z - Moving model to CPU before return...
üêõ DEBUG [00:11:36.257]: Z - Returning result metadata...
üêõ DEBUG: train_worker started for BSX
üêõ DEBUG [00:11:36.257]: Main received result for Z
  ‚öôÔ∏è Training models for BSX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - BSX: Initiating feature extraction for training.
  [DIAGNOSTIC] BSX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BSX: rows after features available: 126
üéØ BSX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BSX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BSX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BSX: Training LSTM (50 epochs)...
       ‚úÖ EBAY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.7232 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EBAY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BSX LSTM: Epoch 10/50 (20%)
       ‚úÖ EBAY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.5976 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EBAY XGBoost: Starting GridSearchCV fit...
      ‚è≥ BSX LSTM: Epoch 20/50 (40%)
      ‚è≥ BSX LSTM: Epoch 30/50 (60%)
      ‚è≥ BSX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.185313
         RMSE: 0.430480
         R¬≤ Score: -0.8781 (Poor - 87.8% variance explained)
      üîπ BSX: Training TCN (50 epochs)...
      ‚è≥ BSX TCN: Epoch 10/50 (20%)
      ‚è≥ BSX TCN: Epoch 20/50 (40%)
      ‚è≥ BSX TCN: Epoch 30/50 (60%)
      ‚è≥ BSX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.099468
         RMSE: 0.315385
         R¬≤ Score: -0.0081
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BSX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BSX Random Forest: Starting GridSearchCV fit...
       ‚úÖ BSX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5781 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BSX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BSX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.3551 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BSX XGBoost: Starting GridSearchCV fit...
       ‚úÖ EQH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.2145 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.3288
    - TCN: MSE=0.3199
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3199
        ‚Ä¢ LSTM: MSE=0.3288
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.3645
        ‚Ä¢ Random Forest: MSE=14.5039
        ‚Ä¢ XGBoost: MSE=20.2145
   ‚úÖ EQH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EQH (TargetReturn): TCN with MSE=0.3199
üêõ DEBUG: EQH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EQH.
üêõ DEBUG: EQH - Moving model to CPU before return...
üêõ DEBUG [00:11:49.217]: EQH - Returning result metadata...
üêõ DEBUG [00:11:49.218]: Main received result for EQH
üêõ DEBUG: train_worker started for OUNZ
  ‚öôÔ∏è Training models for OUNZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - OUNZ: Initiating feature extraction for training.
  [DIAGNOSTIC] OUNZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OUNZ: rows after features available: 126
üéØ OUNZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OUNZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OUNZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OUNZ: Training LSTM (50 epochs)...
      ‚è≥ OUNZ LSTM: Epoch 10/50 (20%)
      ‚è≥ OUNZ LSTM: Epoch 20/50 (40%)
      ‚è≥ OUNZ LSTM: Epoch 30/50 (60%)
       ‚úÖ TDY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.8637 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.2s
    - LSTM: MSE=0.5400
    - TCN: MSE=0.5476
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5400
        ‚Ä¢ TCN: MSE=0.5476
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7539
        ‚Ä¢ Random Forest: MSE=6.7185
        ‚Ä¢ XGBoost: MSE=6.8637
   ‚úÖ TDY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TDY (TargetReturn): LSTM with MSE=0.5400
üêõ DEBUG: TDY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TDY.
üêõ DEBUG: TDY - Moving model to CPU before return...
üêõ DEBUG [00:11:50.770]: TDY - Returning result metadata...
üêõ DEBUG: train_worker started for AAAU
üêõ DEBUG [00:11:50.772]: Main received result for TDY
  ‚öôÔ∏è Training models for AAAU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AAAU: Initiating feature extraction for training.
  [DIAGNOSTIC] AAAU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AAAU: rows after features available: 126
üéØ AAAU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AAAU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AAAU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AAAU: Training LSTM (50 epochs)...
      ‚è≥ OUNZ LSTM: Epoch 40/50 (80%)
      ‚è≥ AAAU LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.272482
         RMSE: 0.521998
         R¬≤ Score: -0.4545 (Poor - 45.5% variance explained)
      üîπ OUNZ: Training TCN (50 epochs)...
      ‚è≥ OUNZ TCN: Epoch 10/50 (20%)
      ‚è≥ OUNZ TCN: Epoch 20/50 (40%)
      ‚è≥ AAAU LSTM: Epoch 20/50 (40%)
      ‚è≥ OUNZ TCN: Epoch 30/50 (60%)
      ‚è≥ OUNZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.289890
         RMSE: 0.538414
         R¬≤ Score: -0.5474
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OUNZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OUNZ Random Forest: Starting GridSearchCV fit...
      ‚è≥ AAAU LSTM: Epoch 30/50 (60%)
      ‚è≥ AAAU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.328420
         RMSE: 0.573079
         R¬≤ Score: -0.7560 (Poor - 75.6% variance explained)
      üîπ AAAU: Training TCN (50 epochs)...
      ‚è≥ AAAU TCN: Epoch 10/50 (20%)
      ‚è≥ AAAU TCN: Epoch 20/50 (40%)
      ‚è≥ AAAU TCN: Epoch 30/50 (60%)
      ‚è≥ AAAU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.318900
         RMSE: 0.564713
         R¬≤ Score: -0.7051
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AAAU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AAAU Random Forest: Starting GridSearchCV fit...
       ‚úÖ OUNZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.9346 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OUNZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OUNZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.8510 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OUNZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ AAAU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.3817 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AAAU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AAAU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.5445 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AAAU XGBoost: Starting GridSearchCV fit...
       ‚úÖ OPEN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=1823.1660 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.2s
    - LSTM: MSE=0.4786
    - TCN: MSE=0.2567
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2567
        ‚Ä¢ LSTM: MSE=0.4786
        ‚Ä¢ Random Forest: MSE=588.4943
        ‚Ä¢ LightGBM Regressor (CPU): MSE=961.0696
        ‚Ä¢ XGBoost: MSE=1823.1660
   ‚úÖ OPEN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OPEN (TargetReturn): TCN with MSE=0.2567
üêõ DEBUG: OPEN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OPEN.
üêõ DEBUG: OPEN - Moving model to CPU before return...
üêõ DEBUG [00:11:58.724]: OPEN - Returning result metadata...
üêõ DEBUG [00:11:58.724]: Main received result for OPEN
üêõ DEBUG: Training progress: 800/959 done
üêõ DEBUG: train_worker started for KR
  ‚öôÔ∏è Training models for KR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - KR: Initiating feature extraction for training.
  [DIAGNOSTIC] KR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KR: rows after features available: 126
üéØ KR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KR: Training LSTM (50 epochs)...
      ‚è≥ KR LSTM: Epoch 10/50 (20%)
      ‚è≥ KR LSTM: Epoch 20/50 (40%)
      ‚è≥ KR LSTM: Epoch 30/50 (60%)
      ‚è≥ KR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.312022
         RMSE: 0.558590
         R¬≤ Score: -0.7856 (Poor - 78.6% variance explained)
      üîπ KR: Training TCN (50 epochs)...
      ‚è≥ KR TCN: Epoch 10/50 (20%)
      ‚è≥ KR TCN: Epoch 20/50 (40%)
      ‚è≥ KR TCN: Epoch 30/50 (60%)
      ‚è≥ KR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.220333
         RMSE: 0.469396
         R¬≤ Score: -0.2609
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KR Random Forest: Starting GridSearchCV fit...
       ‚úÖ KR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.6339 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.5041 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KR XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.8690 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.2056
    - TCN: MSE=0.1338
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1338
        ‚Ä¢ LSTM: MSE=0.2056
        ‚Ä¢ Random Forest: MSE=4.1623
        ‚Ä¢ XGBoost: MSE=4.8690
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7278
   ‚úÖ FDT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDT (TargetReturn): TCN with MSE=0.1338
üêõ DEBUG: FDT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDT.
üêõ DEBUG: FDT - Moving model to CPU before return...
üêõ DEBUG [00:12:15.256]: FDT - Returning result metadata...
üêõ DEBUG: train_worker started for HACK
üêõ DEBUG [00:12:15.257]: Main received result for FDT
  ‚öôÔ∏è Training models for HACK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - HACK: Initiating feature extraction for training.
  [DIAGNOSTIC] HACK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HACK: rows after features available: 126
üéØ HACK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HACK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HACK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HACK: Training LSTM (50 epochs)...
      ‚è≥ HACK LSTM: Epoch 10/50 (20%)
      ‚è≥ HACK LSTM: Epoch 20/50 (40%)
      ‚è≥ HACK LSTM: Epoch 30/50 (60%)
      ‚è≥ HACK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.450518
         RMSE: 0.671206
         R¬≤ Score: -0.9670 (Poor - 96.7% variance explained)
      üîπ HACK: Training TCN (50 epochs)...
      ‚è≥ HACK TCN: Epoch 10/50 (20%)
      ‚è≥ HACK TCN: Epoch 20/50 (40%)
      ‚è≥ HACK TCN: Epoch 30/50 (60%)
      ‚è≥ HACK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.385772
         RMSE: 0.621105
         R¬≤ Score: -0.6843
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HACK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HACK Random Forest: Starting GridSearchCV fit...
       ‚úÖ HACK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.2734 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HACK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SAP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.9498 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.1s
    - LSTM: MSE=0.2806
    - TCN: MSE=0.1807
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1807
        ‚Ä¢ LSTM: MSE=0.2806
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.8572
        ‚Ä¢ XGBoost: MSE=10.9498
        ‚Ä¢ Random Forest: MSE=13.2211
   ‚úÖ SAP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SAP (TargetReturn): TCN with MSE=0.1807
üêõ DEBUG: SAP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SAP.
üêõ DEBUG: SAP - Moving model to CPU before return...
üêõ DEBUG [00:12:21.150]: SAP - Returning result metadata...
üêõ DEBUG: train_worker started for FINX
üêõ DEBUG [00:12:21.153]: Main received result for SAP
  ‚öôÔ∏è Training models for FINX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - FINX: Initiating feature extraction for training.
  [DIAGNOSTIC] FINX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FINX: rows after features available: 126
üéØ FINX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FINX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FINX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FINX: Training LSTM (50 epochs)...
       ‚úÖ HACK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.6246 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HACK XGBoost: Starting GridSearchCV fit...
      ‚è≥ FINX LSTM: Epoch 10/50 (20%)
      ‚è≥ FINX LSTM: Epoch 20/50 (40%)
      ‚è≥ FINX LSTM: Epoch 30/50 (60%)
      ‚è≥ FINX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.344572
         RMSE: 0.587002
         R¬≤ Score: -0.2341 (Poor - 23.4% variance explained)
      üîπ FINX: Training TCN (50 epochs)...
      ‚è≥ FINX TCN: Epoch 10/50 (20%)
      ‚è≥ FINX TCN: Epoch 20/50 (40%)
      ‚è≥ FINX TCN: Epoch 30/50 (60%)
      ‚è≥ FINX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.546084
         RMSE: 0.738975
         R¬≤ Score: -0.9558
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FINX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FINX Random Forest: Starting GridSearchCV fit...
       ‚úÖ FINX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.4553 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FINX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FINX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.6164 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FINX XGBoost: Starting GridSearchCV fit...
       ‚úÖ TEO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.4613 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.5s
    - LSTM: MSE=0.0469
    - TCN: MSE=0.0278
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0278
        ‚Ä¢ LSTM: MSE=0.0469
        ‚Ä¢ XGBoost: MSE=14.4613
        ‚Ä¢ Random Forest: MSE=16.0701
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.3884
   ‚úÖ TEO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TEO (TargetReturn): TCN with MSE=0.0278
üêõ DEBUG: TEO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TEO.
üêõ DEBUG: TEO - Moving model to CPU before return...
üêõ DEBUG [00:12:36.616]: TEO - Returning result metadata...
üêõ DEBUG [00:12:36.616]: Main received result for TEO
üêõ DEBUG: train_worker started for IAU
  ‚öôÔ∏è Training models for IAU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - IAU: Initiating feature extraction for training.
  [DIAGNOSTIC] IAU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IAU: rows after features available: 126
üéØ IAU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IAU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IAU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IAU: Training LSTM (50 epochs)...
      ‚è≥ IAU LSTM: Epoch 10/50 (20%)
      ‚è≥ IAU LSTM: Epoch 20/50 (40%)
      ‚è≥ IAU LSTM: Epoch 30/50 (60%)
      ‚è≥ IAU LSTM: Epoch 40/50 (80%)
       ‚úÖ RELY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.2231 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 123.1s
    - LSTM: MSE=0.0498
    - TCN: MSE=0.0269
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0269
        ‚Ä¢ LSTM: MSE=0.0498
        ‚Ä¢ Random Forest: MSE=14.3772
        ‚Ä¢ XGBoost: MSE=17.2231
        ‚Ä¢ LightGBM Regressor (CPU): MSE=21.9901
   ‚úÖ RELY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RELY (TargetReturn): TCN with MSE=0.0269
üêõ DEBUG: RELY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RELY.
üêõ DEBUG: RELY - Moving model to CPU before return...
üêõ DEBUG [00:12:38.568]: RELY - Returning result metadata...
üêõ DEBUG: train_worker started for ASTE
üêõ DEBUG [00:12:38.569]: Main received result for RELY
üêõ DEBUG: Training progress: 804/959 done
  ‚öôÔ∏è Training models for ASTE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - ASTE: Initiating feature extraction for training.
  [DIAGNOSTIC] ASTE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ASTE: rows after features available: 126
üéØ ASTE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ASTE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ASTE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ASTE: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.333393
         RMSE: 0.577402
         R¬≤ Score: -0.7700 (Poor - 77.0% variance explained)
      üîπ IAU: Training TCN (50 epochs)...
      ‚è≥ IAU TCN: Epoch 10/50 (20%)
      ‚è≥ ASTE LSTM: Epoch 10/50 (20%)
      ‚è≥ IAU TCN: Epoch 20/50 (40%)
      ‚è≥ IAU TCN: Epoch 30/50 (60%)
      ‚è≥ IAU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.322683
         RMSE: 0.568052
         R¬≤ Score: -0.7132
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IAU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IAU Random Forest: Starting GridSearchCV fit...
      ‚è≥ ASTE LSTM: Epoch 20/50 (40%)
      ‚è≥ ASTE LSTM: Epoch 30/50 (60%)
      ‚è≥ ASTE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.207640
         RMSE: 0.455676
         R¬≤ Score: -0.9035 (Poor - 90.4% variance explained)
      üîπ ASTE: Training TCN (50 epochs)...
      ‚è≥ ASTE TCN: Epoch 10/50 (20%)
      ‚è≥ ASTE TCN: Epoch 20/50 (40%)
      ‚è≥ ASTE TCN: Epoch 30/50 (60%)
      ‚è≥ ASTE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.109437
         RMSE: 0.330813
         R¬≤ Score: -0.0032
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ASTE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ASTE Random Forest: Starting GridSearchCV fit...
       ‚úÖ IAU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.8050 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IAU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IAU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.8127 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IAU XGBoost: Starting GridSearchCV fit...
       ‚úÖ ASTE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.5761 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ASTE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ASTE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.9517 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ASTE XGBoost: Starting GridSearchCV fit...
       ‚úÖ IAUM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.2416 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.3085
    - TCN: MSE=0.2912
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2912
        ‚Ä¢ LSTM: MSE=0.3085
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.4986
        ‚Ä¢ Random Forest: MSE=5.4214
        ‚Ä¢ XGBoost: MSE=6.2416
   ‚úÖ IAUM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IAUM (TargetReturn): TCN with MSE=0.2912
üêõ DEBUG: IAUM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IAUM.
üêõ DEBUG: IAUM - Moving model to CPU before return...
üêõ DEBUG [00:12:46.526]: IAUM - Returning result metadata...
üêõ DEBUG [00:12:46.527]: Main received result for IAUM
üêõ DEBUG: train_worker started for CLSK
  ‚öôÔ∏è Training models for CLSK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - CLSK: Initiating feature extraction for training.
  [DIAGNOSTIC] CLSK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CLSK: rows after features available: 126
üéØ CLSK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CLSK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CLSK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CLSK: Training LSTM (50 epochs)...
      ‚è≥ CLSK LSTM: Epoch 10/50 (20%)
      ‚è≥ CLSK LSTM: Epoch 20/50 (40%)
      ‚è≥ CLSK LSTM: Epoch 30/50 (60%)
      ‚è≥ CLSK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.265221
         RMSE: 0.514996
         R¬≤ Score: -0.0701 (Poor - 7.0% variance explained)
      üîπ CLSK: Training TCN (50 epochs)...
      ‚è≥ CLSK TCN: Epoch 10/50 (20%)
      ‚è≥ CLSK TCN: Epoch 20/50 (40%)
      ‚è≥ CLSK TCN: Epoch 30/50 (60%)
      ‚è≥ CLSK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.563947
         RMSE: 0.750964
         R¬≤ Score: -1.2754
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CLSK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CLSK Random Forest: Starting GridSearchCV fit...
       ‚úÖ CLSK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=66.9565 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CLSK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CLSK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=64.8842 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CLSK XGBoost: Starting GridSearchCV fit...
       ‚úÖ GLDM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.6152 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.3071
    - TCN: MSE=0.3066
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3066
        ‚Ä¢ LSTM: MSE=0.3071
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.4485
        ‚Ä¢ XGBoost: MSE=6.6152
        ‚Ä¢ Random Forest: MSE=7.0021
   ‚úÖ GLDM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GLDM (TargetReturn): TCN with MSE=0.3066
üêõ DEBUG: GLDM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GLDM.
üêõ DEBUG: GLDM - Moving model to CPU before return...
üêõ DEBUG [00:12:54.094]: GLDM - Returning result metadata...
üêõ DEBUG [00:12:54.095]: Main received result for GLDM
üêõ DEBUG: train_worker started for UROY
  ‚öôÔ∏è Training models for UROY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - UROY: Initiating feature extraction for training.
  [DIAGNOSTIC] UROY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UROY: rows after features available: 126
üéØ UROY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UROY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UROY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UROY: Training LSTM (50 epochs)...
      ‚è≥ UROY LSTM: Epoch 10/50 (20%)
      ‚è≥ UROY LSTM: Epoch 20/50 (40%)
      ‚è≥ UROY LSTM: Epoch 30/50 (60%)
      ‚è≥ UROY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.523758
         RMSE: 0.723711
         R¬≤ Score: -0.6878 (Poor - 68.8% variance explained)
      üîπ UROY: Training TCN (50 epochs)...
      ‚è≥ UROY TCN: Epoch 10/50 (20%)
      ‚è≥ UROY TCN: Epoch 20/50 (40%)
      ‚è≥ UROY TCN: Epoch 30/50 (60%)
      ‚è≥ UROY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.398753
         RMSE: 0.631469
         R¬≤ Score: -0.2850
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UROY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UROY Random Forest: Starting GridSearchCV fit...
       ‚úÖ UROY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=35.6524 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UROY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UROY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=37.3332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UROY XGBoost: Starting GridSearchCV fit...
       ‚úÖ BAR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.8985 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.4080
    - TCN: MSE=0.2953
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2953
        ‚Ä¢ LSTM: MSE=0.4080
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.4749
        ‚Ä¢ Random Forest: MSE=5.4454
        ‚Ä¢ XGBoost: MSE=6.8985
   ‚úÖ BAR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BAR (TargetReturn): TCN with MSE=0.2953
üêõ DEBUG: BAR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BAR.
üêõ DEBUG: BAR - Moving model to CPU before return...
üêõ DEBUG [00:13:04.746]: BAR - Returning result metadata...
üêõ DEBUG [00:13:04.747]: Main received result for BAR
üêõ DEBUG: train_worker started for ARGT
  ‚öôÔ∏è Training models for ARGT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - ARGT: Initiating feature extraction for training.
  [DIAGNOSTIC] ARGT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARGT: rows after features available: 126
üéØ ARGT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARGT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARGT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARGT: Training LSTM (50 epochs)...
      ‚è≥ ARGT LSTM: Epoch 10/50 (20%)
      ‚è≥ ARGT LSTM: Epoch 20/50 (40%)
      ‚è≥ ARGT LSTM: Epoch 30/50 (60%)
      ‚è≥ ARGT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.172018
         RMSE: 0.414750
         R¬≤ Score: -0.8377 (Poor - 83.8% variance explained)
      üîπ ARGT: Training TCN (50 epochs)...
      ‚è≥ ARGT TCN: Epoch 10/50 (20%)
      ‚è≥ ARGT TCN: Epoch 20/50 (40%)
      ‚è≥ ARGT TCN: Epoch 30/50 (60%)
      ‚è≥ ARGT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.096818
         RMSE: 0.311157
         R¬≤ Score: -0.0343
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARGT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARGT Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARGT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.9379 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARGT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARGT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.0219 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARGT XGBoost: Starting GridSearchCV fit...
       ‚úÖ DK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=67.5182 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.1s
    - LSTM: MSE=0.5589
    - TCN: MSE=0.3394
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3394
        ‚Ä¢ LSTM: MSE=0.5589
        ‚Ä¢ Random Forest: MSE=44.5461
        ‚Ä¢ XGBoost: MSE=67.5182
        ‚Ä¢ LightGBM Regressor (CPU): MSE=106.5717
   ‚úÖ DK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DK (TargetReturn): TCN with MSE=0.3394
üêõ DEBUG: DK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DK.
üêõ DEBUG: DK - Moving model to CPU before return...
üêõ DEBUG [00:13:16.882]: DK - Returning result metadata...
üêõ DEBUG [00:13:16.882]: Main received result for DK
üêõ DEBUG: Training progress: 808/959 done
üêõ DEBUG: train_worker started for GLD
  ‚öôÔ∏è Training models for GLD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - GLD: Initiating feature extraction for training.
  [DIAGNOSTIC] GLD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GLD: rows after features available: 126
üéØ GLD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GLD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GLD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GLD: Training LSTM (50 epochs)...
      ‚è≥ GLD LSTM: Epoch 10/50 (20%)
       ‚úÖ BULZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=96.3336 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.5481
    - TCN: MSE=0.3592
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3592
        ‚Ä¢ LSTM: MSE=0.5481
        ‚Ä¢ Random Forest: MSE=91.4914
        ‚Ä¢ XGBoost: MSE=96.3336
        ‚Ä¢ LightGBM Regressor (CPU): MSE=228.8818
   ‚úÖ BULZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BULZ (TargetReturn): TCN with MSE=0.3592
üêõ DEBUG: BULZ - train_and_evaluate_models completed
      ‚è≥ GLD LSTM: Epoch 20/50 (40%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BULZ.
üêõ DEBUG: BULZ - Moving model to CPU before return...
üêõ DEBUG [00:13:17.781]: BULZ - Returning result metadata...
üêõ DEBUG [00:13:17.781]: Main received result for BULZüêõ DEBUG: train_worker started for CPER

  ‚öôÔ∏è Training models for CPER (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CPER: Initiating feature extraction for training.
  [DIAGNOSTIC] CPER: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CPER: rows after features available: 126
üéØ CPER: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CPER: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CPER: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CPER: Training LSTM (50 epochs)...
      ‚è≥ GLD LSTM: Epoch 30/50 (60%)
      ‚è≥ CPER LSTM: Epoch 10/50 (20%)
      ‚è≥ GLD LSTM: Epoch 40/50 (80%)
      ‚è≥ CPER LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.334472
         RMSE: 0.578335
         R¬≤ Score: -0.7731 (Poor - 77.3% variance explained)
      üîπ GLD: Training TCN (50 epochs)...
      ‚è≥ CPER LSTM: Epoch 30/50 (60%)
      ‚è≥ GLD TCN: Epoch 10/50 (20%)
      ‚è≥ GLD TCN: Epoch 20/50 (40%)
      ‚è≥ GLD TCN: Epoch 30/50 (60%)
      ‚è≥ GLD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.292211
         RMSE: 0.540565
         R¬≤ Score: -0.5490
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GLD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GLD Random Forest: Starting GridSearchCV fit...
      ‚è≥ CPER LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.494180
         RMSE: 0.702979
         R¬≤ Score: -0.9202 (Poor - 92.0% variance explained)
      üîπ CPER: Training TCN (50 epochs)...
      ‚è≥ CPER TCN: Epoch 10/50 (20%)
      ‚è≥ CPER TCN: Epoch 20/50 (40%)
      ‚è≥ CPER TCN: Epoch 30/50 (60%)
      ‚è≥ CPER TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.292819
         RMSE: 0.541128
         R¬≤ Score: -0.1378
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CPER: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CPER Random Forest: Starting GridSearchCV fit...
       ‚úÖ GLD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.8434 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GLD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GLD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.2692 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GLD XGBoost: Starting GridSearchCV fit...
       ‚úÖ CPER Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.3639 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CPER LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CPER LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.4130 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CPER XGBoost: Starting GridSearchCV fit...
       ‚úÖ SGOL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.1391 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.3453
    - TCN: MSE=0.2935
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2935
        ‚Ä¢ LSTM: MSE=0.3453
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.8291
        ‚Ä¢ Random Forest: MSE=5.4733
        ‚Ä¢ XGBoost: MSE=8.1391
   ‚úÖ SGOL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SGOL (TargetReturn): TCN with MSE=0.2935
üêõ DEBUG: SGOL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SGOL.
üêõ DEBUG: SGOL - Moving model to CPU before return...
üêõ DEBUG [00:13:32.715]: SGOL - Returning result metadata...
üêõ DEBUG: train_worker started for CIBR
üêõ DEBUG [00:13:32.721]: Main received result for SGOL
  ‚öôÔ∏è Training models for CIBR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CIBR: Initiating feature extraction for training.
  [DIAGNOSTIC] CIBR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CIBR: rows after features available: 126
üéØ CIBR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CIBR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CIBR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CIBR: Training LSTM (50 epochs)...
      ‚è≥ CIBR LSTM: Epoch 10/50 (20%)
      ‚è≥ CIBR LSTM: Epoch 20/50 (40%)
      ‚è≥ CIBR LSTM: Epoch 30/50 (60%)
      ‚è≥ CIBR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.280139
         RMSE: 0.529282
         R¬≤ Score: -0.3706 (Poor - 37.1% variance explained)
      üîπ CIBR: Training TCN (50 epochs)...
      ‚è≥ CIBR TCN: Epoch 10/50 (20%)
      ‚è≥ CIBR TCN: Epoch 20/50 (40%)
      ‚è≥ CIBR TCN: Epoch 30/50 (60%)
      ‚è≥ CIBR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.394188
         RMSE: 0.627844
         R¬≤ Score: -0.9286
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CIBR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CIBR Random Forest: Starting GridSearchCV fit...
       ‚úÖ BSX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.9963 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.2s
    - LSTM: MSE=0.1853
    - TCN: MSE=0.0995
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0995
        ‚Ä¢ LSTM: MSE=0.1853
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.3551
        ‚Ä¢ XGBoost: MSE=4.9963
        ‚Ä¢ Random Forest: MSE=5.5781
   ‚úÖ BSX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BSX (TargetReturn): TCN with MSE=0.0995
üêõ DEBUG: BSX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BSX.
üêõ DEBUG: BSX - Moving model to CPU before return...
üêõ DEBUG [00:13:38.553]: BSX - Returning result metadata...
üêõ DEBUG: train_worker started for DCTH
  ‚öôÔ∏è Training models for DCTH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - DCTH: Initiating feature extraction for training.
  [DIAGNOSTIC] DCTH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DCTH: rows after features available: 126
üéØ DCTH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DCTH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DCTH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DCTH: Training LSTM (50 epochs)...
       ‚úÖ CIBR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.1508 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CIBR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ DCTH LSTM: Epoch 10/50 (20%)
       ‚úÖ EBAY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.6885 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.3s
    - LSTM: MSE=0.2975
    - TCN: MSE=0.3191
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2975
        ‚Ä¢ TCN: MSE=0.3191
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5976
        ‚Ä¢ Random Forest: MSE=13.7232
        ‚Ä¢ XGBoost: MSE=15.6885
   ‚úÖ EBAY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EBAY (TargetReturn): LSTM with MSE=0.2975
üêõ DEBUG: EBAY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EBAY.
üêõ DEBUG: EBAY - Moving model to CPU before return...
üêõ DEBUG [00:13:39.388]: EBAY - Returning result metadata...
üêõ DEBUG [00:13:39.389]: Main received result for EBAY
üêõ DEBUG [00:13:39.389]: Main received result for BSX
üêõ DEBUG: Training progress: 812/959 done
üêõ DEBUG: train_worker started for BJ
       ‚úÖ CIBR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.2277 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CIBR XGBoost: Starting GridSearchCV fit...
  ‚öôÔ∏è Training models for BJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - BJ: Initiating feature extraction for training.
  [DIAGNOSTIC] BJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BJ: rows after features available: 126
üéØ BJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BJ: Training LSTM (50 epochs)...
      ‚è≥ DCTH LSTM: Epoch 20/50 (40%)
      ‚è≥ DCTH LSTM: Epoch 30/50 (60%)
      ‚è≥ BJ LSTM: Epoch 10/50 (20%)
      ‚è≥ DCTH LSTM: Epoch 40/50 (80%)
      ‚è≥ BJ LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.203640
         RMSE: 0.451264
         R¬≤ Score: -0.7804 (Poor - 78.0% variance explained)
      üîπ DCTH: Training TCN (50 epochs)...
      ‚è≥ BJ LSTM: Epoch 30/50 (60%)
      ‚è≥ DCTH TCN: Epoch 10/50 (20%)
      ‚è≥ DCTH TCN: Epoch 20/50 (40%)
      ‚è≥ DCTH TCN: Epoch 30/50 (60%)
      ‚è≥ DCTH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.114588
         RMSE: 0.338509
         R¬≤ Score: -0.0018
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DCTH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DCTH Random Forest: Starting GridSearchCV fit...
      ‚è≥ BJ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.385290
         RMSE: 0.620717
         R¬≤ Score: -1.0771 (Poor - 107.7% variance explained)
      üîπ BJ: Training TCN (50 epochs)...
      ‚è≥ BJ TCN: Epoch 10/50 (20%)
      ‚è≥ BJ TCN: Epoch 20/50 (40%)
      ‚è≥ BJ TCN: Epoch 30/50 (60%)
      ‚è≥ BJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.272389
         RMSE: 0.521909
         R¬≤ Score: -0.4684
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ DCTH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=44.8010 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DCTH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DCTH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=38.8405 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DCTH XGBoost: Starting GridSearchCV fit...
       ‚úÖ BJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.2839 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.9374 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BJ XGBoost: Starting GridSearchCV fit...
       ‚úÖ OUNZ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.7346 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.8s
    - LSTM: MSE=0.2725
    - TCN: MSE=0.2899
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2725
        ‚Ä¢ TCN: MSE=0.2899
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.8510
        ‚Ä¢ Random Forest: MSE=5.9346
        ‚Ä¢ XGBoost: MSE=9.7346
   ‚úÖ OUNZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OUNZ (TargetReturn): LSTM with MSE=0.2725
üêõ DEBUG: OUNZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OUNZ.
üêõ DEBUG: OUNZ - Moving model to CPU before return...
üêõ DEBUG [00:13:52.042]: OUNZ - Returning result metadata...
üêõ DEBUG: train_worker started for FITE
üêõ DEBUG [00:13:52.043]: Main received result for OUNZ
  ‚öôÔ∏è Training models for FITE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - FITE: Initiating feature extraction for training.
  [DIAGNOSTIC] FITE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FITE: rows after features available: 126
üéØ FITE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FITE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FITE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FITE: Training LSTM (50 epochs)...
      ‚è≥ FITE LSTM: Epoch 10/50 (20%)
      ‚è≥ FITE LSTM: Epoch 20/50 (40%)
      ‚è≥ FITE LSTM: Epoch 30/50 (60%)
      ‚è≥ FITE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.486955
         RMSE: 0.697822
         R¬≤ Score: -0.8150 (Poor - 81.5% variance explained)
      üîπ FITE: Training TCN (50 epochs)...
      ‚è≥ FITE TCN: Epoch 10/50 (20%)
      ‚è≥ FITE TCN: Epoch 20/50 (40%)
      ‚è≥ FITE TCN: Epoch 30/50 (60%)
      ‚è≥ FITE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.502199
         RMSE: 0.708660
         R¬≤ Score: -0.8718
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FITE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FITE Random Forest: Starting GridSearchCV fit...
       ‚úÖ FITE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9362 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FITE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FITE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.0464 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FITE XGBoost: Starting GridSearchCV fit...
       ‚úÖ AAAU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.7193 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.6s
    - LSTM: MSE=0.3284
    - TCN: MSE=0.3189
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3189
        ‚Ä¢ LSTM: MSE=0.3284
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.5445
        ‚Ä¢ Random Forest: MSE=5.3817
        ‚Ä¢ XGBoost: MSE=8.7193
   ‚úÖ AAAU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AAAU (TargetReturn): TCN with MSE=0.3189
üêõ DEBUG: AAAU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AAAU.
üêõ DEBUG: AAAU - Moving model to CPU before return...
üêõ DEBUG [00:13:58.613]: AAAU - Returning result metadata...
üêõ DEBUG: train_worker started for ENVA
üêõ DEBUG [00:13:58.614]: Main received result for AAAU
  ‚öôÔ∏è Training models for ENVA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - ENVA: Initiating feature extraction for training.
  [DIAGNOSTIC] ENVA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ENVA: rows after features available: 126
üéØ ENVA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ENVA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ENVA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ENVA: Training LSTM (50 epochs)...
      ‚è≥ ENVA LSTM: Epoch 10/50 (20%)
      ‚è≥ ENVA LSTM: Epoch 20/50 (40%)
       ‚úÖ KR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.6466 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.1s
    - LSTM: MSE=0.3120
    - TCN: MSE=0.2203
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2203
        ‚Ä¢ LSTM: MSE=0.3120
        ‚Ä¢ Random Forest: MSE=7.6339
        ‚Ä¢ XGBoost: MSE=8.6466
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5041
   ‚úÖ KR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KR (TargetReturn): TCN with MSE=0.2203
üêõ DEBUG: KR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KR.
üêõ DEBUG: KR - Moving model to CPU before return...
üêõ DEBUG [00:13:59.711]: KR - Returning result metadata...
üêõ DEBUG: train_worker started for ZG
üêõ DEBUG [00:13:59.712]: Main received result for KR
  ‚öôÔ∏è Training models for ZG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - ZG: Initiating feature extraction for training.
  [DIAGNOSTIC] ZG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ZG: rows after features available: 126
üéØ ZG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ZG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ZG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ZG: Training LSTM (50 epochs)...
      ‚è≥ ENVA LSTM: Epoch 30/50 (60%)
      ‚è≥ ZG LSTM: Epoch 10/50 (20%)
      ‚è≥ ENVA LSTM: Epoch 40/50 (80%)
      ‚è≥ ZG LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.635219
         RMSE: 0.797006
         R¬≤ Score: -0.9164 (Poor - 91.6% variance explained)
      üîπ ENVA: Training TCN (50 epochs)...
      ‚è≥ ENVA TCN: Epoch 10/50 (20%)
      ‚è≥ ZG LSTM: Epoch 30/50 (60%)
      ‚è≥ ENVA TCN: Epoch 20/50 (40%)
      ‚è≥ ENVA TCN: Epoch 30/50 (60%)
      ‚è≥ ENVA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.343613
         RMSE: 0.586185
         R¬≤ Score: -0.0367
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ENVA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ENVA Random Forest: Starting GridSearchCV fit...
      ‚è≥ ZG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.270309
         RMSE: 0.519913
         R¬≤ Score: -0.7096 (Poor - 71.0% variance explained)
      üîπ ZG: Training TCN (50 epochs)...
      ‚è≥ ZG TCN: Epoch 10/50 (20%)
      ‚è≥ ZG TCN: Epoch 20/50 (40%)
      ‚è≥ ZG TCN: Epoch 30/50 (60%)
      ‚è≥ ZG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.219835
         RMSE: 0.468866
         R¬≤ Score: -0.3904
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ZG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ZG Random Forest: Starting GridSearchCV fit...
       ‚úÖ ENVA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.9267 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ENVA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ENVA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.2964 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ENVA XGBoost: Starting GridSearchCV fit...
       ‚úÖ ZG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.5095 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ZG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ZG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.2508 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ZG XGBoost: Starting GridSearchCV fit...
       ‚úÖ HACK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.1731 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.9s
    - LSTM: MSE=0.4505
    - TCN: MSE=0.3858
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3858
        ‚Ä¢ LSTM: MSE=0.4505
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.6246
        ‚Ä¢ Random Forest: MSE=13.2734
        ‚Ä¢ XGBoost: MSE=14.1731
   ‚úÖ HACK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HACK (TargetReturn): TCN with MSE=0.3858
üêõ DEBUG: HACK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HACK.
üêõ DEBUG: HACK - Moving model to CPU before return...
üêõ DEBUG [00:14:20.446]: HACK - Returning result metadata...
üêõ DEBUG: train_worker started for LNG
üêõ DEBUG [00:14:20.446]: Main received result for HACK
üêõ DEBUG: Training progress: 816/959 done
  ‚öôÔ∏è Training models for LNG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - LNG: Initiating feature extraction for training.
  [DIAGNOSTIC] LNG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ LNG: rows after features available: 126
üéØ LNG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] LNG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö LNG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ LNG: Training LSTM (50 epochs)...
      ‚è≥ LNG LSTM: Epoch 10/50 (20%)
      ‚è≥ LNG LSTM: Epoch 20/50 (40%)
      ‚è≥ LNG LSTM: Epoch 30/50 (60%)
      ‚è≥ LNG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.115495
         RMSE: 0.339846
         R¬≤ Score: -0.5792 (Poor - 57.9% variance explained)
      üîπ LNG: Training TCN (50 epochs)...
      ‚è≥ LNG TCN: Epoch 10/50 (20%)
      ‚è≥ LNG TCN: Epoch 20/50 (40%)
      ‚è≥ LNG TCN: Epoch 30/50 (60%)
      ‚è≥ LNG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.073934
         RMSE: 0.271909
         R¬≤ Score: -0.0109
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä LNG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ LNG Random Forest: Starting GridSearchCV fit...
       ‚úÖ FINX XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.4511 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.3s
    - LSTM: MSE=0.3446
    - TCN: MSE=0.5461
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3446
        ‚Ä¢ TCN: MSE=0.5461
        ‚Ä¢ XGBoost: MSE=10.4511
        ‚Ä¢ Random Forest: MSE=11.4553
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6164
   ‚úÖ FINX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FINX (TargetReturn): LSTM with MSE=0.3446
üêõ DEBUG: FINX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FINX.
üêõ DEBUG: FINX - Moving model to CPU before return...
üêõ DEBUG [00:14:24.390]: FINX - Returning result metadata...
üêõ DEBUG: train_worker started for TARS
üêõ DEBUG [00:14:24.391]: Main received result for FINX
  ‚öôÔ∏è Training models for TARS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - TARS: Initiating feature extraction for training.
  [DIAGNOSTIC] TARS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TARS: rows after features available: 126
üéØ TARS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TARS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TARS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TARS: Training LSTM (50 epochs)...
      ‚è≥ TARS LSTM: Epoch 10/50 (20%)
      ‚è≥ TARS LSTM: Epoch 20/50 (40%)
      ‚è≥ TARS LSTM: Epoch 30/50 (60%)
       ‚úÖ LNG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6726 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ LNG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TARS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.100117
         RMSE: 0.316412
         R¬≤ Score: -0.4539 (Poor - 45.4% variance explained)
      üîπ TARS: Training TCN (50 epochs)...
      ‚è≥ TARS TCN: Epoch 10/50 (20%)
      ‚è≥ TARS TCN: Epoch 20/50 (40%)
       ‚úÖ LNG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.6365 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ LNG XGBoost: Starting GridSearchCV fit...
      ‚è≥ TARS TCN: Epoch 30/50 (60%)
      ‚è≥ TARS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.072114
         RMSE: 0.268541
         R¬≤ Score: -0.0473
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TARS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TARS Random Forest: Starting GridSearchCV fit...
       ‚úÖ TARS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.7009 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TARS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TARS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.5931 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TARS XGBoost: Starting GridSearchCV fit...
       ‚úÖ IAU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.5996 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.3334
    - TCN: MSE=0.3227
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3227
        ‚Ä¢ LSTM: MSE=0.3334
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.8127
        ‚Ä¢ Random Forest: MSE=5.8050
        ‚Ä¢ XGBoost: MSE=8.5996
   ‚úÖ IAU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IAU (TargetReturn): TCN with MSE=0.3227
üêõ DEBUG: IAU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IAU.
üêõ DEBUG: IAU - Moving model to CPU before return...
üêõ DEBUG [00:14:44.499]: IAU - Returning result metadata...
üêõ DEBUG [00:14:44.502]: Main received result for IAU
üêõ DEBUG: train_worker started for FER
  ‚öôÔ∏è Training models for FER (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FER: Initiating feature extraction for training.
  [DIAGNOSTIC] FER: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FER: rows after features available: 126
üéØ FER: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FER: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FER: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FER: Training LSTM (50 epochs)...
      ‚è≥ FER LSTM: Epoch 10/50 (20%)
      ‚è≥ FER LSTM: Epoch 20/50 (40%)
      ‚è≥ FER LSTM: Epoch 30/50 (60%)
      ‚è≥ FER LSTM: Epoch 40/50 (80%)
       ‚úÖ ASTE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=14.6522 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.1s
    - LSTM: MSE=0.2076
    - TCN: MSE=0.1094
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1094
        ‚Ä¢ LSTM: MSE=0.2076
        ‚Ä¢ XGBoost: MSE=14.6522
        ‚Ä¢ Random Forest: MSE=17.5761
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.9517
   ‚úÖ ASTE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ASTE (TargetReturn): TCN with MSE=0.1094
üêõ DEBUG: ASTE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ASTE.
üêõ DEBUG: ASTE - Moving model to CPU before return...
üêõ DEBUG [00:14:46.815]: ASTE - Returning result metadata...
üêõ DEBUG [00:14:46.816]: Main received result for ASTE
üêõ DEBUG: train_worker started for KCE
  ‚öôÔ∏è Training models for KCE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - KCE: Initiating feature extraction for training.
  [DIAGNOSTIC] KCE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KCE: rows after features available: 126
üéØ KCE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KCE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KCE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KCE: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.117993
         RMSE: 0.343501
         R¬≤ Score: -0.7547 (Poor - 75.5% variance explained)
      üîπ FER: Training TCN (50 epochs)...
      ‚è≥ FER TCN: Epoch 10/50 (20%)
      ‚è≥ FER TCN: Epoch 20/50 (40%)
      ‚è≥ KCE LSTM: Epoch 10/50 (20%)
      ‚è≥ FER TCN: Epoch 30/50 (60%)
      ‚è≥ FER TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.074918
         RMSE: 0.273712
         R¬≤ Score: -0.1141
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FER: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FER Random Forest: Starting GridSearchCV fit...
      ‚è≥ KCE LSTM: Epoch 20/50 (40%)
      ‚è≥ KCE LSTM: Epoch 30/50 (60%)
      ‚è≥ KCE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.501886
         RMSE: 0.708439
         R¬≤ Score: -0.7544 (Poor - 75.4% variance explained)
      üîπ KCE: Training TCN (50 epochs)...
      ‚è≥ KCE TCN: Epoch 10/50 (20%)
      ‚è≥ KCE TCN: Epoch 20/50 (40%)
      ‚è≥ KCE TCN: Epoch 30/50 (60%)
      ‚è≥ KCE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.482301
         RMSE: 0.694479
         R¬≤ Score: -0.6860
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KCE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KCE Random Forest: Starting GridSearchCV fit...
       ‚úÖ FER Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.5501 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FER LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CLSK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=68.2489 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.2652
    - TCN: MSE=0.5639
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2652
        ‚Ä¢ TCN: MSE=0.5639
        ‚Ä¢ LightGBM Regressor (CPU): MSE=64.8842
        ‚Ä¢ Random Forest: MSE=66.9565
        ‚Ä¢ XGBoost: MSE=68.2489
   ‚úÖ CLSK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CLSK (TargetReturn): LSTM with MSE=0.2652
üêõ DEBUG: CLSK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CLSK.
üêõ DEBUG: CLSK - Moving model to CPU before return...
üêõ DEBUG [00:14:50.525]: CLSK - Returning result metadata...
üêõ DEBUG: train_worker started for SOCL
üêõ DEBUG [00:14:50.527]: Main received result for CLSK
üêõ DEBUG: Training progress: 820/959 done
  ‚öôÔ∏è Training models for SOCL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SOCL: Initiating feature extraction for training.
  [DIAGNOSTIC] SOCL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SOCL: rows after features available: 126
üéØ SOCL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SOCL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SOCL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SOCL: Training LSTM (50 epochs)...
      ‚è≥ SOCL LSTM: Epoch 10/50 (20%)
       ‚úÖ FER LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.9184 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FER XGBoost: Starting GridSearchCV fit...
      ‚è≥ SOCL LSTM: Epoch 20/50 (40%)
      ‚è≥ SOCL LSTM: Epoch 30/50 (60%)
      ‚è≥ SOCL LSTM: Epoch 40/50 (80%)
       ‚úÖ KCE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9618 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KCE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.732453
         RMSE: 0.855835
         R¬≤ Score: -1.0782 (Poor - 107.8% variance explained)
      üîπ SOCL: Training TCN (50 epochs)...
      ‚è≥ SOCL TCN: Epoch 10/50 (20%)
      ‚è≥ SOCL TCN: Epoch 20/50 (40%)
      ‚è≥ SOCL TCN: Epoch 30/50 (60%)
      ‚è≥ SOCL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.738682
         RMSE: 0.859466
         R¬≤ Score: -1.0959
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SOCL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SOCL Random Forest: Starting GridSearchCV fit...
       ‚úÖ KCE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.7885 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KCE XGBoost: Starting GridSearchCV fit...
       ‚úÖ SOCL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.9057 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SOCL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SOCL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.8837 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SOCL XGBoost: Starting GridSearchCV fit...
       ‚úÖ UROY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=51.7969 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.2s
    - LSTM: MSE=0.5238
    - TCN: MSE=0.3988
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3988
        ‚Ä¢ LSTM: MSE=0.5238
        ‚Ä¢ Random Forest: MSE=35.6524
        ‚Ä¢ LightGBM Regressor (CPU): MSE=37.3332
        ‚Ä¢ XGBoost: MSE=51.7969
   ‚úÖ UROY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UROY (TargetReturn): TCN with MSE=0.3988
üêõ DEBUG: UROY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UROY.
üêõ DEBUG: UROY - Moving model to CPU before return...
üêõ DEBUG [00:14:57.505]: UROY - Returning result metadata...
üêõ DEBUG: train_worker started for IGV
üêõ DEBUG [00:14:57.506]: Main received result for UROY
  ‚öôÔ∏è Training models for IGV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - IGV: Initiating feature extraction for training.
  [DIAGNOSTIC] IGV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IGV: rows after features available: 126
üéØ IGV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IGV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IGV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IGV: Training LSTM (50 epochs)...
      ‚è≥ IGV LSTM: Epoch 10/50 (20%)
      ‚è≥ IGV LSTM: Epoch 20/50 (40%)
      ‚è≥ IGV LSTM: Epoch 30/50 (60%)
      ‚è≥ IGV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.428762
         RMSE: 0.654799
         R¬≤ Score: -1.0132 (Poor - 101.3% variance explained)
      üîπ IGV: Training TCN (50 epochs)...
      ‚è≥ IGV TCN: Epoch 10/50 (20%)
      ‚è≥ IGV TCN: Epoch 20/50 (40%)
      ‚è≥ IGV TCN: Epoch 30/50 (60%)
      ‚è≥ IGV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.348572
         RMSE: 0.590400
         R¬≤ Score: -0.6366
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IGV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IGV Random Forest: Starting GridSearchCV fit...
       ‚úÖ IGV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.6148 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IGV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IGV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.3140 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IGV XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARGT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=7.8888 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.1720
    - TCN: MSE=0.0968
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0968
        ‚Ä¢ LSTM: MSE=0.1720
        ‚Ä¢ XGBoost: MSE=7.8888
        ‚Ä¢ Random Forest: MSE=8.9379
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.0219
   ‚úÖ ARGT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARGT (TargetReturn): TCN with MSE=0.0968
üêõ DEBUG: ARGT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARGT.
üêõ DEBUG: ARGT - Moving model to CPU before return...
üêõ DEBUG [00:15:08.209]: ARGT - Returning result metadata...
üêõ DEBUG [00:15:08.209]: Main received result for ARGT
üêõ DEBUG: train_worker started for TBT
  ‚öôÔ∏è Training models for TBT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TBT: Initiating feature extraction for training.
  [DIAGNOSTIC] TBT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TBT: rows after features available: 126
üéØ TBT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TBT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TBT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TBT: Training LSTM (50 epochs)...
      ‚è≥ TBT LSTM: Epoch 10/50 (20%)
      ‚è≥ TBT LSTM: Epoch 20/50 (40%)
      ‚è≥ TBT LSTM: Epoch 30/50 (60%)
      ‚è≥ TBT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.249403
         RMSE: 0.499402
         R¬≤ Score: -1.4810 (Poor - 148.1% variance explained)
      üîπ TBT: Training TCN (50 epochs)...
      ‚è≥ TBT TCN: Epoch 10/50 (20%)
      ‚è≥ TBT TCN: Epoch 20/50 (40%)
      ‚è≥ TBT TCN: Epoch 30/50 (60%)
      ‚è≥ TBT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.102700
         RMSE: 0.320469
         R¬≤ Score: -0.0216
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TBT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TBT Random Forest: Starting GridSearchCV fit...
       ‚úÖ TBT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.0592 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TBT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TBT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.7781 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TBT XGBoost: Starting GridSearchCV fit...
       ‚úÖ GLD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.9774 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.3345
    - TCN: MSE=0.2922
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2922
        ‚Ä¢ LSTM: MSE=0.3345
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.2692
        ‚Ä¢ Random Forest: MSE=5.8434
        ‚Ä¢ XGBoost: MSE=7.9774
   ‚úÖ GLD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GLD (TargetReturn): TCN with MSE=0.2922
üêõ DEBUG: GLD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GLD.
üêõ DEBUG: GLD - Moving model to CPU before return...
üêõ DEBUG [00:15:23.006]: GLD - Returning result metadata...
üêõ DEBUG [00:15:23.006]: Main received result for GLD
üêõ DEBUG: train_worker started for NTNX
  ‚öôÔ∏è Training models for NTNX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - NTNX: Initiating feature extraction for training.
  [DIAGNOSTIC] NTNX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTNX: rows after features available: 126
üéØ NTNX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTNX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTNX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTNX: Training LSTM (50 epochs)...
       ‚úÖ CPER XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.0857 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.4942
    - TCN: MSE=0.2928
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2928
        ‚Ä¢ LSTM: MSE=0.4942
        ‚Ä¢ Random Forest: MSE=9.3639
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.4130
        ‚Ä¢ XGBoost: MSE=10.0857
   ‚úÖ CPER: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CPER (TargetReturn): TCN with MSE=0.2928
üêõ DEBUG: CPER - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CPER.
üêõ DEBUG: CPER - Moving model to CPU before return...
üêõ DEBUG [00:15:23.189]: CPER - Returning result metadata...
üêõ DEBUG [00:15:23.190]: Main received result for CPER
üêõ DEBUG: Training progress: 824/959 done
üêõ DEBUG: train_worker started for IETC
  ‚öôÔ∏è Training models for IETC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - IETC: Initiating feature extraction for training.
  [DIAGNOSTIC] IETC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IETC: rows after features available: 126
üéØ IETC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IETC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IETC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IETC: Training LSTM (50 epochs)...
      ‚è≥ NTNX LSTM: Epoch 10/50 (20%)
      ‚è≥ IETC LSTM: Epoch 10/50 (20%)
      ‚è≥ NTNX LSTM: Epoch 20/50 (40%)
      ‚è≥ IETC LSTM: Epoch 20/50 (40%)
      ‚è≥ NTNX LSTM: Epoch 30/50 (60%)
      ‚è≥ IETC LSTM: Epoch 30/50 (60%)
      ‚è≥ NTNX LSTM: Epoch 40/50 (80%)
      ‚è≥ IETC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.262819
         RMSE: 0.512658
         R¬≤ Score: -0.3071 (Poor - 30.7% variance explained)
      üîπ NTNX: Training TCN (50 epochs)...
      ‚è≥ NTNX TCN: Epoch 10/50 (20%)
      ‚è≥ NTNX TCN: Epoch 20/50 (40%)
      ‚è≥ NTNX TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.312081
         RMSE: 0.558642
         R¬≤ Score: -0.2167 (Poor - 21.7% variance explained)
      üîπ IETC: Training TCN (50 epochs)...
      ‚è≥ NTNX TCN: Epoch 40/50 (80%)
      ‚è≥ IETC TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.204798
         RMSE: 0.452546
         R¬≤ Score: -0.0185
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTNX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTNX Random Forest: Starting GridSearchCV fit...
      ‚è≥ IETC TCN: Epoch 20/50 (40%)
      ‚è≥ IETC TCN: Epoch 30/50 (60%)
      ‚è≥ IETC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.508664
         RMSE: 0.713207
         R¬≤ Score: -0.9832
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IETC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IETC Random Forest: Starting GridSearchCV fit...
       ‚úÖ NTNX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=25.2063 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTNX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IETC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.0134 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IETC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IETC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=15.4250 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IETC XGBoost: Starting GridSearchCV fit...
       ‚úÖ NTNX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=24.1649 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTNX XGBoost: Starting GridSearchCV fit...
       ‚úÖ CIBR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.1656 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.1s
    - LSTM: MSE=0.2801
    - TCN: MSE=0.3942
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.2801
        ‚Ä¢ TCN: MSE=0.3942
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.2277
        ‚Ä¢ Random Forest: MSE=8.1508
        ‚Ä¢ XGBoost: MSE=9.1656
   ‚úÖ CIBR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CIBR (TargetReturn): LSTM with MSE=0.2801
üêõ DEBUG: CIBR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CIBR.
üêõ DEBUG: CIBR - Moving model to CPU before return...
üêõ DEBUG [00:15:35.543]: CIBR - Returning result metadata...
üêõ DEBUG [00:15:35.544]: Main received result for CIBR
üêõ DEBUG: train_worker started for ARTY
  ‚öôÔ∏è Training models for ARTY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - ARTY: Initiating feature extraction for training.
  [DIAGNOSTIC] ARTY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ARTY: rows after features available: 126
üéØ ARTY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ARTY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ARTY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ARTY: Training LSTM (50 epochs)...
      ‚è≥ ARTY LSTM: Epoch 10/50 (20%)
      ‚è≥ ARTY LSTM: Epoch 20/50 (40%)
      ‚è≥ ARTY LSTM: Epoch 30/50 (60%)
      ‚è≥ ARTY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.501945
         RMSE: 0.708481
         R¬≤ Score: -0.6385 (Poor - 63.8% variance explained)
      üîπ ARTY: Training TCN (50 epochs)...
      ‚è≥ ARTY TCN: Epoch 10/50 (20%)
      ‚è≥ ARTY TCN: Epoch 20/50 (40%)
      ‚è≥ ARTY TCN: Epoch 30/50 (60%)
      ‚è≥ ARTY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.671025
         RMSE: 0.819161
         R¬≤ Score: -1.1904
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ARTY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ARTY Random Forest: Starting GridSearchCV fit...
       ‚úÖ ARTY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.2135 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ARTY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ARTY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.3534 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ARTY XGBoost: Starting GridSearchCV fit...
       ‚úÖ DCTH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=52.0793 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.2036
    - TCN: MSE=0.1146
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1146
        ‚Ä¢ LSTM: MSE=0.2036
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.8405
        ‚Ä¢ Random Forest: MSE=44.8010
        ‚Ä¢ XGBoost: MSE=52.0793
   ‚úÖ DCTH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DCTH (TargetReturn): TCN with MSE=0.1146
üêõ DEBUG: DCTH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DCTH.
üêõ DEBUG: DCTH - Moving model to CPU before return...
üêõ DEBUG [00:15:44.494]: DCTH - Returning result metadata...
üêõ DEBUG: train_worker started for KALV
üêõ DEBUG [00:15:44.495]: Main received result for DCTH
       ‚úÖ BJ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.9079 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.3853
    - TCN: MSE=0.2724
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2724
        ‚Ä¢ LSTM: MSE=0.3853
        ‚Ä¢ Random Forest: MSE=8.2839
        ‚Ä¢ XGBoost: MSE=9.9079
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.9374
   ‚úÖ BJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BJ (TargetReturn): TCN with MSE=0.2724
üêõ DEBUG: BJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BJ.
üêõ DEBUG: BJ - Moving model to CPU before return...
üêõ DEBUG [00:15:44.508]: BJ - Returning result metadata...
üêõ DEBUG: train_worker started for CTVA
üêõ DEBUG [00:15:44.509]: Main received result for BJ
  ‚öôÔ∏è Training models for KALV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - KALV: Initiating feature extraction for training.
  [DIAGNOSTIC] KALV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KALV: rows after features available: 126
üéØ KALV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KALV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KALV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KALV: Training LSTM (50 epochs)...
  ‚öôÔ∏è Training models for CTVA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - CTVA: Initiating feature extraction for training.
  [DIAGNOSTIC] CTVA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CTVA: rows after features available: 126
üéØ CTVA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CTVA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CTVA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CTVA: Training LSTM (50 epochs)...
      ‚è≥ CTVA LSTM: Epoch 10/50 (20%)
      ‚è≥ KALV LSTM: Epoch 10/50 (20%)
      ‚è≥ CTVA LSTM: Epoch 20/50 (40%)
      ‚è≥ KALV LSTM: Epoch 20/50 (40%)
      ‚è≥ CTVA LSTM: Epoch 30/50 (60%)
      ‚è≥ KALV LSTM: Epoch 30/50 (60%)
      ‚è≥ CTVA LSTM: Epoch 40/50 (80%)
      ‚è≥ KALV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.327716
         RMSE: 0.572465
         R¬≤ Score: -0.5051 (Poor - 50.5% variance explained)
      üîπ CTVA: Training TCN (50 epochs)...
      ‚è≥ CTVA TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.315895
         RMSE: 0.562045
         R¬≤ Score: -0.9573 (Poor - 95.7% variance explained)
      üîπ KALV: Training TCN (50 epochs)...
      ‚è≥ CTVA TCN: Epoch 20/50 (40%)
      ‚è≥ KALV TCN: Epoch 10/50 (20%)
      ‚è≥ CTVA TCN: Epoch 30/50 (60%)
      ‚è≥ KALV TCN: Epoch 20/50 (40%)
      ‚è≥ KALV TCN: Epoch 30/50 (60%)
      ‚è≥ CTVA TCN: Epoch 40/50 (80%)
      ‚è≥ KALV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.366075
         RMSE: 0.605041
         R¬≤ Score: -0.6813
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CTVA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CTVA Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.162672
         RMSE: 0.403326
         R¬≤ Score: -0.0079
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KALV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KALV Random Forest: Starting GridSearchCV fit...
       ‚úÖ CTVA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.4181 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CTVA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ KALV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=45.8616 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KALV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CTVA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.2307 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CTVA XGBoost: Starting GridSearchCV fit...
       ‚úÖ KALV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=50.1817 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KALV XGBoost: Starting GridSearchCV fit...
       ‚úÖ FITE XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.0864 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.2s
    - LSTM: MSE=0.4870
    - TCN: MSE=0.5022
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4870
        ‚Ä¢ TCN: MSE=0.5022
        ‚Ä¢ XGBoost: MSE=6.0864
        ‚Ä¢ Random Forest: MSE=6.9362
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0464
   ‚úÖ FITE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FITE (TargetReturn): LSTM with MSE=0.4870
üêõ DEBUG: FITE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FITE.
üêõ DEBUG: FITE - Moving model to CPU before return...
üêõ DEBUG [00:15:56.396]: FITE - Returning result metadata...
üêõ DEBUG: train_worker started for SPWH
üêõ DEBUG [00:15:56.397]: Main received result for FITE
üêõ DEBUG: Training progress: 828/959 done
  ‚öôÔ∏è Training models for SPWH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - SPWH: Initiating feature extraction for training.
  [DIAGNOSTIC] SPWH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPWH: rows after features available: 126
üéØ SPWH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPWH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPWH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPWH: Training LSTM (50 epochs)...
      ‚è≥ SPWH LSTM: Epoch 10/50 (20%)
      ‚è≥ SPWH LSTM: Epoch 20/50 (40%)
      ‚è≥ SPWH LSTM: Epoch 30/50 (60%)
      ‚è≥ SPWH LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.342618
         RMSE: 0.585336
         R¬≤ Score: -0.3747 (Poor - 37.5% variance explained)
      üîπ SPWH: Training TCN (50 epochs)...
      ‚è≥ SPWH TCN: Epoch 10/50 (20%)
      ‚è≥ SPWH TCN: Epoch 20/50 (40%)
      ‚è≥ SPWH TCN: Epoch 30/50 (60%)
      ‚è≥ SPWH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.375598
         RMSE: 0.612861
         R¬≤ Score: -0.5071
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPWH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPWH Random Forest: Starting GridSearchCV fit...
       ‚úÖ ZG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.7445 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}) | Time: 115.0s
    - LSTM: MSE=0.2703
    - TCN: MSE=0.2198
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2198
        ‚Ä¢ LSTM: MSE=0.2703
        ‚Ä¢ Random Forest: MSE=11.5095
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.2508
        ‚Ä¢ XGBoost: MSE=18.7445
   ‚úÖ ZG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ZG (TargetReturn): TCN with MSE=0.2198
üêõ DEBUG: ZG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ZG.
üêõ DEBUG: ZG - Moving model to CPU before return...
üêõ DEBUG [00:16:00.926]: ZG - Returning result metadata...
üêõ DEBUG: train_worker started for SIXG
  ‚öôÔ∏è Training models for SIXG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - SIXG: Initiating feature extraction for training.
  [DIAGNOSTIC] SIXG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SIXG: rows after features available: 126
üéØ SIXG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SIXG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SIXG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SIXG: Training LSTM (50 epochs)...
      ‚è≥ SIXG LSTM: Epoch 10/50 (20%)
      ‚è≥ SIXG LSTM: Epoch 20/50 (40%)
      ‚è≥ SIXG LSTM: Epoch 30/50 (60%)
       ‚úÖ SPWH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=183.6109 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPWH LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SIXG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.679534
         RMSE: 0.824339
         R¬≤ Score: -1.3842 (Poor - 138.4% variance explained)
      üîπ SIXG: Training TCN (50 epochs)...
       ‚úÖ SPWH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=445.6410 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPWH XGBoost: Starting GridSearchCV fit...
      ‚è≥ SIXG TCN: Epoch 10/50 (20%)
      ‚è≥ SIXG TCN: Epoch 20/50 (40%)
      ‚è≥ SIXG TCN: Epoch 30/50 (60%)
      ‚è≥ SIXG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.408701
         RMSE: 0.639297
         R¬≤ Score: -0.4340
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SIXG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SIXG Random Forest: Starting GridSearchCV fit...
       ‚úÖ ENVA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.2330 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.6352
    - TCN: MSE=0.3436
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3436
        ‚Ä¢ LSTM: MSE=0.6352
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.2964
        ‚Ä¢ Random Forest: MSE=11.9267
        ‚Ä¢ XGBoost: MSE=12.2330
   ‚úÖ ENVA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ENVA (TargetReturn): TCN with MSE=0.3436
üêõ DEBUG: ENVA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ENVA.
üêõ DEBUG: ENVA - Moving model to CPU before return...
üêõ DEBUG [00:16:05.572]: ENVA - Returning result metadata...
üêõ DEBUG: train_worker started for OIS
üêõ DEBUG [00:16:05.573]: Main received result for ENVA
üêõ DEBUG [00:16:05.573]: Main received result for ZG
  ‚öôÔ∏è Training models for OIS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - OIS: Initiating feature extraction for training.
  [DIAGNOSTIC] OIS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OIS: rows after features available: 126
üéØ OIS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OIS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OIS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OIS: Training LSTM (50 epochs)...
      ‚è≥ OIS LSTM: Epoch 10/50 (20%)
       ‚úÖ SIXG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.5834 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SIXG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ OIS LSTM: Epoch 20/50 (40%)
      ‚è≥ OIS LSTM: Epoch 30/50 (60%)
       ‚úÖ SIXG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.6658 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SIXG XGBoost: Starting GridSearchCV fit...
      ‚è≥ OIS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.576243
         RMSE: 0.759107
         R¬≤ Score: -0.6950 (Poor - 69.5% variance explained)
      üîπ OIS: Training TCN (50 epochs)...
      ‚è≥ OIS TCN: Epoch 10/50 (20%)
      ‚è≥ OIS TCN: Epoch 20/50 (40%)
      ‚è≥ OIS TCN: Epoch 30/50 (60%)
      ‚è≥ OIS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.381810
         RMSE: 0.617908
         R¬≤ Score: -0.1231
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OIS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OIS Random Forest: Starting GridSearchCV fit...
       ‚úÖ OIS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=66.6541 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OIS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OIS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=92.9806 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OIS XGBoost: Starting GridSearchCV fit...
       ‚úÖ LNG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.3110 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.1155
    - TCN: MSE=0.0739
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0739
        ‚Ä¢ LSTM: MSE=0.1155
        ‚Ä¢ XGBoost: MSE=11.3110
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6365
        ‚Ä¢ Random Forest: MSE=11.6726
   ‚úÖ LNG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for LNG (TargetReturn): TCN with MSE=0.0739
üêõ DEBUG: LNG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for LNG.
üêõ DEBUG: LNG - Moving model to CPU before return...
üêõ DEBUG [00:16:25.349]: LNG - Returning result metadata...
üêõ DEBUG: train_worker started for ESAB
üêõ DEBUG [00:16:25.353]: Main received result for LNG
  ‚öôÔ∏è Training models for ESAB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - ESAB: Initiating feature extraction for training.
  [DIAGNOSTIC] ESAB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ESAB: rows after features available: 126
üéØ ESAB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ESAB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ESAB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ESAB: Training LSTM (50 epochs)...
      ‚è≥ ESAB LSTM: Epoch 10/50 (20%)
      ‚è≥ ESAB LSTM: Epoch 20/50 (40%)
      ‚è≥ ESAB LSTM: Epoch 30/50 (60%)
       ‚úÖ TARS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.5094 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.9s
    - LSTM: MSE=0.1001
    - TCN: MSE=0.0721
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0721
        ‚Ä¢ LSTM: MSE=0.1001
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.5931
        ‚Ä¢ Random Forest: MSE=16.7009
        ‚Ä¢ XGBoost: MSE=23.5094
   ‚úÖ TARS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TARS (TargetReturn): TCN with MSE=0.0721
üêõ DEBUG: TARS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TARS.
üêõ DEBUG: TARS - Moving model to CPU before return...
üêõ DEBUG [00:16:27.340]: TARS - Returning result metadata...
üêõ DEBUG [00:16:27.341]: Main received result for TARS
üêõ DEBUG: train_worker started for IYG
üêõ DEBUG: Training progress: 832/959 done
      ‚è≥ ESAB LSTM: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for IYG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - IYG: Initiating feature extraction for training.
  [DIAGNOSTIC] IYG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IYG: rows after features available: 126
üéØ IYG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IYG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IYG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IYG: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.189185
         RMSE: 0.434954
         R¬≤ Score: -0.1696 (Poor - 17.0% variance explained)
      üîπ ESAB: Training TCN (50 epochs)...
      ‚è≥ IYG LSTM: Epoch 10/50 (20%)
      ‚è≥ ESAB TCN: Epoch 10/50 (20%)
      ‚è≥ ESAB TCN: Epoch 20/50 (40%)
      ‚è≥ ESAB TCN: Epoch 30/50 (60%)
      ‚è≥ IYG LSTM: Epoch 20/50 (40%)
      ‚è≥ ESAB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.240815
         RMSE: 0.490729
         R¬≤ Score: -0.4888
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ESAB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ESAB Random Forest: Starting GridSearchCV fit...
      ‚è≥ IYG LSTM: Epoch 30/50 (60%)
      ‚è≥ IYG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.618070
         RMSE: 0.786174
         R¬≤ Score: -1.6322 (Poor - 163.2% variance explained)
      üîπ IYG: Training TCN (50 epochs)...
      ‚è≥ IYG TCN: Epoch 10/50 (20%)
      ‚è≥ IYG TCN: Epoch 20/50 (40%)
      ‚è≥ IYG TCN: Epoch 30/50 (60%)
      ‚è≥ IYG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.247199
         RMSE: 0.497191
         R¬≤ Score: -0.0528
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IYG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IYG Random Forest: Starting GridSearchCV fit...
       ‚úÖ ESAB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.9899 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ESAB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ESAB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.4615 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ESAB XGBoost: Starting GridSearchCV fit...
       ‚úÖ IYG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.3504 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IYG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IYG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.9644 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.2s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IYG XGBoost: Starting GridSearchCV fit...
       ‚úÖ FER XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.0096 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.9s
    - LSTM: MSE=0.1180
    - TCN: MSE=0.0749
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0749
        ‚Ä¢ LSTM: MSE=0.1180
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.9184
        ‚Ä¢ Random Forest: MSE=7.5501
        ‚Ä¢ XGBoost: MSE=8.0096
   ‚úÖ FER: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FER (TargetReturn): TCN with MSE=0.0749
üêõ DEBUG: FER - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FER.
üêõ DEBUG: FER - Moving model to CPU before return...
üêõ DEBUG [00:16:52.040]: FER - Returning result metadata...
üêõ DEBUG [00:16:52.040]: Main received result for FER
üêõ DEBUG: train_worker started for TS
  ‚öôÔ∏è Training models for TS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TS: Initiating feature extraction for training.
  [DIAGNOSTIC] TS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TS: rows after features available: 126
üéØ TS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TS: Training LSTM (50 epochs)...
      ‚è≥ TS LSTM: Epoch 10/50 (20%)
      ‚è≥ TS LSTM: Epoch 20/50 (40%)
      ‚è≥ TS LSTM: Epoch 30/50 (60%)
      ‚è≥ TS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.459331
         RMSE: 0.677740
         R¬≤ Score: -0.6392 (Poor - 63.9% variance explained)
      üîπ TS: Training TCN (50 epochs)...
      ‚è≥ TS TCN: Epoch 10/50 (20%)
      ‚è≥ TS TCN: Epoch 20/50 (40%)
      ‚è≥ TS TCN: Epoch 30/50 (60%)
      ‚è≥ TS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.478528
         RMSE: 0.691757
         R¬≤ Score: -0.7078
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TS Random Forest: Starting GridSearchCV fit...
       ‚úÖ KCE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.1864 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.2s
    - LSTM: MSE=0.5019
    - TCN: MSE=0.4823
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4823
        ‚Ä¢ LSTM: MSE=0.5019
        ‚Ä¢ Random Forest: MSE=10.9618
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.7885
        ‚Ä¢ XGBoost: MSE=17.1864
   ‚úÖ KCE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KCE (TargetReturn): TCN with MSE=0.4823
üêõ DEBUG: KCE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KCE.
üêõ DEBUG: KCE - Moving model to CPU before return...
üêõ DEBUG [00:16:54.771]: KCE - Returning result metadata...
üêõ DEBUG: train_worker started for PHYS
üêõ DEBUG [00:16:54.772]: Main received result for KCE
  ‚öôÔ∏è Training models for PHYS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - PHYS: Initiating feature extraction for training.
  [DIAGNOSTIC] PHYS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PHYS: rows after features available: 126
üéØ PHYS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PHYS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PHYS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PHYS: Training LSTM (50 epochs)...
      ‚è≥ PHYS LSTM: Epoch 10/50 (20%)
      ‚è≥ PHYS LSTM: Epoch 20/50 (40%)
       ‚úÖ SOCL XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=7.5760 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.7325
    - TCN: MSE=0.7387
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7325
        ‚Ä¢ TCN: MSE=0.7387
        ‚Ä¢ XGBoost: MSE=7.5760
        ‚Ä¢ Random Forest: MSE=8.9057
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.8837
   ‚úÖ SOCL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SOCL (TargetReturn): LSTM with MSE=0.7325
üêõ DEBUG: SOCL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SOCL.
üêõ DEBUG: SOCL - Moving model to CPU before return...
üêõ DEBUG [00:16:56.078]: SOCL - Returning result metadata...
üêõ DEBUG: train_worker started for UPWK
üêõ DEBUG [00:16:56.079]: Main received result for SOCL
  ‚öôÔ∏è Training models for UPWK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - UPWK: Initiating feature extraction for training.
  [DIAGNOSTIC] UPWK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UPWK: rows after features available: 126
üéØ UPWK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UPWK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UPWK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UPWK: Training LSTM (50 epochs)...
      ‚è≥ PHYS LSTM: Epoch 30/50 (60%)
      ‚è≥ UPWK LSTM: Epoch 10/50 (20%)
      ‚è≥ PHYS LSTM: Epoch 40/50 (80%)
      ‚è≥ UPWK LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.300530
         RMSE: 0.548206
         R¬≤ Score: -0.7964 (Poor - 79.6% variance explained)
      üîπ PHYS: Training TCN (50 epochs)...
      ‚è≥ PHYS TCN: Epoch 10/50 (20%)
      ‚è≥ PHYS TCN: Epoch 20/50 (40%)
      ‚è≥ UPWK LSTM: Epoch 30/50 (60%)
       ‚úÖ TS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.0566 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ PHYS TCN: Epoch 30/50 (60%)
      ‚è≥ PHYS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.302528
         RMSE: 0.550025
         R¬≤ Score: -0.8084
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PHYS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PHYS Random Forest: Starting GridSearchCV fit...
      ‚è≥ UPWK LSTM: Epoch 40/50 (80%)
       ‚úÖ TS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.9113 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TS XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.157573
         RMSE: 0.396955
         R¬≤ Score: -0.5433 (Poor - 54.3% variance explained)
      üîπ UPWK: Training TCN (50 epochs)...
      ‚è≥ UPWK TCN: Epoch 10/50 (20%)
      ‚è≥ UPWK TCN: Epoch 20/50 (40%)
      ‚è≥ UPWK TCN: Epoch 30/50 (60%)
      ‚è≥ UPWK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.117762
         RMSE: 0.343165
         R¬≤ Score: -0.1534
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UPWK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UPWK Random Forest: Starting GridSearchCV fit...
       ‚úÖ PHYS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.5842 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PHYS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PHYS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.8452 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PHYS XGBoost: Starting GridSearchCV fit...
       ‚úÖ UPWK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.8703 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UPWK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UPWK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=29.4744 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UPWK XGBoost: Starting GridSearchCV fit...
       ‚úÖ IGV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.9835 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.4288
    - TCN: MSE=0.3486
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3486
        ‚Ä¢ LSTM: MSE=0.4288
        ‚Ä¢ Random Forest: MSE=11.6148
        ‚Ä¢ XGBoost: MSE=11.9835
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.3140
   ‚úÖ IGV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IGV (TargetReturn): TCN with MSE=0.3486
üêõ DEBUG: IGV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IGV.
üêõ DEBUG: IGV - Moving model to CPU before return...
üêõ DEBUG [00:17:03.040]: IGV - Returning result metadata...
üêõ DEBUG: train_worker started for VMI
üêõ DEBUG [00:17:03.041]: Main received result for IGV
üêõ DEBUG: Training progress: 836/959 done
  ‚öôÔ∏è Training models for VMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - VMI: Initiating feature extraction for training.
  [DIAGNOSTIC] VMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VMI: rows after features available: 126
üéØ VMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VMI: Training LSTM (50 epochs)...
      ‚è≥ VMI LSTM: Epoch 10/50 (20%)
      ‚è≥ VMI LSTM: Epoch 20/50 (40%)
      ‚è≥ VMI LSTM: Epoch 30/50 (60%)
      ‚è≥ VMI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.337788
         RMSE: 0.581195
         R¬≤ Score: -0.2573 (Poor - 25.7% variance explained)
      üîπ VMI: Training TCN (50 epochs)...
      ‚è≥ VMI TCN: Epoch 10/50 (20%)
      ‚è≥ VMI TCN: Epoch 20/50 (40%)
      ‚è≥ VMI TCN: Epoch 30/50 (60%)
      ‚è≥ VMI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.595637
         RMSE: 0.771775
         R¬≤ Score: -1.2171
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VMI Random Forest: Starting GridSearchCV fit...
       ‚úÖ VMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.3810 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.6881 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VMI XGBoost: Starting GridSearchCV fit...
       ‚úÖ TBT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.2752 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.2494
    - TCN: MSE=0.1027
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1027
        ‚Ä¢ LSTM: MSE=0.2494
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7781
        ‚Ä¢ Random Forest: MSE=6.0592
        ‚Ä¢ XGBoost: MSE=6.2752
   ‚úÖ TBT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TBT (TargetReturn): TCN with MSE=0.1027
üêõ DEBUG: TBT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TBT.
üêõ DEBUG: TBT - Moving model to CPU before return...
üêõ DEBUG [00:17:11.927]: TBT - Returning result metadata...
üêõ DEBUG [00:17:11.927]: Main received result for TBT
üêõ DEBUG: train_worker started for YMAX
  ‚öôÔ∏è Training models for YMAX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - YMAX: Initiating feature extraction for training.
  [DIAGNOSTIC] YMAX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ YMAX: rows after features available: 126
üéØ YMAX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] YMAX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö YMAX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ YMAX: Training LSTM (50 epochs)...
      ‚è≥ YMAX LSTM: Epoch 10/50 (20%)
      ‚è≥ YMAX LSTM: Epoch 20/50 (40%)
      ‚è≥ YMAX LSTM: Epoch 30/50 (60%)
      ‚è≥ YMAX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.594466
         RMSE: 0.771016
         R¬≤ Score: -1.3888 (Poor - 138.9% variance explained)
      üîπ YMAX: Training TCN (50 epochs)...
      ‚è≥ YMAX TCN: Epoch 10/50 (20%)
      ‚è≥ YMAX TCN: Epoch 20/50 (40%)
      ‚è≥ YMAX TCN: Epoch 30/50 (60%)
      ‚è≥ YMAX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.390970
         RMSE: 0.625276
         R¬≤ Score: -0.5711
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä YMAX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ YMAX Random Forest: Starting GridSearchCV fit...
       ‚úÖ YMAX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.6465 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ YMAX LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ YMAX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.5152 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ YMAX XGBoost: Starting GridSearchCV fit...
       ‚úÖ IETC XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.1595 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.6s
    - LSTM: MSE=0.3121
    - TCN: MSE=0.5087
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3121
        ‚Ä¢ TCN: MSE=0.5087
        ‚Ä¢ XGBoost: MSE=11.1595
        ‚Ä¢ Random Forest: MSE=12.0134
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.4250
   ‚úÖ IETC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IETC (TargetReturn): LSTM with MSE=0.3121
üêõ DEBUG: IETC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IETC.
üêõ DEBUG: IETC - Moving model to CPU before return...
üêõ DEBUG [00:17:29.233]: IETC - Returning result metadata...
üêõ DEBUG: train_worker started for PRM
  ‚öôÔ∏è Training models for PRM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - PRM: Initiating feature extraction for training.
  [DIAGNOSTIC] PRM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PRM: rows after features available: 126
üéØ PRM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PRM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PRM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PRM: Training LSTM (50 epochs)...
      ‚è≥ PRM LSTM: Epoch 10/50 (20%)
       ‚úÖ NTNX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.5933 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.2628
    - TCN: MSE=0.2048
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2048
        ‚Ä¢ LSTM: MSE=0.2628
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.1649
        ‚Ä¢ Random Forest: MSE=25.2063
        ‚Ä¢ XGBoost: MSE=29.5933
   ‚úÖ NTNX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTNX (TargetReturn): TCN with MSE=0.2048
üêõ DEBUG: NTNX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTNX.
üêõ DEBUG: NTNX - Moving model to CPU before return...
üêõ DEBUG [00:17:30.225]: NTNX - Returning result metadata...
üêõ DEBUG [00:17:30.226]: Main received result for NTNX
üêõ DEBUG [00:17:30.226]: Main received result for IETC
üêõ DEBUG: train_worker started for DE
  ‚öôÔ∏è Training models for DE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - DE: Initiating feature extraction for training.
  [DIAGNOSTIC] DE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DE: rows after features available: 126
üéØ DE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DE: Training LSTM (50 epochs)...
      ‚è≥ PRM LSTM: Epoch 20/50 (40%)
      ‚è≥ DE LSTM: Epoch 10/50 (20%)
      ‚è≥ PRM LSTM: Epoch 30/50 (60%)
      ‚è≥ DE LSTM: Epoch 20/50 (40%)
      ‚è≥ PRM LSTM: Epoch 40/50 (80%)
      ‚è≥ DE LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.576458
         RMSE: 0.759248
         R¬≤ Score: -0.8863 (Poor - 88.6% variance explained)
      üîπ PRM: Training TCN (50 epochs)...
      ‚è≥ PRM TCN: Epoch 10/50 (20%)
      ‚è≥ PRM TCN: Epoch 20/50 (40%)
      ‚è≥ PRM TCN: Epoch 30/50 (60%)
      ‚è≥ DE LSTM: Epoch 40/50 (80%)
      ‚è≥ PRM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.638437
         RMSE: 0.799022
         R¬≤ Score: -1.0891
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PRM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PRM Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.184072
         RMSE: 0.429036
         R¬≤ Score: -0.5676 (Poor - 56.8% variance explained)
      üîπ DE: Training TCN (50 epochs)...
      ‚è≥ DE TCN: Epoch 10/50 (20%)
      ‚è≥ DE TCN: Epoch 20/50 (40%)
      ‚è≥ DE TCN: Epoch 30/50 (60%)
      ‚è≥ DE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.217390
         RMSE: 0.466251
         R¬≤ Score: -0.8514
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DE Random Forest: Starting GridSearchCV fit...
       ‚úÖ PRM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.2728 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PRM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PRM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=34.8831 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PRM XGBoost: Starting GridSearchCV fit...
       ‚úÖ DE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.4355 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.5545 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DE XGBoost: Starting GridSearchCV fit...
       ‚úÖ ARTY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.3865 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.5019
    - TCN: MSE=0.6710
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5019
        ‚Ä¢ TCN: MSE=0.6710
        ‚Ä¢ Random Forest: MSE=11.2135
        ‚Ä¢ XGBoost: MSE=13.3865
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.3534
   ‚úÖ ARTY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ARTY (TargetReturn): LSTM with MSE=0.5019
üêõ DEBUG: ARTY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ARTY.
üêõ DEBUG: ARTY - Moving model to CPU before return...
üêõ DEBUG [00:17:38.211]: ARTY - Returning result metadata...
üêõ DEBUG [00:17:38.212]: Main received result for ARTY
üêõ DEBUG: Training progress: 840/959 done
üêõ DEBUG: train_worker started for GLTR
  ‚öôÔ∏è Training models for GLTR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - GLTR: Initiating feature extraction for training.
  [DIAGNOSTIC] GLTR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GLTR: rows after features available: 126
üéØ GLTR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GLTR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GLTR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GLTR: Training LSTM (50 epochs)...
      ‚è≥ GLTR LSTM: Epoch 10/50 (20%)
      ‚è≥ GLTR LSTM: Epoch 20/50 (40%)
      ‚è≥ GLTR LSTM: Epoch 30/50 (60%)
      ‚è≥ GLTR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.109570
         RMSE: 0.331013
         R¬≤ Score: -0.6374 (Poor - 63.7% variance explained)
      üîπ GLTR: Training TCN (50 epochs)...
      ‚è≥ GLTR TCN: Epoch 10/50 (20%)
      ‚è≥ GLTR TCN: Epoch 20/50 (40%)
      ‚è≥ GLTR TCN: Epoch 30/50 (60%)
      ‚è≥ GLTR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.067487
         RMSE: 0.259783
         R¬≤ Score: -0.0085
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GLTR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GLTR Random Forest: Starting GridSearchCV fit...
       ‚úÖ GLTR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.5887 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GLTR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GLTR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.1620 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GLTR XGBoost: Starting GridSearchCV fit...
       ‚úÖ CTVA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.4291 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.3277
    - TCN: MSE=0.3661
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3277
        ‚Ä¢ TCN: MSE=0.3661
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.2307
        ‚Ä¢ Random Forest: MSE=12.4181
        ‚Ä¢ XGBoost: MSE=12.4291
   ‚úÖ CTVA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CTVA (TargetReturn): LSTM with MSE=0.3277
üêõ DEBUG: CTVA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CTVA.
üêõ DEBUG: CTVA - Moving model to CPU before return...
üêõ DEBUG [00:17:50.797]: CTVA - Returning result metadata...
üêõ DEBUG: train_worker started for PBW
  ‚öôÔ∏è Training models for PBW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - PBW: Initiating feature extraction for training.
  [DIAGNOSTIC] PBW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PBW: rows after features available: 126
üéØ PBW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PBW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PBW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PBW: Training LSTM (50 epochs)...
       ‚úÖ KALV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=49.3819 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.3159
    - TCN: MSE=0.1627
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1627
        ‚Ä¢ LSTM: MSE=0.3159
        ‚Ä¢ Random Forest: MSE=45.8616
        ‚Ä¢ XGBoost: MSE=49.3819
        ‚Ä¢ LightGBM Regressor (CPU): MSE=50.1817
   ‚úÖ KALV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KALV (TargetReturn): TCN with MSE=0.1627
üêõ DEBUG: KALV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KALV.
üêõ DEBUG: KALV - Moving model to CPU before return...
üêõ DEBUG [00:17:51.024]: KALV - Returning result metadata...
üêõ DEBUG [00:17:51.024]: Main received result for KALV
üêõ DEBUG [00:17:51.024]: Main received result for CTVAüêõ DEBUG: train_worker started for HLI

  ‚öôÔ∏è Training models for HLI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - HLI: Initiating feature extraction for training.
  [DIAGNOSTIC] HLI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HLI: rows after features available: 126
üéØ HLI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HLI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HLI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HLI: Training LSTM (50 epochs)...
      ‚è≥ PBW LSTM: Epoch 10/50 (20%)
      ‚è≥ HLI LSTM: Epoch 10/50 (20%)
      ‚è≥ PBW LSTM: Epoch 20/50 (40%)
      ‚è≥ HLI LSTM: Epoch 20/50 (40%)
      ‚è≥ PBW LSTM: Epoch 30/50 (60%)
      ‚è≥ HLI LSTM: Epoch 30/50 (60%)
      ‚è≥ PBW LSTM: Epoch 40/50 (80%)
      ‚è≥ HLI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.481715
         RMSE: 0.694057
         R¬≤ Score: -0.7048 (Poor - 70.5% variance explained)
      üîπ PBW: Training TCN (50 epochs)...
      ‚è≥ PBW TCN: Epoch 10/50 (20%)
      ‚è≥ PBW TCN: Epoch 20/50 (40%)
      ‚è≥ PBW TCN: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.566211
         RMSE: 0.752470
         R¬≤ Score: -1.2135 (Poor - 121.3% variance explained)
      üîπ HLI: Training TCN (50 epochs)...
      ‚è≥ PBW TCN: Epoch 40/50 (80%)
      ‚è≥ HLI TCN: Epoch 10/50 (20%)
      üìä TCN Regression Metrics:
         MSE: 0.602133
         RMSE: 0.775972
         R¬≤ Score: -1.1309
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PBW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PBW Random Forest: Starting GridSearchCV fit...
      ‚è≥ HLI TCN: Epoch 20/50 (40%)
      ‚è≥ HLI TCN: Epoch 30/50 (60%)
      ‚è≥ HLI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.358641
         RMSE: 0.598867
         R¬≤ Score: -0.4020
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HLI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HLI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PBW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2408 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PBW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HLI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.0289 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HLI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PBW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=34.9746 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PBW XGBoost: Starting GridSearchCV fit...
       ‚úÖ HLI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.3945 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HLI XGBoost: Starting GridSearchCV fit...
       ‚úÖ SPWH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=228.3206 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 115.0s
    - LSTM: MSE=0.3426
    - TCN: MSE=0.3756
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3426
        ‚Ä¢ TCN: MSE=0.3756
        ‚Ä¢ Random Forest: MSE=183.6109
        ‚Ä¢ XGBoost: MSE=228.3206
        ‚Ä¢ LightGBM Regressor (CPU): MSE=445.6410
   ‚úÖ SPWH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPWH (TargetReturn): LSTM with MSE=0.3426
üêõ DEBUG: SPWH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPWH.
üêõ DEBUG: SPWH - Moving model to CPU before return...
üêõ DEBUG [00:17:58.357]: SPWH - Returning result metadata...
üêõ DEBUG [00:17:58.358]: Main received result for SPWHüêõ DEBUG: train_worker started for PIZ

  ‚öôÔ∏è Training models for PIZ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - PIZ: Initiating feature extraction for training.
  [DIAGNOSTIC] PIZ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PIZ: rows after features available: 126
üéØ PIZ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PIZ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PIZ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PIZ: Training LSTM (50 epochs)...
      ‚è≥ PIZ LSTM: Epoch 10/50 (20%)
       ‚úÖ SIXG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=12.9080 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 112.2s
    - LSTM: MSE=0.6795
    - TCN: MSE=0.4087
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 115.6 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4087
        ‚Ä¢ LSTM: MSE=0.6795
        ‚Ä¢ XGBoost: MSE=12.9080
        ‚Ä¢ Random Forest: MSE=13.5834
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6658
   ‚úÖ SIXG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SIXG (TargetReturn): TCN with MSE=0.4087
üêõ DEBUG: SIXG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SIXG.
üêõ DEBUG: SIXG - Moving model to CPU before return...
üêõ DEBUG [00:17:59.306]: SIXG - Returning result metadata...
üêõ DEBUG: train_worker started for FXI
üêõ DEBUG [00:17:59.307]: Main received result for SIXG
üêõ DEBUG: Training progress: 844/959 done
  ‚öôÔ∏è Training models for FXI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FXI: Initiating feature extraction for training.
  [DIAGNOSTIC] FXI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FXI: rows after features available: 126
üéØ FXI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FXI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FXI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FXI: Training LSTM (50 epochs)...
      ‚è≥ PIZ LSTM: Epoch 20/50 (40%)
      ‚è≥ FXI LSTM: Epoch 10/50 (20%)
      ‚è≥ PIZ LSTM: Epoch 30/50 (60%)
      ‚è≥ FXI LSTM: Epoch 20/50 (40%)
      ‚è≥ PIZ LSTM: Epoch 40/50 (80%)
      ‚è≥ FXI LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.210537
         RMSE: 0.458843
         R¬≤ Score: -1.2850 (Poor - 128.5% variance explained)
      üîπ PIZ: Training TCN (50 epochs)...
      ‚è≥ PIZ TCN: Epoch 10/50 (20%)
      ‚è≥ PIZ TCN: Epoch 20/50 (40%)
      ‚è≥ FXI LSTM: Epoch 40/50 (80%)
      ‚è≥ PIZ TCN: Epoch 30/50 (60%)
      ‚è≥ PIZ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.154866
         RMSE: 0.393530
         R¬≤ Score: -0.6808
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PIZ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PIZ Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.372154
         RMSE: 0.610044
         R¬≤ Score: -1.0026 (Poor - 100.3% variance explained)
      üîπ FXI: Training TCN (50 epochs)...
      ‚è≥ FXI TCN: Epoch 10/50 (20%)
      ‚è≥ FXI TCN: Epoch 20/50 (40%)
      ‚è≥ FXI TCN: Epoch 30/50 (60%)
      ‚è≥ FXI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.294578
         RMSE: 0.542751
         R¬≤ Score: -0.5852
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FXI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FXI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PIZ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.4980 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PIZ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FXI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.2586 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FXI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PIZ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.7942 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PIZ XGBoost: Starting GridSearchCV fit...
       ‚úÖ FXI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.7904 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FXI XGBoost: Starting GridSearchCV fit...
       ‚úÖ OIS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=73.2968 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.0s
    - LSTM: MSE=0.5762
    - TCN: MSE=0.3818
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3818
        ‚Ä¢ LSTM: MSE=0.5762
        ‚Ä¢ Random Forest: MSE=66.6541
        ‚Ä¢ XGBoost: MSE=73.2968
        ‚Ä¢ LightGBM Regressor (CPU): MSE=92.9806
   ‚úÖ OIS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OIS (TargetReturn): TCN with MSE=0.3818
üêõ DEBUG: OIS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OIS.
üêõ DEBUG: OIS - Moving model to CPU before return...
üêõ DEBUG [00:18:12.613]: OIS - Returning result metadata...
üêõ DEBUG [00:18:12.613]: Main received result for OIS
üêõ DEBUG: train_worker started for MVBF
  ‚öôÔ∏è Training models for MVBF (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - MVBF: Initiating feature extraction for training.
  [DIAGNOSTIC] MVBF: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MVBF: rows after features available: 126
üéØ MVBF: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MVBF: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MVBF: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MVBF: Training LSTM (50 epochs)...
      ‚è≥ MVBF LSTM: Epoch 10/50 (20%)
      ‚è≥ MVBF LSTM: Epoch 20/50 (40%)
      ‚è≥ MVBF LSTM: Epoch 30/50 (60%)
      ‚è≥ MVBF LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.676860
         RMSE: 0.822715
         R¬≤ Score: -0.7651 (Poor - 76.5% variance explained)
      üîπ MVBF: Training TCN (50 epochs)...
      ‚è≥ MVBF TCN: Epoch 10/50 (20%)
      ‚è≥ MVBF TCN: Epoch 20/50 (40%)
      ‚è≥ MVBF TCN: Epoch 30/50 (60%)
      ‚è≥ MVBF TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.672519
         RMSE: 0.820073
         R¬≤ Score: -0.7538
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MVBF: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MVBF Random Forest: Starting GridSearchCV fit...
       ‚úÖ MVBF Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.9800 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MVBF LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MVBF LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.4261 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MVBF XGBoost: Starting GridSearchCV fit...
       ‚úÖ IYG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.1882 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.7s
    - LSTM: MSE=0.6181
    - TCN: MSE=0.2472
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2472
        ‚Ä¢ LSTM: MSE=0.6181
        ‚Ä¢ Random Forest: MSE=11.3504
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.9644
        ‚Ä¢ XGBoost: MSE=14.1882
   ‚úÖ IYG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IYG (TargetReturn): TCN with MSE=0.2472
üêõ DEBUG: IYG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IYG.
üêõ DEBUG: IYG - Moving model to CPU before return...
üêõ DEBUG [00:18:29.647]: IYG - Returning result metadata...
üêõ DEBUG: train_worker started for NEM
  ‚öôÔ∏è Training models for NEM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - NEM: Initiating feature extraction for training.
  [DIAGNOSTIC] NEM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NEM: rows after features available: 126
üéØ NEM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NEM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NEM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NEM: Training LSTM (50 epochs)...
      ‚è≥ NEM LSTM: Epoch 10/50 (20%)
      ‚è≥ NEM LSTM: Epoch 20/50 (40%)
      ‚è≥ NEM LSTM: Epoch 30/50 (60%)
       ‚úÖ ESAB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.1132 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.0s
    - LSTM: MSE=0.1892
    - TCN: MSE=0.2408
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1892
        ‚Ä¢ TCN: MSE=0.2408
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.4615
        ‚Ä¢ XGBoost: MSE=9.1132
        ‚Ä¢ Random Forest: MSE=10.9899
   ‚úÖ ESAB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ESAB (TargetReturn): LSTM with MSE=0.1892
üêõ DEBUG: ESAB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ESAB.
üêõ DEBUG: ESAB - Moving model to CPU before return...
üêõ DEBUG [00:18:31.167]: ESAB - Returning result metadata...
üêõ DEBUG: train_worker started for CRBG
üêõ DEBUG [00:18:31.172]: Main received result for ESAB
üêõ DEBUG [00:18:31.172]: Main received result for IYG
  ‚öôÔ∏è Training models for CRBG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CRBG: Initiating feature extraction for training.
  [DIAGNOSTIC] CRBG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRBG: rows after features available: 126
üéØ CRBG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRBG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRBG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRBG: Training LSTM (50 epochs)...
      ‚è≥ NEM LSTM: Epoch 40/50 (80%)
      ‚è≥ CRBG LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.210179
         RMSE: 0.458453
         R¬≤ Score: -0.9848 (Poor - 98.5% variance explained)
      üîπ NEM: Training TCN (50 epochs)...
      ‚è≥ CRBG LSTM: Epoch 20/50 (40%)
      ‚è≥ NEM TCN: Epoch 10/50 (20%)
      ‚è≥ NEM TCN: Epoch 20/50 (40%)
      ‚è≥ NEM TCN: Epoch 30/50 (60%)
      ‚è≥ NEM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.107067
         RMSE: 0.327210
         R¬≤ Score: -0.0111
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NEM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NEM Random Forest: Starting GridSearchCV fit...
      ‚è≥ CRBG LSTM: Epoch 30/50 (60%)
      ‚è≥ CRBG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.479286
         RMSE: 0.692305
         R¬≤ Score: -1.1140 (Poor - 111.4% variance explained)
      üîπ CRBG: Training TCN (50 epochs)...
      ‚è≥ CRBG TCN: Epoch 10/50 (20%)
      ‚è≥ CRBG TCN: Epoch 20/50 (40%)
      ‚è≥ CRBG TCN: Epoch 30/50 (60%)
      ‚è≥ CRBG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.247545
         RMSE: 0.497539
         R¬≤ Score: -0.0918
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRBG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRBG Random Forest: Starting GridSearchCV fit...
       ‚úÖ NEM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.9933 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NEM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NEM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.5176 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NEM XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRBG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.5494 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRBG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRBG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.8198 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRBG XGBoost: Starting GridSearchCV fit...
       ‚úÖ TS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.9251 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.3s
    - LSTM: MSE=0.4593
    - TCN: MSE=0.4785
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4593
        ‚Ä¢ TCN: MSE=0.4785
        ‚Ä¢ Random Forest: MSE=9.0566
        ‚Ä¢ XGBoost: MSE=9.9251
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.9113
   ‚úÖ TS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TS (TargetReturn): LSTM with MSE=0.4593
üêõ DEBUG: TS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TS.
üêõ DEBUG: TS - Moving model to CPU before return...
üêõ DEBUG [00:18:59.683]: TS - Returning result metadata...
üêõ DEBUG: train_worker started for IDMO
üêõ DEBUG [00:18:59.684]: Main received result for TS
üêõ DEBUG: Training progress: 848/959 done
  ‚öôÔ∏è Training models for IDMO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - IDMO: Initiating feature extraction for training.
  [DIAGNOSTIC] IDMO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IDMO: rows after features available: 126
üéØ IDMO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IDMO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IDMO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IDMO: Training LSTM (50 epochs)...
      ‚è≥ IDMO LSTM: Epoch 10/50 (20%)
       ‚úÖ UPWK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.0162 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.8s
    - LSTM: MSE=0.1576
    - TCN: MSE=0.1178
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1178
        ‚Ä¢ LSTM: MSE=0.1576
        ‚Ä¢ XGBoost: MSE=16.0162
        ‚Ä¢ Random Forest: MSE=20.8703
        ‚Ä¢ LightGBM Regressor (CPU): MSE=29.4744
   ‚úÖ UPWK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UPWK (TargetReturn): TCN with MSE=0.1178
üêõ DEBUG: UPWK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UPWK.
üêõ DEBUG: UPWK - Moving model to CPU before return...
üêõ DEBUG [00:19:00.236]: UPWK - Returning result metadata...
üêõ DEBUG: train_worker started for GEOS
  ‚öôÔ∏è Training models for GEOS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - GEOS: Initiating feature extraction for training.
  [DIAGNOSTIC] GEOS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GEOS: rows after features available: 126
üéØ GEOS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GEOS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GEOS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GEOS: Training LSTM (50 epochs)...
      ‚è≥ IDMO LSTM: Epoch 20/50 (40%)
      ‚è≥ GEOS LSTM: Epoch 10/50 (20%)
      ‚è≥ GEOS LSTM: Epoch 20/50 (40%)
      ‚è≥ IDMO LSTM: Epoch 30/50 (60%)
      ‚è≥ GEOS LSTM: Epoch 30/50 (60%)
      ‚è≥ IDMO LSTM: Epoch 40/50 (80%)
       ‚úÖ PHYS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.7376 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.3005
    - TCN: MSE=0.3025
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3005
        ‚Ä¢ TCN: MSE=0.3025
        ‚Ä¢ Random Forest: MSE=6.5842
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.8452
        ‚Ä¢ XGBoost: MSE=7.7376
   ‚úÖ PHYS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PHYS (TargetReturn): LSTM with MSE=0.3005
üêõ DEBUG: PHYS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PHYS.
üêõ DEBUG: PHYS - Moving model to CPU before return...
üêõ DEBUG [00:19:01.929]: PHYS - Returning result metadata...
üêõ DEBUG [00:19:01.930]: Main received result for PHYS
üêõ DEBUG [00:19:01.930]: Main received result for UPWK
üêõ DEBUG: train_worker started for DBP
  ‚öôÔ∏è Training models for DBP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - DBP: Initiating feature extraction for training.
  [DIAGNOSTIC] DBP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DBP: rows after features available: 126
üéØ DBP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DBP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DBP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DBP: Training LSTM (50 epochs)...
      ‚è≥ GEOS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.129482
         RMSE: 0.359836
         R¬≤ Score: -0.6431 (Poor - 64.3% variance explained)
      üîπ IDMO: Training TCN (50 epochs)...
      ‚è≥ IDMO TCN: Epoch 10/50 (20%)
      ‚è≥ DBP LSTM: Epoch 10/50 (20%)
      ‚è≥ IDMO TCN: Epoch 20/50 (40%)
      ‚è≥ IDMO TCN: Epoch 30/50 (60%)
      ‚è≥ IDMO TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.795717
         RMSE: 0.892030
         R¬≤ Score: -0.8375 (Poor - 83.7% variance explained)
      üîπ GEOS: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.104648
         RMSE: 0.323494
         R¬≤ Score: -0.3279
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IDMO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IDMO Random Forest: Starting GridSearchCV fit...
      ‚è≥ GEOS TCN: Epoch 10/50 (20%)
      ‚è≥ GEOS TCN: Epoch 20/50 (40%)
      ‚è≥ DBP LSTM: Epoch 20/50 (40%)
      ‚è≥ GEOS TCN: Epoch 30/50 (60%)
      ‚è≥ GEOS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.621445
         RMSE: 0.788318
         R¬≤ Score: -0.4350
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GEOS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GEOS Random Forest: Starting GridSearchCV fit...
      ‚è≥ DBP LSTM: Epoch 30/50 (60%)
      ‚è≥ DBP LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.171150
         RMSE: 0.413702
         R¬≤ Score: -0.2734 (Poor - 27.3% variance explained)
      üîπ DBP: Training TCN (50 epochs)...
      ‚è≥ DBP TCN: Epoch 10/50 (20%)
      ‚è≥ DBP TCN: Epoch 20/50 (40%)
      ‚è≥ DBP TCN: Epoch 30/50 (60%)
      ‚è≥ DBP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.176661
         RMSE: 0.420311
         R¬≤ Score: -0.3144
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DBP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DBP Random Forest: Starting GridSearchCV fit...
       ‚úÖ IDMO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.9386 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IDMO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GEOS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=190.4827 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GEOS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IDMO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.9789 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IDMO XGBoost: Starting GridSearchCV fit...
       ‚úÖ GEOS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=333.0895 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GEOS XGBoost: Starting GridSearchCV fit...
       ‚úÖ DBP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.2692 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DBP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DBP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.9714 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DBP XGBoost: Starting GridSearchCV fit...
       ‚úÖ VMI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.3058 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.3378
    - TCN: MSE=0.5956
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3378
        ‚Ä¢ TCN: MSE=0.5956
        ‚Ä¢ Random Forest: MSE=13.3810
        ‚Ä¢ XGBoost: MSE=14.3058
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.6881
   ‚úÖ VMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VMI (TargetReturn): LSTM with MSE=0.3378
üêõ DEBUG: VMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VMI.
üêõ DEBUG: VMI - Moving model to CPU before return...
üêõ DEBUG [00:19:10.544]: VMI - Returning result metadata...
üêõ DEBUG: train_worker started for RIVN
üêõ DEBUG [00:19:10.545]: Main received result for VMI
  ‚öôÔ∏è Training models for RIVN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - RIVN: Initiating feature extraction for training.
  [DIAGNOSTIC] RIVN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RIVN: rows after features available: 126
üéØ RIVN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RIVN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RIVN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RIVN: Training LSTM (50 epochs)...
      ‚è≥ RIVN LSTM: Epoch 10/50 (20%)
      ‚è≥ RIVN LSTM: Epoch 20/50 (40%)
      ‚è≥ RIVN LSTM: Epoch 30/50 (60%)
      ‚è≥ RIVN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.256201
         RMSE: 0.506163
         R¬≤ Score: -1.3321 (Poor - 133.2% variance explained)
      üîπ RIVN: Training TCN (50 epochs)...
      ‚è≥ RIVN TCN: Epoch 10/50 (20%)
      ‚è≥ RIVN TCN: Epoch 20/50 (40%)
      ‚è≥ RIVN TCN: Epoch 30/50 (60%)
      ‚è≥ RIVN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.112500
         RMSE: 0.335410
         R¬≤ Score: -0.0241
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RIVN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RIVN Random Forest: Starting GridSearchCV fit...
       ‚úÖ YMAX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.6522 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.5s
    - LSTM: MSE=0.5945
    - TCN: MSE=0.3910
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3910
        ‚Ä¢ LSTM: MSE=0.5945
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5152
        ‚Ä¢ Random Forest: MSE=9.6465
        ‚Ä¢ XGBoost: MSE=10.6522
   ‚úÖ YMAX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for YMAX (TargetReturn): TCN with MSE=0.3910
üêõ DEBUG: YMAX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for YMAX.
üêõ DEBUG: YMAX - Moving model to CPU before return...
üêõ DEBUG [00:19:13.613]: YMAX - Returning result metadata...
üêõ DEBUG [00:19:13.614]: Main received result for YMAX
üêõ DEBUG: Training progress: 852/959 done
üêõ DEBUG: train_worker started for SLVO
  ‚öôÔ∏è Training models for SLVO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SLVO: Initiating feature extraction for training.
  [DIAGNOSTIC] SLVO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SLVO: rows after features available: 126
üéØ SLVO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SLVO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SLVO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SLVO: Training LSTM (50 epochs)...
      ‚è≥ SLVO LSTM: Epoch 10/50 (20%)
      ‚è≥ SLVO LSTM: Epoch 20/50 (40%)
      ‚è≥ SLVO LSTM: Epoch 30/50 (60%)
      ‚è≥ SLVO LSTM: Epoch 40/50 (80%)
       ‚úÖ RIVN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=40.6039 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RIVN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.324783
         RMSE: 0.569898
         R¬≤ Score: -1.2811 (Poor - 128.1% variance explained)
      üîπ SLVO: Training TCN (50 epochs)...
      ‚è≥ SLVO TCN: Epoch 10/50 (20%)
      ‚è≥ SLVO TCN: Epoch 20/50 (40%)
       ‚úÖ RIVN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.7705 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RIVN XGBoost: Starting GridSearchCV fit...
      ‚è≥ SLVO TCN: Epoch 30/50 (60%)
      ‚è≥ SLVO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.227772
         RMSE: 0.477255
         R¬≤ Score: -0.5998
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SLVO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SLVO Random Forest: Starting GridSearchCV fit...
       ‚úÖ SLVO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.7512 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SLVO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SLVO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.0164 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SLVO XGBoost: Starting GridSearchCV fit...
       ‚úÖ PRM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=66.8412 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.3s
    - LSTM: MSE=0.5765
    - TCN: MSE=0.6384
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5765
        ‚Ä¢ TCN: MSE=0.6384
        ‚Ä¢ Random Forest: MSE=22.2728
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.8831
        ‚Ä¢ XGBoost: MSE=66.8412
   ‚úÖ PRM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PRM (TargetReturn): LSTM with MSE=0.5765
üêõ DEBUG: PRM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PRM.
üêõ DEBUG: PRM - Moving model to CPU before return...
üêõ DEBUG [00:19:33.841]: PRM - Returning result metadata...
üêõ DEBUG [00:19:33.842]: Main received result for PRM
üêõ DEBUG: train_worker started for META
  ‚öôÔ∏è Training models for META (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - META: Initiating feature extraction for training.
  [DIAGNOSTIC] META: fetch_training_data - Initial data rows: 205
   ‚Ü≥ META: rows after features available: 126
üéØ META: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] META: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö META: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ META: Training LSTM (50 epochs)...
      ‚è≥ META LSTM: Epoch 10/50 (20%)
      ‚è≥ META LSTM: Epoch 20/50 (40%)
      ‚è≥ META LSTM: Epoch 30/50 (60%)
      ‚è≥ META LSTM: Epoch 40/50 (80%)
       ‚úÖ DE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.6412 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.1841
    - TCN: MSE=0.2174
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1841
        ‚Ä¢ TCN: MSE=0.2174
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.5545
        ‚Ä¢ Random Forest: MSE=9.4355
        ‚Ä¢ XGBoost: MSE=11.6412
   ‚úÖ DE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DE (TargetReturn): LSTM with MSE=0.1841
üêõ DEBUG: DE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DE.
üêõ DEBUG: DE - Moving model to CPU before return...
üêõ DEBUG [00:19:35.864]: DE - Returning result metadata...
üêõ DEBUG: train_worker started for HWKN
üêõ DEBUG [00:19:35.870]: Main received result for DE
  ‚öôÔ∏è Training models for HWKN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - HWKN: Initiating feature extraction for training.
  [DIAGNOSTIC] HWKN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HWKN: rows after features available: 126
üéØ HWKN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HWKN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HWKN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HWKN: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.877513
         RMSE: 0.936757
         R¬≤ Score: -1.3182 (Poor - 131.8% variance explained)
      üîπ META: Training TCN (50 epochs)...
      ‚è≥ META TCN: Epoch 10/50 (20%)
      ‚è≥ META TCN: Epoch 20/50 (40%)
      ‚è≥ META TCN: Epoch 30/50 (60%)
      ‚è≥ HWKN LSTM: Epoch 10/50 (20%)
      ‚è≥ META TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.737278
         RMSE: 0.858649
         R¬≤ Score: -0.9477
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä META: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ META Random Forest: Starting GridSearchCV fit...
      ‚è≥ HWKN LSTM: Epoch 20/50 (40%)
      ‚è≥ HWKN LSTM: Epoch 30/50 (60%)
      ‚è≥ HWKN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.306541
         RMSE: 0.553662
         R¬≤ Score: -2.4391 (Poor - 243.9% variance explained)
      üîπ HWKN: Training TCN (50 epochs)...
      ‚è≥ HWKN TCN: Epoch 10/50 (20%)
      ‚è≥ HWKN TCN: Epoch 20/50 (40%)
      ‚è≥ HWKN TCN: Epoch 30/50 (60%)
      ‚è≥ HWKN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.138077
         RMSE: 0.371587
         R¬≤ Score: -0.5491
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HWKN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HWKN Random Forest: Starting GridSearchCV fit...
       ‚úÖ META Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.3438 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ META LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ META LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.4244 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ META XGBoost: Starting GridSearchCV fit...
       ‚úÖ GLTR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.5486 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.1096
    - TCN: MSE=0.0675
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0675
        ‚Ä¢ LSTM: MSE=0.1096
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.1620
        ‚Ä¢ XGBoost: MSE=5.5486
        ‚Ä¢ Random Forest: MSE=5.5887
   ‚úÖ GLTR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GLTR (TargetReturn): TCN with MSE=0.0675
üêõ DEBUG: GLTR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GLTR.
üêõ DEBUG: GLTR - Moving model to CPU before return...
üêõ DEBUG [00:19:41.527]: GLTR - Returning result metadata...
üêõ DEBUG: train_worker started for GLW
üêõ DEBUG [00:19:41.528]: Main received result for GLTR
  ‚öôÔ∏è Training models for GLW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - GLW: Initiating feature extraction for training.
  [DIAGNOSTIC] GLW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GLW: rows after features available: 126
üéØ GLW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GLW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GLW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GLW: Training LSTM (50 epochs)...
       ‚úÖ HWKN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.9196 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HWKN LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ GLW LSTM: Epoch 10/50 (20%)
       ‚úÖ HWKN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.4332 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HWKN XGBoost: Starting GridSearchCV fit...
      ‚è≥ GLW LSTM: Epoch 20/50 (40%)
      ‚è≥ GLW LSTM: Epoch 30/50 (60%)
      ‚è≥ GLW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.532987
         RMSE: 0.730060
         R¬≤ Score: -0.8476 (Poor - 84.8% variance explained)
      üîπ GLW: Training TCN (50 epochs)...
      ‚è≥ GLW TCN: Epoch 10/50 (20%)
      ‚è≥ GLW TCN: Epoch 20/50 (40%)
      ‚è≥ GLW TCN: Epoch 30/50 (60%)
      ‚è≥ GLW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.503980
         RMSE: 0.709916
         R¬≤ Score: -0.7471
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GLW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GLW Random Forest: Starting GridSearchCV fit...
       ‚úÖ GLW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.1026 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GLW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GLW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=20.1533 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GLW XGBoost: Starting GridSearchCV fit...
       ‚úÖ PBW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.9856 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.1s
    - LSTM: MSE=0.4817
    - TCN: MSE=0.6021
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4817
        ‚Ä¢ TCN: MSE=0.6021
        ‚Ä¢ Random Forest: MSE=14.2408
        ‚Ä¢ XGBoost: MSE=19.9856
        ‚Ä¢ LightGBM Regressor (CPU): MSE=34.9746
   ‚úÖ PBW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PBW (TargetReturn): LSTM with MSE=0.4817
üêõ DEBUG: PBW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PBW.
üêõ DEBUG: PBW - Moving model to CPU before return...
üêõ DEBUG [00:19:54.148]: PBW - Returning result metadata...
üêõ DEBUG [00:19:54.148]: Main received result for PBW
üêõ DEBUG: Training progress: 856/959 done
üêõ DEBUG: train_worker started for ASLE
  ‚öôÔ∏è Training models for ASLE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - ASLE: Initiating feature extraction for training.
  [DIAGNOSTIC] ASLE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ASLE: rows after features available: 126
üéØ ASLE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ASLE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ASLE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ASLE: Training LSTM (50 epochs)...
      ‚è≥ ASLE LSTM: Epoch 10/50 (20%)
      ‚è≥ ASLE LSTM: Epoch 20/50 (40%)
      ‚è≥ ASLE LSTM: Epoch 30/50 (60%)
      ‚è≥ ASLE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.176233
         RMSE: 0.419802
         R¬≤ Score: -1.1674 (Poor - 116.7% variance explained)
      üîπ ASLE: Training TCN (50 epochs)...
      ‚è≥ ASLE TCN: Epoch 10/50 (20%)
      ‚è≥ ASLE TCN: Epoch 20/50 (40%)
      ‚è≥ ASLE TCN: Epoch 30/50 (60%)
      ‚è≥ ASLE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.089998
         RMSE: 0.299996
         R¬≤ Score: -0.1068
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ASLE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ASLE Random Forest: Starting GridSearchCV fit...
       ‚úÖ HLI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=11.2842 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.5662
    - TCN: MSE=0.3586
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3586
        ‚Ä¢ LSTM: MSE=0.5662
        ‚Ä¢ Random Forest: MSE=11.0289
        ‚Ä¢ XGBoost: MSE=11.2842
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.3945
   ‚úÖ HLI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HLI (TargetReturn): TCN with MSE=0.3586
üêõ DEBUG: HLI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HLI.
üêõ DEBUG: HLI - Moving model to CPU before return...
üêõ DEBUG [00:19:57.525]: HLI - Returning result metadata...
üêõ DEBUG [00:19:57.526]: Main received result for HLI
üêõ DEBUG: train_worker started for CLM
  ‚öôÔ∏è Training models for CLM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CLM: Initiating feature extraction for training.
  [DIAGNOSTIC] CLM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CLM: rows after features available: 126
üéØ CLM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CLM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CLM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CLM: Training LSTM (50 epochs)...
      ‚è≥ CLM LSTM: Epoch 10/50 (20%)
      ‚è≥ CLM LSTM: Epoch 20/50 (40%)
      ‚è≥ CLM LSTM: Epoch 30/50 (60%)
      ‚è≥ CLM LSTM: Epoch 40/50 (80%)
       ‚úÖ ASLE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=22.4384 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ASLE LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.752026
         RMSE: 0.867194
         R¬≤ Score: -0.9760 (Poor - 97.6% variance explained)
      üîπ CLM: Training TCN (50 epochs)...
      ‚è≥ CLM TCN: Epoch 10/50 (20%)
      ‚è≥ CLM TCN: Epoch 20/50 (40%)
       ‚úÖ ASLE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.7208 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 100}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ASLE XGBoost: Starting GridSearchCV fit...
      ‚è≥ CLM TCN: Epoch 30/50 (60%)
      ‚è≥ CLM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.452835
         RMSE: 0.672930
         R¬≤ Score: -0.1898
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CLM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CLM Random Forest: Starting GridSearchCV fit...
       ‚úÖ PIZ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.7824 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.2105
    - TCN: MSE=0.1549
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1549
        ‚Ä¢ LSTM: MSE=0.2105
        ‚Ä¢ XGBoost: MSE=4.7824
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.7942
        ‚Ä¢ Random Forest: MSE=5.4980
   ‚úÖ PIZ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PIZ (TargetReturn): TCN with MSE=0.1549
üêõ DEBUG: PIZ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PIZ.
üêõ DEBUG: PIZ - Moving model to CPU before return...
üêõ DEBUG [00:20:02.409]: PIZ - Returning result metadata...
üêõ DEBUG: train_worker started for PODD
üêõ DEBUG [00:20:02.410]: Main received result for PIZ
  ‚öôÔ∏è Training models for PODD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - PODD: Initiating feature extraction for training.
  [DIAGNOSTIC] PODD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PODD: rows after features available: 126
üéØ PODD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PODD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PODD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PODD: Training LSTM (50 epochs)...
       ‚úÖ FXI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.2999 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.3722
    - TCN: MSE=0.2946
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2946
        ‚Ä¢ LSTM: MSE=0.3722
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.7904
        ‚Ä¢ Random Forest: MSE=10.2586
        ‚Ä¢ XGBoost: MSE=14.2999
   ‚úÖ FXI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FXI (TargetReturn): TCN with MSE=0.2946
üêõ DEBUG: FXI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FXI.
üêõ DEBUG: FXI - Moving model to CPU before return...
üêõ DEBUG [00:20:02.835]: FXI - Returning result metadata...
üêõ DEBUG [00:20:02.836]: Main received result for FXIüêõ DEBUG: train_worker started for WUGI

  ‚öôÔ∏è Training models for WUGI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - WUGI: Initiating feature extraction for training.
  [DIAGNOSTIC] WUGI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WUGI: rows after features available: 126
üéØ WUGI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WUGI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WUGI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WUGI: Training LSTM (50 epochs)...
      ‚è≥ PODD LSTM: Epoch 10/50 (20%)
      ‚è≥ WUGI LSTM: Epoch 10/50 (20%)
      ‚è≥ PODD LSTM: Epoch 20/50 (40%)
       ‚úÖ CLM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9483 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CLM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ WUGI LSTM: Epoch 20/50 (40%)
      ‚è≥ PODD LSTM: Epoch 30/50 (60%)
       ‚úÖ CLM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.6872 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CLM XGBoost: Starting GridSearchCV fit...
      ‚è≥ WUGI LSTM: Epoch 30/50 (60%)
      ‚è≥ PODD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.436301
         RMSE: 0.660531
         R¬≤ Score: -0.9642 (Poor - 96.4% variance explained)
      üîπ PODD: Training TCN (50 epochs)...
      ‚è≥ WUGI LSTM: Epoch 40/50 (80%)
      ‚è≥ PODD TCN: Epoch 10/50 (20%)
      ‚è≥ PODD TCN: Epoch 20/50 (40%)
      ‚è≥ PODD TCN: Epoch 30/50 (60%)
      ‚è≥ PODD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.265691
         RMSE: 0.515453
         R¬≤ Score: -0.1961
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PODD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PODD Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.500155
         RMSE: 0.707216
         R¬≤ Score: -0.8458 (Poor - 84.6% variance explained)
      üîπ WUGI: Training TCN (50 epochs)...
      ‚è≥ WUGI TCN: Epoch 10/50 (20%)
      ‚è≥ WUGI TCN: Epoch 20/50 (40%)
      ‚è≥ WUGI TCN: Epoch 30/50 (60%)
      ‚è≥ WUGI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.504953
         RMSE: 0.710601
         R¬≤ Score: -0.8635
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WUGI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WUGI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PODD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.5504 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PODD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WUGI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.0721 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WUGI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PODD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=17.8610 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PODD XGBoost: Starting GridSearchCV fit...
       ‚úÖ WUGI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.8630 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WUGI XGBoost: Starting GridSearchCV fit...
       ‚úÖ MVBF XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=13.5053 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.6769
    - TCN: MSE=0.6725
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6725
        ‚Ä¢ LSTM: MSE=0.6769
        ‚Ä¢ Random Forest: MSE=9.9800
        ‚Ä¢ XGBoost: MSE=13.5053
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.4261
   ‚úÖ MVBF: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MVBF (TargetReturn): TCN with MSE=0.6725
üêõ DEBUG: MVBF - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MVBF.
üêõ DEBUG: MVBF - Moving model to CPU before return...
üêõ DEBUG [00:20:17.086]: MVBF - Returning result metadata...
üêõ DEBUG: train_worker started for RIGL
üêõ DEBUG [00:20:17.087]: Main received result for MVBF
üêõ DEBUG: Training progress: 860/959 done
  ‚öôÔ∏è Training models for RIGL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - RIGL: Initiating feature extraction for training.
  [DIAGNOSTIC] RIGL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RIGL: rows after features available: 126
üéØ RIGL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RIGL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RIGL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RIGL: Training LSTM (50 epochs)...
      ‚è≥ RIGL LSTM: Epoch 10/50 (20%)
      ‚è≥ RIGL LSTM: Epoch 20/50 (40%)
      ‚è≥ RIGL LSTM: Epoch 30/50 (60%)
      ‚è≥ RIGL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.106079
         RMSE: 0.325697
         R¬≤ Score: -1.0324 (Poor - 103.2% variance explained)
      üîπ RIGL: Training TCN (50 epochs)...
      ‚è≥ RIGL TCN: Epoch 10/50 (20%)
      ‚è≥ RIGL TCN: Epoch 20/50 (40%)
      ‚è≥ RIGL TCN: Epoch 30/50 (60%)
      ‚è≥ RIGL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.061289
         RMSE: 0.247567
         R¬≤ Score: -0.1743
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RIGL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RIGL Random Forest: Starting GridSearchCV fit...
       ‚úÖ RIGL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.8643 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RIGL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RIGL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=38.2390 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RIGL XGBoost: Starting GridSearchCV fit...
       ‚úÖ NEM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=26.9577 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.2102
    - TCN: MSE=0.1071
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.1 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1071
        ‚Ä¢ LSTM: MSE=0.2102
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.5176
        ‚Ä¢ Random Forest: MSE=23.9933
        ‚Ä¢ XGBoost: MSE=26.9577
   ‚úÖ NEM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NEM (TargetReturn): TCN with MSE=0.1071
üêõ DEBUG: NEM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NEM.
üêõ DEBUG: NEM - Moving model to CPU before return...
üêõ DEBUG [00:20:33.783]: NEM - Returning result metadata...
üêõ DEBUG: train_worker started for OZK
üêõ DEBUG [00:20:33.783]: Main received result for NEM
  ‚öôÔ∏è Training models for OZK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - OZK: Initiating feature extraction for training.
  [DIAGNOSTIC] OZK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ OZK: rows after features available: 126
üéØ OZK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] OZK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö OZK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ OZK: Training LSTM (50 epochs)...
       ‚úÖ CRBG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=12.8853 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.3s
    - LSTM: MSE=0.4793
    - TCN: MSE=0.2475
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2475
        ‚Ä¢ LSTM: MSE=0.4793
        ‚Ä¢ XGBoost: MSE=12.8853
        ‚Ä¢ Random Forest: MSE=15.5494
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.8198
   ‚úÖ CRBG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRBG (TargetReturn): TCN with MSE=0.2475
üêõ DEBUG: CRBG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRBG.
üêõ DEBUG: CRBG - Moving model to CPU before return...
üêõ DEBUG [00:20:33.983]: CRBG - Returning result metadata...
üêõ DEBUG: train_worker started for HEI-A
üêõ DEBUG [00:20:33.985]: Main received result for CRBG
  ‚öôÔ∏è Training models for HEI-A (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - HEI-A: Initiating feature extraction for training.
  [DIAGNOSTIC] HEI-A: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HEI-A: rows after features available: 126
üéØ HEI-A: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HEI-A: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HEI-A: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HEI-A: Training LSTM (50 epochs)...
      ‚è≥ OZK LSTM: Epoch 10/50 (20%)
      ‚è≥ HEI-A LSTM: Epoch 10/50 (20%)
      ‚è≥ OZK LSTM: Epoch 20/50 (40%)
      ‚è≥ HEI-A LSTM: Epoch 20/50 (40%)
      ‚è≥ OZK LSTM: Epoch 30/50 (60%)
      ‚è≥ HEI-A LSTM: Epoch 30/50 (60%)
      ‚è≥ HEI-A LSTM: Epoch 40/50 (80%)
      ‚è≥ OZK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.221051
         RMSE: 0.470160
         R¬≤ Score: -1.3143 (Poor - 131.4% variance explained)
      üîπ HEI-A: Training TCN (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.533593
         RMSE: 0.730475
         R¬≤ Score: -0.6825 (Poor - 68.3% variance explained)
      üîπ OZK: Training TCN (50 epochs)...
      ‚è≥ HEI-A TCN: Epoch 10/50 (20%)
      ‚è≥ OZK TCN: Epoch 10/50 (20%)
      ‚è≥ HEI-A TCN: Epoch 20/50 (40%)
      ‚è≥ OZK TCN: Epoch 20/50 (40%)
      ‚è≥ HEI-A TCN: Epoch 30/50 (60%)
      ‚è≥ OZK TCN: Epoch 30/50 (60%)
      ‚è≥ OZK TCN: Epoch 40/50 (80%)
      ‚è≥ HEI-A TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.422069
         RMSE: 0.649668
         R¬≤ Score: -0.3309
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä OZK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ OZK Random Forest: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.100844
         RMSE: 0.317559
         R¬≤ Score: -0.0558
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HEI-A: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HEI-A Random Forest: Starting GridSearchCV fit...
       ‚úÖ HEI-A Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.1211 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HEI-A LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ OZK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.5079 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ OZK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HEI-A LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.8297 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HEI-A XGBoost: Starting GridSearchCV fit...
       ‚úÖ OZK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.5289 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ OZK XGBoost: Starting GridSearchCV fit...
       ‚úÖ GEOS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=198.1466 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 113.5s
    - LSTM: MSE=0.7957
    - TCN: MSE=0.6214
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6214
        ‚Ä¢ LSTM: MSE=0.7957
        ‚Ä¢ Random Forest: MSE=190.4827
        ‚Ä¢ XGBoost: MSE=198.1466
        ‚Ä¢ LightGBM Regressor (CPU): MSE=333.0895
   ‚úÖ GEOS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GEOS (TargetReturn): TCN with MSE=0.6214
üêõ DEBUG: GEOS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GEOS.
üêõ DEBUG: GEOS - Moving model to CPU before return...
üêõ DEBUG [00:21:00.399]: GEOS - Returning result metadata...
üêõ DEBUG: train_worker started for NTB
  ‚öôÔ∏è Training models for NTB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - NTB: Initiating feature extraction for training.
  [DIAGNOSTIC] NTB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NTB: rows after features available: 126
üéØ NTB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NTB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NTB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NTB: Training LSTM (50 epochs)...
      ‚è≥ NTB LSTM: Epoch 10/50 (20%)
      ‚è≥ NTB LSTM: Epoch 20/50 (40%)
      ‚è≥ NTB LSTM: Epoch 30/50 (60%)
      ‚è≥ NTB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.309822
         RMSE: 0.556616
         R¬≤ Score: -0.8519 (Poor - 85.2% variance explained)
      üîπ NTB: Training TCN (50 epochs)...
      ‚è≥ NTB TCN: Epoch 10/50 (20%)
      ‚è≥ NTB TCN: Epoch 20/50 (40%)
      ‚è≥ NTB TCN: Epoch 30/50 (60%)
      ‚è≥ NTB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.207084
         RMSE: 0.455065
         R¬≤ Score: -0.2378
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NTB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NTB Random Forest: Starting GridSearchCV fit...
       ‚úÖ NTB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.5615 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NTB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IDMO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.3547 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.0s
    - LSTM: MSE=0.1295
    - TCN: MSE=0.1046
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1046
        ‚Ä¢ LSTM: MSE=0.1295
        ‚Ä¢ XGBoost: MSE=4.3547
        ‚Ä¢ Random Forest: MSE=4.9386
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.9789
   ‚úÖ IDMO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IDMO (TargetReturn): TCN with MSE=0.1046
üêõ DEBUG: IDMO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IDMO.
üêõ DEBUG: IDMO - Moving model to CPU before return...
üêõ DEBUG [00:21:06.287]: IDMO - Returning result metadata...
üêõ DEBUG: train_worker started for FSK
üêõ DEBUG [00:21:06.290]: Main received result for IDMO
üêõ DEBUG [00:21:06.290]: Main received result for GEOS
üêõ DEBUG: Training progress: 864/959 done
  ‚öôÔ∏è Training models for FSK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - FSK: Initiating feature extraction for training.
  [DIAGNOSTIC] FSK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FSK: rows after features available: 126
üéØ FSK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FSK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FSK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FSK: Training LSTM (50 epochs)...
       ‚úÖ NTB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.9638 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NTB XGBoost: Starting GridSearchCV fit...
      ‚è≥ FSK LSTM: Epoch 10/50 (20%)
      ‚è≥ FSK LSTM: Epoch 20/50 (40%)
      ‚è≥ FSK LSTM: Epoch 30/50 (60%)
      ‚è≥ FSK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 1.033691
         RMSE: 1.016706
         R¬≤ Score: -1.7371 (Poor - 173.7% variance explained)
      üîπ FSK: Training TCN (50 epochs)...
      ‚è≥ FSK TCN: Epoch 10/50 (20%)
       ‚úÖ DBP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.4915 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.1711
    - TCN: MSE=0.1767
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1711
        ‚Ä¢ TCN: MSE=0.1767
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.9714
        ‚Ä¢ Random Forest: MSE=6.2692
        ‚Ä¢ XGBoost: MSE=7.4915
   ‚úÖ DBP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DBP (TargetReturn): LSTM with MSE=0.1711
üêõ DEBUG: DBP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DBP.
üêõ DEBUG: DBP - Moving model to CPU before return...
üêõ DEBUG [00:21:08.851]: DBP - Returning result metadata...
üêõ DEBUG: train_worker started for PSKY
üêõ DEBUG [00:21:08.852]: Main received result for DBP
      ‚è≥ FSK TCN: Epoch 20/50 (40%)
  ‚öôÔ∏è Training models for PSKY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - PSKY: Initiating feature extraction for training.
  [DIAGNOSTIC] PSKY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PSKY: rows after features available: 126
üéØ PSKY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PSKY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PSKY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PSKY: Training LSTM (50 epochs)...
      ‚è≥ FSK TCN: Epoch 30/50 (60%)
      ‚è≥ FSK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.576326
         RMSE: 0.759162
         R¬≤ Score: -0.5260
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FSK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FSK Random Forest: Starting GridSearchCV fit...
      ‚è≥ PSKY LSTM: Epoch 10/50 (20%)
      ‚è≥ PSKY LSTM: Epoch 20/50 (40%)
      ‚è≥ PSKY LSTM: Epoch 30/50 (60%)
      ‚è≥ PSKY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.452263
         RMSE: 0.672505
         R¬≤ Score: -1.6345 (Poor - 163.5% variance explained)
      üîπ PSKY: Training TCN (50 epochs)...
      ‚è≥ PSKY TCN: Epoch 10/50 (20%)
      ‚è≥ PSKY TCN: Epoch 20/50 (40%)
      ‚è≥ PSKY TCN: Epoch 30/50 (60%)
      ‚è≥ PSKY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.180366
         RMSE: 0.424696
         R¬≤ Score: -0.0507
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PSKY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PSKY Random Forest: Starting GridSearchCV fit...
       ‚úÖ FSK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.4508 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FSK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FSK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.2863 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FSK XGBoost: Starting GridSearchCV fit...
       ‚úÖ PSKY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.2589 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PSKY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PSKY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.2055 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PSKY XGBoost: Starting GridSearchCV fit...
       ‚úÖ SLVO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=4.7201 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.2s
    - LSTM: MSE=0.3248
    - TCN: MSE=0.2278
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2278
        ‚Ä¢ LSTM: MSE=0.3248
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.0164
        ‚Ä¢ XGBoost: MSE=4.7201
        ‚Ä¢ Random Forest: MSE=4.7512
   ‚úÖ SLVO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SLVO (TargetReturn): TCN with MSE=0.2278
üêõ DEBUG: SLVO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SLVO.
üêõ DEBUG: SLVO - Moving model to CPU before return...
üêõ DEBUG [00:21:17.030]: SLVO - Returning result metadata...
üêõ DEBUG: train_worker started for NI
  ‚öôÔ∏è Training models for NI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - NI: Initiating feature extraction for training.
  [DIAGNOSTIC] NI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NI: rows after features available: 126
üéØ NI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NI: Training LSTM (50 epochs)...
      ‚è≥ NI LSTM: Epoch 10/50 (20%)
      ‚è≥ NI LSTM: Epoch 20/50 (40%)
       ‚úÖ RIVN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=37.9710 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.9s
    - LSTM: MSE=0.2562
    - TCN: MSE=0.1125
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1125
        ‚Ä¢ LSTM: MSE=0.2562
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.7705
        ‚Ä¢ XGBoost: MSE=37.9710
        ‚Ä¢ Random Forest: MSE=40.6039
   ‚úÖ RIVN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RIVN (TargetReturn): TCN with MSE=0.1125
üêõ DEBUG: RIVN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RIVN.
üêõ DEBUG: RIVN - Moving model to CPU before return...
üêõ DEBUG [00:21:18.355]: RIVN - Returning result metadata...
üêõ DEBUG [00:21:18.356]: Main received result for RIVN
üêõ DEBUG: train_worker started for HDB
üêõ DEBUG [00:21:18.356]: Main received result for SLVO
      ‚è≥ NI LSTM: Epoch 30/50 (60%)
  ‚öôÔ∏è Training models for HDB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - HDB: Initiating feature extraction for training.
  [DIAGNOSTIC] HDB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HDB: rows after features available: 126
üéØ HDB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HDB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HDB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HDB: Training LSTM (50 epochs)...
      ‚è≥ NI LSTM: Epoch 40/50 (80%)
      ‚è≥ HDB LSTM: Epoch 10/50 (20%)
      ‚è≥ HDB LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.257344
         RMSE: 0.507291
         R¬≤ Score: -0.6979 (Poor - 69.8% variance explained)
      üîπ NI: Training TCN (50 epochs)...
      ‚è≥ NI TCN: Epoch 10/50 (20%)
      ‚è≥ NI TCN: Epoch 20/50 (40%)
      ‚è≥ NI TCN: Epoch 30/50 (60%)
      ‚è≥ HDB LSTM: Epoch 30/50 (60%)
      ‚è≥ NI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.152389
         RMSE: 0.390371
         R¬≤ Score: -0.0054
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NI Random Forest: Starting GridSearchCV fit...
      ‚è≥ HDB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.185106
         RMSE: 0.430240
         R¬≤ Score: -1.0951 (Poor - 109.5% variance explained)
      üîπ HDB: Training TCN (50 epochs)...
      ‚è≥ HDB TCN: Epoch 10/50 (20%)
      ‚è≥ HDB TCN: Epoch 20/50 (40%)
      ‚è≥ HDB TCN: Epoch 30/50 (60%)
      ‚è≥ HDB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.095881
         RMSE: 0.309647
         R¬≤ Score: -0.0852
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HDB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HDB Random Forest: Starting GridSearchCV fit...
       ‚úÖ NI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.2144 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.2746 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NI XGBoost: Starting GridSearchCV fit...
       ‚úÖ HDB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.8174 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HDB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HDB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=10.9945 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HDB XGBoost: Starting GridSearchCV fit...
       ‚úÖ META XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.8184 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 120.4s
    - LSTM: MSE=0.8775
    - TCN: MSE=0.7373
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.8 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.7373
        ‚Ä¢ LSTM: MSE=0.8775
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.4244
        ‚Ä¢ Random Forest: MSE=15.3438
        ‚Ä¢ XGBoost: MSE=17.8184
   ‚úÖ META: Phase 3/3 - Model selection complete!
  üèÜ WINNER for META (TargetReturn): TCN with MSE=0.7373
üêõ DEBUG: META - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for META.
üêõ DEBUG: META - Moving model to CPU before return...
üêõ DEBUG [00:21:40.281]: META - Returning result metadata...
üêõ DEBUG [00:21:40.282]: Main received result for META
üêõ DEBUG: Training progress: 868/959 doneüêõ DEBUG: train_worker started for TG

  ‚öôÔ∏è Training models for TG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - TG: Initiating feature extraction for training.
  [DIAGNOSTIC] TG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TG: rows after features available: 126
üéØ TG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TG: Training LSTM (50 epochs)...
      ‚è≥ TG LSTM: Epoch 10/50 (20%)
      ‚è≥ TG LSTM: Epoch 20/50 (40%)
      ‚è≥ TG LSTM: Epoch 30/50 (60%)
      ‚è≥ TG LSTM: Epoch 40/50 (80%)
       ‚úÖ HWKN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.0351 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.1s
    - LSTM: MSE=0.3065
    - TCN: MSE=0.1381
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1381
        ‚Ä¢ LSTM: MSE=0.3065
        ‚Ä¢ XGBoost: MSE=10.0351
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.4332
        ‚Ä¢ Random Forest: MSE=28.9196
   ‚úÖ HWKN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HWKN (TargetReturn): TCN with MSE=0.1381
üêõ DEBUG: HWKN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HWKN.
üêõ DEBUG: HWKN - Moving model to CPU before return...
üêõ DEBUG [00:21:42.444]: HWKN - Returning result metadata...
üêõ DEBUG: train_worker started for TD
üêõ DEBUG [00:21:42.447]: Main received result for HWKN
  ‚öôÔ∏è Training models for TD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - TD: Initiating feature extraction for training.
  [DIAGNOSTIC] TD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TD: rows after features available: 126
üéØ TD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TD: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.298643
         RMSE: 0.546482
         R¬≤ Score: -0.7668 (Poor - 76.7% variance explained)
      üîπ TG: Training TCN (50 epochs)...
      ‚è≥ TG TCN: Epoch 10/50 (20%)
      ‚è≥ TG TCN: Epoch 20/50 (40%)
      ‚è≥ TD LSTM: Epoch 10/50 (20%)
      ‚è≥ TG TCN: Epoch 30/50 (60%)
      ‚è≥ TG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.250687
         RMSE: 0.500687
         R¬≤ Score: -0.4831
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TG Random Forest: Starting GridSearchCV fit...
      ‚è≥ TD LSTM: Epoch 20/50 (40%)
      ‚è≥ TD LSTM: Epoch 30/50 (60%)
      ‚è≥ TD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.329258
         RMSE: 0.573810
         R¬≤ Score: -0.7628 (Poor - 76.3% variance explained)
      üîπ TD: Training TCN (50 epochs)...
      ‚è≥ TD TCN: Epoch 10/50 (20%)
      ‚è≥ TD TCN: Epoch 20/50 (40%)
      ‚è≥ TD TCN: Epoch 30/50 (60%)
      ‚è≥ TD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.395641
         RMSE: 0.629000
         R¬≤ Score: -1.1182
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TD Random Forest: Starting GridSearchCV fit...
       ‚úÖ GLW XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=12.8522 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.0s
    - LSTM: MSE=0.5330
    - TCN: MSE=0.5040
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5040
        ‚Ä¢ LSTM: MSE=0.5330
        ‚Ä¢ XGBoost: MSE=12.8522
        ‚Ä¢ Random Forest: MSE=15.1026
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.1533
   ‚úÖ GLW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GLW (TargetReturn): TCN with MSE=0.5040
üêõ DEBUG: GLW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GLW.
üêõ DEBUG: GLW - Moving model to CPU before return...
üêõ DEBUG [00:21:45.738]: GLW - Returning result metadata...
üêõ DEBUG: train_worker started for NBN
üêõ DEBUG [00:21:45.743]: Main received result for GLW
  ‚öôÔ∏è Training models for NBN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - NBN: Initiating feature extraction for training.
  [DIAGNOSTIC] NBN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NBN: rows after features available: 126
üéØ NBN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NBN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NBN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NBN: Training LSTM (50 epochs)...
       ‚úÖ TG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.2351 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TG LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NBN LSTM: Epoch 10/50 (20%)
       ‚úÖ TG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.3322 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TG XGBoost: Starting GridSearchCV fit...
      ‚è≥ NBN LSTM: Epoch 20/50 (40%)
      ‚è≥ NBN LSTM: Epoch 30/50 (60%)
      ‚è≥ NBN LSTM: Epoch 40/50 (80%)
       ‚úÖ TD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.7643 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TD LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.408629
         RMSE: 0.639241
         R¬≤ Score: -0.5606 (Poor - 56.1% variance explained)
      üîπ NBN: Training TCN (50 epochs)...
      ‚è≥ NBN TCN: Epoch 10/50 (20%)
      ‚è≥ NBN TCN: Epoch 20/50 (40%)
      ‚è≥ NBN TCN: Epoch 30/50 (60%)
      ‚è≥ NBN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.364594
         RMSE: 0.603816
         R¬≤ Score: -0.3924
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NBN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NBN Random Forest: Starting GridSearchCV fit...
       ‚úÖ TD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=5.7948 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TD XGBoost: Starting GridSearchCV fit...
       ‚úÖ NBN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.8072 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NBN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NBN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.7879 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NBN XGBoost: Starting GridSearchCV fit...
       ‚úÖ ASLE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=20.9265 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.1762
    - TCN: MSE=0.0900
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0900
        ‚Ä¢ LSTM: MSE=0.1762
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.7208
        ‚Ä¢ XGBoost: MSE=20.9265
        ‚Ä¢ Random Forest: MSE=22.4384
   ‚úÖ ASLE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ASLE (TargetReturn): TCN with MSE=0.0900
üêõ DEBUG: ASLE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ASLE.
üêõ DEBUG: ASLE - Moving model to CPU before return...
üêõ DEBUG [00:21:57.708]: ASLE - Returning result metadata...
üêõ DEBUG: train_worker started for IDV
üêõ DEBUG [00:21:57.709]: Main received result for ASLE
  ‚öôÔ∏è Training models for IDV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - IDV: Initiating feature extraction for training.
  [DIAGNOSTIC] IDV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IDV: rows after features available: 126
üéØ IDV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IDV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IDV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IDV: Training LSTM (50 epochs)...
      ‚è≥ IDV LSTM: Epoch 10/50 (20%)
      ‚è≥ IDV LSTM: Epoch 20/50 (40%)
      ‚è≥ IDV LSTM: Epoch 30/50 (60%)
      ‚è≥ IDV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.124701
         RMSE: 0.353130
         R¬≤ Score: -1.0594 (Poor - 105.9% variance explained)
      üîπ IDV: Training TCN (50 epochs)...
      ‚è≥ IDV TCN: Epoch 10/50 (20%)
      ‚è≥ IDV TCN: Epoch 20/50 (40%)
       ‚úÖ CLM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.6265 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.7520
    - TCN: MSE=0.4528
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4528
        ‚Ä¢ LSTM: MSE=0.7520
        ‚Ä¢ XGBoost: MSE=4.6265
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.6872
        ‚Ä¢ Random Forest: MSE=6.9483
   ‚úÖ CLM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CLM (TargetReturn): TCN with MSE=0.4528
üêõ DEBUG: CLM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CLM.
üêõ DEBUG: CLM - Moving model to CPU before return...
üêõ DEBUG [00:22:00.246]: CLM - Returning result metadata...
üêõ DEBUG: train_worker started for AMZY
üêõ DEBUG [00:22:00.247]: Main received result for CLM
üêõ DEBUG: Training progress: 872/959 done
  ‚öôÔ∏è Training models for AMZY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - AMZY: Initiating feature extraction for training.
  [DIAGNOSTIC] AMZY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMZY: rows after features available: 126
üéØ AMZY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ IDV TCN: Epoch 30/50 (60%)
  [DIAGNOSTIC] AMZY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMZY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMZY: Training LSTM (50 epochs)...
      ‚è≥ IDV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.068407
         RMSE: 0.261547
         R¬≤ Score: -0.1297
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IDV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IDV Random Forest: Starting GridSearchCV fit...
      ‚è≥ AMZY LSTM: Epoch 10/50 (20%)
      ‚è≥ AMZY LSTM: Epoch 20/50 (40%)
      ‚è≥ AMZY LSTM: Epoch 30/50 (60%)
      ‚è≥ AMZY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.587708
         RMSE: 0.766621
         R¬≤ Score: -0.9959 (Poor - 99.6% variance explained)
      üîπ AMZY: Training TCN (50 epochs)...
      ‚è≥ AMZY TCN: Epoch 10/50 (20%)
      ‚è≥ AMZY TCN: Epoch 20/50 (40%)
      ‚è≥ AMZY TCN: Epoch 30/50 (60%)
      ‚è≥ AMZY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.615065
         RMSE: 0.784261
         R¬≤ Score: -1.0888
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMZY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMZY Random Forest: Starting GridSearchCV fit...
       ‚úÖ IDV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.5850 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IDV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IDV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=3.7965 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IDV XGBoost: Starting GridSearchCV fit...
       ‚úÖ WUGI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=10.2557 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.3s
    - LSTM: MSE=0.5002
    - TCN: MSE=0.5050
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 119.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5002
        ‚Ä¢ TCN: MSE=0.5050
        ‚Ä¢ XGBoost: MSE=10.2557
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.8630
        ‚Ä¢ Random Forest: MSE=12.0721
   ‚úÖ WUGI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WUGI (TargetReturn): LSTM with MSE=0.5002
üêõ DEBUG: WUGI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WUGI.
üêõ DEBUG: WUGI - Moving model to CPU before return...
üêõ DEBUG [00:22:05.612]: WUGI - Returning result metadata...
üêõ DEBUG: train_worker started for NRIM
  ‚öôÔ∏è Training models for NRIM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - NRIM: Initiating feature extraction for training.
  [DIAGNOSTIC] NRIM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NRIM: rows after features available: 126
üéØ NRIM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NRIM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NRIM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NRIM: Training LSTM (50 epochs)...
      ‚è≥ NRIM LSTM: Epoch 10/50 (20%)
       ‚úÖ AMZY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.0112 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMZY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ NRIM LSTM: Epoch 20/50 (40%)
       ‚úÖ PODD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=21.5154 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.7s
    - LSTM: MSE=0.4363
    - TCN: MSE=0.2657
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2657
        ‚Ä¢ LSTM: MSE=0.4363
        ‚Ä¢ Random Forest: MSE=15.5504
        ‚Ä¢ LightGBM Regressor (CPU): MSE=17.8610
        ‚Ä¢ XGBoost: MSE=21.5154
   ‚úÖ PODD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PODD (TargetReturn): TCN with MSE=0.2657
üêõ DEBUG: PODD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PODD.
üêõ DEBUG: PODD - Moving model to CPU before return...
üêõ DEBUG [00:22:06.779]: PODD - Returning result metadata...
üêõ DEBUG [00:22:06.779]: Main received result for PODD
üêõ DEBUG [00:22:06.779]: Main received result for WUGI
üêõ DEBUG: train_worker started for JTEK
  ‚öôÔ∏è Training models for JTEK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - JTEK: Initiating feature extraction for training.
  [DIAGNOSTIC] JTEK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ JTEK: rows after features available: 126
üéØ JTEK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] JTEK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö JTEK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ JTEK: Training LSTM (50 epochs)...
       ‚úÖ AMZY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.5857 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMZY XGBoost: Starting GridSearchCV fit...
      ‚è≥ NRIM LSTM: Epoch 30/50 (60%)
      ‚è≥ JTEK LSTM: Epoch 10/50 (20%)
      ‚è≥ NRIM LSTM: Epoch 40/50 (80%)
      ‚è≥ JTEK LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.429816
         RMSE: 0.655604
         R¬≤ Score: -0.6945 (Poor - 69.4% variance explained)
      üîπ NRIM: Training TCN (50 epochs)...
      ‚è≥ NRIM TCN: Epoch 10/50 (20%)
      ‚è≥ NRIM TCN: Epoch 20/50 (40%)
      ‚è≥ NRIM TCN: Epoch 30/50 (60%)
      ‚è≥ JTEK LSTM: Epoch 30/50 (60%)
      ‚è≥ NRIM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.468846
         RMSE: 0.684723
         R¬≤ Score: -0.8484
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NRIM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NRIM Random Forest: Starting GridSearchCV fit...
      ‚è≥ JTEK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.577691
         RMSE: 0.760060
         R¬≤ Score: -1.1000 (Poor - 110.0% variance explained)
      üîπ JTEK: Training TCN (50 epochs)...
      ‚è≥ JTEK TCN: Epoch 10/50 (20%)
      ‚è≥ JTEK TCN: Epoch 20/50 (40%)
      ‚è≥ JTEK TCN: Epoch 30/50 (60%)
      ‚è≥ JTEK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.589595
         RMSE: 0.767851
         R¬≤ Score: -1.1433
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä JTEK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ JTEK Random Forest: Starting GridSearchCV fit...
       ‚úÖ NRIM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.4566 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NRIM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ NRIM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=20.6709 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NRIM XGBoost: Starting GridSearchCV fit...
       ‚úÖ JTEK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=12.7327 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ JTEK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ JTEK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.8134 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ JTEK XGBoost: Starting GridSearchCV fit...
       ‚úÖ RIGL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.5785 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.0s
    - LSTM: MSE=0.1061
    - TCN: MSE=0.0613
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0613
        ‚Ä¢ LSTM: MSE=0.1061
        ‚Ä¢ Random Forest: MSE=19.8643
        ‚Ä¢ XGBoost: MSE=25.5785
        ‚Ä¢ LightGBM Regressor (CPU): MSE=38.2390
   ‚úÖ RIGL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RIGL (TargetReturn): TCN with MSE=0.0613
üêõ DEBUG: RIGL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RIGL.
üêõ DEBUG: RIGL - Moving model to CPU before return...
üêõ DEBUG [00:22:20.840]: RIGL - Returning result metadata...
üêõ DEBUG: train_worker started for AMZN
üêõ DEBUG [00:22:20.841]: Main received result for RIGL
  ‚öôÔ∏è Training models for AMZN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - AMZN: Initiating feature extraction for training.
  [DIAGNOSTIC] AMZN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AMZN: rows after features available: 126
üéØ AMZN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AMZN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AMZN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AMZN: Training LSTM (50 epochs)...
      ‚è≥ AMZN LSTM: Epoch 10/50 (20%)
      ‚è≥ AMZN LSTM: Epoch 20/50 (40%)
      ‚è≥ AMZN LSTM: Epoch 30/50 (60%)
      ‚è≥ AMZN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.537754
         RMSE: 0.733317
         R¬≤ Score: -0.8234 (Poor - 82.3% variance explained)
      üîπ AMZN: Training TCN (50 epochs)...
      ‚è≥ AMZN TCN: Epoch 10/50 (20%)
      ‚è≥ AMZN TCN: Epoch 20/50 (40%)
      ‚è≥ AMZN TCN: Epoch 30/50 (60%)
      ‚è≥ AMZN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.597313
         RMSE: 0.772860
         R¬≤ Score: -1.0254
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AMZN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AMZN Random Forest: Starting GridSearchCV fit...
       ‚úÖ AMZN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.2529 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.4s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AMZN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AMZN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=9.0589 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AMZN XGBoost: Starting GridSearchCV fit...
       ‚úÖ OZK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.5853 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 119.3s
    - LSTM: MSE=0.5336
    - TCN: MSE=0.4221
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4221
        ‚Ä¢ LSTM: MSE=0.5336
        ‚Ä¢ Random Forest: MSE=16.5079
        ‚Ä¢ XGBoost: MSE=19.5853
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.5289
   ‚úÖ OZK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for OZK (TargetReturn): TCN with MSE=0.4221
üêõ DEBUG: OZK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for OZK.
üêõ DEBUG: OZK - Moving model to CPU before return...
üêõ DEBUG [00:22:39.680]: OZK - Returning result metadata...
üêõ DEBUG [00:22:39.681]: Main received result for OZK
üêõ DEBUG: Training progress: 876/959 done
üêõ DEBUG: train_worker started for CFG
  ‚öôÔ∏è Training models for CFG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CFG: Initiating feature extraction for training.
  [DIAGNOSTIC] CFG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CFG: rows after features available: 126
üéØ CFG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CFG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CFG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CFG: Training LSTM (50 epochs)...
       ‚úÖ HEI-A XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=12.9167 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.2211
    - TCN: MSE=0.1008
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1008
        ‚Ä¢ LSTM: MSE=0.2211
        ‚Ä¢ XGBoost: MSE=12.9167
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.8297
        ‚Ä¢ Random Forest: MSE=19.1211
   ‚úÖ HEI-A: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HEI-A (TargetReturn): TCN with MSE=0.1008
üêõ DEBUG: HEI-A - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HEI-A.
üêõ DEBUG: HEI-A - Moving model to CPU before return...
üêõ DEBUG [00:22:39.730]: HEI-A - Returning result metadata...
üêõ DEBUG [00:22:39.730]: Main received result for HEI-A
üêõ DEBUG: train_worker started for CRMT
  ‚öôÔ∏è Training models for CRMT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - CRMT: Initiating feature extraction for training.
  [DIAGNOSTIC] CRMT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CRMT: rows after features available: 126
üéØ CRMT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CRMT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CRMT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CRMT: Training LSTM (50 epochs)...
      ‚è≥ CFG LSTM: Epoch 10/50 (20%)
      ‚è≥ CRMT LSTM: Epoch 10/50 (20%)
      ‚è≥ CFG LSTM: Epoch 20/50 (40%)
      ‚è≥ CRMT LSTM: Epoch 20/50 (40%)
      ‚è≥ CFG LSTM: Epoch 30/50 (60%)
      ‚è≥ CRMT LSTM: Epoch 30/50 (60%)
      ‚è≥ CFG LSTM: Epoch 40/50 (80%)
      ‚è≥ CRMT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.707302
         RMSE: 0.841013
         R¬≤ Score: -0.9777 (Poor - 97.8% variance explained)
      üîπ CFG: Training TCN (50 epochs)...
      ‚è≥ CFG TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.288614
         RMSE: 0.537228
         R¬≤ Score: -1.0727 (Poor - 107.3% variance explained)
      üîπ CRMT: Training TCN (50 epochs)...
      ‚è≥ CFG TCN: Epoch 20/50 (40%)
      ‚è≥ CRMT TCN: Epoch 10/50 (20%)
      ‚è≥ CFG TCN: Epoch 30/50 (60%)
      ‚è≥ CRMT TCN: Epoch 20/50 (40%)
      ‚è≥ CFG TCN: Epoch 40/50 (80%)
      ‚è≥ CRMT TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.678124
         RMSE: 0.823483
         R¬≤ Score: -0.8961
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CFG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CFG Random Forest: Starting GridSearchCV fit...
      ‚è≥ CRMT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.161722
         RMSE: 0.402147
         R¬≤ Score: -0.1614
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CRMT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CRMT Random Forest: Starting GridSearchCV fit...
       ‚úÖ CFG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.7059 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CFG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CRMT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.7500 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CRMT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CFG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=12.1561 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CFG XGBoost: Starting GridSearchCV fit...
       ‚úÖ CRMT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.7290 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CRMT XGBoost: Starting GridSearchCV fit...
       ‚úÖ NTB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.8765 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 115.3s
    - LSTM: MSE=0.3098
    - TCN: MSE=0.2071
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2071
        ‚Ä¢ LSTM: MSE=0.3098
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.9638
        ‚Ä¢ Random Forest: MSE=9.5615
        ‚Ä¢ XGBoost: MSE=12.8765
   ‚úÖ NTB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NTB (TargetReturn): TCN with MSE=0.2071
üêõ DEBUG: NTB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NTB.
üêõ DEBUG: NTB - Moving model to CPU before return...
üêõ DEBUG [00:23:01.690]: NTB - Returning result metadata...
üêõ DEBUG [00:23:01.690]: Main received result for NTB
üêõ DEBUG: train_worker started for DTST
  ‚öôÔ∏è Training models for DTST (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - DTST: Initiating feature extraction for training.
  [DIAGNOSTIC] DTST: fetch_training_data - Initial data rows: 205
   ‚Ü≥ DTST: rows after features available: 126
üéØ DTST: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] DTST: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö DTST: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ DTST: Training LSTM (50 epochs)...
      ‚è≥ DTST LSTM: Epoch 10/50 (20%)
      ‚è≥ DTST LSTM: Epoch 20/50 (40%)
      ‚è≥ DTST LSTM: Epoch 30/50 (60%)
      ‚è≥ DTST LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.266559
         RMSE: 0.516294
         R¬≤ Score: -1.0866 (Poor - 108.7% variance explained)
      üîπ DTST: Training TCN (50 epochs)...
      ‚è≥ DTST TCN: Epoch 10/50 (20%)
      ‚è≥ DTST TCN: Epoch 20/50 (40%)
      ‚è≥ DTST TCN: Epoch 30/50 (60%)
      ‚è≥ DTST TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.158933
         RMSE: 0.398664
         R¬≤ Score: -0.2441
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä DTST: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ DTST Random Forest: Starting GridSearchCV fit...
       ‚úÖ DTST Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=69.2873 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ DTST LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ DTST LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=61.7289 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 1.3s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ DTST XGBoost: Starting GridSearchCV fit...
       ‚úÖ FSK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.5077 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.2s
    - LSTM: MSE=1.0337
    - TCN: MSE=0.5763
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5763
        ‚Ä¢ LSTM: MSE=1.0337
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.2863
        ‚Ä¢ Random Forest: MSE=4.4508
        ‚Ä¢ XGBoost: MSE=7.5077
   ‚úÖ FSK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FSK (TargetReturn): TCN with MSE=0.5763
üêõ DEBUG: FSK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FSK.
üêõ DEBUG: FSK - Moving model to CPU before return...
üêõ DEBUG [00:23:12.859]: FSK - Returning result metadata...
üêõ DEBUG [00:23:12.859]: Main received result for FSK
üêõ DEBUG: train_worker started for NVMI
  ‚öôÔ∏è Training models for NVMI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - NVMI: Initiating feature extraction for training.
  [DIAGNOSTIC] NVMI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ NVMI: rows after features available: 126
üéØ NVMI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] NVMI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö NVMI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ NVMI: Training LSTM (50 epochs)...
      ‚è≥ NVMI LSTM: Epoch 10/50 (20%)
      ‚è≥ NVMI LSTM: Epoch 20/50 (40%)
      ‚è≥ NVMI LSTM: Epoch 30/50 (60%)
      ‚è≥ NVMI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.735433
         RMSE: 0.857574
         R¬≤ Score: -0.8318 (Poor - 83.2% variance explained)
      üîπ NVMI: Training TCN (50 epochs)...
      ‚è≥ NVMI TCN: Epoch 10/50 (20%)
      ‚è≥ NVMI TCN: Epoch 20/50 (40%)
      ‚è≥ NVMI TCN: Epoch 30/50 (60%)
      ‚è≥ NVMI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 1.014097
         RMSE: 1.007024
         R¬≤ Score: -1.5259
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä NVMI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ NVMI Random Forest: Starting GridSearchCV fit...
       ‚úÖ PSKY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.1577 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.7s
    - LSTM: MSE=0.4523
    - TCN: MSE=0.1804
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1804
        ‚Ä¢ LSTM: MSE=0.4523
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.2055
        ‚Ä¢ XGBoost: MSE=7.1577
        ‚Ä¢ Random Forest: MSE=7.2589
   ‚úÖ PSKY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PSKY (TargetReturn): TCN with MSE=0.1804
üêõ DEBUG: PSKY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PSKY.
üêõ DEBUG: PSKY - Moving model to CPU before return...
üêõ DEBUG [00:23:15.546]: PSKY - Returning result metadata...
üêõ DEBUG: train_worker started for SNX
üêõ DEBUG [00:23:15.547]: Main received result for PSKY
üêõ DEBUG: Training progress: 880/959 done
  ‚öôÔ∏è Training models for SNX (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - SNX: Initiating feature extraction for training.
  [DIAGNOSTIC] SNX: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SNX: rows after features available: 126
üéØ SNX: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SNX: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SNX: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SNX: Training LSTM (50 epochs)...
      ‚è≥ SNX LSTM: Epoch 10/50 (20%)
      ‚è≥ SNX LSTM: Epoch 20/50 (40%)
      ‚è≥ SNX LSTM: Epoch 30/50 (60%)
      ‚è≥ SNX LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.761255
         RMSE: 0.872500
         R¬≤ Score: -0.9056 (Poor - 90.6% variance explained)
      üîπ SNX: Training TCN (50 epochs)...
      ‚è≥ SNX TCN: Epoch 10/50 (20%)
      ‚è≥ SNX TCN: Epoch 20/50 (40%)
      ‚è≥ SNX TCN: Epoch 30/50 (60%)
       ‚úÖ NVMI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.2141 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ NVMI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ SNX TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.840150
         RMSE: 0.916597
         R¬≤ Score: -1.1031
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SNX: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SNX Random Forest: Starting GridSearchCV fit...
       ‚úÖ NVMI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=35.9439 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ NVMI XGBoost: Starting GridSearchCV fit...
       ‚úÖ NI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3.3570 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.1s
    - LSTM: MSE=0.2573
    - TCN: MSE=0.1524
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1524
        ‚Ä¢ LSTM: MSE=0.2573
        ‚Ä¢ Random Forest: MSE=3.2144
        ‚Ä¢ XGBoost: MSE=3.3570
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.2746
   ‚úÖ NI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NI (TargetReturn): TCN with MSE=0.1524
üêõ DEBUG: NI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NI.
üêõ DEBUG: NI - Moving model to CPU before return...
üêõ DEBUG [00:23:20.334]: NI - Returning result metadata...
üêõ DEBUG: train_worker started for BRW
üêõ DEBUG [00:23:20.335]: Main received result for NI
  ‚öôÔ∏è Training models for BRW (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BRW: Initiating feature extraction for training.
  [DIAGNOSTIC] BRW: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BRW: rows after features available: 126
üéØ BRW: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BRW: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BRW: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BRW: Training LSTM (50 epochs)...
      ‚è≥ BRW LSTM: Epoch 10/50 (20%)
       ‚úÖ SNX Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=16.2454 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SNX LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ BRW LSTM: Epoch 20/50 (40%)
      ‚è≥ BRW LSTM: Epoch 30/50 (60%)
       ‚úÖ SNX LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=19.1985 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SNX XGBoost: Starting GridSearchCV fit...
      ‚è≥ BRW LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.435834
         RMSE: 0.660177
         R¬≤ Score: -0.7221 (Poor - 72.2% variance explained)
      üîπ BRW: Training TCN (50 epochs)...
      ‚è≥ BRW TCN: Epoch 10/50 (20%)
      ‚è≥ BRW TCN: Epoch 20/50 (40%)
      ‚è≥ BRW TCN: Epoch 30/50 (60%)
      ‚è≥ BRW TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.424542
         RMSE: 0.651569
         R¬≤ Score: -0.6775
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BRW: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BRW Random Forest: Starting GridSearchCV fit...
       ‚úÖ BRW Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=2.8493 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BRW LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BRW LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=2.6080 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BRW XGBoost: Starting GridSearchCV fit...
       ‚úÖ HDB XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.5453 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.1851
    - TCN: MSE=0.0959
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.0 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0959
        ‚Ä¢ LSTM: MSE=0.1851
        ‚Ä¢ XGBoost: MSE=6.5453
        ‚Ä¢ Random Forest: MSE=9.8174
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.9945
   ‚úÖ HDB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HDB (TargetReturn): TCN with MSE=0.0959
üêõ DEBUG: HDB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HDB.
üêõ DEBUG: HDB - Moving model to CPU before return...
üêõ DEBUG [00:23:26.200]: HDB - Returning result metadata...
üêõ DEBUG: train_worker started for GNE
üêõ DEBUG [00:23:26.202]: Main received result for HDB
  ‚öôÔ∏è Training models for GNE (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - GNE: Initiating feature extraction for training.
  [DIAGNOSTIC] GNE: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GNE: rows after features available: 126
üéØ GNE: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GNE: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GNE: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GNE: Training LSTM (50 epochs)...
      ‚è≥ GNE LSTM: Epoch 10/50 (20%)
      ‚è≥ GNE LSTM: Epoch 20/50 (40%)
      ‚è≥ GNE LSTM: Epoch 30/50 (60%)
      ‚è≥ GNE LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.625229
         RMSE: 0.790714
         R¬≤ Score: -1.5998 (Poor - 160.0% variance explained)
      üîπ GNE: Training TCN (50 epochs)...
      ‚è≥ GNE TCN: Epoch 10/50 (20%)
      ‚è≥ GNE TCN: Epoch 20/50 (40%)
      ‚è≥ GNE TCN: Epoch 30/50 (60%)
      ‚è≥ GNE TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.478896
         RMSE: 0.692023
         R¬≤ Score: -0.9913
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GNE: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GNE Random Forest: Starting GridSearchCV fit...
       ‚úÖ GNE Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=32.2844 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GNE LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GNE LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.7663 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GNE XGBoost: Starting GridSearchCV fit...
       ‚úÖ TG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.6053 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.2986
    - TCN: MSE=0.2507
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.1 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2507
        ‚Ä¢ LSTM: MSE=0.2986
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.3322
        ‚Ä¢ Random Forest: MSE=23.2351
        ‚Ä¢ XGBoost: MSE=23.6053
   ‚úÖ TG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TG (TargetReturn): TCN with MSE=0.2507
üêõ DEBUG: TG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TG.
üêõ DEBUG: TG - Moving model to CPU before return...
üêõ DEBUG [00:23:46.289]: TG - Returning result metadata...
üêõ DEBUG: train_worker started for CHWY
üêõ DEBUG [00:23:46.292]: Main received result for TG
  ‚öôÔ∏è Training models for CHWY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CHWY: Initiating feature extraction for training.
  [DIAGNOSTIC] CHWY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CHWY: rows after features available: 126
üéØ CHWY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CHWY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CHWY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CHWY: Training LSTM (50 epochs)...
      ‚è≥ CHWY LSTM: Epoch 10/50 (20%)
      ‚è≥ CHWY LSTM: Epoch 20/50 (40%)
      ‚è≥ CHWY LSTM: Epoch 30/50 (60%)
      ‚è≥ CHWY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.480920
         RMSE: 0.693484
         R¬≤ Score: -1.0202 (Poor - 102.0% variance explained)
      üîπ CHWY: Training TCN (50 epochs)...
      ‚è≥ CHWY TCN: Epoch 10/50 (20%)
       ‚úÖ TD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=6.0086 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.8s
    - LSTM: MSE=0.3293
    - TCN: MSE=0.3956
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3293
        ‚Ä¢ TCN: MSE=0.3956
        ‚Ä¢ Random Forest: MSE=3.7643
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.7948
        ‚Ä¢ XGBoost: MSE=6.0086
   ‚úÖ TD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TD (TargetReturn): LSTM with MSE=0.3293
üêõ DEBUG: TD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TD.
üêõ DEBUG: TD - Moving model to CPU before return...
üêõ DEBUG [00:23:48.913]: TD - Returning result metadata...
üêõ DEBUG: train_worker started for WRB
üêõ DEBUG [00:23:48.914]: Main received result for TD
üêõ DEBUG: Training progress: 884/959 done
  ‚öôÔ∏è Training models for WRB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - WRB: Initiating feature extraction for training.
  [DIAGNOSTIC] WRB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WRB: rows after features available: 126
üéØ WRB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WRB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WRB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WRB: Training LSTM (50 epochs)...
      ‚è≥ CHWY TCN: Epoch 20/50 (40%)
      ‚è≥ CHWY TCN: Epoch 30/50 (60%)
      ‚è≥ CHWY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.257872
         RMSE: 0.507811
         R¬≤ Score: -0.0832
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CHWY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CHWY Random Forest: Starting GridSearchCV fit...
      ‚è≥ WRB LSTM: Epoch 10/50 (20%)
      ‚è≥ WRB LSTM: Epoch 20/50 (40%)
       ‚úÖ NBN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.8595 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.4s
    - LSTM: MSE=0.4086
    - TCN: MSE=0.3646
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3646
        ‚Ä¢ LSTM: MSE=0.4086
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.7879
        ‚Ä¢ Random Forest: MSE=8.8072
        ‚Ä¢ XGBoost: MSE=8.8595
   ‚úÖ NBN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NBN (TargetReturn): TCN with MSE=0.3646
üêõ DEBUG: NBN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NBN.
üêõ DEBUG: NBN - Moving model to CPU before return...
üêõ DEBUG [00:23:50.276]: NBN - Returning result metadata...
üêõ DEBUG [00:23:50.276]: Main received result for NBN
üêõ DEBUG: train_worker started for IQM
  ‚öôÔ∏è Training models for IQM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - IQM: Initiating feature extraction for training.
  [DIAGNOSTIC] IQM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IQM: rows after features available: 126
üéØ IQM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IQM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IQM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IQM: Training LSTM (50 epochs)...
      ‚è≥ WRB LSTM: Epoch 30/50 (60%)
      ‚è≥ IQM LSTM: Epoch 10/50 (20%)
      ‚è≥ WRB LSTM: Epoch 40/50 (80%)
      ‚è≥ IQM LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.542204
         RMSE: 0.736345
         R¬≤ Score: -0.5292 (Poor - 52.9% variance explained)
      üîπ WRB: Training TCN (50 epochs)...
      ‚è≥ WRB TCN: Epoch 10/50 (20%)
      ‚è≥ WRB TCN: Epoch 20/50 (40%)
      ‚è≥ WRB TCN: Epoch 30/50 (60%)
      ‚è≥ WRB TCN: Epoch 40/50 (80%)
      ‚è≥ IQM LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.627321
         RMSE: 0.792036
         R¬≤ Score: -0.7693
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WRB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WRB Random Forest: Starting GridSearchCV fit...
       ‚úÖ CHWY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.7475 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CHWY LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IQM LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.415674
         RMSE: 0.644728
         R¬≤ Score: -0.7100 (Poor - 71.0% variance explained)
      üîπ IQM: Training TCN (50 epochs)...
       ‚úÖ CHWY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=19.5732 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CHWY XGBoost: Starting GridSearchCV fit...
      ‚è≥ IQM TCN: Epoch 10/50 (20%)
      ‚è≥ IQM TCN: Epoch 20/50 (40%)
      ‚è≥ IQM TCN: Epoch 30/50 (60%)
      ‚è≥ IQM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.490837
         RMSE: 0.700598
         R¬≤ Score: -1.0192
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IQM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IQM Random Forest: Starting GridSearchCV fit...
       ‚úÖ WRB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.0135 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WRB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WRB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.6137 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WRB XGBoost: Starting GridSearchCV fit...
       ‚úÖ IQM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.6776 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IQM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IQM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=24.6909 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IQM XGBoost: Starting GridSearchCV fit...
       ‚úÖ IDV XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3.4020 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.5s
    - LSTM: MSE=0.1247
    - TCN: MSE=0.0684
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0684
        ‚Ä¢ LSTM: MSE=0.1247
        ‚Ä¢ XGBoost: MSE=3.4020
        ‚Ä¢ Random Forest: MSE=3.5850
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.7965
   ‚úÖ IDV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IDV (TargetReturn): TCN with MSE=0.0684
üêõ DEBUG: IDV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IDV.
üêõ DEBUG: IDV - Moving model to CPU before return...
üêõ DEBUG [00:24:02.100]: IDV - Returning result metadata...
üêõ DEBUG [00:24:02.100]: Main received result for IDV
üêõ DEBUG: train_worker started for HEI
  ‚öôÔ∏è Training models for HEI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - HEI: Initiating feature extraction for training.
  [DIAGNOSTIC] HEI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HEI: rows after features available: 126
üéØ HEI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HEI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HEI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HEI: Training LSTM (50 epochs)...
      ‚è≥ HEI LSTM: Epoch 10/50 (20%)
      ‚è≥ HEI LSTM: Epoch 20/50 (40%)
      ‚è≥ HEI LSTM: Epoch 30/50 (60%)
      ‚è≥ HEI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.165587
         RMSE: 0.406924
         R¬≤ Score: -0.6942 (Poor - 69.4% variance explained)
      üîπ HEI: Training TCN (50 epochs)...
      ‚è≥ HEI TCN: Epoch 10/50 (20%)
      ‚è≥ HEI TCN: Epoch 20/50 (40%)
      ‚è≥ HEI TCN: Epoch 30/50 (60%)
       ‚úÖ AMZY XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=5.3010 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.5877
    - TCN: MSE=0.6151
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5877
        ‚Ä¢ TCN: MSE=0.6151
        ‚Ä¢ XGBoost: MSE=5.3010
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.5857
        ‚Ä¢ Random Forest: MSE=7.0112
   ‚úÖ AMZY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMZY (TargetReturn): LSTM with MSE=0.5877
üêõ DEBUG: AMZY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMZY.
üêõ DEBUG: AMZY - Moving model to CPU before return...
üêõ DEBUG [00:24:05.050]: AMZY - Returning result metadata...
üêõ DEBUG: train_worker started for TRMK
üêõ DEBUG [00:24:05.052]: Main received result for AMZY
      ‚è≥ HEI TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for TRMK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - TRMK: Initiating feature extraction for training.
  [DIAGNOSTIC] TRMK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TRMK: rows after features available: 126
üéØ TRMK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TRMK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TRMK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TRMK: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.099099
         RMSE: 0.314800
         R¬≤ Score: -0.0139
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HEI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HEI Random Forest: Starting GridSearchCV fit...
      ‚è≥ TRMK LSTM: Epoch 10/50 (20%)
      ‚è≥ TRMK LSTM: Epoch 20/50 (40%)
      ‚è≥ TRMK LSTM: Epoch 30/50 (60%)
      ‚è≥ TRMK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.556660
         RMSE: 0.746097
         R¬≤ Score: -0.8397 (Poor - 84.0% variance explained)
      üîπ TRMK: Training TCN (50 epochs)...
      ‚è≥ TRMK TCN: Epoch 10/50 (20%)
      ‚è≥ TRMK TCN: Epoch 20/50 (40%)
      ‚è≥ TRMK TCN: Epoch 30/50 (60%)
       ‚úÖ JTEK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.1409 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.7s
    - LSTM: MSE=0.5777
    - TCN: MSE=0.5896
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.9 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5777
        ‚Ä¢ TCN: MSE=0.5896
        ‚Ä¢ Random Forest: MSE=12.7327
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.8134
        ‚Ä¢ XGBoost: MSE=16.1409
   ‚úÖ JTEK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for JTEK (TargetReturn): LSTM with MSE=0.5777
üêõ DEBUG: JTEK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for JTEK.
üêõ DEBUG: JTEK - Moving model to CPU before return...
üêõ DEBUG [00:24:07.948]: JTEK - Returning result metadata...
üêõ DEBUG: train_worker started for EVC
  ‚öôÔ∏è Training models for EVC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - EVC: Initiating feature extraction for training.
  [DIAGNOSTIC] EVC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EVC: rows after features available: 126
üéØ EVC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
      ‚è≥ TRMK TCN: Epoch 40/50 (80%)
  [DIAGNOSTIC] EVC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EVC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EVC: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.450054
         RMSE: 0.670861
         R¬≤ Score: -0.4873
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TRMK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TRMK Random Forest: Starting GridSearchCV fit...
       ‚úÖ HEI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=29.0320 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HEI LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ EVC LSTM: Epoch 10/50 (20%)
       ‚úÖ HEI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=25.1408 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HEI XGBoost: Starting GridSearchCV fit...
       ‚úÖ NRIM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=35.7202 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.7s
    - LSTM: MSE=0.4298
    - TCN: MSE=0.4688
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4298
        ‚Ä¢ TCN: MSE=0.4688
        ‚Ä¢ LightGBM Regressor (CPU): MSE=20.6709
        ‚Ä¢ Random Forest: MSE=28.4566
        ‚Ä¢ XGBoost: MSE=35.7202
   ‚úÖ NRIM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NRIM (TargetReturn): LSTM with MSE=0.4298
üêõ DEBUG: NRIM - train_and_evaluate_models completed
      ‚è≥ EVC LSTM: Epoch 20/50 (40%)
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NRIM.
üêõ DEBUG: NRIM - Moving model to CPU before return...
üêõ DEBUG [00:24:08.961]: NRIM - Returning result metadata...
üêõ DEBUG: train_worker started for PAM
üêõ DEBUG [00:24:08.962]: Main received result for NRIM
üêõ DEBUG: Training progress: 888/959 done
üêõ DEBUG [00:24:08.962]: Main received result for JTEK
  ‚öôÔ∏è Training models for PAM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - PAM: Initiating feature extraction for training.
  [DIAGNOSTIC] PAM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PAM: rows after features available: 126
üéØ PAM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PAM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PAM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PAM: Training LSTM (50 epochs)...
      ‚è≥ EVC LSTM: Epoch 30/50 (60%)
      ‚è≥ PAM LSTM: Epoch 10/50 (20%)
      ‚è≥ EVC LSTM: Epoch 40/50 (80%)
      ‚è≥ PAM LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.586419
         RMSE: 0.765780
         R¬≤ Score: -1.2725 (Poor - 127.2% variance explained)
      üîπ EVC: Training TCN (50 epochs)...
      ‚è≥ EVC TCN: Epoch 10/50 (20%)
      ‚è≥ PAM LSTM: Epoch 30/50 (60%)
      ‚è≥ EVC TCN: Epoch 20/50 (40%)
      ‚è≥ EVC TCN: Epoch 30/50 (60%)
      ‚è≥ EVC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.393355
         RMSE: 0.627180
         R¬≤ Score: -0.5243
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EVC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EVC Random Forest: Starting GridSearchCV fit...
      ‚è≥ PAM LSTM: Epoch 40/50 (80%)
       ‚úÖ TRMK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9766 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TRMK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.134033
         RMSE: 0.366105
         R¬≤ Score: -1.5915 (Poor - 159.2% variance explained)
      üîπ PAM: Training TCN (50 epochs)...
      ‚è≥ PAM TCN: Epoch 10/50 (20%)
      ‚è≥ PAM TCN: Epoch 20/50 (40%)
      ‚è≥ PAM TCN: Epoch 30/50 (60%)
      ‚è≥ PAM TCN: Epoch 40/50 (80%)
       ‚úÖ TRMK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.2738 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TRMK XGBoost: Starting GridSearchCV fit...
      üìä TCN Regression Metrics:
         MSE: 0.053184
         RMSE: 0.230616
         R¬≤ Score: -0.0283
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PAM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PAM Random Forest: Starting GridSearchCV fit...
       ‚úÖ EVC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.0108 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EVC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EVC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=23.0925 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EVC XGBoost: Starting GridSearchCV fit...
       ‚úÖ PAM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.6689 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PAM LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PAM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.8726 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PAM XGBoost: Starting GridSearchCV fit...
       ‚úÖ AMZN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.2395 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.7s
    - LSTM: MSE=0.5378
    - TCN: MSE=0.5973
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5378
        ‚Ä¢ TCN: MSE=0.5973
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.0589
        ‚Ä¢ Random Forest: MSE=11.2529
        ‚Ä¢ XGBoost: MSE=12.2395
   ‚úÖ AMZN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AMZN (TargetReturn): LSTM with MSE=0.5378
üêõ DEBUG: AMZN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AMZN.
üêõ DEBUG: AMZN - Moving model to CPU before return...
üêõ DEBUG [00:24:26.427]: AMZN - Returning result metadata...
üêõ DEBUG: train_worker started for COWG
üêõ DEBUG [00:24:26.428]: Main received result for AMZN
  ‚öôÔ∏è Training models for COWG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - COWG: Initiating feature extraction for training.
  [DIAGNOSTIC] COWG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ COWG: rows after features available: 126
üéØ COWG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] COWG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö COWG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ COWG: Training LSTM (50 epochs)...
      ‚è≥ COWG LSTM: Epoch 10/50 (20%)
      ‚è≥ COWG LSTM: Epoch 20/50 (40%)
      ‚è≥ COWG LSTM: Epoch 30/50 (60%)
      ‚è≥ COWG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.336860
         RMSE: 0.580396
         R¬≤ Score: -0.6269 (Poor - 62.7% variance explained)
      üîπ COWG: Training TCN (50 epochs)...
      ‚è≥ COWG TCN: Epoch 10/50 (20%)
      ‚è≥ COWG TCN: Epoch 20/50 (40%)
      ‚è≥ COWG TCN: Epoch 30/50 (60%)
      ‚è≥ COWG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.361833
         RMSE: 0.601526
         R¬≤ Score: -0.7475
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä COWG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ COWG Random Forest: Starting GridSearchCV fit...
       ‚úÖ COWG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7736 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ COWG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ COWG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.2289 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ COWG XGBoost: Starting GridSearchCV fit...
       ‚úÖ CFG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.7892 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.1s
    - LSTM: MSE=0.7073
    - TCN: MSE=0.6781
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.6 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6781
        ‚Ä¢ LSTM: MSE=0.7073
        ‚Ä¢ Random Forest: MSE=8.7059
        ‚Ä¢ XGBoost: MSE=8.7892
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.1561
   ‚úÖ CFG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CFG (TargetReturn): TCN with MSE=0.6781
üêõ DEBUG: CFG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CFG.
üêõ DEBUG: CFG - Moving model to CPU before return...
üêõ DEBUG [00:24:44.096]: CFG - Returning result metadata...
üêõ DEBUG: train_worker started for FDP
üêõ DEBUG [00:24:44.100]: Main received result for CFG
  ‚öôÔ∏è Training models for FDP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - FDP: Initiating feature extraction for training.
  [DIAGNOSTIC] FDP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDP: rows after features available: 126
üéØ FDP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDP: Training LSTM (50 epochs)...
      ‚è≥ FDP LSTM: Epoch 10/50 (20%)
      ‚è≥ FDP LSTM: Epoch 20/50 (40%)
      ‚è≥ FDP LSTM: Epoch 30/50 (60%)
      ‚è≥ FDP LSTM: Epoch 40/50 (80%)
       ‚úÖ CRMT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=27.0110 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 120.3s
    - LSTM: MSE=0.2886
    - TCN: MSE=0.1617
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1617
        ‚Ä¢ LSTM: MSE=0.2886
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.7290
        ‚Ä¢ XGBoost: MSE=27.0110
        ‚Ä¢ Random Forest: MSE=28.7500
   ‚úÖ CRMT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CRMT (TargetReturn): TCN with MSE=0.1617
üêõ DEBUG: CRMT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CRMT.
üêõ DEBUG: CRMT - Moving model to CPU before return...
üêõ DEBUG [00:24:46.279]: CRMT - Returning result metadata...
üêõ DEBUG [00:24:46.279]: Main received result for CRMTüêõ DEBUG: train_worker started for PSLV

üêõ DEBUG: Training progress: 892/959 done
  ‚öôÔ∏è Training models for PSLV (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - PSLV: Initiating feature extraction for training.
  [DIAGNOSTIC] PSLV: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PSLV: rows after features available: 126
üéØ PSLV: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PSLV: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PSLV: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PSLV: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.108347
         RMSE: 0.329161
         R¬≤ Score: -0.7669 (Poor - 76.7% variance explained)
      üîπ FDP: Training TCN (50 epochs)...
      ‚è≥ FDP TCN: Epoch 10/50 (20%)
      ‚è≥ PSLV LSTM: Epoch 10/50 (20%)
      ‚è≥ FDP TCN: Epoch 20/50 (40%)
      ‚è≥ FDP TCN: Epoch 30/50 (60%)
      ‚è≥ FDP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.067137
         RMSE: 0.259109
         R¬≤ Score: -0.0949
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDP Random Forest: Starting GridSearchCV fit...
      ‚è≥ PSLV LSTM: Epoch 20/50 (40%)
      ‚è≥ PSLV LSTM: Epoch 30/50 (60%)
      ‚è≥ PSLV LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.363549
         RMSE: 0.602951
         R¬≤ Score: -1.5374 (Poor - 153.7% variance explained)
      üîπ PSLV: Training TCN (50 epochs)...
      ‚è≥ PSLV TCN: Epoch 10/50 (20%)
      ‚è≥ PSLV TCN: Epoch 20/50 (40%)
      ‚è≥ PSLV TCN: Epoch 30/50 (60%)
      ‚è≥ PSLV TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.254322
         RMSE: 0.504303
         R¬≤ Score: -0.7750
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PSLV: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PSLV Random Forest: Starting GridSearchCV fit...
       ‚úÖ FDP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9254 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.2955 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDP XGBoost: Starting GridSearchCV fit...
       ‚úÖ PSLV Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.8152 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PSLV LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PSLV LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=6.4050 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PSLV XGBoost: Starting GridSearchCV fit...
       ‚úÖ DTST XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=141.4334 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 117.4s
    - LSTM: MSE=0.2666
    - TCN: MSE=0.1589
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1589
        ‚Ä¢ LSTM: MSE=0.2666
        ‚Ä¢ LightGBM Regressor (CPU): MSE=61.7289
        ‚Ä¢ Random Forest: MSE=69.2873
        ‚Ä¢ XGBoost: MSE=141.4334
   ‚úÖ DTST: Phase 3/3 - Model selection complete!
  üèÜ WINNER for DTST (TargetReturn): TCN with MSE=0.1589
üêõ DEBUG: DTST - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for DTST.
üêõ DEBUG: DTST - Moving model to CPU before return...
üêõ DEBUG [00:25:05.983]: DTST - Returning result metadata...
üêõ DEBUG [00:25:05.983]: Main received result for DTST
üêõ DEBUG: train_worker started for MO
  ‚öôÔ∏è Training models for MO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - MO: Initiating feature extraction for training.
  [DIAGNOSTIC] MO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MO: rows after features available: 126
üéØ MO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MO: Training LSTM (50 epochs)...
      ‚è≥ MO LSTM: Epoch 10/50 (20%)
      ‚è≥ MO LSTM: Epoch 20/50 (40%)
      ‚è≥ MO LSTM: Epoch 30/50 (60%)
      ‚è≥ MO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.404132
         RMSE: 0.635714
         R¬≤ Score: -1.2992 (Poor - 129.9% variance explained)
      üîπ MO: Training TCN (50 epochs)...
      ‚è≥ MO TCN: Epoch 10/50 (20%)
      ‚è≥ MO TCN: Epoch 20/50 (40%)
      ‚è≥ MO TCN: Epoch 30/50 (60%)
      ‚è≥ MO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.195769
         RMSE: 0.442457
         R¬≤ Score: -0.1138
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MO Random Forest: Starting GridSearchCV fit...
       ‚úÖ MO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.0747 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.6980 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MO XGBoost: Starting GridSearchCV fit...
       ‚úÖ NVMI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=41.0870 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.4s
    - LSTM: MSE=0.7354
    - TCN: MSE=1.0141
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7354
        ‚Ä¢ TCN: MSE=1.0141
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.9439
        ‚Ä¢ Random Forest: MSE=39.2141
        ‚Ä¢ XGBoost: MSE=41.0870
   ‚úÖ NVMI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for NVMI (TargetReturn): LSTM with MSE=0.7354
üêõ DEBUG: NVMI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for NVMI.
üêõ DEBUG: NVMI - Moving model to CPU before return...
üêõ DEBUG [00:25:26.565]: NVMI - Returning result metadata...
üêõ DEBUG [00:25:26.565]: Main received result for NVMI
üêõ DEBUG: train_worker started for TQQQ
  ‚öôÔ∏è Training models for TQQQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TQQQ: Initiating feature extraction for training.
  [DIAGNOSTIC] TQQQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TQQQ: rows after features available: 126
üéØ TQQQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TQQQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TQQQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TQQQ: Training LSTM (50 epochs)...
      ‚è≥ TQQQ LSTM: Epoch 10/50 (20%)
      ‚è≥ TQQQ LSTM: Epoch 20/50 (40%)
       ‚úÖ BRW XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=3.0927 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.6s
    - LSTM: MSE=0.4358
    - TCN: MSE=0.4245
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.7 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4245
        ‚Ä¢ LSTM: MSE=0.4358
        ‚Ä¢ LightGBM Regressor (CPU): MSE=2.6080
        ‚Ä¢ Random Forest: MSE=2.8493
        ‚Ä¢ XGBoost: MSE=3.0927
   ‚úÖ BRW: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BRW (TargetReturn): TCN with MSE=0.4245
üêõ DEBUG: BRW - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BRW.
üêõ DEBUG: BRW - Moving model to CPU before return...
üêõ DEBUG [00:25:27.727]: BRW - Returning result metadata...
üêõ DEBUG: train_worker started for TRS
  ‚öôÔ∏è Training models for TRS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - TRS: Initiating feature extraction for training.
  [DIAGNOSTIC] TRS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TRS: rows after features available: 126
üéØ TRS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TRS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TRS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TRS: Training LSTM (50 epochs)...
      ‚è≥ TQQQ LSTM: Epoch 30/50 (60%)
      ‚è≥ TRS LSTM: Epoch 10/50 (20%)
      ‚è≥ TQQQ LSTM: Epoch 40/50 (80%)
      ‚è≥ TRS LSTM: Epoch 20/50 (40%)
       ‚úÖ SNX XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.0713 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.1s
    - LSTM: MSE=0.7613
    - TCN: MSE=0.8402
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.7613
        ‚Ä¢ TCN: MSE=0.8402
        ‚Ä¢ Random Forest: MSE=16.2454
        ‚Ä¢ XGBoost: MSE=18.0713
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.1985
   ‚úÖ SNX: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SNX (TargetReturn): LSTM with MSE=0.7613
üêõ DEBUG: SNX - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SNX.
üêõ DEBUG: SNX - Moving model to CPU before return...
üêõ DEBUG [00:25:28.936]: SNX - Returning result metadata...
üêõ DEBUG [00:25:28.937]: Main received result for SNX
üêõ DEBUG [00:25:28.937]: Main received result for BRW
üêõ DEBUG: Training progress: 896/959 done
üêõ DEBUG: train_worker started for MTSI
  ‚öôÔ∏è Training models for MTSI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - MTSI: Initiating feature extraction for training.
  [DIAGNOSTIC] MTSI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MTSI: rows after features available: 126
üéØ MTSI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MTSI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MTSI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MTSI: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.479089
         RMSE: 0.692162
         R¬≤ Score: -0.6593 (Poor - 65.9% variance explained)
      üîπ TQQQ: Training TCN (50 epochs)...
      ‚è≥ TRS LSTM: Epoch 30/50 (60%)
      ‚è≥ TQQQ TCN: Epoch 10/50 (20%)
      ‚è≥ TQQQ TCN: Epoch 20/50 (40%)
      ‚è≥ MTSI LSTM: Epoch 10/50 (20%)
      ‚è≥ TQQQ TCN: Epoch 30/50 (60%)
      ‚è≥ TRS LSTM: Epoch 40/50 (80%)
      ‚è≥ TQQQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.481901
         RMSE: 0.694191
         R¬≤ Score: -0.6691
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TQQQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TQQQ Random Forest: Starting GridSearchCV fit...
      ‚è≥ MTSI LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.442981
         RMSE: 0.665568
         R¬≤ Score: -0.7070 (Poor - 70.7% variance explained)
      üîπ TRS: Training TCN (50 epochs)...
      ‚è≥ TRS TCN: Epoch 10/50 (20%)
      ‚è≥ TRS TCN: Epoch 20/50 (40%)
      ‚è≥ TRS TCN: Epoch 30/50 (60%)
      ‚è≥ TRS TCN: Epoch 40/50 (80%)
      ‚è≥ MTSI LSTM: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.402060
         RMSE: 0.634082
         R¬≤ Score: -0.5494
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TRS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TRS Random Forest: Starting GridSearchCV fit...
      ‚è≥ MTSI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.444900
         RMSE: 0.667009
         R¬≤ Score: -0.8300 (Poor - 83.0% variance explained)
      üîπ MTSI: Training TCN (50 epochs)...
      ‚è≥ MTSI TCN: Epoch 10/50 (20%)
      ‚è≥ MTSI TCN: Epoch 20/50 (40%)
      ‚è≥ MTSI TCN: Epoch 30/50 (60%)
      ‚è≥ MTSI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.412833
         RMSE: 0.642521
         R¬≤ Score: -0.6981
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MTSI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MTSI Random Forest: Starting GridSearchCV fit...
       ‚úÖ TQQQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=63.7404 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TQQQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TQQQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=85.5131 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TQQQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ TRS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.6082 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TRS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ TRS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.6290 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TRS XGBoost: Starting GridSearchCV fit...
       ‚úÖ MTSI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.4641 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MTSI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MTSI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=22.3277 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MTSI XGBoost: Starting GridSearchCV fit...
       ‚úÖ GNE XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=43.1038 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.2s
    - LSTM: MSE=0.6252
    - TCN: MSE=0.4789
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.5 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4789
        ‚Ä¢ LSTM: MSE=0.6252
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.7663
        ‚Ä¢ Random Forest: MSE=32.2844
        ‚Ä¢ XGBoost: MSE=43.1038
   ‚úÖ GNE: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GNE (TargetReturn): TCN with MSE=0.4789
üêõ DEBUG: GNE - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GNE.
üêõ DEBUG: GNE - Moving model to CPU before return...
üêõ DEBUG [00:25:39.482]: GNE - Returning result metadata...
üêõ DEBUG [00:25:39.483]: Main received result for GNEüêõ DEBUG: train_worker started for WCBR

  ‚öôÔ∏è Training models for WCBR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - WCBR: Initiating feature extraction for training.
  [DIAGNOSTIC] WCBR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WCBR: rows after features available: 126
üéØ WCBR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WCBR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WCBR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WCBR: Training LSTM (50 epochs)...
      ‚è≥ WCBR LSTM: Epoch 10/50 (20%)
      ‚è≥ WCBR LSTM: Epoch 20/50 (40%)
      ‚è≥ WCBR LSTM: Epoch 30/50 (60%)
      ‚è≥ WCBR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.478324
         RMSE: 0.691610
         R¬≤ Score: -0.8600 (Poor - 86.0% variance explained)
      üîπ WCBR: Training TCN (50 epochs)...
      ‚è≥ WCBR TCN: Epoch 10/50 (20%)
      ‚è≥ WCBR TCN: Epoch 20/50 (40%)
      ‚è≥ WCBR TCN: Epoch 30/50 (60%)
      ‚è≥ WCBR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.445169
         RMSE: 0.667210
         R¬≤ Score: -0.7311
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WCBR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WCBR Random Forest: Starting GridSearchCV fit...
       ‚úÖ WCBR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2205 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WCBR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WCBR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.2181 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WCBR XGBoost: Starting GridSearchCV fit...
       ‚úÖ CHWY XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.8219 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.9s
    - LSTM: MSE=0.4809
    - TCN: MSE=0.2579
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2579
        ‚Ä¢ LSTM: MSE=0.4809
        ‚Ä¢ LightGBM Regressor (CPU): MSE=19.5732
        ‚Ä¢ Random Forest: MSE=26.7475
        ‚Ä¢ XGBoost: MSE=33.8219
   ‚úÖ CHWY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CHWY (TargetReturn): TCN with MSE=0.2579
üêõ DEBUG: CHWY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CHWY.
üêõ DEBUG: CHWY - Moving model to CPU before return...
üêõ DEBUG [00:26:01.899]: CHWY - Returning result metadata...
üêõ DEBUG [00:26:01.900]: Main received result for CHWY
üêõ DEBUG: train_worker started for MAMA
  ‚öôÔ∏è Training models for MAMA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - MAMA: Initiating feature extraction for training.
  [DIAGNOSTIC] MAMA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MAMA: rows after features available: 126
üéØ MAMA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MAMA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MAMA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MAMA: Training LSTM (50 epochs)...
      ‚è≥ MAMA LSTM: Epoch 10/50 (20%)
       ‚úÖ IQM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.8551 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 125.8s
    - LSTM: MSE=0.4157
    - TCN: MSE=0.4908
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4157
        ‚Ä¢ TCN: MSE=0.4908
        ‚Ä¢ XGBoost: MSE=15.8551
        ‚Ä¢ Random Forest: MSE=17.6776
        ‚Ä¢ LightGBM Regressor (CPU): MSE=24.6909
   ‚úÖ IQM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IQM (TargetReturn): LSTM with MSE=0.4157
üêõ DEBUG: IQM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IQM.
üêõ DEBUG: IQM - Moving model to CPU before return...
üêõ DEBUG [00:26:02.496]: IQM - Returning result metadata...
üêõ DEBUG: train_worker started for FRSH
  ‚öôÔ∏è Training models for FRSH (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FRSH: Initiating feature extraction for training.
  [DIAGNOSTIC] FRSH: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FRSH: rows after features available: 126
üéØ FRSH: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FRSH: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FRSH: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FRSH: Training LSTM (50 epochs)...
      ‚è≥ MAMA LSTM: Epoch 20/50 (40%)
      ‚è≥ FRSH LSTM: Epoch 10/50 (20%)
       ‚úÖ WRB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.8191 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.8s
    - LSTM: MSE=0.5422
    - TCN: MSE=0.6273
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.5422
        ‚Ä¢ TCN: MSE=0.6273
        ‚Ä¢ Random Forest: MSE=8.0135
        ‚Ä¢ XGBoost: MSE=8.8191
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6137
   ‚úÖ WRB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WRB (TargetReturn): LSTM with MSE=0.5422
üêõ DEBUG: WRB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WRB.
üêõ DEBUG: WRB - Moving model to CPU before return...
üêõ DEBUG [00:26:03.150]: WRB - Returning result metadata...
üêõ DEBUG [00:26:03.151]: Main received result for WRB
üêõ DEBUG [00:26:03.151]: Main received result for IQM
üêõ DEBUG: Training progress: 900/959 done
üêõ DEBUG: train_worker started for SOBO
  ‚öôÔ∏è Training models for SOBO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - SOBO: Initiating feature extraction for training.
  [DIAGNOSTIC] SOBO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SOBO: rows after features available: 123
üéØ SOBO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SOBO: train_and_evaluate_models - Rows after dropping NaNs: 123
    - Using MSE loss for regression (predicting returns)
   üìö SOBO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SOBO: Training LSTM (50 epochs)...
      ‚è≥ MAMA LSTM: Epoch 30/50 (60%)
      ‚è≥ FRSH LSTM: Epoch 20/50 (40%)
      ‚è≥ SOBO LSTM: Epoch 10/50 (20%)
      ‚è≥ MAMA LSTM: Epoch 40/50 (80%)
      ‚è≥ FRSH LSTM: Epoch 30/50 (60%)
      ‚è≥ SOBO LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.406820
         RMSE: 0.637824
         R¬≤ Score: -0.8305 (Poor - 83.0% variance explained)
      üîπ MAMA: Training TCN (50 epochs)...
      ‚è≥ FRSH LSTM: Epoch 40/50 (80%)
      ‚è≥ MAMA TCN: Epoch 10/50 (20%)
      ‚è≥ SOBO LSTM: Epoch 30/50 (60%)
      ‚è≥ MAMA TCN: Epoch 20/50 (40%)
      ‚è≥ MAMA TCN: Epoch 30/50 (60%)
      ‚è≥ MAMA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.272348
         RMSE: 0.521869
         R¬≤ Score: -0.2254
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MAMA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MAMA Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.152367
         RMSE: 0.390342
         R¬≤ Score: -0.8025 (Poor - 80.3% variance explained)
      üîπ FRSH: Training TCN (50 epochs)...
      ‚è≥ SOBO LSTM: Epoch 40/50 (80%)
      ‚è≥ FRSH TCN: Epoch 10/50 (20%)
      ‚è≥ FRSH TCN: Epoch 20/50 (40%)
      ‚è≥ FRSH TCN: Epoch 30/50 (60%)
      ‚è≥ FRSH TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.139481
         RMSE: 0.373471
         R¬≤ Score: -0.6501
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FRSH: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FRSH Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.139751
         RMSE: 0.373833
         R¬≤ Score: -0.1295 (Poor - 13.0% variance explained)
      üîπ SOBO: Training TCN (50 epochs)...
      ‚è≥ SOBO TCN: Epoch 10/50 (20%)
      ‚è≥ SOBO TCN: Epoch 20/50 (40%)
      ‚è≥ SOBO TCN: Epoch 30/50 (60%)
      ‚è≥ SOBO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.140132
         RMSE: 0.374342
         R¬≤ Score: -0.1326
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SOBO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SOBO Random Forest: Starting GridSearchCV fit...
       ‚úÖ MAMA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.3920 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MAMA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FRSH Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=23.3093 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FRSH LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MAMA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=26.9582 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MAMA XGBoost: Starting GridSearchCV fit...
       ‚úÖ SOBO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.7394 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SOBO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FRSH LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=39.5448 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FRSH XGBoost: Starting GridSearchCV fit...
       ‚úÖ SOBO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.1062 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SOBO XGBoost: Starting GridSearchCV fit...
       ‚úÖ HEI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=28.4152 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.6s
    - LSTM: MSE=0.1656
    - TCN: MSE=0.0991
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0991
        ‚Ä¢ LSTM: MSE=0.1656
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.1408
        ‚Ä¢ XGBoost: MSE=28.4152
        ‚Ä¢ Random Forest: MSE=29.0320
   ‚úÖ HEI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HEI (TargetReturn): TCN with MSE=0.0991
üêõ DEBUG: HEI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HEI.
üêõ DEBUG: HEI - Moving model to CPU before return...
üêõ DEBUG [00:26:18.573]: HEI - Returning result metadata...
üêõ DEBUG: train_worker started for GGAL
üêõ DEBUG [00:26:18.574]: Main received result for HEI
  ‚öôÔ∏è Training models for GGAL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - GGAL: Initiating feature extraction for training.
  [DIAGNOSTIC] GGAL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GGAL: rows after features available: 126
üéØ GGAL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GGAL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GGAL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GGAL: Training LSTM (50 epochs)...
      ‚è≥ GGAL LSTM: Epoch 10/50 (20%)
      ‚è≥ GGAL LSTM: Epoch 20/50 (40%)
      ‚è≥ GGAL LSTM: Epoch 30/50 (60%)
      ‚è≥ GGAL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.170955
         RMSE: 0.413467
         R¬≤ Score: -1.0877 (Poor - 108.8% variance explained)
      üîπ GGAL: Training TCN (50 epochs)...
      ‚è≥ GGAL TCN: Epoch 10/50 (20%)
      ‚è≥ GGAL TCN: Epoch 20/50 (40%)
      ‚è≥ GGAL TCN: Epoch 30/50 (60%)
      ‚è≥ GGAL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.090952
         RMSE: 0.301582
         R¬≤ Score: -0.1107
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GGAL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GGAL Random Forest: Starting GridSearchCV fit...
       ‚úÖ TRMK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.7692 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.8s
    - LSTM: MSE=0.5567
    - TCN: MSE=0.4501
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4501
        ‚Ä¢ LSTM: MSE=0.5567
        ‚Ä¢ XGBoost: MSE=6.7692
        ‚Ä¢ Random Forest: MSE=6.9766
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.2738
   ‚úÖ TRMK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TRMK (TargetReturn): TCN with MSE=0.4501
üêõ DEBUG: TRMK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TRMK.
üêõ DEBUG: TRMK - Moving model to CPU before return...
üêõ DEBUG [00:26:21.823]: TRMK - Returning result metadata...
üêõ DEBUG [00:26:21.823]: Main received result for TRMK
üêõ DEBUG: train_worker started for AXS
  ‚öôÔ∏è Training models for AXS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - AXS: Initiating feature extraction for training.
  [DIAGNOSTIC] AXS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AXS: rows after features available: 126
üéØ AXS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AXS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AXS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AXS: Training LSTM (50 epochs)...
      ‚è≥ AXS LSTM: Epoch 10/50 (20%)
      ‚è≥ AXS LSTM: Epoch 20/50 (40%)
      ‚è≥ AXS LSTM: Epoch 30/50 (60%)
      ‚è≥ AXS LSTM: Epoch 40/50 (80%)
       ‚úÖ GGAL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.2964 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GGAL LightGBM Regressor (CPU): Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.222411
         RMSE: 0.471604
         R¬≤ Score: -0.3609 (Poor - 36.1% variance explained)
      üîπ AXS: Training TCN (50 epochs)...
      ‚è≥ AXS TCN: Epoch 10/50 (20%)
      ‚è≥ AXS TCN: Epoch 20/50 (40%)
      ‚è≥ AXS TCN: Epoch 30/50 (60%)
      ‚è≥ AXS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.169101
         RMSE: 0.411218
         R¬≤ Score: -0.0347
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AXS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AXS Random Forest: Starting GridSearchCV fit...
       ‚úÖ PAM XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=19.7898 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.6s
    - LSTM: MSE=0.1340
    - TCN: MSE=0.0532
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0532
        ‚Ä¢ LSTM: MSE=0.1340
        ‚Ä¢ Random Forest: MSE=17.6689
        ‚Ä¢ XGBoost: MSE=19.7898
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.8726
   ‚úÖ PAM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PAM (TargetReturn): TCN with MSE=0.0532
üêõ DEBUG: PAM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PAM.
üêõ DEBUG: PAM - Moving model to CPU before return...
üêõ DEBUG [00:26:25.058]: PAM - Returning result metadata...
üêõ DEBUG: train_worker started for MASI
  ‚öôÔ∏è Training models for MASI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - MASI: Initiating feature extraction for training.
  [DIAGNOSTIC] MASI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ MASI: rows after features available: 126
üéØ MASI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] MASI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö MASI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ MASI: Training LSTM (50 epochs)...
       ‚úÖ GGAL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.4048 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GGAL XGBoost: Starting GridSearchCV fit...
       ‚úÖ EVC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=29.6694 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.0s
    - LSTM: MSE=0.5864
    - TCN: MSE=0.3934
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.3 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3934
        ‚Ä¢ LSTM: MSE=0.5864
        ‚Ä¢ LightGBM Regressor (CPU): MSE=23.0925
        ‚Ä¢ Random Forest: MSE=24.0108
        ‚Ä¢ XGBoost: MSE=29.6694
   ‚úÖ EVC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EVC (TargetReturn): TCN with MSE=0.3934
üêõ DEBUG: EVC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EVC.
üêõ DEBUG: EVC - Moving model to CPU before return...
üêõ DEBUG [00:26:26.253]: EVC - Returning result metadata...
üêõ DEBUG [00:26:26.253]: Main received result for EVC
üêõ DEBUG [00:26:26.253]: Main received result for PAM
üêõ DEBUG: Training progress: 904/959 done
üêõ DEBUG: train_worker started for RSG
  ‚öôÔ∏è Training models for RSG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - RSG: Initiating feature extraction for training.
  [DIAGNOSTIC] RSG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ RSG: rows after features available: 126
üéØ RSG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] RSG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö RSG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ RSG: Training LSTM (50 epochs)...
      ‚è≥ MASI LSTM: Epoch 10/50 (20%)
      ‚è≥ RSG LSTM: Epoch 10/50 (20%)
      ‚è≥ MASI LSTM: Epoch 20/50 (40%)
      ‚è≥ RSG LSTM: Epoch 20/50 (40%)
      ‚è≥ MASI LSTM: Epoch 30/50 (60%)
      ‚è≥ RSG LSTM: Epoch 30/50 (60%)
      ‚è≥ MASI LSTM: Epoch 40/50 (80%)
      ‚è≥ RSG LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.221554
         RMSE: 0.470695
         R¬≤ Score: -0.8369 (Poor - 83.7% variance explained)
      üîπ MASI: Training TCN (50 epochs)...
      ‚è≥ MASI TCN: Epoch 10/50 (20%)
      ‚è≥ MASI TCN: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.337506
         RMSE: 0.580953
         R¬≤ Score: -1.1727 (Poor - 117.3% variance explained)
      üîπ RSG: Training TCN (50 epochs)...
       ‚úÖ AXS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.9303 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 4.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AXS LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ MASI TCN: Epoch 30/50 (60%)
      ‚è≥ RSG TCN: Epoch 10/50 (20%)
      ‚è≥ MASI TCN: Epoch 40/50 (80%)
      ‚è≥ RSG TCN: Epoch 20/50 (40%)
      üìä TCN Regression Metrics:
         MSE: 0.146601
         RMSE: 0.382885
         R¬≤ Score: -0.2154
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä MASI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ MASI Random Forest: Starting GridSearchCV fit...
      ‚è≥ RSG TCN: Epoch 30/50 (60%)
      ‚è≥ RSG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.185930
         RMSE: 0.431196
         R¬≤ Score: -0.1969
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä RSG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ RSG Random Forest: Starting GridSearchCV fit...
       ‚úÖ AXS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=7.1639 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AXS XGBoost: Starting GridSearchCV fit...
       ‚úÖ MASI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.5580 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ MASI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ RSG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=3.0590 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ RSG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MASI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=13.7241 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ MASI XGBoost: Starting GridSearchCV fit...
       ‚úÖ RSG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=4.7828 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ RSG XGBoost: Starting GridSearchCV fit...
       ‚úÖ COWG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.2355 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.2s
    - LSTM: MSE=0.3369
    - TCN: MSE=0.3618
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.3 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.3369
        ‚Ä¢ TCN: MSE=0.3618
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.2289
        ‚Ä¢ Random Forest: MSE=7.7736
        ‚Ä¢ XGBoost: MSE=8.2355
   ‚úÖ COWG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for COWG (TargetReturn): LSTM with MSE=0.3369
üêõ DEBUG: COWG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for COWG.
üêõ DEBUG: COWG - Moving model to CPU before return...
üêõ DEBUG [00:26:45.317]: COWG - Returning result metadata...
üêõ DEBUG [00:26:45.318]: Main received result for COWG
üêõ DEBUG: train_worker started for IRON
  ‚öôÔ∏è Training models for IRON (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - IRON: Initiating feature extraction for training.
  [DIAGNOSTIC] IRON: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IRON: rows after features available: 126
üéØ IRON: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IRON: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IRON: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IRON: Training LSTM (50 epochs)...
      ‚è≥ IRON LSTM: Epoch 10/50 (20%)
      ‚è≥ IRON LSTM: Epoch 20/50 (40%)
      ‚è≥ IRON LSTM: Epoch 30/50 (60%)
      ‚è≥ IRON LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.445089
         RMSE: 0.667150
         R¬≤ Score: -1.2164 (Poor - 121.6% variance explained)
      üîπ IRON: Training TCN (50 epochs)...
      ‚è≥ IRON TCN: Epoch 10/50 (20%)
      ‚è≥ IRON TCN: Epoch 20/50 (40%)
      ‚è≥ IRON TCN: Epoch 30/50 (60%)
      ‚è≥ IRON TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.346079
         RMSE: 0.588285
         R¬≤ Score: -0.7234
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IRON: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IRON Random Forest: Starting GridSearchCV fit...
       ‚úÖ IRON Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=20.2878 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IRON LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IRON LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=27.4897 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IRON XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=10.8478 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.5s
    - LSTM: MSE=0.1083
    - TCN: MSE=0.0671
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 130.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0671
        ‚Ä¢ LSTM: MSE=0.1083
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.2955
        ‚Ä¢ Random Forest: MSE=6.9254
        ‚Ä¢ XGBoost: MSE=10.8478
   ‚úÖ FDP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDP (TargetReturn): TCN with MSE=0.0671
üêõ DEBUG: FDP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDP.
üêõ DEBUG: FDP - Moving model to CPU before return...
üêõ DEBUG [00:26:58.292]: FDP - Returning result metadata...
üêõ DEBUG [00:26:58.292]: Main received result for FDP
üêõ DEBUG: train_worker started for CPA
  ‚öôÔ∏è Training models for CPA (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - CPA: Initiating feature extraction for training.
  [DIAGNOSTIC] CPA: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CPA: rows after features available: 126
üéØ CPA: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CPA: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CPA: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CPA: Training LSTM (50 epochs)...
      ‚è≥ CPA LSTM: Epoch 10/50 (20%)
      ‚è≥ CPA LSTM: Epoch 20/50 (40%)
      ‚è≥ CPA LSTM: Epoch 30/50 (60%)
      ‚è≥ CPA LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.478961
         RMSE: 0.692070
         R¬≤ Score: -1.6239 (Poor - 162.4% variance explained)
      üîπ CPA: Training TCN (50 epochs)...
      ‚è≥ CPA TCN: Epoch 10/50 (20%)
      ‚è≥ CPA TCN: Epoch 20/50 (40%)
      ‚è≥ CPA TCN: Epoch 30/50 (60%)
      ‚è≥ CPA TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.241404
         RMSE: 0.491329
         R¬≤ Score: -0.3225
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CPA: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CPA Random Forest: Starting GridSearchCV fit...
       ‚úÖ CPA Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.6971 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CPA LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PSLV XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.4173 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.1s
    - LSTM: MSE=0.3635
    - TCN: MSE=0.2543
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2543
        ‚Ä¢ LSTM: MSE=0.3635
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.4050
        ‚Ä¢ XGBoost: MSE=9.4173
        ‚Ä¢ Random Forest: MSE=10.8152
   ‚úÖ PSLV: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PSLV (TargetReturn): TCN with MSE=0.2543
üêõ DEBUG: PSLV - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PSLV.
üêõ DEBUG: PSLV - Moving model to CPU before return...
üêõ DEBUG [00:27:05.013]: PSLV - Returning result metadata...
üêõ DEBUG [00:27:05.013]: Main received result for PSLV
üêõ DEBUG: train_worker started for GEL
  ‚öôÔ∏è Training models for GEL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - GEL: Initiating feature extraction for training.
  [DIAGNOSTIC] GEL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GEL: rows after features available: 126
üéØ GEL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GEL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GEL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GEL: Training LSTM (50 epochs)...
       ‚úÖ CPA LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.6804 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CPA XGBoost: Starting GridSearchCV fit...
      ‚è≥ GEL LSTM: Epoch 10/50 (20%)
      ‚è≥ GEL LSTM: Epoch 20/50 (40%)
      ‚è≥ GEL LSTM: Epoch 30/50 (60%)
      ‚è≥ GEL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.112525
         RMSE: 0.335448
         R¬≤ Score: -0.9017 (Poor - 90.2% variance explained)
      üîπ GEL: Training TCN (50 epochs)...
      ‚è≥ GEL TCN: Epoch 10/50 (20%)
      ‚è≥ GEL TCN: Epoch 20/50 (40%)
      ‚è≥ GEL TCN: Epoch 30/50 (60%)
      ‚è≥ GEL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.059624
         RMSE: 0.244179
         R¬≤ Score: -0.0076
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GEL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GEL Random Forest: Starting GridSearchCV fit...
       ‚úÖ GEL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.6056 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GEL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ GEL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=31.5052 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GEL XGBoost: Starting GridSearchCV fit...
       ‚úÖ MO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=4.3669 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.3s
    - LSTM: MSE=0.4041
    - TCN: MSE=0.1958
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1958
        ‚Ä¢ LSTM: MSE=0.4041
        ‚Ä¢ XGBoost: MSE=4.3669
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.6980
        ‚Ä¢ Random Forest: MSE=5.0747
   ‚úÖ MO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MO (TargetReturn): TCN with MSE=0.1958
üêõ DEBUG: MO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MO.
üêõ DEBUG: MO - Moving model to CPU before return...
üêõ DEBUG [00:27:20.600]: MO - Returning result metadata...
üêõ DEBUG: train_worker started for SIMO
üêõ DEBUG [00:27:20.601]: Main received result for MO
üêõ DEBUG: Training progress: 908/959 done
  ‚öôÔ∏è Training models for SIMO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - SIMO: Initiating feature extraction for training.
  [DIAGNOSTIC] SIMO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SIMO: rows after features available: 126
üéØ SIMO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SIMO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SIMO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SIMO: Training LSTM (50 epochs)...
      ‚è≥ SIMO LSTM: Epoch 10/50 (20%)
      ‚è≥ SIMO LSTM: Epoch 20/50 (40%)
      ‚è≥ SIMO LSTM: Epoch 30/50 (60%)
      ‚è≥ SIMO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.898929
         RMSE: 0.948118
         R¬≤ Score: -1.8847 (Poor - 188.5% variance explained)
      üîπ SIMO: Training TCN (50 epochs)...
      ‚è≥ SIMO TCN: Epoch 10/50 (20%)
      ‚è≥ SIMO TCN: Epoch 20/50 (40%)
      ‚è≥ SIMO TCN: Epoch 30/50 (60%)
      ‚è≥ SIMO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.371279
         RMSE: 0.609327
         R¬≤ Score: -0.1915
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SIMO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SIMO Random Forest: Starting GridSearchCV fit...
       ‚úÖ SIMO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=100.7528 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.9s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SIMO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SIMO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=92.4011 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SIMO XGBoost: Starting GridSearchCV fit...
       ‚úÖ TQQQ XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=63.1317 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 131.3s
    - LSTM: MSE=0.4791
    - TCN: MSE=0.4819
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4791
        ‚Ä¢ TCN: MSE=0.4819
        ‚Ä¢ XGBoost: MSE=63.1317
        ‚Ä¢ Random Forest: MSE=63.7404
        ‚Ä¢ LightGBM Regressor (CPU): MSE=85.5131
   ‚úÖ TQQQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TQQQ (TargetReturn): LSTM with MSE=0.4791
üêõ DEBUG: TQQQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TQQQ.
üêõ DEBUG: TQQQ - Moving model to CPU before return...
üêõ DEBUG [00:27:44.751]: TQQQ - Returning result metadata...
üêõ DEBUG [00:27:44.751]: Main received result for TQQQ
üêõ DEBUG: train_worker started for GT
  ‚öôÔ∏è Training models for GT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - GT: Initiating feature extraction for training.
  [DIAGNOSTIC] GT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ GT: rows after features available: 126
üéØ GT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] GT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö GT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ GT: Training LSTM (50 epochs)...
      ‚è≥ GT LSTM: Epoch 10/50 (20%)
       ‚úÖ TRS XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=19.8160 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 131.1s
    - LSTM: MSE=0.4430
    - TCN: MSE=0.4021
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4021
        ‚Ä¢ LSTM: MSE=0.4430
        ‚Ä¢ XGBoost: MSE=19.8160
        ‚Ä¢ Random Forest: MSE=21.6082
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.6290
   ‚úÖ TRS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TRS (TargetReturn): TCN with MSE=0.4021
üêõ DEBUG: TRS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TRS.
üêõ DEBUG: TRS - Moving model to CPU before return...
üêõ DEBUG [00:27:45.371]: TRS - Returning result metadata...
üêõ DEBUG [00:27:45.371]: Main received result for TRS
üêõ DEBUG: train_worker started for PEJ
  ‚öôÔ∏è Training models for PEJ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - PEJ: Initiating feature extraction for training.
  [DIAGNOSTIC] PEJ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PEJ: rows after features available: 126
üéØ PEJ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PEJ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PEJ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PEJ: Training LSTM (50 epochs)...
      ‚è≥ GT LSTM: Epoch 20/50 (40%)
      ‚è≥ PEJ LSTM: Epoch 10/50 (20%)
      ‚è≥ GT LSTM: Epoch 30/50 (60%)
      ‚è≥ PEJ LSTM: Epoch 20/50 (40%)
      ‚è≥ GT LSTM: Epoch 40/50 (80%)
      ‚è≥ PEJ LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.186922
         RMSE: 0.432344
         R¬≤ Score: -0.6559 (Poor - 65.6% variance explained)
      üîπ GT: Training TCN (50 epochs)...
      ‚è≥ PEJ LSTM: Epoch 40/50 (80%)
      ‚è≥ GT TCN: Epoch 10/50 (20%)
      ‚è≥ GT TCN: Epoch 20/50 (40%)
      ‚è≥ GT TCN: Epoch 30/50 (60%)
      ‚è≥ GT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.121148
         RMSE: 0.348063
         R¬≤ Score: -0.0732
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä GT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ GT Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 1.022366
         RMSE: 1.011121
         R¬≤ Score: -2.1040 (Poor - 210.4% variance explained)
      üîπ PEJ: Training TCN (50 epochs)...
      ‚è≥ PEJ TCN: Epoch 10/50 (20%)
      ‚è≥ PEJ TCN: Epoch 20/50 (40%)
      ‚è≥ PEJ TCN: Epoch 30/50 (60%)
      ‚è≥ PEJ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.540906
         RMSE: 0.735463
         R¬≤ Score: -0.6422
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PEJ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PEJ Random Forest: Starting GridSearchCV fit...
       ‚úÖ GT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=39.7061 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ GT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ MTSI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=17.4558 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 135.0s
    - LSTM: MSE=0.4449
    - TCN: MSE=0.4128
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 138.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4128
        ‚Ä¢ LSTM: MSE=0.4449
        ‚Ä¢ XGBoost: MSE=17.4558
        ‚Ä¢ Random Forest: MSE=21.4641
        ‚Ä¢ LightGBM Regressor (CPU): MSE=22.3277
   ‚úÖ MTSI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MTSI (TargetReturn): TCN with MSE=0.4128
üêõ DEBUG: MTSI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MTSI.
üêõ DEBUG: MTSI - Moving model to CPU before return...
üêõ DEBUG [00:27:50.630]: MTSI - Returning result metadata...
üêõ DEBUG: train_worker started for FEDU
üêõ DEBUG [00:27:50.632]: Main received result for MTSI
  ‚öôÔ∏è Training models for FEDU (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - FEDU: Initiating feature extraction for training.
  [DIAGNOSTIC] FEDU: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FEDU: rows after features available: 126
üéØ FEDU: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FEDU: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FEDU: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FEDU: Training LSTM (50 epochs)...
       ‚úÖ PEJ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.3233 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PEJ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FEDU LSTM: Epoch 10/50 (20%)
       ‚úÖ GT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=36.7080 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ GT XGBoost: Starting GridSearchCV fit...
      ‚è≥ FEDU LSTM: Epoch 20/50 (40%)
       ‚úÖ PEJ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.6154 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PEJ XGBoost: Starting GridSearchCV fit...
      ‚è≥ FEDU LSTM: Epoch 30/50 (60%)
      ‚è≥ FEDU LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.659628
         RMSE: 0.812175
         R¬≤ Score: -1.3404 (Poor - 134.0% variance explained)
      üîπ FEDU: Training TCN (50 epochs)...
      ‚è≥ FEDU TCN: Epoch 10/50 (20%)
      ‚è≥ FEDU TCN: Epoch 20/50 (40%)
      ‚è≥ FEDU TCN: Epoch 30/50 (60%)
      ‚è≥ FEDU TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.429876
         RMSE: 0.655649
         R¬≤ Score: -0.5252
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FEDU: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FEDU Random Forest: Starting GridSearchCV fit...
       ‚úÖ FEDU Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=101.7186 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FEDU LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FEDU LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=274.9566 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FEDU XGBoost: Starting GridSearchCV fit...
       ‚úÖ WCBR XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.5057 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 134.4s
    - LSTM: MSE=0.4783
    - TCN: MSE=0.4452
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.8 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4452
        ‚Ä¢ LSTM: MSE=0.4783
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.2181
        ‚Ä¢ Random Forest: MSE=14.2205
        ‚Ä¢ XGBoost: MSE=14.5057
   ‚úÖ WCBR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WCBR (TargetReturn): TCN with MSE=0.4452
üêõ DEBUG: WCBR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WCBR.
üêõ DEBUG: WCBR - Moving model to CPU before return...
üêõ DEBUG [00:28:00.474]: WCBR - Returning result metadata...
üêõ DEBUG [00:28:00.474]: Main received result for WCBR
üêõ DEBUG: Training progress: 912/959 done
üêõ DEBUG: train_worker started for SPRY
  ‚öôÔ∏è Training models for SPRY (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - SPRY: Initiating feature extraction for training.
  [DIAGNOSTIC] SPRY: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPRY: rows after features available: 126
üéØ SPRY: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPRY: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPRY: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPRY: Training LSTM (50 epochs)...
      ‚è≥ SPRY LSTM: Epoch 10/50 (20%)
      ‚è≥ SPRY LSTM: Epoch 20/50 (40%)
      ‚è≥ SPRY LSTM: Epoch 30/50 (60%)
      ‚è≥ SPRY LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.251938
         RMSE: 0.501934
         R¬≤ Score: -1.5890 (Poor - 158.9% variance explained)
      üîπ SPRY: Training TCN (50 epochs)...
      ‚è≥ SPRY TCN: Epoch 10/50 (20%)
      ‚è≥ SPRY TCN: Epoch 20/50 (40%)
      ‚è≥ SPRY TCN: Epoch 30/50 (60%)
      ‚è≥ SPRY TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.104671
         RMSE: 0.323529
         R¬≤ Score: -0.0756
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPRY: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPRY Random Forest: Starting GridSearchCV fit...
       ‚úÖ SPRY Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=64.8796 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPRY LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ SPRY LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=53.5601 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPRY XGBoost: Starting GridSearchCV fit...
       ‚úÖ FRSH XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=60.3118 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.2s
    - LSTM: MSE=0.1524
    - TCN: MSE=0.1395
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.7 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1395
        ‚Ä¢ LSTM: MSE=0.1524
        ‚Ä¢ Random Forest: MSE=23.3093
        ‚Ä¢ LightGBM Regressor (CPU): MSE=39.5448
        ‚Ä¢ XGBoost: MSE=60.3118
   ‚úÖ FRSH: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FRSH (TargetReturn): TCN with MSE=0.1395
üêõ DEBUG: FRSH - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FRSH.
üêõ DEBUG: FRSH - Moving model to CPU before return...
üêõ DEBUG [00:28:20.414]: FRSH - Returning result metadata...
üêõ DEBUG: train_worker started for CFR
  ‚öôÔ∏è Training models for CFR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - CFR: Initiating feature extraction for training.
  [DIAGNOSTIC] CFR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CFR: rows after features available: 126
üéØ CFR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CFR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CFR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CFR: Training LSTM (50 epochs)...
       ‚úÖ SOBO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.7410 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 130.9s
    - LSTM: MSE=0.1398
    - TCN: MSE=0.1401
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.3 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.1398
        ‚Ä¢ TCN: MSE=0.1401
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.1062
        ‚Ä¢ Random Forest: MSE=7.7394
        ‚Ä¢ XGBoost: MSE=7.7410
   ‚úÖ SOBO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SOBO (TargetReturn): LSTM with MSE=0.1398
üêõ DEBUG: SOBO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SOBO.
üêõ DEBUG: SOBO - Moving model to CPU before return...
üêõ DEBUG [00:28:20.505]: SOBO - Returning result metadata...
üêõ DEBUG: train_worker started for CSGP
  ‚öôÔ∏è Training models for CSGP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - CSGP: Initiating feature extraction for training.
  [DIAGNOSTIC] CSGP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CSGP: rows after features available: 126
üéØ CSGP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CSGP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CSGP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CSGP: Training LSTM (50 epochs)...
      ‚è≥ CFR LSTM: Epoch 10/50 (20%)
      ‚è≥ CSGP LSTM: Epoch 10/50 (20%)
       ‚úÖ MAMA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=34.3068 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.5s
    - LSTM: MSE=0.4068
    - TCN: MSE=0.2723
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 136.3 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2723
        ‚Ä¢ LSTM: MSE=0.4068
        ‚Ä¢ Random Forest: MSE=26.3920
        ‚Ä¢ LightGBM Regressor (CPU): MSE=26.9582
        ‚Ä¢ XGBoost: MSE=34.3068
   ‚úÖ MAMA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MAMA (TargetReturn): TCN with MSE=0.2723
üêõ DEBUG: MAMA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MAMA.
üêõ DEBUG: MAMA - Moving model to CPU before return...
üêõ DEBUG [00:28:21.397]: MAMA - Returning result metadata...
üêõ DEBUG [00:28:21.397]: Main received result for MAMA
üêõ DEBUG [00:28:21.397]: Main received result for FRSH
üêõ DEBUG [00:28:21.397]: Main received result for SOBO
üêõ DEBUG: train_worker started for CPNG
  ‚öôÔ∏è Training models for CPNG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - CPNG: Initiating feature extraction for training.
  [DIAGNOSTIC] CPNG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CPNG: rows after features available: 126
üéØ CPNG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CPNG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CPNG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CPNG: Training LSTM (50 epochs)...
      ‚è≥ CFR LSTM: Epoch 20/50 (40%)
      ‚è≥ CSGP LSTM: Epoch 20/50 (40%)
      ‚è≥ CPNG LSTM: Epoch 10/50 (20%)
      ‚è≥ CFR LSTM: Epoch 30/50 (60%)
      ‚è≥ CSGP LSTM: Epoch 30/50 (60%)
      ‚è≥ CPNG LSTM: Epoch 20/50 (40%)
      ‚è≥ CFR LSTM: Epoch 40/50 (80%)
      ‚è≥ CSGP LSTM: Epoch 40/50 (80%)
      ‚è≥ CPNG LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.779590
         RMSE: 0.882944
         R¬≤ Score: -1.6773 (Poor - 167.7% variance explained)
      üîπ CFR: Training TCN (50 epochs)...
      ‚è≥ CFR TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.312968
         RMSE: 0.559436
         R¬≤ Score: -0.6729 (Poor - 67.3% variance explained)
      üîπ CSGP: Training TCN (50 epochs)...
      ‚è≥ CFR TCN: Epoch 20/50 (40%)
      ‚è≥ CSGP TCN: Epoch 10/50 (20%)
      ‚è≥ CFR TCN: Epoch 30/50 (60%)
      ‚è≥ CSGP TCN: Epoch 20/50 (40%)
      ‚è≥ CPNG LSTM: Epoch 40/50 (80%)
      ‚è≥ CFR TCN: Epoch 40/50 (80%)
      ‚è≥ CSGP TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.384010
         RMSE: 0.619685
         R¬≤ Score: -0.3188
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CFR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CFR Random Forest: Starting GridSearchCV fit...
      ‚è≥ CSGP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.196315
         RMSE: 0.443074
         R¬≤ Score: -0.0493
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CSGP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CSGP Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.461026
         RMSE: 0.678989
         R¬≤ Score: -1.0125 (Poor - 101.3% variance explained)
      üîπ CPNG: Training TCN (50 epochs)...
      ‚è≥ CPNG TCN: Epoch 10/50 (20%)
      ‚è≥ CPNG TCN: Epoch 20/50 (40%)
      ‚è≥ CPNG TCN: Epoch 30/50 (60%)
      ‚è≥ CPNG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.392849
         RMSE: 0.626777
         R¬≤ Score: -0.7149
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CPNG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CPNG Random Forest: Starting GridSearchCV fit...
       ‚úÖ CFR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.4902 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CFR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CSGP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.6515 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CSGP LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CFR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.9520 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CFR XGBoost: Starting GridSearchCV fit...
       ‚úÖ CPNG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=24.6129 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CPNG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CSGP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=5.8758 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CSGP XGBoost: Starting GridSearchCV fit...
       ‚úÖ CPNG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=14.9010 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CPNG XGBoost: Starting GridSearchCV fit...
       ‚úÖ AXS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.3644 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100}) | Time: 126.8s
    - LSTM: MSE=0.2224
    - TCN: MSE=0.1691
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1691
        ‚Ä¢ LSTM: MSE=0.2224
        ‚Ä¢ Random Forest: MSE=6.9303
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.1639
        ‚Ä¢ XGBoost: MSE=7.3644
   ‚úÖ AXS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AXS (TargetReturn): TCN with MSE=0.1691
üêõ DEBUG: AXS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AXS.
üêõ DEBUG: AXS - Moving model to CPU before return...
üêõ DEBUG [00:28:36.626]: AXS - Returning result metadata...
üêõ DEBUG: train_worker started for UIVM
  ‚öôÔ∏è Training models for UIVM (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - UIVM: Initiating feature extraction for training.
  [DIAGNOSTIC] UIVM: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UIVM: rows after features available: 126
üéØ UIVM: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UIVM: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UIVM: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UIVM: Training LSTM (50 epochs)...
      ‚è≥ UIVM LSTM: Epoch 10/50 (20%)
      ‚è≥ UIVM LSTM: Epoch 20/50 (40%)
       ‚úÖ GGAL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.5588 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.7s
    - LSTM: MSE=0.1710
    - TCN: MSE=0.0910
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.4 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0910
        ‚Ä¢ LSTM: MSE=0.1710
        ‚Ä¢ Random Forest: MSE=20.2964
        ‚Ä¢ XGBoost: MSE=24.5588
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.4048
   ‚úÖ GGAL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GGAL (TargetReturn): TCN with MSE=0.0910
üêõ DEBUG: GGAL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GGAL.
üêõ DEBUG: GGAL - Moving model to CPU before return...
üêõ DEBUG [00:28:38.988]: GGAL - Returning result metadata...
üêõ DEBUG [00:28:38.988]: Main received result for GGALüêõ DEBUG: train_worker started for KEMQ

üêõ DEBUG: Training progress: 916/959 done
üêõ DEBUG [00:28:38.989]: Main received result for AXS
  ‚öôÔ∏è Training models for KEMQ (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - KEMQ: Initiating feature extraction for training.
  [DIAGNOSTIC] KEMQ: fetch_training_data - Initial data rows: 205
   ‚Ü≥ KEMQ: rows after features available: 126
üéØ KEMQ: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] KEMQ: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö KEMQ: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ KEMQ: Training LSTM (50 epochs)...
      ‚è≥ UIVM LSTM: Epoch 30/50 (60%)
      ‚è≥ KEMQ LSTM: Epoch 10/50 (20%)
      ‚è≥ UIVM LSTM: Epoch 40/50 (80%)
      ‚è≥ KEMQ LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.117182
         RMSE: 0.342319
         R¬≤ Score: -0.7723 (Poor - 77.2% variance explained)
      üîπ UIVM: Training TCN (50 epochs)...
      ‚è≥ UIVM TCN: Epoch 10/50 (20%)
      ‚è≥ UIVM TCN: Epoch 20/50 (40%)
      ‚è≥ KEMQ LSTM: Epoch 30/50 (60%)
      ‚è≥ UIVM TCN: Epoch 30/50 (60%)
      ‚è≥ UIVM TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.087485
         RMSE: 0.295779
         R¬≤ Score: -0.3231
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UIVM: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UIVM Random Forest: Starting GridSearchCV fit...
      ‚è≥ KEMQ LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.403586
         RMSE: 0.635284
         R¬≤ Score: -0.8154 (Poor - 81.5% variance explained)
      üîπ KEMQ: Training TCN (50 epochs)...
      ‚è≥ KEMQ TCN: Epoch 10/50 (20%)
      ‚è≥ KEMQ TCN: Epoch 20/50 (40%)
      ‚è≥ KEMQ TCN: Epoch 30/50 (60%)
      ‚è≥ KEMQ TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.491679
         RMSE: 0.701199
         R¬≤ Score: -1.2116
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä KEMQ: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ KEMQ Random Forest: Starting GridSearchCV fit...
       ‚úÖ RSG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=2.7627 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.4s
    - LSTM: MSE=0.3375
    - TCN: MSE=0.1859
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.9 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1859
        ‚Ä¢ LSTM: MSE=0.3375
        ‚Ä¢ XGBoost: MSE=2.7627
        ‚Ä¢ Random Forest: MSE=3.0590
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.7828
   ‚úÖ RSG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for RSG (TargetReturn): TCN with MSE=0.1859
üêõ DEBUG: RSG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for RSG.
üêõ DEBUG: RSG - Moving model to CPU before return...
üêõ DEBUG [00:28:42.363]: RSG - Returning result metadata...
üêõ DEBUG: train_worker started for WTFC
       ‚úÖ MASI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=15.7596 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 129.5s
    - LSTM: MSE=0.2216
    - TCN: MSE=0.1466
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1466
        ‚Ä¢ LSTM: MSE=0.2216
        ‚Ä¢ Random Forest: MSE=13.5580
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.7241
        ‚Ä¢ XGBoost: MSE=15.7596
   ‚úÖ MASI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for MASI (TargetReturn): TCN with MSE=0.1466
üêõ DEBUG: MASI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for MASI.
üêõ DEBUG: MASI - Moving model to CPU before return...
üêõ DEBUG [00:28:42.368]: MASI - Returning result metadata...
üêõ DEBUG: train_worker started for FDTS
üêõ DEBUG [00:28:42.369]: Main received result for MASI
üêõ DEBUG [00:28:42.369]: Main received result for RSG
  ‚öôÔ∏è Training models for FDTS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - FDTS: Initiating feature extraction for training.
  [DIAGNOSTIC] FDTS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDTS: rows after features available: 126
üéØ FDTS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  ‚öôÔ∏è Training models for WTFC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - WTFC: Initiating feature extraction for training.
  [DIAGNOSTIC] WTFC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WTFC: rows after features available: 126
üéØ WTFC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDTS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDTS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDTS: Training LSTM (50 epochs)...
  [DIAGNOSTIC] WTFC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WTFC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WTFC: Training LSTM (50 epochs)...
      ‚è≥ WTFC LSTM: Epoch 10/50 (20%)
      ‚è≥ FDTS LSTM: Epoch 10/50 (20%)
      ‚è≥ WTFC LSTM: Epoch 20/50 (40%)
      ‚è≥ FDTS LSTM: Epoch 20/50 (40%)
      ‚è≥ WTFC LSTM: Epoch 30/50 (60%)
       ‚úÖ UIVM Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.7054 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UIVM LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ FDTS LSTM: Epoch 30/50 (60%)
      ‚è≥ WTFC LSTM: Epoch 40/50 (80%)
      ‚è≥ FDTS LSTM: Epoch 40/50 (80%)
       ‚úÖ UIVM LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.0348 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UIVM XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.403215
         RMSE: 0.634992
         R¬≤ Score: -0.6225 (Poor - 62.3% variance explained)
      üîπ WTFC: Training TCN (50 epochs)...
      ‚è≥ WTFC TCN: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.335221
         RMSE: 0.578983
         R¬≤ Score: -1.4578 (Poor - 145.8% variance explained)
      üîπ FDTS: Training TCN (50 epochs)...
      ‚è≥ WTFC TCN: Epoch 20/50 (40%)
      ‚è≥ FDTS TCN: Epoch 10/50 (20%)
       ‚úÖ KEMQ Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.2074 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 3.3s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ KEMQ LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ WTFC TCN: Epoch 30/50 (60%)
      ‚è≥ FDTS TCN: Epoch 20/50 (40%)
      ‚è≥ WTFC TCN: Epoch 40/50 (80%)
      ‚è≥ FDTS TCN: Epoch 30/50 (60%)
      üìä TCN Regression Metrics:
         MSE: 0.460493
         RMSE: 0.678597
         R¬≤ Score: -0.8530
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WTFC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WTFC Random Forest: Starting GridSearchCV fit...
      ‚è≥ FDTS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.219977
         RMSE: 0.469017
         R¬≤ Score: -0.6128
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDTS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDTS Random Forest: Starting GridSearchCV fit...
       ‚úÖ KEMQ LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.2778 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ KEMQ XGBoost: Starting GridSearchCV fit...
       ‚úÖ WTFC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.0083 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WTFC LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDTS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.6025 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDTS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WTFC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.0251 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WTFC XGBoost: Starting GridSearchCV fit...
       ‚úÖ FDTS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.1480 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDTS XGBoost: Starting GridSearchCV fit...
       ‚úÖ IRON XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=24.4682 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}) | Time: 130.5s
    - LSTM: MSE=0.4451
    - TCN: MSE=0.3461
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 133.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3461
        ‚Ä¢ LSTM: MSE=0.4451
        ‚Ä¢ Random Forest: MSE=20.2878
        ‚Ä¢ XGBoost: MSE=24.4682
        ‚Ä¢ LightGBM Regressor (CPU): MSE=27.4897
   ‚úÖ IRON: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IRON (TargetReturn): TCN with MSE=0.3461
üêõ DEBUG: IRON - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IRON.
üêõ DEBUG: IRON - Moving model to CPU before return...
üêõ DEBUG [00:29:02.173]: IRON - Returning result metadata...
üêõ DEBUG: train_worker started for STEL
üêõ DEBUG [00:29:02.182]: Main received result for IRON
üêõ DEBUG: Training progress: 920/959 done
  ‚öôÔ∏è Training models for STEL (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - STEL: Initiating feature extraction for training.
  [DIAGNOSTIC] STEL: fetch_training_data - Initial data rows: 205
   ‚Ü≥ STEL: rows after features available: 126
üéØ STEL: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] STEL: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö STEL: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ STEL: Training LSTM (50 epochs)...
      ‚è≥ STEL LSTM: Epoch 10/50 (20%)
      ‚è≥ STEL LSTM: Epoch 20/50 (40%)
      ‚è≥ STEL LSTM: Epoch 30/50 (60%)
      ‚è≥ STEL LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.717336
         RMSE: 0.846957
         R¬≤ Score: -0.8199 (Poor - 82.0% variance explained)
      üîπ STEL: Training TCN (50 epochs)...
      ‚è≥ STEL TCN: Epoch 10/50 (20%)
      ‚è≥ STEL TCN: Epoch 20/50 (40%)
      ‚è≥ STEL TCN: Epoch 30/50 (60%)
      ‚è≥ STEL TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.561919
         RMSE: 0.749612
         R¬≤ Score: -0.4256
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä STEL: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ STEL Random Forest: Starting GridSearchCV fit...
       ‚úÖ STEL Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=6.4086 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ STEL LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ STEL LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.4546 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ STEL XGBoost: Starting GridSearchCV fit...
       ‚úÖ CPA XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.9720 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.8s
    - LSTM: MSE=0.4790
    - TCN: MSE=0.2414
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.2 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2414
        ‚Ä¢ LSTM: MSE=0.4790
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.6804
        ‚Ä¢ XGBoost: MSE=14.9720
        ‚Ä¢ Random Forest: MSE=15.6971
   ‚úÖ CPA: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CPA (TargetReturn): TCN with MSE=0.2414
üêõ DEBUG: CPA - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CPA.
üêõ DEBUG: CPA - Moving model to CPU before return...
üêõ DEBUG [00:29:14.166]: CPA - Returning result metadata...
üêõ DEBUG: train_worker started for XITK
üêõ DEBUG [00:29:14.166]: Main received result for CPA
  ‚öôÔ∏è Training models for XITK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - XITK: Initiating feature extraction for training.
  [DIAGNOSTIC] XITK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ XITK: rows after features available: 126
üéØ XITK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] XITK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö XITK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ XITK: Training LSTM (50 epochs)...
      ‚è≥ XITK LSTM: Epoch 10/50 (20%)
      ‚è≥ XITK LSTM: Epoch 20/50 (40%)
      ‚è≥ XITK LSTM: Epoch 30/50 (60%)
      ‚è≥ XITK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.471259
         RMSE: 0.686483
         R¬≤ Score: -0.9335 (Poor - 93.4% variance explained)
      üîπ XITK: Training TCN (50 epochs)...
      ‚è≥ XITK TCN: Epoch 10/50 (20%)
      ‚è≥ XITK TCN: Epoch 20/50 (40%)
      ‚è≥ XITK TCN: Epoch 30/50 (60%)
      ‚è≥ XITK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.437717
         RMSE: 0.661602
         R¬≤ Score: -0.7959
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä XITK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ XITK Random Forest: Starting GridSearchCV fit...
       ‚úÖ XITK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.3285 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ XITK LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ XITK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.5794 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ XITK XGBoost: Starting GridSearchCV fit...
       ‚úÖ GEL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=25.6803 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.6s
    - LSTM: MSE=0.1125
    - TCN: MSE=0.0596
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.2 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0596
        ‚Ä¢ LSTM: MSE=0.1125
        ‚Ä¢ Random Forest: MSE=19.6056
        ‚Ä¢ XGBoost: MSE=25.6803
        ‚Ä¢ LightGBM Regressor (CPU): MSE=31.5052
   ‚úÖ GEL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GEL (TargetReturn): TCN with MSE=0.0596
üêõ DEBUG: GEL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GEL.
üêõ DEBUG: GEL - Moving model to CPU before return...
üêõ DEBUG [00:29:23.406]: GEL - Returning result metadata...
üêõ DEBUG: train_worker started for VSAT
üêõ DEBUG [00:29:23.407]: Main received result for GEL
  ‚öôÔ∏è Training models for VSAT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - VSAT: Initiating feature extraction for training.
  [DIAGNOSTIC] VSAT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ VSAT: rows after features available: 126
üéØ VSAT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] VSAT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö VSAT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ VSAT: Training LSTM (50 epochs)...
      ‚è≥ VSAT LSTM: Epoch 10/50 (20%)
      ‚è≥ VSAT LSTM: Epoch 20/50 (40%)
      ‚è≥ VSAT LSTM: Epoch 30/50 (60%)
      ‚è≥ VSAT LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.574331
         RMSE: 0.757846
         R¬≤ Score: -1.2389 (Poor - 123.9% variance explained)
      üîπ VSAT: Training TCN (50 epochs)...
      ‚è≥ VSAT TCN: Epoch 10/50 (20%)
      ‚è≥ VSAT TCN: Epoch 20/50 (40%)
      ‚è≥ VSAT TCN: Epoch 30/50 (60%)
      ‚è≥ VSAT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.325632
         RMSE: 0.570641
         R¬≤ Score: -0.2694
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä VSAT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ VSAT Random Forest: Starting GridSearchCV fit...
       ‚úÖ VSAT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=144.9372 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.5s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ VSAT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ VSAT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=152.2011 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ VSAT XGBoost: Starting GridSearchCV fit...
       ‚úÖ SIMO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=119.2178 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 130.8s
    - LSTM: MSE=0.8989
    - TCN: MSE=0.3713
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 134.4 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3713
        ‚Ä¢ LSTM: MSE=0.8989
        ‚Ä¢ LightGBM Regressor (CPU): MSE=92.4011
        ‚Ä¢ Random Forest: MSE=100.7528
        ‚Ä¢ XGBoost: MSE=119.2178
   ‚úÖ SIMO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SIMO (TargetReturn): TCN with MSE=0.3713
üêõ DEBUG: SIMO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SIMO.
üêõ DEBUG: SIMO - Moving model to CPU before return...
üêõ DEBUG [00:29:37.920]: SIMO - Returning result metadata...
üêõ DEBUG: train_worker started for EPR
üêõ DEBUG [00:29:37.921]: Main received result for SIMO
  ‚öôÔ∏è Training models for EPR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - EPR: Initiating feature extraction for training.
  [DIAGNOSTIC] EPR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ EPR: rows after features available: 126
üéØ EPR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] EPR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö EPR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ EPR: Training LSTM (50 epochs)...
      ‚è≥ EPR LSTM: Epoch 10/50 (20%)
      ‚è≥ EPR LSTM: Epoch 20/50 (40%)
      ‚è≥ EPR LSTM: Epoch 30/50 (60%)
      ‚è≥ EPR LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.396251
         RMSE: 0.629484
         R¬≤ Score: -1.2409 (Poor - 124.1% variance explained)
      üîπ EPR: Training TCN (50 epochs)...
      ‚è≥ EPR TCN: Epoch 10/50 (20%)
      ‚è≥ EPR TCN: Epoch 20/50 (40%)
      ‚è≥ EPR TCN: Epoch 30/50 (60%)
      ‚è≥ EPR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.265783
         RMSE: 0.515542
         R¬≤ Score: -0.5031
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä EPR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ EPR Random Forest: Starting GridSearchCV fit...
       ‚úÖ EPR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.9143 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ EPR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ EPR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=8.3801 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ EPR XGBoost: Starting GridSearchCV fit...
       ‚úÖ PEJ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.3947 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.6s
    - LSTM: MSE=1.0224
    - TCN: MSE=0.5409
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5409
        ‚Ä¢ LSTM: MSE=1.0224
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.6154
        ‚Ä¢ Random Forest: MSE=8.3233
        ‚Ä¢ XGBoost: MSE=8.3947
   ‚úÖ PEJ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PEJ (TargetReturn): TCN with MSE=0.5409
üêõ DEBUG: PEJ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PEJ.
üêõ DEBUG: PEJ - Moving model to CPU before return...
üêõ DEBUG [00:30:00.506]: PEJ - Returning result metadata...
üêõ DEBUG: train_worker started for SPOK
  ‚öôÔ∏è Training models for SPOK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - SPOK: Initiating feature extraction for training.
  [DIAGNOSTIC] SPOK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SPOK: rows after features available: 126
üéØ SPOK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SPOK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SPOK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SPOK: Training LSTM (50 epochs)...
      ‚è≥ SPOK LSTM: Epoch 10/50 (20%)
      ‚è≥ SPOK LSTM: Epoch 20/50 (40%)
      ‚è≥ SPOK LSTM: Epoch 30/50 (60%)
      ‚è≥ SPOK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.463144
         RMSE: 0.680547
         R¬≤ Score: -1.7956 (Poor - 179.6% variance explained)
      üîπ SPOK: Training TCN (50 epochs)...
      ‚è≥ SPOK TCN: Epoch 10/50 (20%)
      ‚è≥ SPOK TCN: Epoch 20/50 (40%)
      ‚è≥ SPOK TCN: Epoch 30/50 (60%)
      ‚è≥ SPOK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.186206
         RMSE: 0.431516
         R¬≤ Score: -0.1240
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SPOK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SPOK Random Forest: Starting GridSearchCV fit...
       ‚úÖ GT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=44.0742 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 133.7s
    - LSTM: MSE=0.1869
    - TCN: MSE=0.1211
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 137.0 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1211
        ‚Ä¢ LSTM: MSE=0.1869
        ‚Ä¢ LightGBM Regressor (CPU): MSE=36.7080
        ‚Ä¢ Random Forest: MSE=39.7061
        ‚Ä¢ XGBoost: MSE=44.0742
   ‚úÖ GT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for GT (TargetReturn): TCN with MSE=0.1211
üêõ DEBUG: GT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for GT.
üêõ DEBUG: GT - Moving model to CPU before return...
üêõ DEBUG [00:30:04.857]: GT - Returning result metadata...
üêõ DEBUG [00:30:04.858]: Main received result for GT
üêõ DEBUG: Training progress: 924/959 done
üêõ DEBUG [00:30:04.858]: Main received result for PEJ
üêõ DEBUG: train_worker started for TRIP
  ‚öôÔ∏è Training models for TRIP (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - TRIP: Initiating feature extraction for training.
  [DIAGNOSTIC] TRIP: fetch_training_data - Initial data rows: 205
   ‚Ü≥ TRIP: rows after features available: 126
üéØ TRIP: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] TRIP: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö TRIP: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ TRIP: Training LSTM (50 epochs)...
      ‚è≥ TRIP LSTM: Epoch 10/50 (20%)
      ‚è≥ TRIP LSTM: Epoch 20/50 (40%)
       ‚úÖ SPOK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=13.2057 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.0s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SPOK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ TRIP LSTM: Epoch 30/50 (60%)
      ‚è≥ TRIP LSTM: Epoch 40/50 (80%)
       ‚úÖ SPOK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=12.1040 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SPOK XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.609366
         RMSE: 0.780619
         R¬≤ Score: -0.8665 (Poor - 86.7% variance explained)
      üîπ TRIP: Training TCN (50 epochs)...
      ‚è≥ TRIP TCN: Epoch 10/50 (20%)
      ‚è≥ TRIP TCN: Epoch 20/50 (40%)
      ‚è≥ TRIP TCN: Epoch 30/50 (60%)
      ‚è≥ TRIP TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.444968
         RMSE: 0.667059
         R¬≤ Score: -0.3630
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä TRIP: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ TRIP Random Forest: Starting GridSearchCV fit...
       ‚úÖ FEDU XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=207.6880 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 132.1s
    - LSTM: MSE=0.6596
    - TCN: MSE=0.4299
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.5 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4299
        ‚Ä¢ LSTM: MSE=0.6596
        ‚Ä¢ Random Forest: MSE=101.7186
        ‚Ä¢ XGBoost: MSE=207.6880
        ‚Ä¢ LightGBM Regressor (CPU): MSE=274.9566
   ‚úÖ FEDU: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FEDU (TargetReturn): TCN with MSE=0.4299
üêõ DEBUG: FEDU - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FEDU.
üêõ DEBUG: FEDU - Moving model to CPU before return...
üêõ DEBUG [00:30:09.036]: FEDU - Returning result metadata...
üêõ DEBUG: train_worker started for IXG
üêõ DEBUG [00:30:09.037]: Main received result for FEDU
  ‚öôÔ∏è Training models for IXG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - IXG: Initiating feature extraction for training.
  [DIAGNOSTIC] IXG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ IXG: rows after features available: 126
üéØ IXG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] IXG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö IXG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ IXG: Training LSTM (50 epochs)...
      ‚è≥ IXG LSTM: Epoch 10/50 (20%)
      ‚è≥ IXG LSTM: Epoch 20/50 (40%)
       ‚úÖ TRIP Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=30.2720 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ TRIP LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ IXG LSTM: Epoch 30/50 (60%)
      ‚è≥ IXG LSTM: Epoch 40/50 (80%)
       ‚úÖ TRIP LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.0468 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ TRIP XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.299542
         RMSE: 0.547304
         R¬≤ Score: -1.0048 (Poor - 100.5% variance explained)
      üîπ IXG: Training TCN (50 epochs)...
      ‚è≥ IXG TCN: Epoch 10/50 (20%)
      ‚è≥ IXG TCN: Epoch 20/50 (40%)
      ‚è≥ IXG TCN: Epoch 30/50 (60%)
      ‚è≥ IXG TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.242893
         RMSE: 0.492841
         R¬≤ Score: -0.6257
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä IXG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ IXG Random Forest: Starting GridSearchCV fit...
       ‚úÖ IXG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.6788 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ IXG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ IXG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=6.1806 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ IXG XGBoost: Starting GridSearchCV fit...
       ‚úÖ SPRY XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=50.5453 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 131.7s
    - LSTM: MSE=0.2519
    - TCN: MSE=0.1047
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 135.9 seconds (2.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1047
        ‚Ä¢ LSTM: MSE=0.2519
        ‚Ä¢ XGBoost: MSE=50.5453
        ‚Ä¢ LightGBM Regressor (CPU): MSE=53.5601
        ‚Ä¢ Random Forest: MSE=64.8796
   ‚úÖ SPRY: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPRY (TargetReturn): TCN with MSE=0.1047
üêõ DEBUG: SPRY - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPRY.
üêõ DEBUG: SPRY - Moving model to CPU before return...
üêõ DEBUG [00:30:19.087]: SPRY - Returning result metadata...
üêõ DEBUG [00:30:19.087]: Main received result for SPRYüêõ DEBUG: train_worker started for PDLB

  ‚öôÔ∏è Training models for PDLB (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - PDLB: Initiating feature extraction for training.
  [DIAGNOSTIC] PDLB: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PDLB: rows after features available: 126
üéØ PDLB: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PDLB: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PDLB: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PDLB: Training LSTM (50 epochs)...
      ‚è≥ PDLB LSTM: Epoch 10/50 (20%)
      ‚è≥ PDLB LSTM: Epoch 20/50 (40%)
      ‚è≥ PDLB LSTM: Epoch 30/50 (60%)
      ‚è≥ PDLB LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.362213
         RMSE: 0.601841
         R¬≤ Score: -0.7572 (Poor - 75.7% variance explained)
      üîπ PDLB: Training TCN (50 epochs)...
      ‚è≥ PDLB TCN: Epoch 10/50 (20%)
      ‚è≥ PDLB TCN: Epoch 20/50 (40%)
      ‚è≥ PDLB TCN: Epoch 30/50 (60%)
      ‚è≥ PDLB TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.331024
         RMSE: 0.575347
         R¬≤ Score: -0.6059
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PDLB: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PDLB Random Forest: Starting GridSearchCV fit...
       ‚úÖ PDLB Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.1908 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PDLB LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PDLB LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.0604 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PDLB XGBoost: Starting GridSearchCV fit...
       ‚úÖ CFR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=6.3108 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 126.1s
    - LSTM: MSE=0.7796
    - TCN: MSE=0.3840
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.6 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3840
        ‚Ä¢ LSTM: MSE=0.7796
        ‚Ä¢ XGBoost: MSE=6.3108
        ‚Ä¢ Random Forest: MSE=8.4902
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.9520
   ‚úÖ CFR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CFR (TargetReturn): TCN with MSE=0.3840
üêõ DEBUG: CFR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CFR.
üêõ DEBUG: CFR - Moving model to CPU before return...
üêõ DEBUG [00:30:33.196]: CFR - Returning result metadata...
üêõ DEBUG: train_worker started for FTDR
üêõ DEBUG [00:30:33.197]: Main received result for CFR
üêõ DEBUG: Training progress: 928/959 done
  ‚öôÔ∏è Training models for FTDR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FTDR: Initiating feature extraction for training.
  [DIAGNOSTIC] FTDR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FTDR: rows after features available: 126
üéØ FTDR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FTDR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FTDR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FTDR: Training LSTM (50 epochs)...
      ‚è≥ FTDR LSTM: Epoch 10/50 (20%)
      ‚è≥ FTDR LSTM: Epoch 20/50 (40%)
      ‚è≥ FTDR LSTM: Epoch 30/50 (60%)
      ‚è≥ FTDR LSTM: Epoch 40/50 (80%)
       ‚úÖ CPNG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=40.6623 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 127.5s
    - LSTM: MSE=0.4610
    - TCN: MSE=0.3928
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 131.0 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3928
        ‚Ä¢ LSTM: MSE=0.4610
        ‚Ä¢ LightGBM Regressor (CPU): MSE=14.9010
        ‚Ä¢ Random Forest: MSE=24.6129
        ‚Ä¢ XGBoost: MSE=40.6623
   ‚úÖ CPNG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CPNG (TargetReturn): TCN with MSE=0.3928
üêõ DEBUG: CPNG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CPNG.
üêõ DEBUG: CPNG - Moving model to CPU before return...
üêõ DEBUG [00:30:35.380]: CPNG - Returning result metadata...
üêõ DEBUG: train_worker started for QLD
  ‚öôÔ∏è Training models for QLD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - QLD: Initiating feature extraction for training.
  [DIAGNOSTIC] QLD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ QLD: rows after features available: 126
üéØ QLD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] QLD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö QLD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ QLD: Training LSTM (50 epochs)...
      üìä LSTM Regression Metrics:
         MSE: 0.984111
         RMSE: 0.992024
         R¬≤ Score: -1.2526 (Poor - 125.3% variance explained)
      üîπ FTDR: Training TCN (50 epochs)...
      ‚è≥ FTDR TCN: Epoch 10/50 (20%)
      ‚è≥ FTDR TCN: Epoch 20/50 (40%)
      ‚è≥ FTDR TCN: Epoch 30/50 (60%)
       ‚úÖ CSGP XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.4268 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 128.6s
    - LSTM: MSE=0.3130
    - TCN: MSE=0.1963
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 132.1 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1963
        ‚Ä¢ LSTM: MSE=0.3130
        ‚Ä¢ LightGBM Regressor (CPU): MSE=5.8758
        ‚Ä¢ Random Forest: MSE=6.6515
        ‚Ä¢ XGBoost: MSE=7.4268
   ‚úÖ CSGP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CSGP (TargetReturn): TCN with MSE=0.1963
üêõ DEBUG: CSGP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CSGP.
üêõ DEBUG: CSGP - Moving model to CPU before return...
üêõ DEBUG [00:30:35.935]: CSGP - Returning result metadata...
üêõ DEBUG [00:30:35.936]: Main received result for CSGP
üêõ DEBUG [00:30:35.936]: Main received result for CPNG
üêõ DEBUG: train_worker started for HURN
      ‚è≥ QLD LSTM: Epoch 10/50 (20%)
      ‚è≥ FTDR TCN: Epoch 40/50 (80%)
  ‚öôÔ∏è Training models for HURN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-74 - HURN: Initiating feature extraction for training.
  [DIAGNOSTIC] HURN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ HURN: rows after features available: 126
üéØ HURN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] HURN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö HURN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ HURN: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.567858
         RMSE: 0.753564
         R¬≤ Score: -0.2998
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FTDR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FTDR Random Forest: Starting GridSearchCV fit...
      ‚è≥ QLD LSTM: Epoch 20/50 (40%)
      ‚è≥ HURN LSTM: Epoch 10/50 (20%)
      ‚è≥ QLD LSTM: Epoch 30/50 (60%)
      ‚è≥ HURN LSTM: Epoch 20/50 (40%)
      ‚è≥ QLD LSTM: Epoch 40/50 (80%)
      ‚è≥ HURN LSTM: Epoch 30/50 (60%)
      üìä LSTM Regression Metrics:
         MSE: 0.824729
         RMSE: 0.908146
         R¬≤ Score: -2.0806 (Poor - 208.1% variance explained)
      üîπ QLD: Training TCN (50 epochs)...
      ‚è≥ HURN LSTM: Epoch 40/50 (80%)
      ‚è≥ QLD TCN: Epoch 10/50 (20%)
      ‚è≥ QLD TCN: Epoch 20/50 (40%)
      ‚è≥ QLD TCN: Epoch 30/50 (60%)
      ‚è≥ QLD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.451474
         RMSE: 0.671918
         R¬≤ Score: -0.6864
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä QLD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ QLD Random Forest: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.462777
         RMSE: 0.680277
         R¬≤ Score: -0.6026 (Poor - 60.3% variance explained)
      üîπ HURN: Training TCN (50 epochs)...
      ‚è≥ HURN TCN: Epoch 10/50 (20%)
      ‚è≥ HURN TCN: Epoch 20/50 (40%)
       ‚úÖ FTDR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.1793 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FTDR LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ HURN TCN: Epoch 30/50 (60%)
      ‚è≥ HURN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.409083
         RMSE: 0.639596
         R¬≤ Score: -0.4166
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä HURN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ HURN Random Forest: Starting GridSearchCV fit...
       ‚úÖ FTDR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.2462 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FTDR XGBoost: Starting GridSearchCV fit...
       ‚úÖ QLD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=28.9702 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ QLD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ HURN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=14.2609 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ HURN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ QLD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=42.9651 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ QLD XGBoost: Starting GridSearchCV fit...
       ‚úÖ HURN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=13.3537 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ HURN XGBoost: Starting GridSearchCV fit...
       ‚úÖ UIVM XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3.8551 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.5s
    - LSTM: MSE=0.1172
    - TCN: MSE=0.0875
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 127.5 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.0875
        ‚Ä¢ LSTM: MSE=0.1172
        ‚Ä¢ XGBoost: MSE=3.8551
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.0348
        ‚Ä¢ Random Forest: MSE=4.7054
   ‚úÖ UIVM: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UIVM (TargetReturn): TCN with MSE=0.0875
üêõ DEBUG: UIVM - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UIVM.
üêõ DEBUG: UIVM - Moving model to CPU before return...
üêõ DEBUG [00:30:48.303]: UIVM - Returning result metadata...
üêõ DEBUG: train_worker started for CECO
üêõ DEBUG [00:30:48.304]: Main received result for UIVM
  ‚öôÔ∏è Training models for CECO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-65 - CECO: Initiating feature extraction for training.
  [DIAGNOSTIC] CECO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CECO: rows after features available: 126
üéØ CECO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CECO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CECO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CECO: Training LSTM (50 epochs)...
      ‚è≥ CECO LSTM: Epoch 10/50 (20%)
      ‚è≥ CECO LSTM: Epoch 20/50 (40%)
      ‚è≥ CECO LSTM: Epoch 30/50 (60%)
       ‚úÖ FDTS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.3732 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.8s
    - LSTM: MSE=0.3352
    - TCN: MSE=0.2200
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.2 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2200
        ‚Ä¢ LSTM: MSE=0.3352
        ‚Ä¢ Random Forest: MSE=4.6025
        ‚Ä¢ XGBoost: MSE=5.3732
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.1480
   ‚úÖ FDTS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDTS (TargetReturn): TCN with MSE=0.2200
üêõ DEBUG: FDTS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDTS.
üêõ DEBUG: FDTS - Moving model to CPU before return...
üêõ DEBUG [00:30:50.856]: FDTS - Returning result metadata...
üêõ DEBUG: train_worker started for WTS
  ‚öôÔ∏è Training models for WTS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-73 - WTS: Initiating feature extraction for training.
  [DIAGNOSTIC] WTS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ WTS: rows after features available: 126
üéØ WTS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] WTS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö WTS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ WTS: Training LSTM (50 epochs)...
      ‚è≥ CECO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.407697
         RMSE: 0.638511
         R¬≤ Score: -0.7857 (Poor - 78.6% variance explained)
      üîπ CECO: Training TCN (50 epochs)...
      ‚è≥ WTS LSTM: Epoch 10/50 (20%)
      ‚è≥ CECO TCN: Epoch 10/50 (20%)
      ‚è≥ CECO TCN: Epoch 20/50 (40%)
      ‚è≥ CECO TCN: Epoch 30/50 (60%)
      ‚è≥ CECO TCN: Epoch 40/50 (80%)
       ‚úÖ KEMQ XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.4430 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 125.7s
    - LSTM: MSE=0.4036
    - TCN: MSE=0.4917
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 129.8 seconds (2.2 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4036
        ‚Ä¢ TCN: MSE=0.4917
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.2778
        ‚Ä¢ XGBoost: MSE=18.4430
        ‚Ä¢ Random Forest: MSE=19.2074
   ‚úÖ KEMQ: Phase 3/3 - Model selection complete!
  üèÜ WINNER for KEMQ (TargetReturn): LSTM with MSE=0.4036
üêõ DEBUG: KEMQ - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for KEMQ.
üêõ DEBUG: KEMQ - Moving model to CPU before return...
üêõ DEBUG [00:30:51.798]: KEMQ - Returning result metadata...
üêõ DEBUG: train_worker started for ALG
üêõ DEBUG [00:30:51.801]: Main received result for KEMQ
üêõ DEBUG: Training progress: 932/959 done
  ‚öôÔ∏è Training models for ALG (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-64 - ALG: Initiating feature extraction for training.
  [DIAGNOSTIC] ALG: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ALG: rows after features available: 126
üéØ ALG: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ALG: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ALG: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ALG: Training LSTM (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.340154
         RMSE: 0.583227
         R¬≤ Score: -0.4899
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CECO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CECO Random Forest: Starting GridSearchCV fit...
      ‚è≥ WTS LSTM: Epoch 20/50 (40%)
       ‚úÖ WTFC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=17.8000 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 123.4s
    - LSTM: MSE=0.4032
    - TCN: MSE=0.4605
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.9 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ LSTM: MSE=0.4032
        ‚Ä¢ TCN: MSE=0.4605
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.0251
        ‚Ä¢ XGBoost: MSE=17.8000
        ‚Ä¢ Random Forest: MSE=19.0083
   ‚úÖ WTFC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WTFC (TargetReturn): LSTM with MSE=0.4032
üêõ DEBUG: WTFC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WTFC.
üêõ DEBUG: WTFC - Moving model to CPU before return...
üêõ DEBUG [00:30:52.415]: WTFC - Returning result metadata...
üêõ DEBUG: train_worker started for CMPO
üêõ DEBUG [00:30:52.416]: Main received result for WTFC
üêõ DEBUG [00:30:52.416]: Main received result for FDTS
      ‚è≥ WTS LSTM: Epoch 30/50 (60%)
      ‚è≥ ALG LSTM: Epoch 10/50 (20%)
  ‚öôÔ∏è Training models for CMPO (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-61 - CMPO: Initiating feature extraction for training.
  [DIAGNOSTIC] CMPO: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CMPO: rows after features available: 126
üéØ CMPO: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CMPO: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CMPO: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CMPO: Training LSTM (50 epochs)...
      ‚è≥ WTS LSTM: Epoch 40/50 (80%)
      ‚è≥ ALG LSTM: Epoch 20/50 (40%)
      ‚è≥ CMPO LSTM: Epoch 10/50 (20%)
      ‚è≥ CMPO LSTM: Epoch 20/50 (40%)
      üìä LSTM Regression Metrics:
         MSE: 0.318835
         RMSE: 0.564655
         R¬≤ Score: -0.8524 (Poor - 85.2% variance explained)
      üîπ WTS: Training TCN (50 epochs)...
      ‚è≥ ALG LSTM: Epoch 30/50 (60%)
      ‚è≥ WTS TCN: Epoch 10/50 (20%)
      ‚è≥ WTS TCN: Epoch 20/50 (40%)
      ‚è≥ WTS TCN: Epoch 30/50 (60%)
      ‚è≥ WTS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.214885
         RMSE: 0.463556
         R¬≤ Score: -0.2484
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä WTS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ WTS Random Forest: Starting GridSearchCV fit...
      ‚è≥ ALG LSTM: Epoch 40/50 (80%)
      ‚è≥ CMPO LSTM: Epoch 30/50 (60%)
      ‚è≥ CMPO LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.833001
         RMSE: 0.912689
         R¬≤ Score: -2.0762 (Poor - 207.6% variance explained)
      üîπ ALG: Training TCN (50 epochs)...
      ‚è≥ ALG TCN: Epoch 10/50 (20%)
      ‚è≥ ALG TCN: Epoch 20/50 (40%)
      ‚è≥ ALG TCN: Epoch 30/50 (60%)
       ‚úÖ CECO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=26.0179 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.1s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CECO LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ALG TCN: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.640898
         RMSE: 0.800561
         R¬≤ Score: -1.0822 (Poor - 108.2% variance explained)
      üîπ CMPO: Training TCN (50 epochs)...
      üìä TCN Regression Metrics:
         MSE: 0.478862
         RMSE: 0.691999
         R¬≤ Score: -0.7684
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ALG: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ALG Random Forest: Starting GridSearchCV fit...
      ‚è≥ CMPO TCN: Epoch 10/50 (20%)
      ‚è≥ CMPO TCN: Epoch 20/50 (40%)
      ‚è≥ CMPO TCN: Epoch 30/50 (60%)
      ‚è≥ CMPO TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.470180
         RMSE: 0.685697
         R¬≤ Score: -0.5276
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CMPO: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CMPO Random Forest: Starting GridSearchCV fit...
       ‚úÖ CECO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=41.3145 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CECO XGBoost: Starting GridSearchCV fit...
       ‚úÖ WTS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=7.0776 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ WTS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ WTS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=9.5692 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ WTS XGBoost: Starting GridSearchCV fit...
       ‚úÖ CMPO Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=19.5452 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CMPO LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ALG Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=21.9736 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 3.2s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ALG LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CMPO LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=25.0122 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CMPO XGBoost: Starting GridSearchCV fit...
       ‚úÖ ALG LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.9180 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ALG XGBoost: Starting GridSearchCV fit...
       ‚úÖ STEL XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.6630 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 122.8s
    - LSTM: MSE=0.7173
    - TCN: MSE=0.5619
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 126.3 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5619
        ‚Ä¢ LSTM: MSE=0.7173
        ‚Ä¢ Random Forest: MSE=6.4086
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.4546
        ‚Ä¢ XGBoost: MSE=7.6630
   ‚úÖ STEL: Phase 3/3 - Model selection complete!
  üèÜ WINNER for STEL (TargetReturn): TCN with MSE=0.5619
üêõ DEBUG: STEL - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for STEL.
üêõ DEBUG: STEL - Moving model to CPU before return...
üêõ DEBUG [00:31:11.316]: STEL - Returning result metadata...
üêõ DEBUG: train_worker started for SMBK
üêõ DEBUG [00:31:11.317]: Main received result for STEL
  ‚öôÔ∏è Training models for SMBK (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-69 - SMBK: Initiating feature extraction for training.
  [DIAGNOSTIC] SMBK: fetch_training_data - Initial data rows: 205
   ‚Ü≥ SMBK: rows after features available: 126
üéØ SMBK: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] SMBK: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö SMBK: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ SMBK: Training LSTM (50 epochs)...
      ‚è≥ SMBK LSTM: Epoch 10/50 (20%)
      ‚è≥ SMBK LSTM: Epoch 20/50 (40%)
      ‚è≥ SMBK LSTM: Epoch 30/50 (60%)
      ‚è≥ SMBK LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.917004
         RMSE: 0.957603
         R¬≤ Score: -1.5636 (Poor - 156.4% variance explained)
      üîπ SMBK: Training TCN (50 epochs)...
      ‚è≥ SMBK TCN: Epoch 10/50 (20%)
      ‚è≥ SMBK TCN: Epoch 20/50 (40%)
      ‚è≥ SMBK TCN: Epoch 30/50 (60%)
      ‚è≥ SMBK TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.386746
         RMSE: 0.621889
         R¬≤ Score: -0.0812
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä SMBK: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ SMBK Random Forest: Starting GridSearchCV fit...
       ‚úÖ XITK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=12.9691 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 115.1s
    - LSTM: MSE=0.4713
    - TCN: MSE=0.4377
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 118.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4377
        ‚Ä¢ LSTM: MSE=0.4713
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.5794
        ‚Ä¢ XGBoost: MSE=12.9691
        ‚Ä¢ Random Forest: MSE=14.3285
   ‚úÖ XITK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for XITK (TargetReturn): TCN with MSE=0.4377
üêõ DEBUG: XITK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for XITK.
üêõ DEBUG: XITK - Moving model to CPU before return...
üêõ DEBUG [00:31:15.558]: XITK - Returning result metadata...
üêõ DEBUG: train_worker started for AIT
üêõ DEBUG [00:31:15.561]: Main received result for XITK
üêõ DEBUG: Training progress: 936/959 done
  ‚öôÔ∏è Training models for AIT (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-62 - AIT: Initiating feature extraction for training.
  [DIAGNOSTIC] AIT: fetch_training_data - Initial data rows: 205
   ‚Ü≥ AIT: rows after features available: 126
üéØ AIT: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] AIT: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö AIT: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ AIT: Training LSTM (50 epochs)...
      ‚è≥ AIT LSTM: Epoch 10/50 (20%)
      ‚è≥ AIT LSTM: Epoch 20/50 (40%)
       ‚úÖ SMBK Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=11.3903 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ SMBK LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ AIT LSTM: Epoch 30/50 (60%)
      ‚è≥ AIT LSTM: Epoch 40/50 (80%)
       ‚úÖ SMBK LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=11.0250 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.9s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ SMBK XGBoost: Starting GridSearchCV fit...
      üìä LSTM Regression Metrics:
         MSE: 0.619907
         RMSE: 0.787341
         R¬≤ Score: -1.2551 (Poor - 125.5% variance explained)
      üîπ AIT: Training TCN (50 epochs)...
      ‚è≥ AIT TCN: Epoch 10/50 (20%)
      ‚è≥ AIT TCN: Epoch 20/50 (40%)
      ‚è≥ AIT TCN: Epoch 30/50 (60%)
      ‚è≥ AIT TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.368170
         RMSE: 0.606770
         R¬≤ Score: -0.3393
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä AIT: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ AIT Random Forest: Starting GridSearchCV fit...
       ‚úÖ AIT Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.2075 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ AIT LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ AIT LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=10.0960 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ AIT XGBoost: Starting GridSearchCV fit...
       ‚úÖ VSAT XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=131.8738 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 121.4s
    - LSTM: MSE=0.5743
    - TCN: MSE=0.3256
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 124.6 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3256
        ‚Ä¢ LSTM: MSE=0.5743
        ‚Ä¢ XGBoost: MSE=131.8738
        ‚Ä¢ Random Forest: MSE=144.9372
        ‚Ä¢ LightGBM Regressor (CPU): MSE=152.2011
   ‚úÖ VSAT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for VSAT (TargetReturn): TCN with MSE=0.3256
üêõ DEBUG: VSAT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for VSAT.
üêõ DEBUG: VSAT - Moving model to CPU before return...
üêõ DEBUG [00:31:30.991]: VSAT - Returning result metadata...
üêõ DEBUG [00:31:30.993]: Main received result for VSATüêõ DEBUG: train_worker started for FDNI

  ‚öôÔ∏è Training models for FDNI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-75 - FDNI: Initiating feature extraction for training.
  [DIAGNOSTIC] FDNI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FDNI: rows after features available: 126
üéØ FDNI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FDNI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FDNI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FDNI: Training LSTM (50 epochs)...
      ‚è≥ FDNI LSTM: Epoch 10/50 (20%)
      ‚è≥ FDNI LSTM: Epoch 20/50 (40%)
      ‚è≥ FDNI LSTM: Epoch 30/50 (60%)
      ‚è≥ FDNI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.652857
         RMSE: 0.807996
         R¬≤ Score: -1.6310 (Poor - 163.1% variance explained)
      üîπ FDNI: Training TCN (50 epochs)...
      ‚è≥ FDNI TCN: Epoch 10/50 (20%)
      ‚è≥ FDNI TCN: Epoch 20/50 (40%)
      ‚è≥ FDNI TCN: Epoch 30/50 (60%)
      ‚è≥ FDNI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.307170
         RMSE: 0.554229
         R¬≤ Score: -0.2379
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FDNI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FDNI Random Forest: Starting GridSearchCV fit...
       ‚úÖ FDNI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=8.0163 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.8s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FDNI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FDNI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=7.3919 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FDNI XGBoost: Starting GridSearchCV fit...
       ‚úÖ EPR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=7.6480 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.0s
    - LSTM: MSE=0.3963
    - TCN: MSE=0.2658
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2658
        ‚Ä¢ LSTM: MSE=0.3963
        ‚Ä¢ XGBoost: MSE=7.6480
        ‚Ä¢ Random Forest: MSE=7.9143
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.3801
   ‚úÖ EPR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for EPR (TargetReturn): TCN with MSE=0.2658
üêõ DEBUG: EPR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for EPR.
üêõ DEBUG: EPR - Moving model to CPU before return...
üêõ DEBUG [00:31:41.431]: EPR - Returning result metadata...
üêõ DEBUG: train_worker started for CDNS
üêõ DEBUG [00:31:41.432]: Main received result for EPR
  ‚öôÔ∏è Training models for CDNS (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-63 - CDNS: Initiating feature extraction for training.
  [DIAGNOSTIC] CDNS: fetch_training_data - Initial data rows: 205
   ‚Ü≥ CDNS: rows after features available: 126
üéØ CDNS: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] CDNS: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö CDNS: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ CDNS: Training LSTM (50 epochs)...
      ‚è≥ CDNS LSTM: Epoch 10/50 (20%)
      ‚è≥ CDNS LSTM: Epoch 20/50 (40%)
      ‚è≥ CDNS LSTM: Epoch 30/50 (60%)
      ‚è≥ CDNS LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.197373
         RMSE: 0.444267
         R¬≤ Score: -0.5845 (Poor - 58.5% variance explained)
      üîπ CDNS: Training TCN (50 epochs)...
      ‚è≥ CDNS TCN: Epoch 10/50 (20%)
      ‚è≥ CDNS TCN: Epoch 20/50 (40%)
      ‚è≥ CDNS TCN: Epoch 30/50 (60%)
      ‚è≥ CDNS TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.162104
         RMSE: 0.402622
         R¬≤ Score: -0.3014
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä CDNS: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ CDNS Random Forest: Starting GridSearchCV fit...
       ‚úÖ CDNS Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=17.8027 (Best Params: {'max_depth': 10, 'n_estimators': 100}) | Time: 2.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ CDNS LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ CDNS LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=16.4128 (Best Params: {'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ CDNS XGBoost: Starting GridSearchCV fit...
       ‚úÖ SPOK XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.3102 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 116.6s
    - LSTM: MSE=0.4631
    - TCN: MSE=0.1862
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.3 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1862
        ‚Ä¢ LSTM: MSE=0.4631
        ‚Ä¢ XGBoost: MSE=11.3102
        ‚Ä¢ LightGBM Regressor (CPU): MSE=12.1040
        ‚Ä¢ Random Forest: MSE=13.2057
   ‚úÖ SPOK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SPOK (TargetReturn): TCN with MSE=0.1862
üêõ DEBUG: SPOK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SPOK.
üêõ DEBUG: SPOK - Moving model to CPU before return...
üêõ DEBUG [00:32:03.542]: SPOK - Returning result metadata...
üêõ DEBUG [00:32:03.543]: Main received result for SPOK
üêõ DEBUG: train_worker started for BBSI
  ‚öôÔ∏è Training models for BBSI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-68 - BBSI: Initiating feature extraction for training.
  [DIAGNOSTIC] BBSI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ BBSI: rows after features available: 126
üéØ BBSI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] BBSI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö BBSI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ BBSI: Training LSTM (50 epochs)...
      ‚è≥ BBSI LSTM: Epoch 10/50 (20%)
      ‚è≥ BBSI LSTM: Epoch 20/50 (40%)
      ‚è≥ BBSI LSTM: Epoch 30/50 (60%)
      ‚è≥ BBSI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.181253
         RMSE: 0.425738
         R¬≤ Score: -0.5661 (Poor - 56.6% variance explained)
      üîπ BBSI: Training TCN (50 epochs)...
      ‚è≥ BBSI TCN: Epoch 10/50 (20%)
      ‚è≥ BBSI TCN: Epoch 20/50 (40%)
      ‚è≥ BBSI TCN: Epoch 30/50 (60%)
      ‚è≥ BBSI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.120648
         RMSE: 0.347344
         R¬≤ Score: -0.0425
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä BBSI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ BBSI Random Forest: Starting GridSearchCV fit...
       ‚úÖ BBSI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=4.1123 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ BBSI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ BBSI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=3.4848 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.8s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ BBSI XGBoost: Starting GridSearchCV fit...
       ‚úÖ TRIP XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=26.9794 (Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 200}) | Time: 122.0s
    - LSTM: MSE=0.6094
    - TCN: MSE=0.4450
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 125.4 seconds (2.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4450
        ‚Ä¢ LSTM: MSE=0.6094
        ‚Ä¢ XGBoost: MSE=26.9794
        ‚Ä¢ Random Forest: MSE=30.2720
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.0468
   ‚úÖ TRIP: Phase 3/3 - Model selection complete!
  üèÜ WINNER for TRIP (TargetReturn): TCN with MSE=0.4450
üêõ DEBUG: TRIP - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for TRIP.
üêõ DEBUG: TRIP - Moving model to CPU before return...
üêõ DEBUG [00:32:13.185]: TRIP - Returning result metadata...
üêõ DEBUG [00:32:13.186]: Main received result for TRIP
üêõ DEBUG: Training progress: 940/959 done
üêõ DEBUG: train_worker started for PCOR
  ‚öôÔ∏è Training models for PCOR (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-66 - PCOR: Initiating feature extraction for training.
  [DIAGNOSTIC] PCOR: fetch_training_data - Initial data rows: 205
   ‚Ü≥ PCOR: rows after features available: 126
üéØ PCOR: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] PCOR: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö PCOR: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ PCOR: Training LSTM (50 epochs)...
      ‚è≥ PCOR LSTM: Epoch 10/50 (20%)
      ‚è≥ PCOR LSTM: Epoch 20/50 (40%)
      ‚è≥ PCOR LSTM: Epoch 30/50 (60%)
       ‚úÖ IXG XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=3.3886 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.4s
    - LSTM: MSE=0.2995
    - TCN: MSE=0.2429
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2429
        ‚Ä¢ LSTM: MSE=0.2995
        ‚Ä¢ XGBoost: MSE=3.3886
        ‚Ä¢ Random Forest: MSE=4.6788
        ‚Ä¢ LightGBM Regressor (CPU): MSE=6.1806
   ‚úÖ IXG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for IXG (TargetReturn): TCN with MSE=0.2429
üêõ DEBUG: IXG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for IXG.
üêõ DEBUG: IXG - Moving model to CPU before return...
üêõ DEBUG [00:32:14.593]: IXG - Returning result metadata...
üêõ DEBUG: train_worker started for FGD
üêõ DEBUG [00:32:14.594]: Main received result for IXG
  ‚öôÔ∏è Training models for FGD (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-72 - FGD: Initiating feature extraction for training.
  [DIAGNOSTIC] FGD: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FGD: rows after features available: 126
üéØ FGD: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FGD: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FGD: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FGD: Training LSTM (50 epochs)...
      ‚è≥ PCOR LSTM: Epoch 40/50 (80%)
      ‚è≥ FGD LSTM: Epoch 10/50 (20%)
      üìä LSTM Regression Metrics:
         MSE: 0.834556
         RMSE: 0.913541
         R¬≤ Score: -1.9671 (Poor - 196.7% variance explained)
      üîπ PCOR: Training TCN (50 epochs)...
      ‚è≥ PCOR TCN: Epoch 10/50 (20%)
      ‚è≥ FGD LSTM: Epoch 20/50 (40%)
      ‚è≥ PCOR TCN: Epoch 20/50 (40%)
      ‚è≥ PCOR TCN: Epoch 30/50 (60%)
      ‚è≥ PCOR TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.414678
         RMSE: 0.643955
         R¬≤ Score: -0.4743
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä PCOR: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ PCOR Random Forest: Starting GridSearchCV fit...
      ‚è≥ FGD LSTM: Epoch 30/50 (60%)
      ‚è≥ FGD LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.224558
         RMSE: 0.473876
         R¬≤ Score: -1.3477 (Poor - 134.8% variance explained)
      üîπ FGD: Training TCN (50 epochs)...
      ‚è≥ FGD TCN: Epoch 10/50 (20%)
      ‚è≥ FGD TCN: Epoch 20/50 (40%)
      ‚è≥ FGD TCN: Epoch 30/50 (60%)
      ‚è≥ FGD TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.156656
         RMSE: 0.395797
         R¬≤ Score: -0.6378
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FGD: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FGD Random Forest: Starting GridSearchCV fit...
       ‚úÖ PCOR Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=15.2522 (Best Params: {'max_depth': 10, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ PCOR LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ PCOR LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=15.0085 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ PCOR XGBoost: Starting GridSearchCV fit...
       ‚úÖ FGD Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=5.4282 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FGD LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ FGD LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=4.3819 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FGD XGBoost: Starting GridSearchCV fit...
       ‚úÖ PDLB XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.6063 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.7s
    - LSTM: MSE=0.3622
    - TCN: MSE=0.3310
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3310
        ‚Ä¢ LSTM: MSE=0.3622
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.0604
        ‚Ä¢ XGBoost: MSE=7.6063
        ‚Ä¢ Random Forest: MSE=10.1908
   ‚úÖ PDLB: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PDLB (TargetReturn): TCN with MSE=0.3310
üêõ DEBUG: PDLB - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PDLB.
üêõ DEBUG: PDLB - Moving model to CPU before return...
üêõ DEBUG [00:32:23.746]: PDLB - Returning result metadata...
üêõ DEBUG [00:32:23.747]: Main received result for PDLB
üêõ DEBUG: train_worker started for UXI
  ‚öôÔ∏è Training models for UXI (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-67 - UXI: Initiating feature extraction for training.
  [DIAGNOSTIC] UXI: fetch_training_data - Initial data rows: 205
   ‚Ü≥ UXI: rows after features available: 126
üéØ UXI: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] UXI: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö UXI: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ UXI: Training LSTM (50 epochs)...
      ‚è≥ UXI LSTM: Epoch 10/50 (20%)
      ‚è≥ UXI LSTM: Epoch 20/50 (40%)
      ‚è≥ UXI LSTM: Epoch 30/50 (60%)
      ‚è≥ UXI LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.571907
         RMSE: 0.756245
         R¬≤ Score: -1.6077 (Poor - 160.8% variance explained)
      üîπ UXI: Training TCN (50 epochs)...
      ‚è≥ UXI TCN: Epoch 10/50 (20%)
      ‚è≥ UXI TCN: Epoch 20/50 (40%)
      ‚è≥ UXI TCN: Epoch 30/50 (60%)
      ‚è≥ UXI TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.291234
         RMSE: 0.539661
         R¬≤ Score: -0.3279
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä UXI: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ UXI Random Forest: Starting GridSearchCV fit...
       ‚úÖ UXI Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=18.2214 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 3.6s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ UXI LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ UXI LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=35.2823 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.6s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ UXI XGBoost: Starting GridSearchCV fit...
       ‚úÖ FTDR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=16.0926 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 114.2s
    - LSTM: MSE=0.9841
    - TCN: MSE=0.5679
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.8 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.5679
        ‚Ä¢ LSTM: MSE=0.9841
        ‚Ä¢ XGBoost: MSE=16.0926
        ‚Ä¢ Random Forest: MSE=18.1793
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.2462
   ‚úÖ FTDR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FTDR (TargetReturn): TCN with MSE=0.5679
üêõ DEBUG: FTDR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FTDR.
üêõ DEBUG: FTDR - Moving model to CPU before return...
üêõ DEBUG [00:32:33.889]: FTDR - Returning result metadata...
üêõ DEBUG [00:32:33.889]: Main received result for FTDR
üêõ DEBUG: train_worker started for FBNC
  ‚öôÔ∏è Training models for FBNC (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-71 - FBNC: Initiating feature extraction for training.
  [DIAGNOSTIC] FBNC: fetch_training_data - Initial data rows: 205
   ‚Ü≥ FBNC: rows after features available: 126
üéØ FBNC: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] FBNC: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö FBNC: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ FBNC: Training LSTM (50 epochs)...
      ‚è≥ FBNC LSTM: Epoch 10/50 (20%)
      ‚è≥ FBNC LSTM: Epoch 20/50 (40%)
      ‚è≥ FBNC LSTM: Epoch 30/50 (60%)
      ‚è≥ FBNC LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.514187
         RMSE: 0.717068
         R¬≤ Score: -1.0729 (Poor - 107.3% variance explained)
      üîπ FBNC: Training TCN (50 epochs)...
      ‚è≥ FBNC TCN: Epoch 10/50 (20%)
      ‚è≥ FBNC TCN: Epoch 20/50 (40%)
      ‚è≥ FBNC TCN: Epoch 30/50 (60%)
      ‚è≥ FBNC TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.344380
         RMSE: 0.586839
         R¬≤ Score: -0.3883
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä FBNC: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ FBNC Random Forest: Starting GridSearchCV fit...
       ‚úÖ QLD XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=23.5548 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 116.8s
    - LSTM: MSE=0.8247
    - TCN: MSE=0.4515
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 120.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4515
        ‚Ä¢ LSTM: MSE=0.8247
        ‚Ä¢ XGBoost: MSE=23.5548
        ‚Ä¢ Random Forest: MSE=28.9702
        ‚Ä¢ LightGBM Regressor (CPU): MSE=42.9651
   ‚úÖ QLD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for QLD (TargetReturn): TCN with MSE=0.4515
üêõ DEBUG: QLD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for QLD.
üêõ DEBUG: QLD - Moving model to CPU before return...
üêõ DEBUG [00:32:38.787]: QLD - Returning result metadata...
üêõ DEBUG: train_worker started for ONLN
üêõ DEBUG [00:32:38.789]: Main received result for QLD
üêõ DEBUG: Training progress: 944/959 done
  ‚öôÔ∏è Training models for ONLN (FORCE_TRAINING is True, CONTINUE_TRAINING_FROM_EXISTING is False)...
  [DEBUG] SpawnPoolWorker-70 - ONLN: Initiating feature extraction for training.
  [DIAGNOSTIC] ONLN: fetch_training_data - Initial data rows: 205
   ‚Ü≥ ONLN: rows after features available: 126
üéØ ONLN: Starting model training (LSTM‚ÜíTCN‚ÜíML models)...
  [DIAGNOSTIC] ONLN: train_and_evaluate_models - Rows after dropping NaNs: 126
    - Using MSE loss for regression (predicting returns)
   üìö ONLN: Phase 1/3 - Training Deep Learning models (LSTM, TCN)...
      üîπ ONLN: Training LSTM (50 epochs)...
      ‚è≥ ONLN LSTM: Epoch 10/50 (20%)
       ‚úÖ FBNC Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=9.3629 (Best Params: {'max_depth': 15, 'n_estimators': 200}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ FBNC LightGBM Regressor (CPU): Starting GridSearchCV fit...
      ‚è≥ ONLN LSTM: Epoch 20/50 (40%)
       ‚úÖ FBNC LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ ‚ûñ LightGBM Regressor (CPU): MSE=11.3962 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ FBNC XGBoost: Starting GridSearchCV fit...
      ‚è≥ ONLN LSTM: Epoch 30/50 (60%)
      ‚è≥ ONLN LSTM: Epoch 40/50 (80%)
      üìä LSTM Regression Metrics:
         MSE: 0.660165
         RMSE: 0.812506
         R¬≤ Score: -0.9502 (Poor - 95.0% variance explained)
      üîπ ONLN: Training TCN (50 epochs)...
      ‚è≥ ONLN TCN: Epoch 10/50 (20%)
      ‚è≥ ONLN TCN: Epoch 20/50 (40%)
      ‚è≥ ONLN TCN: Epoch 30/50 (60%)
      ‚è≥ ONLN TCN: Epoch 40/50 (80%)
      üìä TCN Regression Metrics:
         MSE: 0.636082
         RMSE: 0.797548
         R¬≤ Score: -0.8791
  üî¨ Comparing regressor performance (MSE via cross-validation with GridSearchCV):
   üìä ONLN: Phase 2/3 - Optimizing 3 traditional ML models with GridSearchCV...
     ‚ö° Using 3-fold cross-validation, parallel processing enabled
    üîç Optimizing Random Forest (1/3) - Testing 4 parameter combinations √ó 3-fold CV = 12 total model trainings...
       ‚è≥ ONLN Random Forest: Starting GridSearchCV fit...
       ‚úÖ HURN XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=13.2278 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 119.6s
    - LSTM: MSE=0.4628
    - TCN: MSE=0.4091
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 123.0 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4091
        ‚Ä¢ LSTM: MSE=0.4628
        ‚Ä¢ XGBoost: MSE=13.2278
        ‚Ä¢ LightGBM Regressor (CPU): MSE=13.3537
        ‚Ä¢ Random Forest: MSE=14.2609
   ‚úÖ HURN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for HURN (TargetReturn): TCN with MSE=0.4091
üêõ DEBUG: HURN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for HURN.
üêõ DEBUG: HURN - Moving model to CPU before return...
üêõ DEBUG [00:32:42.094]: HURN - Returning result metadata...
üêõ DEBUG [00:32:42.094]: Main received result for HURN
       ‚úÖ ONLN Random Forest: GridSearchCV complete
    ‚úÖ üéØ BEST! Random Forest: MSE=10.8982 (Best Params: {'max_depth': 15, 'n_estimators': 100}) | Time: 2.7s
    üîç Optimizing LightGBM Regressor (CPU) (2/3) - Testing 18 parameter combinations √ó 3-fold CV = 54 total model trainings...
       ‚è≥ ONLN LightGBM Regressor (CPU): Starting GridSearchCV fit...
       ‚úÖ ONLN LightGBM Regressor (CPU): GridSearchCV complete
    ‚úÖ üéØ BEST! LightGBM Regressor (CPU): MSE=8.2510 (Best Params: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200}) | Time: 0.7s
    üîç Optimizing XGBoost (3/3) - Testing 8 parameter combinations √ó 3-fold CV = 24 total model trainings...
       ‚è≥ ONLN XGBoost: Starting GridSearchCV fit...
       ‚úÖ CECO XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=22.5931 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.6s
    - LSTM: MSE=0.4077
    - TCN: MSE=0.3402
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.4 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3402
        ‚Ä¢ LSTM: MSE=0.4077
        ‚Ä¢ XGBoost: MSE=22.5931
        ‚Ä¢ Random Forest: MSE=26.0179
        ‚Ä¢ LightGBM Regressor (CPU): MSE=41.3145
   ‚úÖ CECO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CECO (TargetReturn): TCN with MSE=0.3402
üêõ DEBUG: CECO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CECO.
üêõ DEBUG: CECO - Moving model to CPU before return...
üêõ DEBUG [00:32:53.275]: CECO - Returning result metadata...
üêõ DEBUG [00:32:53.275]: Main received result for CECO
       ‚úÖ WTS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=7.6994 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 117.4s
    - LSTM: MSE=0.3188
    - TCN: MSE=0.2149
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.5 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2149
        ‚Ä¢ LSTM: MSE=0.3188
        ‚Ä¢ Random Forest: MSE=7.0776
        ‚Ä¢ XGBoost: MSE=7.6994
        ‚Ä¢ LightGBM Regressor (CPU): MSE=9.5692
   ‚úÖ WTS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for WTS (TargetReturn): TCN with MSE=0.2149
üêõ DEBUG: WTS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for WTS.
üêõ DEBUG: WTS - Moving model to CPU before return...
üêõ DEBUG [00:32:55.518]: WTS - Returning result metadata...
üêõ DEBUG [00:32:55.519]: Main received result for WTS
       ‚úÖ CMPO XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=23.1520 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 117.9s
    - LSTM: MSE=0.6409
    - TCN: MSE=0.4702
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 121.2 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4702
        ‚Ä¢ LSTM: MSE=0.6409
        ‚Ä¢ Random Forest: MSE=19.5452
        ‚Ä¢ XGBoost: MSE=23.1520
        ‚Ä¢ LightGBM Regressor (CPU): MSE=25.0122
   ‚úÖ CMPO: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CMPO (TargetReturn): TCN with MSE=0.4702
üêõ DEBUG: CMPO - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CMPO.
üêõ DEBUG: CMPO - Moving model to CPU before return...
üêõ DEBUG [00:32:56.833]: CMPO - Returning result metadata...
       ‚úÖ ALG XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=33.8949 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 118.8s
    - LSTM: MSE=0.8330
    - TCN: MSE=0.4789
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 122.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4789
        ‚Ä¢ LSTM: MSE=0.8330
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.9180
        ‚Ä¢ Random Forest: MSE=21.9736
        ‚Ä¢ XGBoost: MSE=33.8949
   ‚úÖ ALG: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ALG (TargetReturn): TCN with MSE=0.4789
üêõ DEBUG: ALG - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ALG.
üêõ DEBUG: ALG - Moving model to CPU before return...
üêõ DEBUG [00:32:57.931]: ALG - Returning result metadata...
üêõ DEBUG [00:32:57.932]: Main received result for ALG
üêõ DEBUG: Training progress: 948/959 done
üêõ DEBUG [00:32:57.932]: Main received result for CMPO
       ‚úÖ SMBK XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.7741 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 114.1s
    - LSTM: MSE=0.9170
    - TCN: MSE=0.3867
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 117.7 seconds (2.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3867
        ‚Ä¢ LSTM: MSE=0.9170
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.0250
        ‚Ä¢ Random Forest: MSE=11.3903
        ‚Ä¢ XGBoost: MSE=16.7741
   ‚úÖ SMBK: Phase 3/3 - Model selection complete!
  üèÜ WINNER for SMBK (TargetReturn): TCN with MSE=0.3867
üêõ DEBUG: SMBK - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for SMBK.
üêõ DEBUG: SMBK - Moving model to CPU before return...
üêõ DEBUG [00:33:11.831]: SMBK - Returning result metadata...
üêõ DEBUG [00:33:11.832]: Main received result for SMBK
       ‚úÖ AIT XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=14.0498 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 110.4s
    - LSTM: MSE=0.6199
    - TCN: MSE=0.3682
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 113.9 seconds (1.9 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3682
        ‚Ä¢ LSTM: MSE=0.6199
        ‚Ä¢ LightGBM Regressor (CPU): MSE=10.0960
        ‚Ä¢ XGBoost: MSE=14.0498
        ‚Ä¢ Random Forest: MSE=15.2075
   ‚úÖ AIT: Phase 3/3 - Model selection complete!
  üèÜ WINNER for AIT (TargetReturn): TCN with MSE=0.3682
üêõ DEBUG: AIT - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for AIT.
üêõ DEBUG: AIT - Moving model to CPU before return...
üêõ DEBUG [00:33:12.440]: AIT - Returning result metadata...
üêõ DEBUG [00:33:12.441]: Main received result for AIT
       ‚úÖ FDNI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=9.0667 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 104.1s
    - LSTM: MSE=0.6529
    - TCN: MSE=0.3072
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 107.5 seconds (1.8 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3072
        ‚Ä¢ LSTM: MSE=0.6529
        ‚Ä¢ LightGBM Regressor (CPU): MSE=7.3919
        ‚Ä¢ Random Forest: MSE=8.0163
        ‚Ä¢ XGBoost: MSE=9.0667
   ‚úÖ FDNI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FDNI (TargetReturn): TCN with MSE=0.3072
üêõ DEBUG: FDNI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FDNI.
üêõ DEBUG: FDNI - Moving model to CPU before return...
üêõ DEBUG [00:33:21.057]: FDNI - Returning result metadata...
üêõ DEBUG [00:33:21.058]: Main received result for FDNI
üêõ DEBUG: Training progress: 952/959 done
       ‚úÖ CDNS XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=18.1479 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 96.0s
    - LSTM: MSE=0.1974
    - TCN: MSE=0.1621
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 99.2 seconds (1.7 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1621
        ‚Ä¢ LSTM: MSE=0.1974
        ‚Ä¢ LightGBM Regressor (CPU): MSE=16.4128
        ‚Ä¢ Random Forest: MSE=17.8027
        ‚Ä¢ XGBoost: MSE=18.1479
   ‚úÖ CDNS: Phase 3/3 - Model selection complete!
  üèÜ WINNER for CDNS (TargetReturn): TCN with MSE=0.1621
üêõ DEBUG: CDNS - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for CDNS.
üêõ DEBUG: CDNS - Moving model to CPU before return...
üêõ DEBUG [00:33:23.661]: CDNS - Returning result metadata...
üêõ DEBUG [00:33:23.662]: Main received result for CDNS
       ‚úÖ BBSI XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.6254 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 82.7s
    - LSTM: MSE=0.1813
    - TCN: MSE=0.1206
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 86.2 seconds (1.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1206
        ‚Ä¢ LSTM: MSE=0.1813
        ‚Ä¢ LightGBM Regressor (CPU): MSE=3.4848
        ‚Ä¢ Random Forest: MSE=4.1123
        ‚Ä¢ XGBoost: MSE=5.6254
   ‚úÖ BBSI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for BBSI (TargetReturn): TCN with MSE=0.1206
üêõ DEBUG: BBSI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for BBSI.
üêõ DEBUG: BBSI - Moving model to CPU before return...
üêõ DEBUG [00:33:32.498]: BBSI - Returning result metadata...
üêõ DEBUG [00:33:32.499]: Main received result for BBSI
       ‚úÖ PCOR XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=11.6166 (Best Params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}) | Time: 78.7s
    - LSTM: MSE=0.8346
    - TCN: MSE=0.4147
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 82.0 seconds (1.4 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.4147
        ‚Ä¢ LSTM: MSE=0.8346
        ‚Ä¢ XGBoost: MSE=11.6166
        ‚Ä¢ LightGBM Regressor (CPU): MSE=15.0085
        ‚Ä¢ Random Forest: MSE=15.2522
   ‚úÖ PCOR: Phase 3/3 - Model selection complete!
  üèÜ WINNER for PCOR (TargetReturn): TCN with MSE=0.4147
üêõ DEBUG: PCOR - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for PCOR.
üêõ DEBUG: PCOR - Moving model to CPU before return...
üêõ DEBUG [00:33:38.048]: PCOR - Returning result metadata...
üêõ DEBUG [00:33:38.049]: Main received result for PCOR
       ‚úÖ FGD XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=5.6536 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}) | Time: 77.6s
    - LSTM: MSE=0.2246
    - TCN: MSE=0.1567
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 81.0 seconds (1.3 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.1567
        ‚Ä¢ LSTM: MSE=0.2246
        ‚Ä¢ LightGBM Regressor (CPU): MSE=4.3819
        ‚Ä¢ Random Forest: MSE=5.4282
        ‚Ä¢ XGBoost: MSE=5.6536
   ‚úÖ FGD: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FGD (TargetReturn): TCN with MSE=0.1567
üêõ DEBUG: FGD - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FGD.
üêõ DEBUG: FGD - Moving model to CPU before return...
üêõ DEBUG [00:33:38.531]: FGD - Returning result metadata...
üêõ DEBUG [00:33:38.531]: Main received result for FGD
üêõ DEBUG: Training progress: 956/959 done
       ‚úÖ UXI XGBoost: GridSearchCV complete
    ‚úÖ üéØ BEST! XGBoost: MSE=15.8638 (Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}) | Time: 69.7s
    - LSTM: MSE=0.5719
    - TCN: MSE=0.2912
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 73.9 seconds (1.2 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.2912
        ‚Ä¢ LSTM: MSE=0.5719
        ‚Ä¢ XGBoost: MSE=15.8638
        ‚Ä¢ Random Forest: MSE=18.2214
        ‚Ä¢ LightGBM Regressor (CPU): MSE=35.2823
   ‚úÖ UXI: Phase 3/3 - Model selection complete!
  üèÜ WINNER for UXI (TargetReturn): TCN with MSE=0.2912
üêõ DEBUG: UXI - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for UXI.
üêõ DEBUG: UXI - Moving model to CPU before return...
üêõ DEBUG [00:33:40.368]: UXI - Returning result metadata...
üêõ DEBUG [00:33:40.368]: Main received result for UXI
       ‚úÖ FBNC XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=16.8182 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 61.0s
    - LSTM: MSE=0.5142
    - TCN: MSE=0.3444
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 64.4 seconds (1.1 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.3444
        ‚Ä¢ LSTM: MSE=0.5142
        ‚Ä¢ Random Forest: MSE=9.3629
        ‚Ä¢ LightGBM Regressor (CPU): MSE=11.3962
        ‚Ä¢ XGBoost: MSE=16.8182
   ‚úÖ FBNC: Phase 3/3 - Model selection complete!
  üèÜ WINNER for FBNC (TargetReturn): TCN with MSE=0.3444
üêõ DEBUG: FBNC - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for FBNC.
üêõ DEBUG: FBNC - Moving model to CPU before return...
üêõ DEBUG [00:33:41.184]: FBNC - Returning result metadata...
üêõ DEBUG [00:33:41.185]: Main received result for FBNC
       ‚úÖ ONLN XGBoost: GridSearchCV complete
    ‚úÖ ‚ûñ XGBoost: MSE=8.7401 (Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}) | Time: 56.7s
    - LSTM: MSE=0.6602
    - TCN: MSE=0.6361
  ‚úÖ GridSearchCV optimization complete! Processed 5/3 models successfully
     ‚è±Ô∏è  Total time: 60.1 seconds (1.0 minutes)
     üìä Results summary:
        ‚Ä¢ TCN: MSE=0.6361
        ‚Ä¢ LSTM: MSE=0.6602
        ‚Ä¢ LightGBM Regressor (CPU): MSE=8.2510
        ‚Ä¢ XGBoost: MSE=8.7401
        ‚Ä¢ Random Forest: MSE=10.8982
   ‚úÖ ONLN: Phase 3/3 - Model selection complete!
  üèÜ WINNER for ONLN (TargetReturn): TCN with MSE=0.6361
üêõ DEBUG: ONLN - train_and_evaluate_models completed
  ‚úÖ Single regression model, scaler, y_scaler, and GRU hyperparams saved for ONLN.
üêõ DEBUG: ONLN - Moving model to CPU before return...
üêõ DEBUG [00:33:41.838]: ONLN - Returning result metadata...
üêõ DEBUG [00:33:41.848]: Main received result for ONLN
üêõ DEBUG: Training progress: 959/959 done
üêõ DEBUG: All 959 tasks completed (959 successful).
üêõ DEBUG: Closing pool...
üêõ DEBUG: Joining pool workers (timeout 30s)...
üêõ DEBUG: Pool cleanup complete!
üêõ DEBUG: Starting result collection for 959 training results...
üêõ DEBUG: Beginning dictionary assignment loop...
üêõ DEBUG: Dictionary assignment complete, collected 959 models
üêõ DEBUG: Creating models reference from models_buy...
üêõ DEBUG: Counting trained models...
‚úÖ 3-Month training complete: 959 models trained/loaded.
üêõ DEBUG: Building model selection statistics...
üêõ DEBUG: Returning 959 results...
üêõ DEBUG: train_models_for_period returned, processing 959 results...
  ‚ÑπÔ∏è 0 tickers failed 1-Year model training and will be skipped: 
  ‚úÖ 966 tickers available for AI Portfolio training: OKLO, PRCH, LAES, RYM, AMPX, TMC, ARQQ, AEVA, QNTM, ABVX, KC, DAVE, SEZL, BMNR, IONQ, SRRK, HOOD, SMR, UAMY, PSIX, LEU, PLTR, OUST, BKSY, GRRR, TSSI, ASPI, RCAT, ACHR, NVTS, KOD, GRPN, CLS, CRDO, ROOT, BE, SLDP, SGHC, HIMS, ATAI, QURE, ZVIA, PGY, INOD, GRAL, CRNC, NNE, KOPN, CANG, APP, ONDS, SOFI, AKBA, MPU, RBRK, ALLT, ORLA, LX, SEI, ETON, KTOS, NAGE, QMCO, GEV, AMLX, SOUN, TTMI, SBET, AISP, OPFI, NET, LGCY, BITU, LMND, MSTR, NEXT, BITX, AS, CAR, SRAD, AGX, TPR, INVZ, CRCL, DPRO, EYE, ESLT, RDDT, OPRT, JFIN, CONL, HTZ, SBSW, GRNQ, PAYS, XYF, TRVG, AMBR, COIN, GOGO, SSRM, QSI, OSS, AAPG, GHRS, LUXE, EVEX, EOSE, MASS, AVXL, TESL, DOMO, DAO, FOA, ALNT, IREN, UPST, ASTS, AENT, TARK, IHS, APEI, NN, DOYU, APYX, LFMD, CTMX, GEO, DXPE, KEN, VNET, UI, TWLO, VVOS, ATOM, MGTX, RCL, EAT, WLDN, MESO, VS, DJTWW, DB, MIR, TSAT, TIGR, RBOT, METC, MSTY, ABCL, TBRG, LQDA, RIOT, MYRG, LIF, TLN, VUZI, UTI, AU, LINC, CVNA, AAOI, CCB, MGIC, HWM, CYD, IBKR, WGS, MNY, SPOT, CRK, JBL, BROS, CVAC, GHM, ARKW, STRL, BTCW, HODL, AXON, EZBC, BITB, BRRR, IBIT, FBTC, ARKB, BTCO, REAL, SAN, AMSC, SKE, DEFI, OM, UUUU, FRHC, TPB, GBTC, BBIO, ARKF, LASR, KEP, IRTC, HIMX, GDS, ETHZ, IESC, TME, PARR, APPS, VEON, XMTR, ATRO, FIX, LMB, CIB, TPC, ORGO, INDV, EXEL, PBI, ICCM, DASH, FUTU, TATT, CPS, TENX, BITO, NRG, CRS, ODC, PPTA, KGC, SNEX, CMPX, CTOS, DRS, NXT, TOST, REVG, HNRG, BLOK, MNMD, DNA, SLNO, YALA, JNUG, KD, NBP, MGNI, IVA, AVGO, CMP, AEHR, EVLV, HRTG, ODD, OMAB, UNFI, LLYVA, ZS, NIU, SE, VIK, CIFR, SHLD, CCJ, CRWD, SII, NFLX, HUT, LLYVK, API, VSEC, WEBL, DOUG, RYTM, LITE, CAKE, ARKX, LTM, AFRM, BCS, MPAA, SLI, CXW, ICL, LPLA, GDXU, GFI, WBD, CTRN, SENEA, CUK, MSB, VST, DLTR, TIGO, YBTC, APH, PCT, UFO, ZVRA, MRX, SMLR, ARKQ, ARKK, VRSN, WF, DAPP, PEGA, ORN, BITQ, CCL, NUGT, PRIM, IDCC, GTX, ITRN, APG, UAL, AEM, LUNR, HIPO, PRA, DAN, TGTX, AVPT, RDVT, VSTA, AAMI, ENVX, ASA, C, FNGO, FIVE, YPF, NWG, PW, UPXI, CD, ADTN, RL, SHOP, WPM, AAP, ZLAB, RRGB, GRND, BA, BBAR, BELFB, NGD, HTT, CAE, BTF, ATGE, GREK, LRN, AVAL, ESQ, OR, WFC, BBW, SCHW, MTZ, QTUM, ACAD, ROAD, HSBC, TGS, BTI, MFG, LFCR, WWD, DGP, CG, WOR, SGDM, BRNS, FHN, NFLY, URA, IDT, FINV, ECG, BWMN, PCRX, CTLP, IPI, FLEX, GS, WRBY, BB, UGI, CALM, OSIS, RMBS, CELH, UNTY, ESE, SYF, NTES, CHEF, CNM, SSP, CBRL, VIRT, KMDA, EME, CW, NTRS, CORT, TFPM, SUPV, DRD, ACMR, INBX, FSM, HUYA, OPLN, JOYY, MUFG, GILT, PFIX, MRP, AHR, SERV, SGDJ, IRS, UGL, TE, TTWO, SIL, FGM, NVDU, FDIG, SFM, NLR, JCI, EXK, BFC, NERD, ORCL, EPOL, SLM, LB, GE, OCS, TITN, MCK, CAH, GTES, LOMA, NVDL, KODK, PAYC, SANM, IZRL, EQT, CIEN, BK, OPY, ULS, IFS, COF, UEC, METCB, SXT, EIS, AG, ATLC, MRCY, FROG, LZ, PAY, TSM, NVDA, GOAU, LYG, GDXJ, ATRA, UAN, CRMD, NAMS, HGV, DEC, PLTM, PPLT, DLO, ATI, ESPO, VLN, AMPL, GILD, CTOR, TNL, PWR, EWO, PAAS, JBTM, SLVP, NPK, CENX, GRAB, MOS, CASY, XAR, NIC, QXO, STX, ETH, BKCH, OCUL, ETHW, ETHV, POET, EZET, RSI, FETH, PERI, TETH, ETHA, PTON, LAUR, WGMI, QETH, PLMR, BTSG, RING, LYV, HMY, FTI, USD, TEL, RJF, MAIN, DOCS, ATAT, BYRN, CX, UNM, EUFN, PLNT, CSV, TMV, MCB, UAE, PJT, HNST, MBI, EMR, ING, TMAT, VEEV, GTLS, VRT, TKO, PUK, SPNT, WT, DDS, IAI, PGNY, SGI, STN, ETHE, FAS, BAP, OLLI, NRGV, KLTR, MTW, DCO, TRMB, KT, FHI, GDX, BWXT, ETR, CGNT, SAH, FPX, TBPH, RBA, TCBX, CHAT, WWW, METV, BAM, PM, AVAV, OKTA, EFXT, FFIV, FAST, FBIO, GENI, LGND, IGIC, NGVC, HLF, MTA, GRMN, OSW, EMBJ, ROK, ISRA, FTNT, MEXX, FOX, TOYO, BBCP, CDE, WRLD, STRK, AX, IIIV, TR, BLX, CBNK, FOXA, LANV, BFH, BKNG, IAG, ELPC, LOUP, CFLT, FDD, GLXY, MEDP, ARLO, COCO, BYD, AEIS, AZZ, ECNS, USFD, OMF, MRUS, TXNM, XPP, EWP, WDC, PAC, T, ITA, QUAD, LENZ, PRLB, SHAK, FNGS, MTRX, MCRI, STVN, CNP, MIRM, CUBI, DXCM, RAMP, FWONK, CSCO, CTRI, RPRX, COLO, ULTA, FNV, GSAT, SKYW, TASK, DSP, STT, LIND, CME, AB, SEIC, WBS, PAHC, DAX, HERO, RTX, EWI, BMO, ECVT, EETH, EYPT, CMCM, NDAQ, OGIG, CEPU, PAR, LTL, THFF, SMMT, IMAX, SMCI, BGM, NG, FN, UFCS, NOTV, DG, SMFG, CCEP, MT, V, COMM, AGQ, GOLF, HCI, DTM, KBWB, APLD, NPKI, OSK, DDOG, VCTR, SFST, AGI, HAFC, FEP, WMB, ATEN, RZLT, AL, COR, ONC, NEU, YINN, RBC, PRDO, CSGS, EWS, FWONA, BKR, NMG, MKL, NTR, SONY, BETZ, DUOL, AWI, BEN, DAKT, DBD, NPO, VNM, NVDY, BMBL, NPCE, IVZ, SILJ, PPA, GL, XPO, DOCU, PBYI, WCC, EWG, FDN, DRTS, GLAD, FLUT, NTGR, AFK, WAY, DRI, ALV, SYBT, UBS, SHCO, FSCO, KMI, SPPP, OPPE, WELL, YMM, RDWR, BNT, EGAN, BN, SPMO, FMS, SKWD, BSVN, EWBC, PWRD, CIO, BBUC, FET, NGS, DIS, INTU, GVAL, PAX, ALTG, ITRI, ORLY, VALN, FFTY, R, EXPE, Z, EQH, TDY, OPEN, FDT, SAP, TEO, RELY, IAUM, GLDM, BAR, DK, BULZ, SGOL, EBAY, BSX, OUNZ, AAAU, KR, HACK, FINX, IAU, ASTE, CLSK, UROY, ARGT, GLD, CPER, CIBR, DCTH, BJ, FITE, ENVA, ZG, LNG, TARS, FER, KCE, SOCL, IGV, TBT, NTNX, IETC, ARTY, KALV, CTVA, SPWH, SIXG, OIS, ESAB, IYG, TS, PHYS, UPWK, VMI, YMAX, PRM, DE, GLTR, PBW, HLI, PIZ, FXI, MVBF, ETOR, NEM, CRBG, IDMO, GEOS, DBP, RIVN, SLVO, META, HWKN, GLW, ASLE, CLM, PODD, WUGI, RIGL, OZK, HEI-A, NTB, FSK, PSKY, NI, HDB, TG, TD, NBN, IDV, AMZY, NRIM, JTEK, AMZN, CFG, CRMT, DTST, NVMI, SNX, BRW, GNE, CHWY, WRB, IQM, HEI, TRMK, EVC, PAM, COWG, FDP, PSLV, MO, TQQQ, TRS, MTSI, WCBR, MAMA, FRSH, SOBO, GGAL, AXS, MASI, RSG, IRON, CPA, GEL, SIMO, GT, PEJ, FEDU, SPRY, CFR, CSGP, CPNG, UIVM, KEMQ, WTFC, FDTS, STEL, XITK, VSAT, EPR, SPOK, TRIP, IXG, PDLB, FTDR, QLD, HURN, CECO, WTS, ALG, CMPO, SMBK, AIT, FDNI, CDNS, BBSI, PCOR, FGD, UXI, FBNC, ONLN

üßπ Cleaning up memory before AI Portfolio training...
   ‚úÖ Memory cleanup complete
DEBUG: About to check ENABLE_AI_PORTFOLIO = True
DEBUG: ENABLE_AI_PORTFOLIO is True, starting training...

üß† Training AI Portfolio Rebalancing Model...
DEBUG: Importing ai_portfolio module...
DEBUG: Import successful, calling train_ai_portfolio_model...
DEBUG: AI portfolio training dates: 2024-09-29 to 2025-07-25
   üîí Data Separation: Training ends 1 day(s) before backtest starts
   ‚úÖ No overlap between training and backtest data (preventing look-ahead bias)
   üß† AI Portfolio: Training model to select best 3-stock combinations...
   üìä Training on 966 candidates from 2024-09-29 to 2025-07-25
   üîí Data isolation: Using ONLY 1158045 rows from training period
      Training data range: 2024-09-30 to 2025-07-25
   ‚úÖ Backtest data will NEVER be used during training (preventing look-ahead bias)
   üìê Training parameters:
      Evaluation window: 60 days
      Step size: 7 days
      Target annual return: 50.0%
      ‚Üí Threshold for 60 days: 6.89% (AFTER transaction costs)
